[
  {
    "input": "177 RemovingthePseudoreplication 178 AnalysisofLongitudinalData 178 DerivedVariableAnalysis 179 viii CONTENTS DealingwithPseudoreplication 179 VarianceComponentsAnalysis(VCA) 183 References 184 FurtherReading 184 Chapter9 AnalysisofCovariance 185 FurtherReading 192 Chapter10 MultipleRegression 193 TheStepsInvolvedinModelSimplification 195 Caveats 196 OrderofDeletion 196 CarryingOutaMultipleRegression 197 ATrickierExample 203 FurtherReading 211 Chapter11 Contrasts 212 ContrastCoefficients 213 AnExampleofContrastsinR 214 APrioriContrasts 215 TreatmentContrasts 216 ModelSimplificationbyStepwiseDeletion 218 ContrastSumsofSquaresbyHand 222 TheThreeKindsofContrastsCompared 224 Reference 225 FurtherReading 225 Chapter12 OtherResponseVariables 226 IntroductiontoGeneralizedLinearModels 228 TheErrorStructure 229 TheLinearPredictor 229 FittedValues 230 AGeneralMeasureofVariability 230 TheLinkFunction 231 CanonicalLinkFunctions 232 AkaikesInformationCriterion(AIC)asaMeasureoftheFitofaModel 233 FurtherReading 233 Chapter13 CountData 234 ARegressionwithPoissonErrors 234 AnalysisofDeviancewithCountData 237 CONTENTS ix TheDangerofContingencyTables 244 AnalysisofCovariancewithCountData 247 FrequencyDistributions 250 FurtherReading 255 Chapter14 ProportionData 256 AnalysesofDataonOneandTwoProportions 257 AveragesofProportions 257 CountDataonProportions 257 Odds 259 OverdispersionandHypothesisTesting 260 Applications 261 LogisticRegressionwithBinomialErrors 261 ProportionDatawithCategoricalExplanatoryVariables 264 AnalysisofCovariancewithBinomialData 269 FurtherReading 272 Chapter15 BinaryResponseVariable 273 IncidenceFunctions 275 ANCOVAwithaBinaryResponseVariable 279 FurtherReading 284 Chapter16 DeathandFailureData 285 SurvivalAnalysiswithCensoring 287 FurtherReading 290 Appendix EssentialsoftheRLanguage 291 RasaCalculator 291 Built-inFunctions 292 NumberswithExponents 294 ModuloandIntegerQuotients 294 Assignment 295 Rounding 295 InfinityandThingsthatAreNotaNumber(NaN) 296 MissingValues(NA) 297 Operators 298 CreatingaVector 298 NamedElementswithinVectors 299 VectorFunctions 299 SummaryInformationfromVectorsbyGroups 300 SubscriptsandIndices 301 x CONTENTS WorkingwithVectorsandLogicalSubscripts 301 AddresseswithinVectors 304 TrimmingVectorsUsingNegativeSubscripts 304 LogicalArithmetic 305 Repeats 305 GenerateFactorLevels 306 GeneratingRegularSequencesofNumbers 306 Matrices 307 CharacterStrings 309 WritingFunctionsinR 310 ArithmeticMeanofaSingleSample 310 MedianofaSingleSample 310 LoopsandRepeats 311 TheifelseFunction 312 EvaluatingFunctionswithapply 312 TestingforEquality 313 TestingandCoercinginR 314 DatesandTimesinR 315 CalculationswithDatesandTimes 319 UnderstandingtheStructureofanRObjectUsingstr 320 Reference 322 FurtherReading 322 Index 323 Preface This book is an introduction to the essentials of statistical analysis for students who have littleornobackgroundinmathematicsorstatistics.Theaudienceincludesfirst-andsecond- yearundergraduatestudentsinscience,engineering,medicine andeconomics, alongwith post-experienceandothermaturestudentswhowanttorelearntheirstatistics,ortoswitchto the powerful new language of R. Formanystudents,statisticsistheleastfavouritecourseoftheirentiretimeatuniversity.",
    "target": "Q: What is 177 RemovingthePseudoreplication 178 AnalysisofLongitudinalData 178 DerivedVariableAnalysis 179 viii CONTENTS DealingwithPseudoreplication 179 VarianceComponentsAnalysis(VCA) 183 References 184 FurtherReading 184 Chapter9 AnalysisofCovariance 185 FurtherReading 192 Chapter10 MultipleRegression 193 TheStepsInvolvedinModelSimplification 195 Caveats 196 OrderofDeletion 196 CarryingOutaMultipleRegression 197 ATrickierExample 203 FurtherReading 211 Chapter11 Contrasts 212 ContrastCoefficients 213 AnExampleofContrastsinR 214 APrioriContrasts 215 TreatmentContrasts 216 ModelSimplificationbyStepwiseDeletion 218 ContrastSumsofSquaresbyHand 222 TheThreeKindsofContrastsCompared 224 Reference 225 FurtherReading 225 Chapter12 OtherResponseVariables 226 IntroductiontoGeneralizedLinearModels 228 TheErrorStructure 229 TheLinearPredictor 229 FittedValues 230 AGeneralMeasureofVariability 230 TheLinkFunction 231 CanonicalLinkFunctions 232 AkaikesInformationCriterion(AIC)asaMeasureoftheFitofaModel 233 FurtherReading 233 Chapter13 CountData 234 ARegressionwithPoissonErrors 234 AnalysisofDeviancewithCountData 237 CONTENTS ix TheDangerofContingencyTables 244 AnalysisofCovariancewithCountData 247 FrequencyDistributions 250 FurtherReading 255 Chapter14 ProportionData 256 AnalysesofDataonOneandTwoProportions 257 AveragesofProportions 257 CountDataonProportions 257 Odds 259 OverdispersionandHypothesisTesting 260 Applications 261 LogisticRegressionwithBinomialErrors 261 ProportionDatawithCategoricalExplanatoryVariables 264 AnalysisofCovariancewithBinomialData 269 FurtherReading 272 Chapter15 BinaryResponseVariable 273 IncidenceFunctions 275 ANCOVAwithaBinaryResponseVariable 279 FurtherReading 284 Chapter16 DeathandFailureData 285 SurvivalAnalysiswithCensoring 287 FurtherReading 290 Appendix EssentialsoftheRLanguage 291 RasaCalculator 291 Built-inFunctions 292 NumberswithExponents 294 ModuloandIntegerQuotients 294 Assignment 295 Rounding 295 InfinityandThingsthatAreNotaNumber(NaN) 296 MissingValues(NA) 297 Operators 298 CreatingaVector 298 NamedElementswithinVectors 299 VectorFunctions 299 SummaryInformationfromVectorsbyGroups 300 SubscriptsandIndices 301 x CONTENTS WorkingwithVectorsandLogicalSubscripts 301 AddresseswithinVectors 304 TrimmingVectorsUsingNegativeSubscripts 304 LogicalArithmetic 305 Repeats 305 GenerateFactorLevels 306 GeneratingRegularSequencesofNumbers 306 Matrices 307 CharacterStrings 309 WritingFunctionsinR 310 ArithmeticMeanofaSingleSample 310 MedianofaSingleSample 310 LoopsandRepeats 311 TheifelseFunction 312 EvaluatingFunctionswithapply 312 TestingforEquality 313 TestingandCoercinginR 314 DatesandTimesinR 315 CalculationswithDatesandTimes 319 UnderstandingtheStructureofanRObjectUsingstr 320 Reference 322 FurtherReading 322 Index 323 Preface This book? A: 177 RemovingthePseudoreplication 178 AnalysisofLongitudinalData 178 DerivedVariableAnalysis 179 viii CONTENTS DealingwithPseudoreplication 179 VarianceComponentsAnalysis(VCA) 183 References 184 FurtherReading 184 Chapter9 AnalysisofCovariance 185 FurtherReading 192 Chapter10 MultipleRegression 193 TheStepsInvolvedinModelSimplification 195 Caveats 196 OrderofDeletion 196 CarryingOutaMultipleRegression 197 ATrickierExample 203 FurtherReading 211 Chapter11 Contrasts 212 ContrastCoefficients 213 AnExampleofContrastsinR 214 APrioriContrasts 215 TreatmentContrasts 216 ModelSimplificationbyStepwiseDeletion 218 ContrastSumsofSquaresbyHand 222 TheThreeKindsofContrastsCompared 224 Reference 225 FurtherReading 225 Chapter12 OtherResponseVariables 226 IntroductiontoGeneralizedLinearModels 228 TheErrorStructure 229 TheLinearPredictor 229 FittedValues 230 AGeneralMeasureofVariability 230 TheLinkFunction 231 CanonicalLinkFunctions 232 AkaikesInformationCriterion(AIC)asaMeasureoftheFitofaModel 233 FurtherReading 233 Chapter13 CountData 234 ARegressionwithPoissonErrors 234 AnalysisofDeviancewithCountData 237 CONTENTS ix TheDangerofContingencyTables 244 AnalysisofCovariancewithCountData 247 FrequencyDistributions 250 FurtherReading 255 Chapter14 ProportionData 256 AnalysesofDataonOneandTwoProportions 257 AveragesofProportions 257 CountDataonProportions 257 Odds 259 OverdispersionandHypothesisTesting 260 Applications 261 LogisticRegressionwithBinomialErrors 261 ProportionDatawithCategoricalExplanatoryVariables 264 AnalysisofCovariancewithBinomialData 269 FurtherReading 272 Chapter15 BinaryResponseVariable 273 IncidenceFunctions 275 ANCOVAwithaBinaryResponseVariable 279 FurtherReading 284 Chapter16 DeathandFailureData 285 SurvivalAnalysiswithCensoring 287 FurtherReading 290 Appendix EssentialsoftheRLanguage 291 RasaCalculator 291 Built-inFunctions 292 NumberswithExponents 294 ModuloandIntegerQuotients 294 Assignment 295 Rounding 295 InfinityandThingsthatAreNotaNumber(NaN) 296 MissingValues(NA) 297 Operators 298 CreatingaVector 298 NamedElementswithinVectors 299 VectorFunctions 299 SummaryInformationfromVectorsbyGroups 300 SubscriptsandIndices 301 x CONTENTS WorkingwithVectorsandLogicalSubscripts 301 AddresseswithinVectors 304 TrimmingVectorsUsingNegativeSubscripts 304 LogicalArithmetic 305 Repeats 305 GenerateFactorLevels 306 GeneratingRegularSequencesofNumbers 306 Matrices 307 CharacterStrings 309 WritingFunctionsinR 310 ArithmeticMeanofaSingleSample 310 MedianofaSingleSample 310 LoopsandRepeats 311 TheifelseFunction 312 EvaluatingFunctionswithapply 312 TestingforEquality 313 TestingandCoercinginR 314 DatesandTimesinR 315 CalculationswithDatesandTimes 319 UnderstandingtheStructureofanRObjectUsingstr 320 Reference 322 FurtherReading 322 Index 323 Preface This book is an introduction to the essentials of statistical analysis for students who have littleornobackgroundinmathematicsorstatistics.Theaudienceincludesfirst-andsecond- yearundergraduatestudentsinscience,engineering,medicine andeconomics, alongwith post-experienceandothermaturestudentswhowanttorelearntheirstatistics,ortoswitchto the powerful new language of R. Formanystudents,statisticsistheleastfavouritecourseoftheirentiretimeatuniversity."
  },
  {
    "input": "Theapproachadoptedhereinvolvesvirtuallynostatisticaltheory.Instead,theassump- tionsofthevariousstatisticalmodelsarediscussedatlength,andthepracticeofexposing statisticalmodelstorigorouscriticismisencouraged.Aphilosophyofmodelsimplification is developed in which the emphasis is placed on estimating effect sizes from data, and establishing confidence intervals for these estimates.",
    "target": "Q: What is Theapproachadoptedhereinvolvesvirtuallynostatisticaltheory.Instead,theassump- tionsofthevariousstatisticalmodelsarediscussedatlength,andthepracticeofexposing statisticalmodelstorigorouscriticismisencouraged.Aphilosophyofmodelsimplification? A: Theapproachadoptedhereinvolvesvirtuallynostatisticaltheory.Instead,theassump- tionsofthevariousstatisticalmodelsarediscussedatlength,andthepracticeofexposing statisticalmodelstorigorouscriticismisencouraged.Aphilosophyofmodelsimplification is developed in which the emphasis is placed on estimating effect sizes from data, and establishing confidence intervals for these estimates."
  },
  {
    "input": "The role of hypothesis testing at an arbitrary threshold of significance like 0:05 is played down.",
    "target": "Q: What is The role of hypothesis testing at an arbitrary threshold of significance like 0:05? A: The role of hypothesis testing at an arbitrary threshold of significance like 0:05 is played down."
  },
  {
    "input": "Astopresentation,theideaisthatbackgroundmaterialwouldbecoveredinaseriesof 1-hour lectures, then this book could be used as a guide to the practical sessions and for homework,withthestudentsworkingontheirownatthecomputer.Myexperienceisthat thematerialcanbecoveredin1030lectures,dependingonthebackgroundofthestudents and the depth of coverage it is hoped to achieve.",
    "target": "Q: What is Astopresentation,theideaisthatbackgroundmaterialwouldbecoveredinaseriesof 1-hour lectures, then this book could be used as a guide to the practical sessions and for homework,withthestudentsworkingontheirownatthecomputer.Myexperienceisthat thematerialcanbecoveredin1030lectures,dependingonthebackgroundofthestudents and the depth of coverage it? A: Astopresentation,theideaisthatbackgroundmaterialwouldbecoveredinaseriesof 1-hour lectures, then this book could be used as a guide to the practical sessions and for homework,withthestudentsworkingontheirownatthecomputer.Myexperienceisthat thematerialcanbecoveredin1030lectures,dependingonthebackgroundofthestudents and the depth of coverage it is hoped to achieve."
  },
  {
    "input": "The practical work is designed to be covered in 1015 sessions of about 1 hours each, again depending on the ambition and depthofthecoverage,andontheamountofone-to-onehelpavailabletothestudentsasthey work at their computers.",
    "target": "Q: What is The practical work? A: The practical work is designed to be covered in 1015 sessions of about 1 hours each, again depending on the ambition and depthofthecoverage,andontheamountofone-to-onehelpavailabletothestudentsasthey work at their computers."
  },
  {
    "input": "Their idea was to provide a software tool for professional statisticians who wanted to combine state-of-the-art graphics with powerful model-fitting capability.Sismadeupofthreecomponents.Firstandforemost,itisapowerful toolfor statisticalmodelling.Itenablesyoutospecifyandfitstatisticalmodelstoyourdata,assess thegoodnessoffitanddisplaytheestimates,standarderrorsandpredictedvaluesderived xii PREFACE fromthemodel.Itprovidesyouwiththemeanstodefineandmanipulateyourdata,butthe way you go about the job of modelling is not predetermined, and the user is left with maximum controloverthemodel-fittingprocess.Second,Scanbeusedfordataexplora- tion,intabulatingandsortingdata,indrawingscatterplotstolookfortrendsinyourdata,or to check visually for the presence of outliers.",
    "target": "Q: What is Their idea was to provide a software tool for professional statisticians who wanted to combine state-of-the-art graphics with powerful model-fitting capability.Sismadeupofthreecomponents.Firstandforemost,itisapowerful toolfor statisticalmodelling.Itenablesyoutospecifyandfitstatisticalmodelstoyourdata,assess thegoodnessoffitanddisplaytheestimates,standarderrorsandpredictedvaluesderived xii PREFACE fromthemodel.Itprovidesyouwiththemeanstodefineandmanipulateyourdata,butthe way you go about the job of modelling? A: Their idea was to provide a software tool for professional statisticians who wanted to combine state-of-the-art graphics with powerful model-fitting capability.Sismadeupofthreecomponents.Firstandforemost,itisapowerful toolfor statisticalmodelling.Itenablesyoutospecifyandfitstatisticalmodelstoyourdata,assess thegoodnessoffitanddisplaytheestimates,standarderrorsandpredictedvaluesderived xii PREFACE fromthemodel.Itprovidesyouwiththemeanstodefineandmanipulateyourdata,butthe way you go about the job of modelling is not predetermined, and the user is left with maximum controloverthemodel-fittingprocess.Second,Scanbeusedfordataexplora- tion,intabulatingandsortingdata,indrawingscatterplotstolookfortrendsinyourdata,or to check visually for the presence of outliers."
  },
  {
    "input": "S is especially useful in handling difficult or unusual data sets, because itsflexibilityenablesittocopewithsuchproblemsasunequalreplication,missingvalues, non-orthogonaldesigns,andsoon.Furthermore,theopen-endedstyleofSisparticularly appropriateforfollowingthroughoriginalideasanddevelopingnewconcepts.Oneofthe greatadvantagesoflearningSisthatthesimpleconceptsthatunderlieitprovideaunified frameworkforlearningaboutstatisticalideasingeneral.Byviewingparticularmodelsina generalcontext,Shighlightsthefundamentalsimilaritiesbetweenstatisticaltechniquesand helps play down their superficial differences.",
    "target": "Q: What is S? A: S is especially useful in handling difficult or unusual data sets, because itsflexibilityenablesittocopewithsuchproblemsasunequalreplication,missingvalues, non-orthogonaldesigns,andsoon.Furthermore,theopen-endedstyleofSisparticularly appropriateforfollowingthroughoriginalideasanddevelopingnewconcepts.Oneofthe greatadvantagesoflearningSisthatthesimpleconceptsthatunderlieitprovideaunified frameworkforlearningaboutstatisticalideasingeneral.Byviewingparticularmodelsina generalcontext,Shighlightsthefundamentalsimilaritiesbetweenstatisticaltechniquesand helps play down their superficial differences."
  },
  {
    "input": "This book is written using version 3.0.1, but all the code will run under earlier releases.",
    "target": "Q: What is This book? A: This book is written using version 3.0.1, but all the code will run under earlier releases."
  },
  {
    "input": "Learning R is not easy, but you will not regret investing the effort to master the basics.",
    "target": "Q: What is Learning R? A: Learning R is not easy, but you will not regret investing the effort to master the basics."
  },
  {
    "input": "Thekeyistounderstandwhatkindofresponsevariableyouhavegot,andtoknowthe natureofyourexplanatoryvariables.Theresponsevariableisthethingyouareworkingon:it isthevariablewhosevariationyouareattemptingtounderstand.Thisisthevariablethatgoes ontheyaxisofthegraph(theordinate).Theexplanatoryvariablegoesonthexaxisofthe graph(theabscissa);youareinterestedintheextenttowhichvariationintheresponsevariable is associated with variation in the explanatory variable.",
    "target": "Q: What is Thekeyistounderstandwhatkindofresponsevariableyouhavegot,andtoknowthe natureofyourexplanatoryvariables.Theresponsevariableisthethingyouareworkingon:it isthevariablewhosevariationyouareattemptingtounderstand.Thisisthevariablethatgoes ontheyaxisofthegraph(theordinate).Theexplanatoryvariablegoesonthexaxisofthe graph(theabscissa);youareinterestedintheextenttowhichvariationintheresponsevariable? A: Thekeyistounderstandwhatkindofresponsevariableyouhavegot,andtoknowthe natureofyourexplanatoryvariables.Theresponsevariableisthethingyouareworkingon:it isthevariablewhosevariationyouareattemptingtounderstand.Thisisthevariablethatgoes ontheyaxisofthegraph(theordinate).Theexplanatoryvariablegoesonthexaxisofthe graph(theabscissa);youareinterestedintheextenttowhichvariationintheresponsevariable is associated with variation in the explanatory variable."
  },
  {
    "input": "It is essential, therefore, that you know: (cid:129) which of your variables is the response variable?",
    "target": "Q: What is It? A: It is essential, therefore, that you know: (cid:129) which of your variables is the response variable?"
  },
  {
    "input": "Heterogeneity is universal: spatial hetero- geneity means that places always differ, and temporal heterogeneity means that times always differ.",
    "target": "Q: What is Heterogeneity? A: Heterogeneity is universal: spatial hetero- geneity means that places always differ, and temporal heterogeneity means that times always differ."
  },
  {
    "input": "The key concept is the amount of variation that we would expect to occur by chance alone, when nothing scientifically interesting was going on.",
    "target": "Q: What is The key concept? A: The key concept is the amount of variation that we would expect to occur by chance alone, when nothing scientifically interesting was going on."
  },
  {
    "input": "If we measure bigger differ- encesthanwewouldexpectbychance,wesaythattheresultisstatisticallysignificant.Ifwe measurenomorevariationthanwemightreasonablyexpecttooccurbychancealone,then wesaythatourresultisnotstatisticallysignificant.Itisimportanttounderstandthatthisis not to say that the result is not important.",
    "target": "Q: What is If we measure bigger differ- encesthanwewouldexpectbychance,wesaythattheresultisstatisticallysignificant.Ifwe measurenomorevariationthanwemightreasonablyexpecttooccurbychancealone,then wesaythatourresultisnotstatisticallysignificant.Itisimportanttounderstandthatthisis not to say that the result? A: If we measure bigger differ- encesthanwewouldexpectbychance,wesaythattheresultisstatisticallysignificant.Ifwe measurenomorevariationthanwemightreasonablyexpecttooccurbychancealone,then wesaythatourresultisnotstatisticallysignificant.Itisimportanttounderstandthatthisis not to say that the result is not important."
  },
  {
    "input": "Non-significant differences in human life span betweentwodrugtreatmentsmaybemassivelyimportant(especiallyifyouarethepatient involved).Non-significantisnotthesameasnotdifferent.Thelackofsignificancemaybe due simply to the fact that our replication is too low.",
    "target": "Q: What is Non-significant differences in human life span betweentwodrugtreatmentsmaybemassivelyimportant(especiallyifyouarethepatient involved).Non-significantisnotthesameasnotdifferent.Thelackofsignificancemaybe due simply to the fact that our replication? A: Non-significant differences in human life span betweentwodrugtreatmentsmaybemassivelyimportant(especiallyifyouarethepatient involved).Non-significantisnotthesameasnotdifferent.Thelackofsignificancemaybe due simply to the fact that our replication is too low."
  },
  {
    "input": "Some students think that the only good result is a significant result.",
    "target": "Q: What is Some students think that the only good result? A: Some students think that the only good result is a significant result."
  },
  {
    "input": "This is an understandablefailingofhumannature,butitisnotgoodscience.Thepointisthatwewant toknowthetruth,onewayortheother.Weshouldtrynottocaretoomuchabouttheway thingsturnout.Thisisnotanamoralstance,itjusthappenstobethewaythatscienceworks best.Ofcourse,itishopelesslyidealistictopretendthatthisisthewaythatscientistsreally behave.Scientistsoftenwantpassionatelythataparticularexperimentalresultwillturnout tobestatisticallysignificant,sothattheycangetaNaturepaperandgetpromoted.Butthat does not make it right.",
    "target": "Q: What is This? A: This is an understandablefailingofhumannature,butitisnotgoodscience.Thepointisthatwewant toknowthetruth,onewayortheother.Weshouldtrynottocaretoomuchabouttheway thingsturnout.Thisisnotanamoralstance,itjusthappenstobethewaythatscienceworks best.Ofcourse,itishopelesslyidealistictopretendthatthisisthewaythatscientistsreally behave.Scientistsoftenwantpassionatelythataparticularexperimentalresultwillturnout tobestatisticallysignificant,sothattheycangetaNaturepaperandgetpromoted.Butthat does not make it right."
  },
  {
    "input": "He argued that a good hypothesis is a falsifiable hypothesis.",
    "target": "Q: What is He argued that a good hypothesis? A: He argued that a good hypothesis is a falsifiable hypothesis."
  },
  {
    "input": "Consider the following two assertions: A. there are vultures in the local park B. there are no vultures in the local park Both involve the same essential idea, but one is refutable and the other is not.",
    "target": "Q: What is Consider the following two assertions: A. there are vultures in the local park B. there are no vultures in the local park Both involve the same essential idea, but one? A: Consider the following two assertions: A. there are vultures in the local park B. there are no vultures in the local park Both involve the same essential idea, but one is refutable and the other is not."
  },
  {
    "input": "NullHypotheses Thenullhypothesissaysnothingishappening.Forinstance,whenwearecomparingtwo sample means, the null hypothesis is that the means of the two populations are the same.",
    "target": "Q: What is NullHypotheses Thenullhypothesissaysnothingishappening.Forinstance,whenwearecomparingtwo sample means, the null hypothesis? A: NullHypotheses Thenullhypothesissaysnothingishappening.Forinstance,whenwearecomparingtwo sample means, the null hypothesis is that the means of the two populations are the same."
  },
  {
    "input": "Again,when workingwithagraphofyagainstxinaregressionstudy,thenullhypothesisisthattheslope oftherelationshipiszero(i.e.yisnotafunctionofx,oryisindependentofx).Theessential pointisthatthenullhypothesisisfalsifiable.Werejectthenullhypothesiswhenourdata show that the null hypothesis is sufficiently unlikely.",
    "target": "Q: What is Again,when workingwithagraphofyagainstxinaregressionstudy,thenullhypothesisisthattheslope oftherelationshipiszero(i.e.yisnotafunctionofx,oryisindependentofx).Theessential pointisthatthenullhypothesisisfalsifiable.Werejectthenullhypothesiswhenourdata show that the null hypothesis? A: Again,when workingwithagraphofyagainstxinaregressionstudy,thenullhypothesisisthattheslope oftherelationshipiszero(i.e.yisnotafunctionofx,oryisindependentofx).Theessential pointisthatthenullhypothesisisfalsifiable.Werejectthenullhypothesiswhenourdata show that the null hypothesis is sufficiently unlikely."
  },
  {
    "input": "In particular, a p value is an estimate of the probability that a value of the test statistic, oravaluemoreextremethanthis,couldhaveoccurredbychancewhenthenullhypothesis istrue.Bigvaluesoftheteststatisticindicatethatthenullhypothesisisunlikelytobetrue.",
    "target": "Q: What is In particular, a p value? A: In particular, a p value is an estimate of the probability that a value of the test statistic, oravaluemoreextremethanthis,couldhaveoccurredbychancewhenthenullhypothesis istrue.Bigvaluesoftheteststatisticindicatethatthenullhypothesisisunlikelytobetrue."
  },
  {
    "input": "Note also that saying we do not reject the null hypothesis and the null hypothesis is truearetwoquitedifferent things.Forinstance,wemay havefailed torejectafalsenull hypothesisbecauseoursamplesizewastoolow,orbecauseourmeasurementerrorwastoo large.",
    "target": "Q: What is Note also that saying we do not reject the null hypothesis and the null hypothesis? A: Note also that saying we do not reject the null hypothesis and the null hypothesis is truearetwoquitedifferent things.Forinstance,wemay havefailed torejectafalsenull hypothesisbecauseoursamplesizewastoolow,orbecauseourmeasurementerrorwastoo large."
  },
  {
    "input": "Interpretation Itshouldbeclearbythispointthatwecanmaketwokindsofmistakesintheinterpretation of our statistical models: (cid:129) we can reject the null hypothesis when it is true (cid:129) we can accept the null hypothesis when it is false ThesearereferredtoasTypeIandTypeIIerrors,respectively.Supposingweknewthe true state of affairs (which, of course, we seldom do).",
    "target": "Q: What is Interpretation Itshouldbeclearbythispointthatwecanmaketwokindsofmistakesintheinterpretation of our statistical models: (cid:129) we can reject the null hypothesis when it? A: Interpretation Itshouldbeclearbythispointthatwecanmaketwokindsofmistakesintheinterpretation of our statistical models: (cid:129) we can reject the null hypothesis when it is true (cid:129) we can accept the null hypothesis when it is false ThesearereferredtoasTypeIandTypeIIerrors,respectively.Supposingweknewthe true state of affairs (which, of course, we seldom do)."
  },
  {
    "input": "Model choice is one of the most frequently ignored of the big issues involved in learning statistics.",
    "target": "Q: What is Model choice? A: Model choice is one of the most frequently ignored of the big issues involved in learning statistics."
  },
  {
    "input": "The book also encourages users to understand that in most cases there are literally hundreds of possible models, and that choosing the best model is an essential part of the processofstatisticalanalysis.Whichexplanatoryvariablestoincludeinyourmodel,what transformationtoapplytoeachvariable,whethertoincludeinteractionterms:allofthese are key issues that you need to resolve.",
    "target": "Q: What is The book also encourages users to understand that in most cases there are literally hundreds of possible models, and that choosing the best model? A: The book also encourages users to understand that in most cases there are literally hundreds of possible models, and that choosing the best model is an essential part of the processofstatisticalanalysis.Whichexplanatoryvariablestoincludeinyourmodel,what transformationtoapplytoeachvariable,whethertoincludeinteractionterms:allofthese are key issues that you need to resolve."
  },
  {
    "input": "The issues are most difficult with observational studies where there are large numbers of (possibly correlated) explan- atoryvariables,littleornorandomizationandsmallnumbersofdatapoints.Muchofyour data is likely to come from the second category.",
    "target": "Q: What is The issues are most difficult with observational studies where there are large numbers of (possibly correlated) explan- atoryvariables,littleornorandomizationandsmallnumbersofdatapoints.Muchofyour data? A: The issues are most difficult with observational studies where there are large numbers of (possibly correlated) explan- atoryvariables,littleornorandomizationandsmallnumbersofdatapoints.Muchofyour data is likely to come from the second category."
  },
  {
    "input": "On the contrary, what we are looking for is the minimal adequate modeltodescribethedata.Themodelisfittedtodata,nottheotherwayaround.Thebest model is the model that produces the least unexplained variation (the minimal residual deviance),subjecttotheconstraintthattheparametersinthemodelshouldallbestatistically significant.",
    "target": "Q: What is On the contrary, what we are looking for? A: On the contrary, what we are looking for is the minimal adequate modeltodescribethedata.Themodelisfittedtodata,nottheotherwayaround.Thebest model is the model that produces the least unexplained variation (the minimal residual deviance),subjecttotheconstraintthattheparametersinthemodelshouldallbestatistically significant."
  },
  {
    "input": "It embodies your mechanistic understanding of the factorsinvolved,andofthewaythattheyarerelatedtotheresponsevariable.Wewantthe modeltobeminimalbecauseoftheprincipleofparsimony,andadequatebecausethereis nopointinretaininganinadequatemodelthatdoesnotdescribeasignificantfractionof thevariationinthedata.Itisveryimportanttounderstandthatthereisnotonemodel;this is one of the common implicit errors involved in traditional regression and ANOVA, where the same models are used, often uncritically, over and over again.",
    "target": "Q: What is It embodies your mechanistic understanding of the factorsinvolved,andofthewaythattheyarerelatedtotheresponsevariable.Wewantthe modeltobeminimalbecauseoftheprincipleofparsimony,andadequatebecausethereis nopointinretaininganinadequatemodelthatdoesnotdescribeasignificantfractionof thevariationinthedata.Itisveryimportanttounderstandthatthereisnotonemodel;this? A: It embodies your mechanistic understanding of the factorsinvolved,andofthewaythattheyarerelatedtotheresponsevariable.Wewantthe modeltobeminimalbecauseoftheprincipleofparsimony,andadequatebecausethereis nopointinretaininganinadequatemodelthatdoesnotdescribeasignificantfractionof thevariationinthedata.Itisveryimportanttounderstandthatthereisnotonemodel;this is one of the common implicit errors involved in traditional regression and ANOVA, where the same models are used, often uncritically, over and over again."
  },
  {
    "input": "In most circumstances, there will be a large number of different, more or less plausible models thatmightbefittedtoanygivensetofdata.Partofthejobofdataanalysisistodetermine 6 STATISTICS:ANINTRODUCTIONUSINGR which, if any, of the possible models are adequate, and then, out of the set of adequate models,whichistheminimaladequatemodel.Insomecasestheremaybenosinglebest model and a set of different models may all describe the data equally well (or equally poorly if the variability is great).",
    "target": "Q: What is In most circumstances, there will be a large number of different, more or less plausible models thatmightbefittedtoanygivensetofdata.Partofthejobofdataanalysisistodetermine 6 STATISTICS:ANINTRODUCTIONUSINGR which, if any, of the possible models are adequate, and then, out of the set of adequate models,whichistheminimaladequatemodel.Insomecasestheremaybenosinglebest model and a set of different models may all describe the data equally well (or equally poorly if the variability? A: In most circumstances, there will be a large number of different, more or less plausible models thatmightbefittedtoanygivensetofdata.Partofthejobofdataanalysisistodetermine 6 STATISTICS:ANINTRODUCTIONUSINGR which, if any, of the possible models are adequate, and then, out of the set of adequate models,whichistheminimaladequatemodel.Insomecasestheremaybenosinglebest model and a set of different models may all describe the data equally well (or equally poorly if the variability is great)."
  },
  {
    "input": "The convention we adopt is that our techniques should lead to unbiased, variance minimizing estimators.",
    "target": "Q: What is The convention we adopt? A: The convention we adopt is that our techniques should lead to unbiased, variance minimizing estimators."
  },
  {
    "input": "This is how it works: (cid:129) given the data (cid:129) and given our choice of model (cid:129) what values of the parameters of that model (cid:129) make the observed data most likely?",
    "target": "Q: What is This? A: This is how it works: (cid:129) given the data (cid:129) and given our choice of model (cid:129) what values of the parameters of that model (cid:129) make the observed data most likely?"
  },
  {
    "input": "Note that the point at which the graph cuts the y axis is not the intercept when (as here) you let R decide where to put the axes.",
    "target": "Q: What is Note that the point at which the graph cuts the y axis? A: Note that the point at which the graph cuts the y axis is not the intercept when (as here) you let R decide where to put the axes."
  },
  {
    "input": "The maximum likelihoodofthe data given the model is obtained with a slope of 0.679 (right-hand graph).",
    "target": "Q: What is The maximum likelihoodofthe data given the model? A: The maximum likelihoodofthe data given the model is obtained with a slope of 0.679 (right-hand graph)."
  },
  {
    "input": "This is not how the procedure is carried out in practice, but it makes the point that we judgethemodelonthebasishowlikelythedatawouldbeifthemodelwerecorrect.When we do the analysis in earnest, both parameters are estimated simultaneously.",
    "target": "Q: What is This? A: This is not how the procedure is carried out in practice, but it makes the point that we judgethemodelonthebasishowlikelythedatawouldbeifthemodelwerecorrect.When we do the analysis in earnest, both parameters are estimated simultaneously."
  },
  {
    "input": "But if your experiment is not properly designed, or not thoroughly randomized, or lacking adequate controls, then no matter how good you are at stats, some(orpossiblyevenall)ofyourexperimentaleffortwillhavebeenwasted.Noamountof high-poweredstatisticalanalysiscanturnabadexperimentintoagoodone.Risgood,but not that good.",
    "target": "Q: What is But if your experiment? A: But if your experiment is not properly designed, or not thoroughly randomized, or lacking adequate controls, then no matter how good you are at stats, some(orpossiblyevenall)ofyourexperimentaleffortwillhavebeenwasted.Noamountof high-poweredstatisticalanalysiscanturnabadexperimentintoagoodone.Risgood,but not that good."
  },
  {
    "input": "The principle of parsimony is attributed to the fourteenth-century English nominalist philosopherWilliamofOccamwhoinsistedthat,givenasetofequallygoodexplanations for a given phenomenon, then the correct explanation is the simplest explanation.",
    "target": "Q: What is The principle of parsimony? A: The principle of parsimony is attributed to the fourteenth-century English nominalist philosopherWilliamofOccamwhoinsistedthat,givenasetofequallygoodexplanations for a given phenomenon, then the correct explanation is the simplest explanation."
  },
  {
    "input": "It is called Occams razor because he shaved his explanations down to the bare minimum.",
    "target": "Q: What is It? A: It is called Occams razor because he shaved his explanations down to the bare minimum."
  },
  {
    "input": "In statistical modelling, the principle of parsimony means that: (cid:129) models should have as few parameters as possible (cid:129) linear models should be preferred to non-linear models (cid:129) experimentsrelyingonfewassumptionsshouldbepreferredtothoserelyingonmany (cid:129) models should be pared down until they are minimal adequate (cid:129) simple explanations should be preferred to complex explanations The process of model simplification is an integral part of statistical analysis in R. In general,avariableisretainedinthemodelonlyifitcausesasignificantincreaseindeviance when it is removed from the current model.",
    "target": "Q: What is In statistical modelling, the principle of parsimony means that: (cid:129) models should have as few parameters as possible (cid:129) linear models should be preferred to non-linear models (cid:129) experimentsrelyingonfewassumptionsshouldbepreferredtothoserelyingonmany (cid:129) models should be pared down until they are minimal adequate (cid:129) simple explanations should be preferred to complex explanations The process of model simplification? A: In statistical modelling, the principle of parsimony means that: (cid:129) models should have as few parameters as possible (cid:129) linear models should be preferred to non-linear models (cid:129) experimentsrelyingonfewassumptionsshouldbepreferredtothoserelyingonmany (cid:129) models should be pared down until they are minimal adequate (cid:129) simple explanations should be preferred to complex explanations The process of model simplification is an integral part of statistical analysis in R. In general,avariableisretainedinthemodelonlyifitcausesasignificantincreaseindeviance when it is removed from the current model."
  },
  {
    "input": "There are lots of cases, for example, where it is ethicallyorlogisticallyimpossibletocarryoutmanipulativeexperiments.Inthesecasesitis doublyimportanttoensurethatthestatisticalanalysisleadstoconclusionsthatareascritical and as unambiguous as possible.",
    "target": "Q: What is There are lots of cases, for example, where it? A: There are lots of cases, for example, where it is ethicallyorlogisticallyimpossibletocarryoutmanipulativeexperiments.Inthesecasesitis doublyimportanttoensurethatthestatisticalanalysisleadstoconclusionsthatareascritical and as unambiguous as possible."
  },
  {
    "input": "(cid:129) repeated measures (e.g.from thesameindividualorthesame spatial location) arenot replicates (this is probably the commonest cause of pseudoreplication in statistical work) HowManyReplicates?",
    "target": "Q: What is (cid:129) repeated measures (e.g.from thesameindividualorthesame spatial location) arenot replicates (this? A: (cid:129) repeated measures (e.g.from thesameindividualorthesame spatial location) arenot replicates (this is probably the commonest cause of pseudoreplication in statistical work) HowManyReplicates?"
  },
  {
    "input": "Nevertheless, it is a rule of great practical utility, if only for giving you pause as you design your experiment with 300 replicatesthatperhapsthismightreallybeabitoverthetop.Orwhenyouthinkyoucould get away with just five replicates this time.",
    "target": "Q: What is Nevertheless, it? A: Nevertheless, it is a rule of great practical utility, if only for giving you pause as you design your experiment with 300 replicatesthatperhapsthismightreallybeabitoverthetop.Orwhenyouthinkyoucould get away with just five replicates this time."
  },
  {
    "input": "Nowthepowerofatestisdefinedas1\u00000:8underthestandardassumptions.Thisis 10 STATISTICS:ANINTRODUCTIONUSINGR usedtocalculatethesamplesizesnecessarytodetectaspecifieddifferencewhentheerror variance is known (or can be guessed at).",
    "target": "Q: What is Nowthepowerofatestisdefinedas1\u00000:8underthestandardassumptions.Thisis 10 STATISTICS:ANINTRODUCTIONUSINGR usedtocalculatethesamplesizesnecessarytodetectaspecifieddifferencewhentheerror variance? A: Nowthepowerofatestisdefinedas1\u00000:8underthestandardassumptions.Thisis 10 STATISTICS:ANINTRODUCTIONUSINGR usedtocalculatethesamplesizesnecessarytodetectaspecifieddifferencewhentheerror variance is known (or can be guessed at)."
  },
  {
    "input": "Lets think about the issues involved with power analysis in the context of a Students t-testtocomparetwosamplemeans.Asexplainedonp.91,theteststatisticist=difference/ (the standard error of the difference) and we can rearrange the formula to obtain n, the sample size necessary in order that that a given difference, d, is statistically significant: 2s2t2 n d2 Youcanseethatthelargerthevariances2,andthesmallerthesizeofthedifference,the biggerthesampleweshallneed.Thevalueoftheteststatistictdependsonourdecisions aboutTypeIandTypeIIerrorrates(conventionally0.05and0.2).Forsamplesizesoforder 30,thetvaluesassociatedwiththeseprobabilitiesare1.96and0.84respectively:theseadd to2.80,andthesquare of2.80is7.84.To thenearestwholenumber,theconstants inthe numeratorevaluateto28=16.Soasagoodruleofthumb,thesamplesizeyouneedin each treatment is given by 16s2 n d2 Wesimplyneedtoworkout16timesthesamplevariance(obtainedfromtheliteratureor fromasmallpilotexperiment)anddividebythesquareofthedifferencethatwewanttobe abletodetect.Sosupposethatourcurrentcerealyieldis10t/hawithastandarddeviationof sd=2.8t/ha(givings2=7.84)andwewanttobeabletosaythatayieldincrease(delta)of 2t/ha is significant at 95% with power=80%, then we shall need to have 167.84/ 4=31.36 replicates in each treatment.",
    "target": "Q: What is Lets think about the issues involved with power analysis in the context of a Students t-testtocomparetwosamplemeans.Asexplainedonp.91,theteststatisticist=difference/ (the standard error of the difference) and we can rearrange the formula to obtain n, the sample size necessary in order that that a given difference, d,? A: Lets think about the issues involved with power analysis in the context of a Students t-testtocomparetwosamplemeans.Asexplainedonp.91,theteststatisticist=difference/ (the standard error of the difference) and we can rearrange the formula to obtain n, the sample size necessary in order that that a given difference, d, is statistically significant: 2s2t2 n d2 Youcanseethatthelargerthevariances2,andthesmallerthesizeofthedifference,the biggerthesampleweshallneed.Thevalueoftheteststatistictdependsonourdecisions aboutTypeIandTypeIIerrorrates(conventionally0.05and0.2).Forsamplesizesoforder 30,thetvaluesassociatedwiththeseprobabilitiesare1.96and0.84respectively:theseadd to2.80,andthesquare of2.80is7.84.To thenearestwholenumber,theconstants inthe numeratorevaluateto28=16.Soasagoodruleofthumb,thesamplesizeyouneedin each treatment is given by 16s2 n d2 Wesimplyneedtoworkout16timesthesamplevariance(obtainedfromtheliteratureor fromasmallpilotexperiment)anddividebythesquareofthedifferencethatwewanttobe abletodetect.Sosupposethatourcurrentcerealyieldis10t/hawithastandarddeviationof sd=2.8t/ha(givings2=7.84)andwewanttobeabletosaythatayieldincrease(delta)of 2t/ha is significant at 95% with power=80%, then we shall need to have 167.84/ 4=31.36 replicates in each treatment."
  },
  {
    "input": "Randomization Randomization is something that everybody says they do, but hardly anybody does properly.",
    "target": "Q: What is Randomization Randomization? A: Randomization Randomization is something that everybody says they do, but hardly anybody does properly."
  },
  {
    "input": "Tree (a) has a much greater chance of being selected than tree (b), and so the nearest tree to a random point is not a randomly selected tree.",
    "target": "Q: What is Tree (a) has a much greater chance of being selected than tree (b), and so the nearest tree to a random point? A: Tree (a) has a much greater chance of being selected than tree (b), and so the nearest tree to a random point is not a randomly selected tree."
  },
  {
    "input": "The answer is that to select a tree at random, every single tree in the forest must be numbered (all 24 683 of them, or whatever), and then a random number between 1 and 24 683must be drawn outofa hat.There isno alternative.Anythingless than that is not randomization.",
    "target": "Q: What is The answer? A: The answer is that to select a tree at random, every single tree in the forest must be numbered (all 24 683 of them, or whatever), and then a random number between 1 and 24 683must be drawn outofa hat.There isno alternative.Anythingless than that is not randomization."
  },
  {
    "input": "Nowaskyourselfhowoftenthisisdoneinpractice,andyouwillseewhatImeanwhen I say that randomization is a classic example of Do as I say, and not do as I do.",
    "target": "Q: What is Nowaskyourselfhowoftenthisisdoneinpractice,andyouwillseewhatImeanwhen I say that randomization? A: Nowaskyourselfhowoftenthisisdoneinpractice,andyouwillseewhatImeanwhen I say that randomization is a classic example of Do as I say, and not do as I do."
  },
  {
    "input": "12 STATISTICS:ANINTRODUCTIONUSINGR It is entirely plausible that flour beetles differ in their activity levels (sex differences, differencesinbodyweight,age,etc.",
    "target": "Q: What is 12 STATISTICS:ANINTRODUCTIONUSINGR It? A: 12 STATISTICS:ANINTRODUCTIONUSINGR It is entirely plausible that flour beetles differ in their activity levels (sex differences, differencesinbodyweight,age,etc."
  },
  {
    "input": "14 STATISTICS:ANINTRODUCTIONUSINGR StrongInference One of the most powerful means available to demonstrate the accuracy of an idea is an experimentalconfirmationofapredictionmadebyacarefullyformulatedhypothesis.There are two essential steps to the protocol of strong inference (Platt, 1964): (cid:129) formulate a clear hypothesis (cid:129) devise an acceptable test Neitheroneismuchgoodwithouttheother.Forexample,thehypothesisshouldnotlead to predictions that are likely to occur by other extrinsic means.",
    "target": "Q: What is 14 STATISTICS:ANINTRODUCTIONUSINGR StrongInference One of the most powerful means available to demonstrate the accuracy of an idea? A: 14 STATISTICS:ANINTRODUCTIONUSINGR StrongInference One of the most powerful means available to demonstrate the accuracy of an idea is an experimentalconfirmationofapredictionmadebyacarefullyformulatedhypothesis.There are two essential steps to the protocol of strong inference (Platt, 1964): (cid:129) formulate a clear hypothesis (cid:129) devise an acceptable test Neitheroneismuchgoodwithouttheother.Forexample,thehypothesisshouldnotlead to predictions that are likely to occur by other extrinsic means."
  },
  {
    "input": "Similarly, the test should demonstrate unequivocally whether the hypothesis is true or false.",
    "target": "Q: What is Similarly, the test should demonstrate unequivocally whether the hypothesis? A: Similarly, the test should demonstrate unequivocally whether the hypothesis is true or false."
  },
  {
    "input": "While this approach may be commendable in the early stages of a study, such experiments tend to be weak as an end in themselves, because there will be such a large number of equally plausible explanationsfortheresults.Withoutcontemplationtherewillbenotestablepredictions; without testable predictions there will be no experimental ingenuity; without experi- mental ingenuity there is likely to be inadequate control; in short, equivocal interpreta- tion.",
    "target": "Q: What is While this approach may be commendable in the early stages of a study, such experiments tend to be weak as an end in themselves, because there will be such a large number of equally plausible explanationsfortheresults.Withoutcontemplationtherewillbenotestablepredictions; without testable predictions there will be no experimental ingenuity; without experi- mental ingenuity there? A: While this approach may be commendable in the early stages of a study, such experiments tend to be weak as an end in themselves, because there will be such a large number of equally plausible explanationsfortheresults.Withoutcontemplationtherewillbenotestablepredictions; without testable predictions there will be no experimental ingenuity; without experi- mental ingenuity there is likely to be inadequate control; in short, equivocal interpreta- tion."
  },
  {
    "input": "It is silly to be disparagingaboutthesedata,becausetheyareoftentheonlydatathatwehave.Theaimof good statistical analysis is to obtain the maximum information from a given set of data, bearing the limitations of the data firmly in mind.",
    "target": "Q: What is It? A: It is silly to be disparagingaboutthesedata,becausetheyareoftentheonlydatathatwehave.Theaimof good statistical analysis is to obtain the maximum information from a given set of data, bearing the limitations of the data firmly in mind."
  },
  {
    "input": "Ideally,thedurationofanexperimentshouldbedeterminedinadvance,lestonefallspreyto one of the twin temptations: FUNDAMENTALS 15 (cid:129) to stop the experiment as soon as a pleasing result is obtained (cid:129) to keep going with the experiment until the right result is achieved (the Gregor Mendel effect) In practice, most experiments probably run for too short a period, because of the idiosyncrasies of scientific funding.",
    "target": "Q: What is Ideally,thedurationofanexperimentshouldbedeterminedinadvance,lestonefallspreyto one of the twin temptations: FUNDAMENTALS 15 (cid:129) to stop the experiment as soon as a pleasing result? A: Ideally,thedurationofanexperimentshouldbedeterminedinadvance,lestonefallspreyto one of the twin temptations: FUNDAMENTALS 15 (cid:129) to stop the experiment as soon as a pleasing result is obtained (cid:129) to keep going with the experiment until the right result is achieved (the Gregor Mendel effect) In practice, most experiments probably run for too short a period, because of the idiosyncrasies of scientific funding."
  },
  {
    "input": "This short-term work is particularly dangerous in medicine and the environmental sciences, because the kind of short-term dynamics exhibited after pulse experiments may be entirely different from the long-term dynamics of the same system.",
    "target": "Q: What is This short-term work? A: This short-term work is particularly dangerous in medicine and the environmental sciences, because the kind of short-term dynamics exhibited after pulse experiments may be entirely different from the long-term dynamics of the same system."
  },
  {
    "input": "The other great advantage of long-term experiments is that a wide range of patterns (e.g.",
    "target": "Q: What is The other great advantage of long-term experiments? A: The other great advantage of long-term experiments is that a wide range of patterns (e.g."
  },
  {
    "input": "There are two kinds of pseudoreplication: (cid:129) temporalpseudoreplication,involvingrepeatedmeasurementsfromthesameindividual (cid:129) spatialpseudoreplication,involvingseveralmeasurementstakenfromthesamevicinity Pseudoreplication is a problem because one of the most important assumptions of standard statistical analysis is independence of errors.",
    "target": "Q: What is There are two kinds of pseudoreplication: (cid:129) temporalpseudoreplication,involvingrepeatedmeasurementsfromthesameindividual (cid:129) spatialpseudoreplication,involvingseveralmeasurementstakenfromthesamevicinity Pseudoreplication? A: There are two kinds of pseudoreplication: (cid:129) temporalpseudoreplication,involvingrepeatedmeasurementsfromthesameindividual (cid:129) spatialpseudoreplication,involvingseveralmeasurementstakenfromthesamevicinity Pseudoreplication is a problem because one of the most important assumptions of standard statistical analysis is independence of errors."
  },
  {
    "input": "The reason for this is thatconditionswithineachquadratarequitelikelytobeunique,andsoall 50plantswill experience more or less the same unique set of conditions, irrespective of the spraying treatment they receive.",
    "target": "Q: What is The reason for this? A: The reason for this is thatconditionswithineachquadratarequitelikelytobeunique,andsoall 50plantswill experience more or less the same unique set of conditions, irrespective of the spraying treatment they receive."
  },
  {
    "input": "Theproblemisthat it 16 STATISTICS:ANINTRODUCTIONUSINGR leads to the reporting of masses of spuriously significant results (with 4998 degrees of freedomforerror,itisalmostimpossiblenottohavesignificantdifferences).Thefirstskill to be acquired by the budding experimenter is the ability to plan an experiment that is properly replicated.",
    "target": "Q: What is Theproblemisthat it 16 STATISTICS:ANINTRODUCTIONUSINGR leads to the reporting of masses of spuriously significant results (with 4998 degrees of freedomforerror,itisalmostimpossiblenottohavesignificantdifferences).Thefirstskill to be acquired by the budding experimenter? A: Theproblemisthat it 16 STATISTICS:ANINTRODUCTIONUSINGR leads to the reporting of masses of spuriously significant results (with 4998 degrees of freedomforerror,itisalmostimpossiblenottohavesignificantdifferences).Thefirstskill to be acquired by the budding experimenter is the ability to plan an experiment that is properly replicated."
  },
  {
    "input": "It is often implicitly assumed that all the experimental units were alikeatthebeginningoftheexperiment,butthisneedstobedemonstratedratherthantaken onfaith.One ofthemost important uses of data on initial conditions isasa check onthe efficiencyofrandomization.Forexample,youshouldbeabletorunyourstatisticalanalysis todemonstratethattheindividualorganismswerenotsignificantlydifferentinmeansizeat the beginning of a growth experiment.",
    "target": "Q: What is It? A: It is often implicitly assumed that all the experimental units were alikeatthebeginningoftheexperiment,butthisneedstobedemonstratedratherthantaken onfaith.One ofthemost important uses of data on initial conditions isasa check onthe efficiencyofrandomization.Forexample,youshouldbeabletorunyourstatisticalanalysis todemonstratethattheindividualorganismswerenotsignificantlydifferentinmeansizeat the beginning of a growth experiment."
  },
  {
    "input": "Without measurements of initial size, it is always possible to attribute the end result to differences in initial conditions.",
    "target": "Q: What is Without measurements of initial size, it? A: Without measurements of initial size, it is always possible to attribute the end result to differences in initial conditions."
  },
  {
    "input": "Another reason for measuring initial conditions is that the information can often be used to improve the resolution of the final analysis through analysis of covariance (see Chapter 9).",
    "target": "Q: What is Another reason for measuring initial conditions? A: Another reason for measuring initial conditions is that the information can often be used to improve the resolution of the final analysis through analysis of covariance (see Chapter 9)."
  },
  {
    "input": "Extrinsic aliasing occurs when it is due to the nature of the data.",
    "target": "Q: What is Extrinsic aliasing occurs when it? A: Extrinsic aliasing occurs when it is due to the nature of the data."
  },
  {
    "input": "over-specified models with more parameters than necessary) Ifwehadafactorwithfourlevels(saynone,light,mediumandheavyuse)thenwecould estimatefourmeansfromthedata,oneforeachfactorlevel.Butthemodellookslikethis: y x  x  x  x 1 1 2 2 3 3 4 4 wherethex aredummyvariableshavingthevalue0or1foreachfactorlevel(seep.158),the i  are the effect sizes and  is the overall mean.",
    "target": "Q: What is over-specified models with more parameters than necessary) Ifwehadafactorwithfourlevels(saynone,light,mediumandheavyuse)thenwecould estimatefourmeansfromthedata,oneforeachfactorlevel.Butthemodellookslikethis: y x  x  x  x 1 1 2 2 3 3 4 4 wherethex aredummyvariableshavingthevalue0or1foreachfactorlevel(seep.158),the i  are the effect sizes and? A: over-specified models with more parameters than necessary) Ifwehadafactorwithfourlevels(saynone,light,mediumandheavyuse)thenwecould estimatefourmeansfromthedata,oneforeachfactorlevel.Butthemodellookslikethis: y x  x  x  x 1 1 2 2 3 3 4 4 wherethex aredummyvariableshavingthevalue0or1foreachfactorlevel(seep.158),the i  are the effect sizes and  is the overall mean."
  },
  {
    "input": "Clearly there is no point in having five i parametersinthemodelifwecanestimateonlyfourindependenttermsfromthedata.Oneof theparametersmustbeintrinsicallyaliased.ThistopicisexplainedindetailinChapter11.",
    "target": "Q: What is Clearly there? A: Clearly there is no point in having five i parametersinthemodelifwecanestimateonlyfourindependenttermsfromthedata.Oneof theparametersmustbeintrinsicallyaliased.ThistopicisexplainedindetailinChapter11."
  },
  {
    "input": "2 Ifallofthevaluesofaparticularexplanatoryvariablearesettozeroforagivenlevelofa particular factor, then that level is said to have been intentionally aliased.",
    "target": "Q: What is 2 Ifallofthevaluesofaparticularexplanatoryvariablearesettozeroforagivenlevelofa particular factor, then that level? A: 2 Ifallofthevaluesofaparticularexplanatoryvariablearesettozeroforagivenlevelofa particular factor, then that level is said to have been intentionally aliased."
  },
  {
    "input": "It is extrinsically aliased, and its parameter estimate is set to NA.",
    "target": "Q: What is It? A: It is extrinsically aliased, and its parameter estimate is set to NA."
  },
  {
    "input": "Bonferronis correction is very harsh and will often throw out the baby with the bathwater.",
    "target": "Q: What is Bonferronis correction? A: Bonferronis correction is very harsh and will often throw out the baby with the bathwater."
  },
  {
    "input": "The modern approach is 18 STATISTICS:ANINTRODUCTIONUSINGR to use contrasts wherever possible, and where it is essential to do multiple compari- sons, then to use the wonderfully named Tukeys honestly significant differences (see ?TukeyHSD).",
    "target": "Q: What is The modern approach? A: The modern approach is 18 STATISTICS:ANINTRODUCTIONUSINGR to use contrasts wherever possible, and where it is essential to do multiple compari- sons, then to use the wonderfully named Tukeys honestly significant differences (see ?TukeyHSD)."
  },
  {
    "input": "SummaryofStatisticalModelsinR Modelsarefittedtodata(nottheotherwayround),usingoneofthefollowingmodel-fitting functions: (cid:129) lm:fitsalinearmodelassumingnormalerrorsandconstantvariance;generallythisis usedforregressionanalysisusingcontinuousexplanatoryvariables.Thedefaultoutput is summary.lm (cid:129) aov:analternativetolmwithsummary.aovasthedefaultoutput.Typicallyusedonly when there are complex error terms to be estimated (e.g.",
    "target": "Q: What is SummaryofStatisticalModelsinR Modelsarefittedtodata(nottheotherwayround),usingoneofthefollowingmodel-fitting functions: (cid:129) lm:fitsalinearmodelassumingnormalerrorsandconstantvariance;generallythisis usedforregressionanalysisusingcontinuousexplanatoryvariables.Thedefaultoutput? A: SummaryofStatisticalModelsinR Modelsarefittedtodata(nottheotherwayround),usingoneofthefollowingmodel-fitting functions: (cid:129) lm:fitsalinearmodelassumingnormalerrorsandconstantvariance;generallythisis usedforregressionanalysisusingcontinuousexplanatoryvariables.Thedefaultoutput is summary.lm (cid:129) aov:analternativetolmwithsummary.aovasthedefaultoutput.Typicallyusedonly when there are complex error terms to be estimated (e.g."
  },
  {
    "input": "With a categorical response variable, the tree is called a classification tree, and the model used for classification assumes that the response variable follows a multinomial distribution FUNDAMENTALS 19 Formostofthesemodels,arangeofgenericfunctionscanbeusedtoobtaininformation about the model.",
    "target": "Q: What is With a categorical response variable, the tree? A: With a categorical response variable, the tree is called a classification tree, and the model used for classification assumes that the response variable follows a multinomial distribution FUNDAMENTALS 19 Formostofthesemodels,arangeofgenericfunctionscanbeusedtoobtaininformation about the model."
  },
  {
    "input": "The alternative is to save the script of the whole session just before you finish.",
    "target": "Q: What is The alternative? A: The alternative is to save the script of the whole session just before you finish."
  },
  {
    "input": "This is stored in what R calls the history file.",
    "target": "Q: What is This? A: This is stored in what R calls the history file."
  },
  {
    "input": "Itisabadideatocreateyourscriptsinawordprocessorbecauseseveralofthesymbols you will use may not be readable within R. Double quotes is a classic example of this; yourwordprocessorwillhave(openquotes)and(closequotes)butRwillreadonly \"(simplequotes).However,youmightwanttosavetheresultsfromyourRsessionsina word processor because this can include graphs as well as input and output in the same document.",
    "target": "Q: What is Itisabadideatocreateyourscriptsinawordprocessorbecauseseveralofthesymbols you will use may not be readable within R. Double quotes? A: Itisabadideatocreateyourscriptsinawordprocessorbecauseseveralofthesymbols you will use may not be readable within R. Double quotes is a classic example of this; yourwordprocessorwillhave(openquotes)and(closequotes)butRwillreadonly \"(simplequotes).However,youmightwanttosavetheresultsfromyourRsessionsina word processor because this can include graphs as well as input and output in the same document."
  },
  {
    "input": "HousekeepingwithinR The simplest way to work is to start a new R session for each separate activity.",
    "target": "Q: What is HousekeepingwithinR The simplest way to work? A: HousekeepingwithinR The simplest way to work is to start a new R session for each separate activity."
  },
  {
    "input": "The advantage of working this way is that things from one session do not get mixed up with things from another session.",
    "target": "Q: What is The advantage of working this way? A: The advantage of working this way is that things from one session do not get mixed up with things from another session."
  },
  {
    "input": "Theclassicthingtogowrongisthatyougettwodifferentobjectswiththesamename, andyoudonotknowwhichiswhich.Forinstance,avariablecalledxfromoneanalysismay contain 30 numbers and a different variable called x from another analysis might have FUNDAMENTALS 21 50numbersinit.Atleast,inthatcase,youcantestthelengthoftheobjecttoseewhichoneit is (if it is of length 50 then it must be the x variable from the second analysis).",
    "target": "Q: What is Theclassicthingtogowrongisthatyougettwodifferentobjectswiththesamename, andyoudonotknowwhichiswhich.Forinstance,avariablecalledxfromoneanalysismay contain 30 numbers and a different variable called x from another analysis might have FUNDAMENTALS 21 50numbersinit.Atleast,inthatcase,youcantestthelengthoftheobjecttoseewhichoneit? A: Theclassicthingtogowrongisthatyougettwodifferentobjectswiththesamename, andyoudonotknowwhichiswhich.Forinstance,avariablecalledxfromoneanalysismay contain 30 numbers and a different variable called x from another analysis might have FUNDAMENTALS 21 50numbersinit.Atleast,inthatcase,youcantestthelengthoftheobjecttoseewhichoneit is (if it is of length 50 then it must be the x variable from the second analysis)."
  },
  {
    "input": "Here is the problem situation in full: first.frame<-read.csv(\"c:\\\\temp\\\\test.pollute.csv\") second.frame<-read.csv(\"c:\\\\temp\\\\ozone.data.csv\") attach(first.frame) attach(second.frame) Thefollowingobjectismaskedfromfirst.frame: temp,wind Here is how to avoid the problem first.frame<-read.csv(\"c:\\\\temp\\\\test.pollute.csv\") second.frame<-read.csv(\"c:\\\\temp\\\\ozone.data.csv\") attach(first.frame) .",
    "target": "Q: What is Here? A: Here is the problem situation in full: first.frame<-read.csv(\"c:\\\\temp\\\\test.pollute.csv\") second.frame<-read.csv(\"c:\\\\temp\\\\ozone.data.csv\") attach(first.frame) attach(second.frame) Thefollowingobjectismaskedfromfirst.frame: temp,wind Here is how to avoid the problem first.frame<-read.csv(\"c:\\\\temp\\\\test.pollute.csv\") second.frame<-read.csv(\"c:\\\\temp\\\\ozone.data.csv\") attach(first.frame) ."
  },
  {
    "input": "this is where you work on the information from first.frame.",
    "target": "Q: What is this? A: this is where you work on the information from first.frame."
  },
  {
    "input": "The way to avoid problems like this is to remove all the variables you have calculated before you start on another project during the same session of R. The function for this is rm (or remove) rm(x) Ifyouaskforavariabletoberemovedthatdoesnotexist,thenRwillwarnyouofthisfact: rm(y,z) Warningmessages: 1:Inrm(y,z):objectynotfound 2:Inrm(y,z):objectznotfound We are now in a position to start using R in earnest.",
    "target": "Q: What is The way to avoid problems like this? A: The way to avoid problems like this is to remove all the variables you have calculated before you start on another project during the same session of R. The function for this is rm (or remove) rm(x) Ifyouaskforavariabletoberemovedthatdoesnotexist,thenRwillwarnyouofthisfact: rm(y,z) Warningmessages: 1:Inrm(y,z):objectynotfound 2:Inrm(y,z):objectznotfound We are now in a position to start using R in earnest."
  },
  {
    "input": "The first thing to learn is how to structureadataframeandhowtoreadadataframeintoR.Itisimmenselyirritatingthatthis firststepoftenturnsouttobesodifficultforbeginnerstogetright.OncethedataareintoR, the rest is plain sailing.",
    "target": "Q: What is The first thing to learn? A: The first thing to learn is how to structureadataframeandhowtoreadadataframeintoR.Itisimmenselyirritatingthatthis firststepoftenturnsouttobesodifficultforbeginnerstogetright.OncethedataareintoR, the rest is plain sailing."
  },
  {
    "input": "A dataframe is an object with rows and columns (a bit likeatwo-dimensionalmatrix).Therowscontaindifferentobservationsfromyourstudy, or measurements from your experiment.",
    "target": "Q: What is A dataframe? A: A dataframe is an object with rows and columns (a bit likeatwo-dimensionalmatrix).Therowscontaindifferentobservationsfromyourstudy, or measurements from your experiment."
  },
  {
    "input": "the names of factor levels for categorical variables, like male or female in a variable called gender), they could be calendar dates(like23/5/04),ortheycouldbelogicalvariables(likeTRUEorFALSE).Hereis a spreadsheet in the form of a dataframe with seven variables, the leftmost of which comprises the row names, and other variables are numeric (Area, Slope, Soil pH and Worm density), categorical (Field Name and Vegetation) or logical (Damp is either true=T or false=F).",
    "target": "Q: What is the names of factor levels for categorical variables, like male or female in a variable called gender), they could be calendar dates(like23/5/04),ortheycouldbelogicalvariables(likeTRUEorFALSE).Hereis a spreadsheet in the form of a dataframe with seven variables, the leftmost of which comprises the row names, and other variables are numeric (Area, Slope, Soil pH and Worm density), categorical (Field Name and Vegetation) or logical (Damp? A: the names of factor levels for categorical variables, like male or female in a variable called gender), they could be calendar dates(like23/5/04),ortheycouldbelogicalvariables(likeTRUEorFALSE).Hereis a spreadsheet in the form of a dataframe with seven variables, the leftmost of which comprises the row names, and other variables are numeric (Area, Slope, Soil pH and Worm density), categorical (Field Name and Vegetation) or logical (Damp is either true=T or false=F)."
  },
  {
    "input": "24 STATISTICS:ANINTRODUCTIONUSINGR (Continued) FieldName Area Slope Vegetation SoilpH Damp Wormdensity Cheapside 2.2 8 Scrub 4.7 T 4 PoundHill 4.4 2 Arable 4.5 F 5 GravelPit 2.9 1 Grassland 3.5 F 1 FarmWood 0.8 10 Scrub 5.1 T 3 Perhaps the most important thing about analysing your own data properly is getting your dataframe absolutely right.",
    "target": "Q: What is 24 STATISTICS:ANINTRODUCTIONUSINGR (Continued) FieldName Area Slope Vegetation SoilpH Damp Wormdensity Cheapside 2.2 8 Scrub 4.7 T 4 PoundHill 4.4 2 Arable 4.5 F 5 GravelPit 2.9 1 Grassland 3.5 F 1 FarmWood 0.8 10 Scrub 5.1 T 3 Perhaps the most important thing about analysing your own data properly? A: 24 STATISTICS:ANINTRODUCTIONUSINGR (Continued) FieldName Area Slope Vegetation SoilpH Damp Wormdensity Cheapside 2.2 8 Scrub 4.7 T 4 PoundHill 4.4 2 Arable 4.5 F 5 GravelPit 2.9 1 Grassland 3.5 F 1 FarmWood 0.8 10 Scrub 5.1 T 3 Perhaps the most important thing about analysing your own data properly is getting your dataframe absolutely right."
  },
  {
    "input": "The expectation is that you will have used a spreadsheet likeExceltoenterandeditthedata,andthatyouwillhaveusedplotstocheckforerrors.",
    "target": "Q: What is The expectation? A: The expectation is that you will have used a spreadsheet likeExceltoenterandeditthedata,andthatyouwillhaveusedplotstocheckforerrors."
  },
  {
    "input": "The thing that takes some practice is learning exactly how to put your numbers into the spreadsheet.Therearecountlesswaysofdoingitwrong,butonlyonewayofdoingitright.",
    "target": "Q: What is The thing that takes some practice? A: The thing that takes some practice is learning exactly how to put your numbers into the spreadsheet.Therearecountlesswaysofdoingitwrong,butonlyonewayofdoingitright."
  },
  {
    "input": "And this way is not the way that most people find intuitively to be the most obvious.",
    "target": "Q: What is And this way? A: And this way is not the way that most people find intuitively to be the most obvious."
  },
  {
    "input": "Thekeythingisthis:allthevaluesofthesamevariablemustgointhesamecolumn.It does not sound like much, but this is what people tend to get wrong.",
    "target": "Q: What is Thekeythingisthis:allthevaluesofthesamevariablemustgointhesamecolumn.It does not sound like much, but this? A: Thekeythingisthis:allthevaluesofthesamevariablemustgointhesamecolumn.It does not sound like much, but this is what people tend to get wrong."
  },
  {
    "input": "Thinkofanameforthedataframe(say,wormsinthiscase).Nowusethegetsarrow <\u0000 which is a composite symbol made up of the two characters < (less than) and \u0000 (minus) like this worms<-read.csv(\"c:\\\\temp\\\\worms.csv\") To see which variables are included in this dataframe, we use the names function: names(worms) [1] \"Field.Name\" \"Area\" \"Slope\" \"Vegetation\" [5] \"Soil.pH\" \"Damp\" \"Worm.density\" Inorderthatwecanrefertothevariablenamesdirectly(withoutprefixingthembythe dataframe name) we attach the dataframe: attach(worms) To see the contents of the dataframe, just type its name: worms Field.Name Area Slope Vegetation Soil.pH Damp Worm.density 1 Nashs.Field 3.6 11 Grassland 4.1 FALSE 4 2 Silwood.Bottom 5.1 2 Arable 5.2 FALSE 7 3 Nursery.Field 2.8 3 Grassland 4.3 FALSE 2 4 Rush.Meadow 2.4 5 Meadow 4.9 TRUE 5 5 Gunness.Thicket 3.8 0 Scrub 4.2 FALSE 6 6 Oak.Mead 3.1 2 Grassland 3.9 FALSE 2 7 Church.Field 3.5 3 Grassland 4.2 FALSE 3 8 Ashurst 2.1 0 Arable 4.8 FALSE 4 9 The.Orchard 1.9 0 Orchard 5.7 FALSE 9 10 Rookery.Slope 1.5 4 Grassland 5.0 TRUE 7 11 Garden.Wood 2.9 10 Scrub 5.2 FALSE 8 12 North.Gravel 3.3 1 Grassland 4.1 FALSE 1 13 South.Gravel 3.7 2 Grassland 4.0 FALSE 2 14 Observatory.Ridge 1.8 6 Grassland 3.8 FALSE 0 15 Pond.Field 4.1 0 Meadow 5.0 TRUE 6 16 Water.Meadow 3.9 0 Meadow 4.9 TRUE 8 26 STATISTICS:ANINTRODUCTIONUSINGR 17 Cheapside 2.2 8 Scrub 4.7 TRUE 4 18 Pound.Hill 4.4 2 Arable 4.5 FALSE 5 19 Gravel.Pit 2.9 1 Grassland 3.5 FALSE 1 20 Farm.Wood 0.8 10 Scrub 5.1 TRUE 3 Thevariablenamesappearinrownumber1.NoticethatRhasexpandedourabbreviated T and F into TRUE and FALSE.",
    "target": "Q: What is Thinkofanameforthedataframe(say,wormsinthiscase).Nowusethegetsarrow <\u0000 which? A: Thinkofanameforthedataframe(say,wormsinthiscase).Nowusethegetsarrow <\u0000 which is a composite symbol made up of the two characters < (less than) and \u0000 (minus) like this worms<-read.csv(\"c:\\\\temp\\\\worms.csv\") To see which variables are included in this dataframe, we use the names function: names(worms) [1] \"Field.Name\" \"Area\" \"Slope\" \"Vegetation\" [5] \"Soil.pH\" \"Damp\" \"Worm.density\" Inorderthatwecanrefertothevariablenamesdirectly(withoutprefixingthembythe dataframe name) we attach the dataframe: attach(worms) To see the contents of the dataframe, just type its name: worms Field.Name Area Slope Vegetation Soil.pH Damp Worm.density 1 Nashs.Field 3.6 11 Grassland 4.1 FALSE 4 2 Silwood.Bottom 5.1 2 Arable 5.2 FALSE 7 3 Nursery.Field 2.8 3 Grassland 4.3 FALSE 2 4 Rush.Meadow 2.4 5 Meadow 4.9 TRUE 5 5 Gunness.Thicket 3.8 0 Scrub 4.2 FALSE 6 6 Oak.Mead 3.1 2 Grassland 3.9 FALSE 2 7 Church.Field 3.5 3 Grassland 4.2 FALSE 3 8 Ashurst 2.1 0 Arable 4.8 FALSE 4 9 The.Orchard 1.9 0 Orchard 5.7 FALSE 9 10 Rookery.Slope 1.5 4 Grassland 5.0 TRUE 7 11 Garden.Wood 2.9 10 Scrub 5.2 FALSE 8 12 North.Gravel 3.3 1 Grassland 4.1 FALSE 1 13 South.Gravel 3.7 2 Grassland 4.0 FALSE 2 14 Observatory.Ridge 1.8 6 Grassland 3.8 FALSE 0 15 Pond.Field 4.1 0 Meadow 5.0 TRUE 6 16 Water.Meadow 3.9 0 Meadow 4.9 TRUE 8 26 STATISTICS:ANINTRODUCTIONUSINGR 17 Cheapside 2.2 8 Scrub 4.7 TRUE 4 18 Pound.Hill 4.4 2 Arable 4.5 FALSE 5 19 Gravel.Pit 2.9 1 Grassland 3.5 FALSE 1 20 Farm.Wood 0.8 10 Scrub 5.1 TRUE 3 Thevariablenamesappearinrownumber1.NoticethatRhasexpandedourabbreviated T and F into TRUE and FALSE."
  },
  {
    "input": "This is a very general procedure in R, accomplished using what are called subscripts.",
    "target": "Q: What is This? A: This is a very general procedure in R, accomplished using what are called subscripts."
  },
  {
    "input": "This is in contrast to arguments to functions in R, which appear in round brackets (4,7).",
    "target": "Q: What is This? A: This is in contrast to arguments to functions in R, which appear in round brackets (4,7)."
  },
  {
    "input": "This syntax is difficult to understand on first acquaintance, but [, blankthencommameansalltherowsand,]commathenblankmeansallthecolumns.",
    "target": "Q: What is This syntax? A: This syntax is difficult to understand on first acquaintance, but [, blankthencommameansalltherowsand,]commathenblankmeansallthecolumns."
  },
  {
    "input": "FirstThingsFirst:GettoKnowYourData Once the data are in the computer, the temptation is to rush straight into statistical analysis.Thisisexactlythewrongthingtodo.Youneedtogettoknowyourdatafirst.",
    "target": "Q: What is FirstThingsFirst:GettoKnowYourData Once the data are in the computer, the temptation? A: FirstThingsFirst:GettoKnowYourData Once the data are in the computer, the temptation is to rush straight into statistical analysis.Thisisexactlythewrongthingtodo.Youneedtogettoknowyourdatafirst."
  },
  {
    "input": "It is useful to know what line of the dataframe contains the unusuallylargevalueofy.Thisiseasytoworkoutusingthewhichfunction.Inspectionof theplotshowsthatourquestionabledatapointistheonlyonelargerthan10,sowecanuse the which function like this: which(y>10) [1]50 sotheoutlyingdatapointisonline50ofthespreadsheet.Nowweneedtogobacktothelab book and try to work out what happened.",
    "target": "Q: What is It? A: It is useful to know what line of the dataframe contains the unusuallylargevalueofy.Thisiseasytoworkoutusingthewhichfunction.Inspectionof theplotshowsthatourquestionabledatapointistheonlyonelargerthan10,sowecanuse the which function like this: which(y>10) [1]50 sotheoutlyingdatapointisonline50ofthespreadsheet.Nowweneedtogobacktothelab book and try to work out what happened."
  },
  {
    "input": "We want the 50th value of y: DATAFRAMES 33 y[50] [1]21.79386 Thelabbookshowsthatthevalueshouldbe2.179386.Whathashappenedisthatatyping errorhasputthedecimalpointinthewrongplace.Clearlyweshouldchangetheentryinthe spreadsheetandstartagain.Nowwhenweuseplottocheckthedata,itlookslikethis plot(y) There is no reason to suspect any of the data points.",
    "target": "Q: What is We want the 50th value of y: DATAFRAMES 33 y[50] [1]21.79386 Thelabbookshowsthatthevalueshouldbe2.179386.Whathashappenedisthatatyping errorhasputthedecimalpointinthewrongplace.Clearlyweshouldchangetheentryinthe spreadsheetandstartagain.Nowwhenweuseplottocheckthedata,itlookslikethis plot(y) There? A: We want the 50th value of y: DATAFRAMES 33 y[50] [1]21.79386 Thelabbookshowsthatthevalueshouldbe2.179386.Whathashappenedisthatatyping errorhasputthedecimalpointinthewrongplace.Clearlyweshouldchangetheentryinthe spreadsheetandstartagain.Nowwhenweuseplottocheckthedata,itlookslikethis plot(y) There is no reason to suspect any of the data points."
  },
  {
    "input": "Here is a variable called treatment from an experiment on fertilizerapplicationandplantgrowth.Therearefourlevelsofthefactor:control,nitrogen, phosphorus and both N and P. This is how we check the data: yields<-read.csv(\"c:\\\\temp\\\\fertyield.csv\") attach(yields) head(yields) 34 STATISTICS:ANINTRODUCTIONUSINGR treatment yield 1 control 0.8274156 2 control 3.6126275 3 control 2.6192581 4 control 1.7412190 5 control 0.6590589 6 control 0.4891107 Asyoucansee,thevariablecalledtreatmentisafactorwithtext(controlonthesesix lines) entries to describe which of the four treatments was applied to obtain the yield in column 2.",
    "target": "Q: What is Here? A: Here is a variable called treatment from an experiment on fertilizerapplicationandplantgrowth.Therearefourlevelsofthefactor:control,nitrogen, phosphorus and both N and P. This is how we check the data: yields<-read.csv(\"c:\\\\temp\\\\fertyield.csv\") attach(yields) head(yields) 34 STATISTICS:ANINTRODUCTIONUSINGR treatment yield 1 control 0.8274156 2 control 3.6126275 3 control 2.6192581 4 control 1.7412190 5 control 0.6590589 6 control 0.4891107 Asyoucansee,thevariablecalledtreatmentisafactorwithtext(controlonthesesix lines) entries to describe which of the four treatments was applied to obtain the yield in column 2."
  },
  {
    "input": "This is how we check the factor levels: table(treatment) variable bothNandP control nitogen nitrogen phosphorus 10 10 1 9 10 There are five factor levels printed rather than the four we expected, so it is clear that something is wrong.",
    "target": "Q: What is This? A: This is how we check the factor levels: table(treatment) variable bothNandP control nitogen nitrogen phosphorus 10 10 1 9 10 There are five factor levels printed rather than the four we expected, so it is clear that something is wrong."
  },
  {
    "input": "What has happened is that one of the nitrogen values has been misspelledasnitogenwiththeresultthatthereareonly9nitrogenvalues(notthe10we expect)andanextracolumninthetablewithoneentryforthespellingmistake.Thenext step is to find out which row the mistake occurs in, then armed with this information, go backtotheoriginalspreadsheetandcorrecttheerror.Thisiseasyusingthewhichfunction.",
    "target": "Q: What is What has happened? A: What has happened is that one of the nitrogen values has been misspelledasnitogenwiththeresultthatthereareonly9nitrogenvalues(notthe10we expect)andanextracolumninthetablewithoneentryforthespellingmistake.Thenext step is to find out which row the mistake occurs in, then armed with this information, go backtotheoriginalspreadsheetandcorrecttheerror.Thisiseasyusingthewhichfunction."
  },
  {
    "input": "Now it is time to look at relationships between variables.",
    "target": "Q: What is Now it? A: Now it is time to look at relationships between variables."
  },
  {
    "input": "detach(yields) Relationships The place to start is with pairwise relationships.",
    "target": "Q: What is detach(yields) Relationships The place to start? A: detach(yields) Relationships The place to start is with pairwise relationships."
  },
  {
    "input": "When both variables are continuous, the appropriate graph is a scatterplot: data<-read.csv(\"c:\\\\temp\\\\scatter.csv\") attach(data) head(data) x y 1 0.000000 0.00000 2 5.112000 61.04000 3 1.320000 11.11130 DATAFRAMES 35 4 35.240000 140.65000 5 1.632931 26.15218 6 2.297635 10.00100 Theresponsevariableisyandtheexplanatoryvariableisx,sowewriteplot(x,y)or plot(yx) to see the relationship (the two forms of plot produce the same graph; the choiceofwhichtouseisentirelyuptoyou).Weintroducetwonewfeaturestocustomizethe plot: changing the plotting symbol (pch stands for plotting character) from the default opencircleswehavebeenusingsofartocoloureddiscswithablackedge(pch=21)and select a bright colour for the fill of the disc (the background as it is called in R, using bg=\"red\"): plot(x,y,pch=21,bg=\"red\") Thisplotimmediatelyalertsustotwoimportantissues:(1)therelationshipbetweenthe response and the explanatory variable is curved, not a straight line; and (2) the degree of scatter of the response variable increases from left to right (this is what non-constant variance(heteroscedasticity)lookslike).Thesetwofeaturesofthedataarenotmistakes,but they are very important elements in model selection (despite the positive correlation between x and y, we would not do a linear regression on these data, for instance).",
    "target": "Q: What is When both variables are continuous, the appropriate graph? A: When both variables are continuous, the appropriate graph is a scatterplot: data<-read.csv(\"c:\\\\temp\\\\scatter.csv\") attach(data) head(data) x y 1 0.000000 0.00000 2 5.112000 61.04000 3 1.320000 11.11130 DATAFRAMES 35 4 35.240000 140.65000 5 1.632931 26.15218 6 2.297635 10.00100 Theresponsevariableisyandtheexplanatoryvariableisx,sowewriteplot(x,y)or plot(yx) to see the relationship (the two forms of plot produce the same graph; the choiceofwhichtouseisentirelyuptoyou).Weintroducetwonewfeaturestocustomizethe plot: changing the plotting symbol (pch stands for plotting character) from the default opencircleswehavebeenusingsofartocoloureddiscswithablackedge(pch=21)and select a bright colour for the fill of the disc (the background as it is called in R, using bg=\"red\"): plot(x,y,pch=21,bg=\"red\") Thisplotimmediatelyalertsustotwoimportantissues:(1)therelationshipbetweenthe response and the explanatory variable is curved, not a straight line; and (2) the degree of scatter of the response variable increases from left to right (this is what non-constant variance(heteroscedasticity)lookslike).Thesetwofeaturesofthedataarenotmistakes,but they are very important elements in model selection (despite the positive correlation between x and y, we would not do a linear regression on these data, for instance)."
  },
  {
    "input": "When the explanatory variable is categorical the plot function produces a box-and- whisker plot.",
    "target": "Q: What is When the explanatory variable? A: When the explanatory variable is categorical the plot function produces a box-and- whisker plot."
  },
  {
    "input": "This is very useful for error-checking, as the following example illustrates: data<-read.csv(\"c:\\\\temp\\\\weather.data.csv\") attach(data) head(data) upper lower rain month yr 1 10.8 6.5 12.2 1 1987 2 10.5 4.5 1.3 1 1987 3 7.5 -1.0 0.1 1 1987 36 STATISTICS:ANINTRODUCTIONUSINGR 4 6.5 -3.3 1.1 1 1987 5 10.0 5.0 3.5 1 1987 6 8.0 3.0 0.1 1 1987 Therearethreepotentialcontinuousresponsevariablesinthisdataframe(dailymaximum temperature upper in degrees Celsius, daily minimum temperature lower in degrees Celsius,anddailytotalprecipitationraininmillimetres)andtwoquantitativevariablesthat we might choose to define as categorical (month and year).",
    "target": "Q: What is This? A: This is very useful for error-checking, as the following example illustrates: data<-read.csv(\"c:\\\\temp\\\\weather.data.csv\") attach(data) head(data) upper lower rain month yr 1 10.8 6.5 12.2 1 1987 2 10.5 4.5 1.3 1 1987 3 7.5 -1.0 0.1 1 1987 36 STATISTICS:ANINTRODUCTIONUSINGR 4 6.5 -3.3 1.1 1 1987 5 10.0 5.0 3.5 1 1987 6 8.0 3.0 0.1 1 1987 Therearethreepotentialcontinuousresponsevariablesinthisdataframe(dailymaximum temperature upper in degrees Celsius, daily minimum temperature lower in degrees Celsius,anddailytotalprecipitationraininmillimetres)andtwoquantitativevariablesthat we might choose to define as categorical (month and year)."
  },
  {
    "input": "(inthe jargon, this is known as a statistical interaction).",
    "target": "Q: What is (inthe jargon, this? A: (inthe jargon, this is known as a statistical interaction)."
  },
  {
    "input": "Herewehaveoneresponsevariable(y)andtwocontinuousexplanatoryvariables(xandz): data<-read.csv(\"c:\\\\temp\\\\coplot.csv\") attach(data) head(data) x y z 1 95.73429 107.8087 14.324408 2 36.20660 223.9257 10.190577 3 28.71378 245.2523 12.566815 4 78.36956 132.7344 13.084384 5 38.37717 222.2966 9.960033 6 57.18078 184.8372 10.035677 Twoscatterplotssidebysidelookbetterifwechangetheshapeoftheplottingwindow from the default square (7  7 inches) to a rectangle (7  4 inches) like this: windows(7,4) thenalterthegraphicsparametertospecifytwosetsofaxisonthesamerow(seep.134for details): par(mfrow=c(1,2)) There is no clear relationship between the response and either of the two continuous explanatory variables when they are fitted on their own: plot(x,y) plot(z,y) To look for interactions between continuous explanatory variables (like x and z in this example) we employ a superb graphical function called coplot.",
    "target": "Q: What is Herewehaveoneresponsevariable(y)andtwocontinuousexplanatoryvariables(xandz): data<-read.csv(\"c:\\\\temp\\\\coplot.csv\") attach(data) head(data) x y z 1 95.73429 107.8087 14.324408 2 36.20660 223.9257 10.190577 3 28.71378 245.2523 12.566815 4 78.36956 132.7344 13.084384 5 38.37717 222.2966 9.960033 6 57.18078 184.8372 10.035677 Twoscatterplotssidebysidelookbetterifwechangetheshapeoftheplottingwindow from the default square (7  7 inches) to a rectangle (7  4 inches) like this: windows(7,4) thenalterthegraphicsparametertospecifytwosetsofaxisonthesamerow(seep.134for details): par(mfrow=c(1,2)) There? A: Herewehaveoneresponsevariable(y)andtwocontinuousexplanatoryvariables(xandz): data<-read.csv(\"c:\\\\temp\\\\coplot.csv\") attach(data) head(data) x y z 1 95.73429 107.8087 14.324408 2 36.20660 223.9257 10.190577 3 28.71378 245.2523 12.566815 4 78.36956 132.7344 13.084384 5 38.37717 222.2966 9.960033 6 57.18078 184.8372 10.035677 Twoscatterplotssidebysidelookbetterifwechangetheshapeoftheplottingwindow from the default square (7  7 inches) to a rectangle (7  4 inches) like this: windows(7,4) thenalterthegraphicsparametertospecifytwosetsofaxisonthesamerow(seep.134for details): par(mfrow=c(1,2)) There is no clear relationship between the response and either of the two continuous explanatory variables when they are fitted on their own: plot(x,y) plot(z,y) To look for interactions between continuous explanatory variables (like x and z in this example) we employ a superb graphical function called coplot."
  },
  {
    "input": "The shingles overlap because that is the defaultsetting(see?coplotfordetailsofoverlap=0.5):foranon-edgeplot,halfofthe datapointsaresharedwiththepaneltoitsleft,andhalfaresharedwiththepanelonitsright.",
    "target": "Q: What is The shingles overlap because that? A: The shingles overlap because that is the defaultsetting(see?coplotfordetailsofoverlap=0.5):foranon-edgeplot,halfofthe datapointsaresharedwiththepaneltoitsleft,andhalfaresharedwiththepanelonitsright."
  },
  {
    "input": "You can specify non-overlapping panels if that is what you want (overlap =0).",
    "target": "Q: What is You can specify non-overlapping panels if that? A: You can specify non-overlapping panels if that is what you want (overlap =0)."
  },
  {
    "input": "InteractionsInvolvingCategoricalVariables The following data are from a factorial experiment involving nitrogen and phosphorus fertilizers applied separately and in combination: data<-read.csv(\"c:\\\\temp\\\\np.csv\") attach(data) head(data) yield nitrogen phosphorus 1 0.8274156 no no 2 3.6126275 no no 3 2.6192581 no no 4 1.7412190 no no 5 0.6590589 no no 6 0.4891107 no no There is one continuous response variable (yield) and two categorical explanatory variables(nitrogenandphosphorus)eachwithtwolevels(yesandno,meaningthefertilizer was or was not applied to the plot in question).",
    "target": "Q: What is InteractionsInvolvingCategoricalVariables The following data are from a factorial experiment involving nitrogen and phosphorus fertilizers applied separately and in combination: data<-read.csv(\"c:\\\\temp\\\\np.csv\") attach(data) head(data) yield nitrogen phosphorus 1 0.8274156 no no 2 3.6126275 no no 3 2.6192581 no no 4 1.7412190 no no 5 0.6590589 no no 6 0.4891107 no no There? A: InteractionsInvolvingCategoricalVariables The following data are from a factorial experiment involving nitrogen and phosphorus fertilizers applied separately and in combination: data<-read.csv(\"c:\\\\temp\\\\np.csv\") attach(data) head(data) yield nitrogen phosphorus 1 0.8274156 no no 2 3.6126275 no no 3 2.6192581 no no 4 1.7412190 no no 5 0.6590589 no no 6 0.4891107 no no There is one continuous response variable (yield) and two categorical explanatory variables(nitrogenandphosphorus)eachwithtwolevels(yesandno,meaningthefertilizer was or was not applied to the plot in question)."
  },
  {
    "input": "That is an example of a statistical interaction: the response to one factor depends upon the level of another factor.",
    "target": "Q: What is That? A: That is an example of a statistical interaction: the response to one factor depends upon the level of another factor."
  },
  {
    "input": "Weneedawayofshowinginteractionsgraphically.Therearemanywayofdoingthis, but perhaps the most visually effective is to use the barplot function.",
    "target": "Q: What is Weneedawayofshowinginteractionsgraphically.Therearemanywayofdoingthis, but perhaps the most visually effective? A: Weneedawayofshowinginteractionsgraphically.Therearemanywayofdoingthis, but perhaps the most visually effective is to use the barplot function."
  },
  {
    "input": "We can use the output from tapply directly for this, but it is a good idea to add a legend to show the nitrogen treatments associated using two colours of shading: barplot(tapply(yield,list(nitrogen,phosphorus),mean), beside=TRUE,xlab=\"phosphorus\") Thelocatorfunctionallowsyoutoputthelegendinaplacewhereitdoesnotinterfere withanyofthebars.Putthecursorwhereyouwantthetopleftcornerofthelegendboxto appear, then left click: legend(locator(1),legend=c(\"no\",\"yes\"),title=\"nitrogen\", fill=c(\"black\",\"lightgrey\")) DATAFRAMES 41 Foryourfinalpresentation,youwouldwanttoadderrorbarstotheplot,butweshalldeal withthislater,oncewehavediscussedhowtomeasuretheunreliabilityofeffects(seep.162).",
    "target": "Q: What is We can use the output from tapply directly for this, but it? A: We can use the output from tapply directly for this, but it is a good idea to add a legend to show the nitrogen treatments associated using two colours of shading: barplot(tapply(yield,list(nitrogen,phosphorus),mean), beside=TRUE,xlab=\"phosphorus\") Thelocatorfunctionallowsyoutoputthelegendinaplacewhereitdoesnotinterfere withanyofthebars.Putthecursorwhereyouwantthetopleftcornerofthelegendboxto appear, then left click: legend(locator(1),legend=c(\"no\",\"yes\"),title=\"nitrogen\", fill=c(\"black\",\"lightgrey\")) DATAFRAMES 41 Foryourfinalpresentation,youwouldwanttoadderrorbarstotheplot,butweshalldeal withthislater,oncewehavediscussedhowtomeasuretheunreliabilityofeffects(seep.162)."
  },
  {
    "input": "3 Central Tendency Despitethefactthateverythingvaries,measurementsoftenclusteraroundcertaininterme- diate values; this attribute is called central tendency.",
    "target": "Q: What is 3 Central Tendency Despitethefactthateverythingvaries,measurementsoftenclusteraroundcertaininterme- diate values; this attribute? A: 3 Central Tendency Despitethefactthateverythingvaries,measurementsoftenclusteraroundcertaininterme- diate values; this attribute is called central tendency."
  },
  {
    "input": "..+ y[n] but that is very long-winded, and it supposes that we know the value of n in advance.",
    "target": "Q: What is ..+ y[n] but that? A: ..+ y[n] but that is very long-winded, and it supposes that we know the value of n in advance."
  },
  {
    "input": "This is likely to vary from application to application.",
    "target": "Q: What is This? A: This is likely to vary from application to application."
  },
  {
    "input": "ybar<-total/n [1]1.103464 There is no need to calculate the intermediate values, total and n, so it would be more efficienttowriteybar<-sum(y)/length(y).Toputthislogicintoageneralfunctionwe need to pick a name for the function, say arithmetic.mean, then define it as follows: arithmetic.mean<-function(x)sum(x)/length(x) Noticetwothings:wedonotassigntheanswersum(x)/length(x)toavariablename likeybar;andthenameofthevectorxusedinsidethefunction(x)maybedifferentfrom thenamesonwhichwemightwanttousethefunctioninfuture(y,forinstance).Ifyoutype the name of a function on its own, you get a listing of the contents: arithmetic.mean function(x)sum(x)/length(x) 44 STATISTICS:ANINTRODUCTIONUSINGR Nowwecantestthefunctiononsomedata.Firstweuseasimpledatasetwhereweknow the answer already, so that we can check that the function works properly: say data<-c(3,4,6,7) where we can see immediately that the arithmetic mean is 5. arithmetic.mean(data) [1]5 So thats OK. Now we can try it on a realistically big data set: arithmetic.mean(y) [1]1.103464 YouwillnotbesurprisedtolearnthatRhasabuilt-infunctionforcalculatingarithmetic meansdirectly,andagain,notsurprisingly,itiscalledmean.Itworksinthesamewayasdid our home-made function: mean(y) [1]1.103464 Arithmeticmeanisnottheonlyquantitativemeasureofcentraltendency,andinfactithas someratherunfortunateproperties.Perhapsthemostseriousfailingofthearithmeticmeanis thatitishighlysensitivetooutliers.Justasingleextremelylargeorextremelysmallvalueinthe datasetwillhaveabigeffectonthevalueofthearithmeticmean.Weshallreturntothisissue later,butournextmeasureofcentraltendencydoesnotsufferfrombeingsensitivetooutliers.",
    "target": "Q: What is ybar<-total/n [1]1.103464 There? A: ybar<-total/n [1]1.103464 There is no need to calculate the intermediate values, total and n, so it would be more efficienttowriteybar<-sum(y)/length(y).Toputthislogicintoageneralfunctionwe need to pick a name for the function, say arithmetic.mean, then define it as follows: arithmetic.mean<-function(x)sum(x)/length(x) Noticetwothings:wedonotassigntheanswersum(x)/length(x)toavariablename likeybar;andthenameofthevectorxusedinsidethefunction(x)maybedifferentfrom thenamesonwhichwemightwanttousethefunctioninfuture(y,forinstance).Ifyoutype the name of a function on its own, you get a listing of the contents: arithmetic.mean function(x)sum(x)/length(x) 44 STATISTICS:ANINTRODUCTIONUSINGR Nowwecantestthefunctiononsomedata.Firstweuseasimpledatasetwhereweknow the answer already, so that we can check that the function works properly: say data<-c(3,4,6,7) where we can see immediately that the arithmetic mean is 5. arithmetic.mean(data) [1]5 So thats OK. Now we can try it on a realistically big data set: arithmetic.mean(y) [1]1.103464 YouwillnotbesurprisedtolearnthatRhasabuilt-infunctionforcalculatingarithmetic meansdirectly,andagain,notsurprisingly,itiscalledmean.Itworksinthesamewayasdid our home-made function: mean(y) [1]1.103464 Arithmeticmeanisnottheonlyquantitativemeasureofcentraltendency,andinfactithas someratherunfortunateproperties.Perhapsthemostseriousfailingofthearithmeticmeanis thatitishighlysensitivetooutliers.Justasingleextremelylargeorextremelysmallvalueinthe datasetwillhaveabigeffectonthevalueofthearithmeticmean.Weshallreturntothisissue later,butournextmeasureofcentraltendencydoesnotsufferfrombeingsensitivetooutliers."
  },
  {
    "input": "Itiscalledthemedian,andisthemiddlevalueinthedataset.Towriteafunctiontoworkout themedian,thefirstthingweneedtodoissortthedataintoascendingorder: sorted<-sort(y) Nowwejustneedtofindthemiddlevalue.Thereisslighthitchhere,becauseifthevector containsanevennumberofnumbers,thenthereisnomiddlevalue.Letusstartwiththeeasy casewherethevectorcontainsanoddnumberofnumbers.Thenumberofnumbersinthe vector is given by length(y)and the middle value is half of this: length(y)/2 [1]19.5 Sothemedianvalueisthe20thvalueinthesorteddataset.Toextractthemedianvalueofy weneedtouse20asasubscript,not19.5,soweshouldconvertthevalueoflength(y)/2 intoaninteger.Weuseceiling(thesmallestintegergreaterthan)forthis: ceiling(length(y)/2) [1]20 CENTRALTENDENCY 45 So now we can extract the median value of y: sorted[20] [1]1.108847 or, more generally: sorted[ceiling(length(y)/2)] [1]1.108847 or even more generally, omitting the intermediate variable called sorted: sort(y)[ceiling(length(y)/2)] [1]1.108847 Butwhataboutthecasewherethevectorcontainsanevennumberofnumbers?Letus manufacture such a vector, by dropping the first element from our vector called y using negative subscripts like this: y.even<-y[-1] length(y.even) [1]38 Thelogicisthatweshallworkoutthearithmeticaverageofthetwovaluesofyoneither side of the middle; in this case, the average of the 19th and 20th sorted values: sort(y.even)[19] [1]1.108847 sort(y.even)[20] [1]1.108853 So in this case, the median would be: (sort(y.even)[19]+sort(y.even)[20])/2 [1]1.10885 But to make it general we need to replace the 19 and 20 by ceiling(length(y.even)/2) and ceiling(1+length(y.even)/2) respectively.",
    "target": "Q: What is Itiscalledthemedian,andisthemiddlevalueinthedataset.Towriteafunctiontoworkout themedian,thefirstthingweneedtodoissortthedataintoascendingorder: sorted<-sort(y) Nowwejustneedtofindthemiddlevalue.Thereisslighthitchhere,becauseifthevector containsanevennumberofnumbers,thenthereisnomiddlevalue.Letusstartwiththeeasy casewherethevectorcontainsanoddnumberofnumbers.Thenumberofnumbersinthe vector? A: Itiscalledthemedian,andisthemiddlevalueinthedataset.Towriteafunctiontoworkout themedian,thefirstthingweneedtodoissortthedataintoascendingorder: sorted<-sort(y) Nowwejustneedtofindthemiddlevalue.Thereisslighthitchhere,becauseifthevector containsanevennumberofnumbers,thenthereisnomiddlevalue.Letusstartwiththeeasy casewherethevectorcontainsanoddnumberofnumbers.Thenumberofnumbersinthe vector is given by length(y)and the middle value is half of this: length(y)/2 [1]19.5 Sothemedianvalueisthe20thvalueinthesorteddataset.Toextractthemedianvalueofy weneedtouse20asasubscript,not19.5,soweshouldconvertthevalueoflength(y)/2 intoaninteger.Weuseceiling(thesmallestintegergreaterthan)forthis: ceiling(length(y)/2) [1]20 CENTRALTENDENCY 45 So now we can extract the median value of y: sorted[20] [1]1.108847 or, more generally: sorted[ceiling(length(y)/2)] [1]1.108847 or even more generally, omitting the intermediate variable called sorted: sort(y)[ceiling(length(y)/2)] [1]1.108847 Butwhataboutthecasewherethevectorcontainsanevennumberofnumbers?Letus manufacture such a vector, by dropping the first element from our vector called y using negative subscripts like this: y.even<-y[-1] length(y.even) [1]38 Thelogicisthatweshallworkoutthearithmeticaverageofthetwovaluesofyoneither side of the middle; in this case, the average of the 19th and 20th sorted values: sort(y.even)[19] [1]1.108847 sort(y.even)[20] [1]1.108853 So in this case, the median would be: (sort(y.even)[19]+sort(y.even)[20])/2 [1]1.10885 But to make it general we need to replace the 19 and 20 by ceiling(length(y.even)/2) and ceiling(1+length(y.even)/2) respectively."
  },
  {
    "input": "Thetrickhereistousemodulo.Thisistheremainder(theamountleftover)whenone integerisdividedbyanother.Anevennumberhasmodulo0whendividedby2,andanodd numberhasmodulo1.ThemodulofunctioninRis%%(twosuccessivepercentsymbols)and itisusedwhereyouwoulduseslash/tocarryoutaregulardivision.Youcanseethisin action with an even number, 38, and odd number, 39: 38%%2 [1]0 39%%2 [1]1 Nowwehaveallthetoolsweneedtowriteageneralfunctiontocalculatemedians.Letus call the function med and define it like this: med<-function(x){ modulo<-length(x)%%2 if(modulo==0) (sort(x)[ceiling(length(x)/2)]+sort(x)[ceiling (1+length(x)/2)])/2 else sort(x)[ceiling(length(x)/2)] } Noticethatwhentheifstatementistrue(i.e.wehaveanevennumberofnumbers)then the expression immediately following the if statement is evaluated (this is the code for calculatingthemedianwithanevennumberofnumbers).",
    "target": "Q: What is Thetrickhereistousemodulo.Thisistheremainder(theamountleftover)whenone integerisdividedbyanother.Anevennumberhasmodulo0whendividedby2,andanodd numberhasmodulo1.ThemodulofunctioninRis%%(twosuccessivepercentsymbols)and itisusedwhereyouwoulduseslash/tocarryoutaregulardivision.Youcanseethisin action with an even number, 38, and odd number, 39: 38%%2 [1]0 39%%2 [1]1 Nowwehaveallthetoolsweneedtowriteageneralfunctiontocalculatemedians.Letus call the function med and define it like this: med<-function(x){ modulo<-length(x)%%2 if(modulo==0) (sort(x)[ceiling(length(x)/2)]+sort(x)[ceiling (1+length(x)/2)])/2 else sort(x)[ceiling(length(x)/2)] } Noticethatwhentheifstatementistrue(i.e.wehaveanevennumberofnumbers)then the expression immediately following the if statement? A: Thetrickhereistousemodulo.Thisistheremainder(theamountleftover)whenone integerisdividedbyanother.Anevennumberhasmodulo0whendividedby2,andanodd numberhasmodulo1.ThemodulofunctioninRis%%(twosuccessivepercentsymbols)and itisusedwhereyouwoulduseslash/tocarryoutaregulardivision.Youcanseethisin action with an even number, 38, and odd number, 39: 38%%2 [1]0 39%%2 [1]1 Nowwehaveallthetoolsweneedtowriteageneralfunctiontocalculatemedians.Letus call the function med and define it like this: med<-function(x){ modulo<-length(x)%%2 if(modulo==0) (sort(x)[ceiling(length(x)/2)]+sort(x)[ceiling (1+length(x)/2)])/2 else sort(x)[ceiling(length(x)/2)] } Noticethatwhentheifstatementistrue(i.e.wehaveanevennumberofnumbers)then the expression immediately following the if statement is evaluated (this is the code for calculatingthemedianwithanevennumberofnumbers)."
  },
  {
    "input": "Whenthe ifstatementisfalse (i.e.wehaveanoddnumberofnumbers,andmodulo==1)thentheexpressionfollowing the else statement is evaluated (this is the code for calculating the median with an odd numberofnumbers).Letustryitout,firstwiththeodd-numberedvectory,thenwiththe even-numbered vector y.even, to check against the values we obtained earlier.",
    "target": "Q: What is Whenthe ifstatementisfalse (i.e.wehaveanoddnumberofnumbers,andmodulo==1)thentheexpressionfollowing the else statement? A: Whenthe ifstatementisfalse (i.e.wehaveanoddnumberofnumbers,andmodulo==1)thentheexpressionfollowing the else statement is evaluated (this is the code for calculating the median with an odd numberofnumbers).Letustryitout,firstwiththeodd-numberedvectory,thenwiththe even-numbered vector y.even, to check against the values we obtained earlier."
  },
  {
    "input": "med(y) [1]1.108847 med(y.even) [1]1.10885 Bothofthesecheckout.Again,youwillnotbesurprisedthatthereisabuilt-infunction for calculating medians, and helpfully it is called median: median(y) [1]1.108847 median(y.even) [1]1.10885 CENTRALTENDENCY 47 For processes that change multiplicatively rather than additively, then neither the arithmetic mean nor the median is an ideal measure of central tendency.",
    "target": "Q: What is med(y) [1]1.108847 med(y.even) [1]1.10885 Bothofthesecheckout.Again,youwillnotbesurprisedthatthereisabuilt-infunction for calculating medians, and helpfully it? A: med(y) [1]1.108847 med(y.even) [1]1.10885 Bothofthesecheckout.Again,youwillnotbesurprisedthatthereisabuilt-infunction for calculating medians, and helpfully it is called median: median(y) [1]1.108847 median(y.even) [1]1.10885 CENTRALTENDENCY 47 For processes that change multiplicatively rather than additively, then neither the arithmetic mean nor the median is an ideal measure of central tendency."
  },
  {
    "input": "Under these conditions,theappropriatemeasureisthegeometricmean.Theformaldefinitionofthisis somewhatabstract:thegeometricmeanisthenthrootoftheproductofthedata.Ifweuse capitalGreekpi()torepresentmultiplication,andyhat(^y)torepresentthegeometric mean, then pffiffiffiffiffiffiffi ^y n y Letustakeasimpleexamplewecanworkoutbyhand:thenumbersofinsectsonfive differentplantswereasfollows:10,1,1000,1,10.Multiplyingthenumberstogethergives 100000.Therearefivenumbers,sowewantthefifthrootofthis.Rootsarehardtodoin yourhead,sowewilluseRasacalculator.Rememberthatrootsarefractionalpowers,so thefifthrootisanumberraisedtothepower1/5=0.2.InR,powersaredenotedbythe^ symbol (the caret), which is found above number 6 on the keyboard: 100000^0.2 [1]10 Sothegeometricmeanoftheseinsectnumbersis10insectsperstem.Notethattwoofthe fivecountswereexactlythisnumber,soitseemsareasonableestimateofcentraltendency.",
    "target": "Q: What is Under these conditions,theappropriatemeasureisthegeometricmean.Theformaldefinitionofthisis somewhatabstract:thegeometricmeanisthenthrootoftheproductofthedata.Ifweuse capitalGreekpi()torepresentmultiplication,andyhat(^y)torepresentthegeometric mean, then pffiffiffiffiffiffiffi ^y n y Letustakeasimpleexamplewecanworkoutbyhand:thenumbersofinsectsonfive differentplantswereasfollows:10,1,1000,1,10.Multiplyingthenumberstogethergives 100000.Therearefivenumbers,sowewantthefifthrootofthis.Rootsarehardtodoin yourhead,sowewilluseRasacalculator.Rememberthatrootsarefractionalpowers,so thefifthrootisanumberraisedtothepower1/5=0.2.InR,powersaredenotedbythe^ symbol (the caret), which? A: Under these conditions,theappropriatemeasureisthegeometricmean.Theformaldefinitionofthisis somewhatabstract:thegeometricmeanisthenthrootoftheproductofthedata.Ifweuse capitalGreekpi()torepresentmultiplication,andyhat(^y)torepresentthegeometric mean, then pffiffiffiffiffiffiffi ^y n y Letustakeasimpleexamplewecanworkoutbyhand:thenumbersofinsectsonfive differentplantswereasfollows:10,1,1000,1,10.Multiplyingthenumberstogethergives 100000.Therearefivenumbers,sowewantthefifthrootofthis.Rootsarehardtodoin yourhead,sowewilluseRasacalculator.Rememberthatrootsarefractionalpowers,so thefifthrootisanumberraisedtothepower1/5=0.2.InR,powersaredenotedbythe^ symbol (the caret), which is found above number 6 on the keyboard: 100000^0.2 [1]10 Sothegeometricmeanoftheseinsectnumbersis10insectsperstem.Notethattwoofthe fivecountswereexactlythisnumber,soitseemsareasonableestimateofcentraltendency."
  },
  {
    "input": "Thearithmeticmean,ontheotherhand,ishopelessforthesedata,becausethelargevalue (1000)issoinfluential:10+1+1000+1+10=1022and1022/5=204.4.Notethatnone of the counts were close to 204.4, so the arithmetic mean is a poor estimate of central tendency in this case.",
    "target": "Q: What is Thearithmeticmean,ontheotherhand,ishopelessforthesedata,becausethelargevalue (1000)issoinfluential:10+1+1000+1+10=1022and1022/5=204.4.Notethatnone of the counts were close to 204.4, so the arithmetic mean? A: Thearithmeticmean,ontheotherhand,ishopelessforthesedata,becausethelargevalue (1000)issoinfluential:10+1+1000+1+10=1022and1022/5=204.4.Notethatnone of the counts were close to 204.4, so the arithmetic mean is a poor estimate of central tendency in this case."
  },
  {
    "input": "Soweshouldbeabletocalculate ageometricmeanbyfindingthe antilog (exp) of the average of the logarithms (log) of the data: exp(mean(log(insects))) [1]10 Writing a general function to compute geometric means is left to you as an exercise.",
    "target": "Q: What is Soweshouldbeabletocalculate ageometricmeanbyfindingthe antilog (exp) of the average of the logarithms (log) of the data: exp(mean(log(insects))) [1]10 Writing a general function to compute geometric means? A: Soweshouldbeabletocalculate ageometricmeanbyfindingthe antilog (exp) of the average of the logarithms (log) of the data: exp(mean(log(insects))) [1]10 Writing a general function to compute geometric means is left to you as an exercise."
  },
  {
    "input": "Now ask yourself Which population is the more variable?",
    "target": "Q: What is Now ask yourself Which population? A: Now ask yourself Which population is the more variable?"
  },
  {
    "input": "Chances are, you will pick the upper line: 48 STATISTICS:ANINTRODUCTIONUSINGR Butnowlookatthescaleontheyaxis.Theupperpopulationisfluctuating100,200,100, 200andsoon.Inotherwords,itisdoublingandhalving,doublingandhalving.Thelower curve is fluctuating 10, 20, 10, 20, 10, 20 and so on.",
    "target": "Q: What is Chances are, you will pick the upper line: 48 STATISTICS:ANINTRODUCTIONUSINGR Butnowlookatthescaleontheyaxis.Theupperpopulationisfluctuating100,200,100, 200andsoon.Inotherwords,itisdoublingandhalving,doublingandhalving.Thelower curve? A: Chances are, you will pick the upper line: 48 STATISTICS:ANINTRODUCTIONUSINGR Butnowlookatthescaleontheyaxis.Theupperpopulationisfluctuating100,200,100, 200andsoon.Inotherwords,itisdoublingandhalving,doublingandhalving.Thelower curve is fluctuating 10, 20, 10, 20, 10, 20 and so on."
  },
  {
    "input": "It, too, is doubling and halving, doublingandhalving.Sotheanswertothequestionis:Theyareequallyvariable.Itisjust thatonepopulationhasahighermeanvaluethantheother(150vs.15inthiscase).Inorder nottofallintothetrapofsayingthattheuppercurveismorevariablethanthelowercurve,it isgoodpracticetographthelogarithmsratherthantherawvaluesofthingslikepopulation sizes that change multiplicatively.",
    "target": "Q: What is It, too,? A: It, too, is doubling and halving, doublingandhalving.Sotheanswertothequestionis:Theyareequallyvariable.Itisjust thatonepopulationhasahighermeanvaluethantheother(150vs.15inthiscase).Inorder nottofallintothetrapofsayingthattheuppercurveismorevariablethanthelowercurve,it isgoodpracticetographthelogarithmsratherthantherawvaluesofthingslikepopulation sizes that change multiplicatively."
  },
  {
    "input": "CENTRALTENDENCY 49 Now it is clear that both populations are equally variable.",
    "target": "Q: What is CENTRALTENDENCY 49 Now it? A: CENTRALTENDENCY 49 Now it is clear that both populations are equally variable."
  },
  {
    "input": "An elephant has a territory which is a square of side 2km.",
    "target": "Q: What is An elephant has a territory which? A: An elephant has a territory which is a square of side 2km."
  },
  {
    "input": "So the average speed is not 2km/hr but 8/5.5=1.4545km/hr.",
    "target": "Q: What is So the average speed? A: So the average speed is not 2km/hr but 8/5.5=1.4545km/hr."
  },
  {
    "input": "The reciprocal of this average is the 1 2 4 1 harmonicmean1=0:68751:4545.Insymbols,therefore,theharmonicmean,~y,(ycurl) is given by 1 n ~y P1  P1 y y n In R, we would write either v<-c(1,2,4,1) length(v)/sum(1/v) [1]1.454545 or 1/mean(1/v) [1]1.454545 FurtherReading Zar,J.H.",
    "target": "Q: What is The reciprocal of this average? A: The reciprocal of this average is the 1 2 4 1 harmonicmean1=0:68751:4545.Insymbols,therefore,theharmonicmean,~y,(ycurl) is given by 1 n ~y P1  P1 y y n In R, we would write either v<-c(1,2,4,1) length(v)/sum(1/v) [1]1.454545 or 1/mean(1/v) [1]1.454545 FurtherReading Zar,J.H."
  },
  {
    "input": "Perhaps the simplest measure is the range of y values: range(y) [1] 515 plot(1:11,y,ylim=c(0,20),pch=16,col=\"blue\") lines(c(4.5,4.5),c(5,15),col=\"brown\") lines(c(4.5,3.5),c(5,5),col=\"brown\",lty=2) lines(c(4.5,5.5),c(15,15),col=\"brown\",lty=2) Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.",
    "target": "Q: What is Perhaps the simplest measure? A: Perhaps the simplest measure is the range of y values: range(y) [1] 515 plot(1:11,y,ylim=c(0,20),pch=16,col=\"blue\") lines(c(4.5,4.5),c(5,15),col=\"brown\") lines(c(4.5,3.5),c(5,5),col=\"brown\",lty=2) lines(c(4.5,5.5),c(15,15),col=\"brown\",lty=2) Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley."
  },
  {
    "input": "In fact, it is easy to prove that this quantity y\u0000y is zero, no matter what the variation in the data, so thats no good (see Box 4.1).",
    "target": "Q: What is In fact, it? A: In fact, it is easy to prove that this quantity y\u0000y is zero, no matter what the variation in the data, so thats no good (see Box 4.1)."
  },
  {
    "input": "This is perhaps the most important single quantity in all of statistics.",
    "target": "Q: What is This? A: This is perhaps the most important single quantity in all of statistics."
  },
  {
    "input": "It is called, somewhat unimaginatively, the sum of squares.",
    "target": "Q: What is It? A: It is called, somewhat unimaginatively, the sum of squares."
  },
  {
    "input": "Thesumofthedifferences y\u0000yiszero Start by writing down the differences explicitly: X X d  y\u0000y P Take through the brackets: X X X d  y\u0000 y P The important point is that y is the same as n:y so X X d  y\u0000ny P and we know that y y=n, so P X X n y d  y\u0000 n The ns cancel, leaving X X X d  y\u0000 y0 (y-mean(y))^2 [1] 11.3140496 6.9504132 21.4958678 5.5867769 0.4049587 28.7685950 [7] 13.2231405 1.8595041 0.4049587 6.9504132 5.5867769 then adding up all these squared differences: sum((y-mean(y))^2) [1]102.5455 Sothesumofsquaresforourdatais102.5455.Butwhatareitsunits?Wellthatdepends ontheunitsinwhichyismeasured.Supposetheyvalueswerelengthsinmm.Sotheunitsof the sum of squares are mm2 (like an area).",
    "target": "Q: What is Thesumofthedifferences y\u0000yiszero Start by writing down the differences explicitly: X X d  y\u0000y P Take through the brackets: X X X d  y\u0000 y P The important point? A: Thesumofthedifferences y\u0000yiszero Start by writing down the differences explicitly: X X d  y\u0000y P Take through the brackets: X X X d  y\u0000 y P The important point is that y is the same as n:y so X X d  y\u0000ny P and we know that y y=n, so P X X n y d  y\u0000 n The ns cancel, leaving X X X d  y\u0000 y0 (y-mean(y))^2 [1] 11.3140496 6.9504132 21.4958678 5.5867769 0.4049587 28.7685950 [7] 13.2231405 1.8595041 0.4049587 6.9504132 5.5867769 then adding up all these squared differences: sum((y-mean(y))^2) [1]102.5455 Sothesumofsquaresforourdatais102.5455.Butwhatareitsunits?Wellthatdepends ontheunitsinwhichyismeasured.Supposetheyvalueswerelengthsinmm.Sotheunitsof the sum of squares are mm2 (like an area)."
  },
  {
    "input": "2 7 4 0 Nowthen.Howmanyvaluescouldthelastnumbertake?Justone.Ithastobeanother7 because the numbers have to add up to 20 because the mean of the five numbers is 4.",
    "target": "Q: What is 2 7 4 0 Nowthen.Howmanyvaluescouldthelastnumbertake?Justone.Ithastobeanother7 because the numbers have to add up to 20 because the mean of the five numbers? A: 2 7 4 0 Nowthen.Howmanyvaluescouldthelastnumbertake?Justone.Ithastobeanother7 because the numbers have to add up to 20 because the mean of the five numbers is 4."
  },
  {
    "input": "Nowwecanformalizeourdefinitionofthemeasurethatweshallusethroughoutthebook forquantifyingvariability.Itiscalledvarianceanditisrepresentedconventionallybys2: sum of squares variance degrees of freedom This is one of the most important definitions in the book, and you should commit it to memory.",
    "target": "Q: What is Nowwecanformalizeourdefinitionofthemeasurethatweshallusethroughoutthebook forquantifyingvariability.Itiscalledvarianceanditisrepresentedconventionallybys2: sum of squares variance degrees of freedom This? A: Nowwecanformalizeourdefinitionofthemeasurethatweshallusethroughoutthebook forquantifyingvariability.Itiscalledvarianceanditisrepresentedconventionallybys2: sum of squares variance degrees of freedom This is one of the most important definitions in the book, and you should commit it to memory."
  },
  {
    "input": "Our quantification of the variation we saw in the first plot is the samplevariance,s2=10.25455.YouwillnotbesurprisedthatRprovidesitsown,built-in functionforcalculatingvariance,andithasanevensimplernamethanthefunctionwejust wrote: var var(y) [1]10.25455 Varianceisusedincountlesswaysinstatisticalanalysis,sothissectionisprobablythe mostimportantinthewholebook,andyoushouldrereadituntilyouaresurethatyouknow exactly what variance is, and precisely what it measures (Box 4.2).",
    "target": "Q: What is Our quantification of the variation we saw in the first plot? A: Our quantification of the variation we saw in the first plot is the samplevariance,s2=10.25455.YouwillnotbesurprisedthatRprovidesitsown,built-in functionforcalculatingvariance,andithasanevensimplernamethanthefunctionwejust wrote: var var(y) [1]10.25455 Varianceisusedincountlesswaysinstatisticalanalysis,sothissectionisprobablythe mostimportantinthewholebook,andyoushouldrereadituntilyouaresurethatyouknow exactly what variance is, and precisely what it measures (Box 4.2)."
  },
  {
    "input": "Shortcutformulaforthesumofsquares y\u0000y2 The main problem with the formula defining variance is that it involves all those subtractions,y\u0000y.Itwouldbegoodtofindasimplerwayofcalculatingthesumof squares.Letusexpandthebracketedterm y\u0000y2toseeifwecanmakeanyprogress towards a subtraction-free solution: y\u0000y2  y\u0000y y\u0000yy2\u00002yyy2 So far, so good.",
    "target": "Q: What is Shortcutformulaforthesumofsquares y\u0000y2 The main problem with the formula defining variance? A: Shortcutformulaforthesumofsquares y\u0000y2 The main problem with the formula defining variance is that it involves all those subtractions,y\u0000y.Itwouldbegoodtofindasimplerwayofcalculatingthesumof squares.Letusexpandthebracketedterm y\u0000y2toseeifwecanmakeanyprogress towards a subtraction-free solution: y\u0000y2  y\u0000y y\u0000yy2\u00002yyy2 So far, so good."
  },
  {
    "input": "Now we apply the summation P (cid:2)P (cid:3) X X X yX y 2 y2\u00002y yny2  y2\u00002 yn n n P NotethatonlytheystakPethesummationsign.Thisisbecausewecanreplace yby ny.Nowreplaceywith y=nontheright-handside,thencancelthensandcollectthe terms: (cid:4)P (cid:5) (cid:4)P (cid:5) (cid:4)P (cid:5) X y 2 y 2 X y 2 y2\u00002 n  y2\u0000 n n2 n This is the shortcut formula for computing the sum of squares.",
    "target": "Q: What is Now we apply the summation P (cid:2)P (cid:3) X X X yX y 2 y2\u00002y yny2  y2\u00002 yn n n P NotethatonlytheystakPethesummationsign.Thisisbecausewecanreplace yby ny.Nowreplaceywith y=nontheright-handside,thencancelthensandcollectthe terms: (cid:4)P (cid:5) (cid:4)P (cid:5) (cid:4)P (cid:5) X y 2 y 2 X y 2 y2\u00002 n  y2\u0000 n n2 n This? A: Now we apply the summation P (cid:2)P (cid:3) X X X yX y 2 y2\u00002y yny2  y2\u00002 yn n n P NotethatonlytheystakPethesummationsign.Thisisbecausewecanreplace yby ny.Nowreplaceywith y=nontheright-handside,thencancelthensandcollectthe terms: (cid:4)P (cid:5) (cid:4)P (cid:5) (cid:4)P (cid:5) X y 2 y 2 X y 2 y2\u00002 n  y2\u0000 n n2 n This is the shortcut formula for computing the sum of squares."
  },
  {
    "input": "ozone<-read.csv(\"c:\\\\temp\\\\gardens.csv\") attach(ozone) ozone gardenA gardenB gardenC 1 3 5 3 2 4 5 3 3 4 6 2 4 3 7 1 5 2 4 10 6 3 4 4 7 1 3 3 8 3 5 11 9 5 6 3 10 2 5 10 56 STATISTICS:ANINTRODUCTIONUSINGR The first step in calculating variance is to work out the mean: mean(gardenA) [1]3 Now we subtract the mean value (3) from each of the data points: gardenA-mean(gardenA) [1] 0 1 1 0 -1 0 -2 0 2 -1 This produces a vector of differences (of length = 10).",
    "target": "Q: What is ozone<-read.csv(\"c:\\\\temp\\\\gardens.csv\") attach(ozone) ozone gardenA gardenB gardenC 1 3 5 3 2 4 5 3 3 4 6 2 4 3 7 1 5 2 4 10 6 3 4 4 7 1 3 3 8 3 5 11 9 5 6 3 10 2 5 10 56 STATISTICS:ANINTRODUCTIONUSINGR The first step in calculating variance? A: ozone<-read.csv(\"c:\\\\temp\\\\gardens.csv\") attach(ozone) ozone gardenA gardenB gardenC 1 3 5 3 2 4 5 3 3 4 6 2 4 3 7 1 5 2 4 10 6 3 4 4 7 1 3 3 8 3 5 11 9 5 6 3 10 2 5 10 56 STATISTICS:ANINTRODUCTIONUSINGR The first step in calculating variance is to work out the mean: mean(gardenA) [1]3 Now we subtract the mean value (3) from each of the data points: gardenA-mean(gardenA) [1] 0 1 1 0 -1 0 -2 0 2 -1 This produces a vector of differences (of length = 10)."
  },
  {
    "input": "We need to square these differences: (gardenA-mean(gardenA))^2 [1] 0 1 1 0 1 0 4 0 4 1 then add up the squared differences: sum((gardenA-mean(gardenA))^2) [1]12 This important quantity is called the sum of squares.",
    "target": "Q: What is We need to square these differences: (gardenA-mean(gardenA))^2 [1] 0 1 1 0 1 0 4 0 4 1 then add up the squared differences: sum((gardenA-mean(gardenA))^2) [1]12 This important quantity? A: We need to square these differences: (gardenA-mean(gardenA))^2 [1] 0 1 1 0 1 0 4 0 4 1 then add up the squared differences: sum((gardenA-mean(gardenA))^2) [1]12 This important quantity is called the sum of squares."
  },
  {
    "input": "Variance is the sum of squares divided by degrees of freedom.",
    "target": "Q: What is Variance? A: Variance is the sum of squares divided by degrees of freedom."
  },
  {
    "input": "sum((gardenA-mean(gardenA))^2)/9 [1]1.333333 So the mean ozone concentration in garden A is 3.0 and the variance in ozone concentration is 1.33.",
    "target": "Q: What is sum((gardenA-mean(gardenA))^2)/9 [1]1.333333 So the mean ozone concentration in garden A? A: sum((gardenA-mean(gardenA))^2)/9 [1]1.333333 So the mean ozone concentration in garden A is 3.0 and the variance in ozone concentration is 1.33."
  },
  {
    "input": "gardenB-mean(gardenB) [1] 0 0 1 2 -1 -1 -2 0 10 (gardenB-mean(gardenB))^2 [1] 0 0 1 4 1 1 4 0 10 sum((gardenB-mean(gardenB))^2) [1]12 VARIANCE 57 sum((gardenB-mean(gardenB))^2)/9 [1]1.333333 This is interesting: so although the mean values are quite different, the variances are exactly the same (both have s2=1.33333).",
    "target": "Q: What is gardenB-mean(gardenB) [1] 0 0 1 2 -1 -1 -2 0 10 (gardenB-mean(gardenB))^2 [1] 0 0 1 4 1 1 4 0 10 sum((gardenB-mean(gardenB))^2) [1]12 VARIANCE 57 sum((gardenB-mean(gardenB))^2)/9 [1]1.333333 This? A: gardenB-mean(gardenB) [1] 0 0 1 2 -1 -1 -2 0 10 (gardenB-mean(gardenB))^2 [1] 0 0 1 4 1 1 4 0 10 sum((gardenB-mean(gardenB))^2) [1]12 VARIANCE 57 sum((gardenB-mean(gardenB))^2)/9 [1]1.333333 This is interesting: so although the mean values are quite different, the variances are exactly the same (both have s2=1.33333)."
  },
  {
    "input": "mean(gardenC) [1]5 Its mean ozone concentration is exactly the same as in garden B. gardenC-mean(gardenC) [1] -2 -2 -3 -4 5 -1 -2 6 -2 5 (gardenC-mean(gardenC))^2 [1] 4 4 9 16 25 1 4 36 4 25 sum((gardenC-mean(gardenC))^2) [1]128 sum((gardenC-mean(gardenC))^2)/9 [1]14.22222 So,althoughthemeansingardensBandCareidentical,thevariancesarequitedifferent (1.33and14.22,respectively).Arethevariancessignificantlydifferent?WedoanFtestfor this, dividing the larger variance by the smaller variance: var(gardenC)/var(gardenB) [1]10.66667 ThenlookuptheprobabilityofgettinganFratioasbigasthisbychancealoneifthetwo variances were really the same.",
    "target": "Q: What is mean(gardenC) [1]5 Its mean ozone concentration? A: mean(gardenC) [1]5 Its mean ozone concentration is exactly the same as in garden B. gardenC-mean(gardenC) [1] -2 -2 -3 -4 5 -1 -2 6 -2 5 (gardenC-mean(gardenC))^2 [1] 4 4 9 16 25 1 4 36 4 25 sum((gardenC-mean(gardenC))^2) [1]128 sum((gardenC-mean(gardenC))^2)/9 [1]14.22222 So,althoughthemeansingardensBandCareidentical,thevariancesarequitedifferent (1.33and14.22,respectively).Arethevariancessignificantlydifferent?WedoanFtestfor this, dividing the larger variance by the smaller variance: var(gardenC)/var(gardenB) [1]10.66667 ThenlookuptheprobabilityofgettinganFratioasbigasthisbychancealoneifthetwo variances were really the same."
  },
  {
    "input": "When we look at the data, we see that this is completely wrong:thereisozonedamage30%ofthetimeingardenCandnoneofthetimeingardenB.",
    "target": "Q: What is When we look at the data, we see that this? A: When we look at the data, we see that this is completely wrong:thereisozonedamage30%ofthetimeingardenCandnoneofthetimeingardenB."
  },
  {
    "input": "VarianceandSampleSize Itisimportanttounderstandtherelationshipbetweenthesizeofasample(thereplication,n) and the value of variance that is estimated.",
    "target": "Q: What is VarianceandSampleSize Itisimportanttounderstandtherelationshipbetweenthesizeofasample(thereplication,n) and the value of variance that? A: VarianceandSampleSize Itisimportanttounderstandtherelationshipbetweenthesizeofasample(thereplication,n) and the value of variance that is estimated."
  },
  {
    "input": "We can do a simple simulation experiment to investigate this: plot(c(0,32),c(0,15),type=\"n\",xlab=\"Samplesize\",ylab=\"Variance\") The plan is to select random numbers from a normal distribution using the function rnorm.Ourdistributionisdefinedashavingameanof10andastandarddeviationof2(this VARIANCE 59 isthesquarerootofthevariance,sos2=4).Weshallworkoutthevarianceforsamplesizes between n=3 and n=31, and plot 30 independent instances of variance at each of the selected sample sizes: for(ninseq(3,31,2)){ for(iin1:30){ x<-rnorm(n,mean=10,sd=2) points(n,var(x))}} You see that as sample size declines, the range of the estimates of sample variance increases dramatically (remember that the population variance is constant at s2=4 throughout).Theproblembecomesseverebelowsamplesof13orso,andisveryserious forsamplesof7orfewer.Evenforreasonablylargesamples(liken=31)thevariancevaries morethanthreefoldinjust30trials(youcanseethattherightmostgroupofpointsvaryfrom about 2 to about 6).",
    "target": "Q: What is We can do a simple simulation experiment to investigate this: plot(c(0,32),c(0,15),type=\"n\",xlab=\"Samplesize\",ylab=\"Variance\") The plan? A: We can do a simple simulation experiment to investigate this: plot(c(0,32),c(0,15),type=\"n\",xlab=\"Samplesize\",ylab=\"Variance\") The plan is to select random numbers from a normal distribution using the function rnorm.Ourdistributionisdefinedashavingameanof10andastandarddeviationof2(this VARIANCE 59 isthesquarerootofthevariance,sos2=4).Weshallworkoutthevarianceforsamplesizes between n=3 and n=31, and plot 30 independent instances of variance at each of the selected sample sizes: for(ninseq(3,31,2)){ for(iin1:30){ x<-rnorm(n,mean=10,sd=2) points(n,var(x))}} You see that as sample size declines, the range of the estimates of sample variance increases dramatically (remember that the population variance is constant at s2=4 throughout).Theproblembecomesseverebelowsamplesof13orso,andisveryserious forsamplesof7orfewer.Evenforreasonablylargesamples(liken=31)thevariancevaries morethanthreefoldinjust30trials(youcanseethattherightmostgroupofpointsvaryfrom about 2 to about 6)."
  },
  {
    "input": "This means that for small samples, the estimated variance is badly behaved, and this has serious consequences for estimation and hypothesis testing.",
    "target": "Q: What is This means that for small samples, the estimated variance? A: This means that for small samples, the estimated variance is badly behaved, and this has serious consequences for estimation and hypothesis testing."
  },
  {
    "input": "UsingVariance Variance is used in two main ways: (cid:129) for establishing measures of unreliability (e.g.",
    "target": "Q: What is UsingVariance Variance? A: UsingVariance Variance is used in two main ways: (cid:129) for establishing measures of unreliability (e.g."
  },
  {
    "input": "It would make good sense to have the dimensions of the unreliability measure the same as the dimensions of the parameter whose unreliability is beingmeasured.Thatiswhyallunreliabilitymeasuresareenclosedinsideabigsquare-root term.Unreliabilitymeasuresarecalledstandarderrors.Whatwehavejustworkedoutisthe standard error of the mean: rffiffiffiffi s2 SE  y n Thisisaveryimportantequationandshouldbememorized.Letuscalculatethestandard errors of each of our market garden means: sqrt(var(gardenA)/10) [1]0.3651484 sqrt(var(gardenB)/10) [1]0.3651484 sqrt(var(gardenC)/10) [1]1.19257 In written work one shows the unreliability of any estimated parameter in a formal, structured way like this: The mean ozone concentration in garden A was 3.00.365 pphm (1 s.e., n=10) You write plus or minus, then the unreliability measure, the units (parts per hundred millioninthiscase)then,inbrackets,tellthereaderwhattheunreliabilitymeasureis(inthis caseonestandarderror)andthesizeofthesampleonwhichtheparameter estimatewas VARIANCE 61 based(inthiscase10).Thismayseemratherstilted,unnecessaryeven.Buttheproblemis thatunlessyoudothis,thereaderwillnotknowwhatkindofunreliabilitymeasureyouhave used.",
    "target": "Q: What is It would make good sense to have the dimensions of the unreliability measure the same as the dimensions of the parameter whose unreliability? A: It would make good sense to have the dimensions of the unreliability measure the same as the dimensions of the parameter whose unreliability is beingmeasured.Thatiswhyallunreliabilitymeasuresareenclosedinsideabigsquare-root term.Unreliabilitymeasuresarecalledstandarderrors.Whatwehavejustworkedoutisthe standard error of the mean: rffiffiffiffi s2 SE  y n Thisisaveryimportantequationandshouldbememorized.Letuscalculatethestandard errors of each of our market garden means: sqrt(var(gardenA)/10) [1]0.3651484 sqrt(var(gardenB)/10) [1]0.3651484 sqrt(var(gardenC)/10) [1]1.19257 In written work one shows the unreliability of any estimated parameter in a formal, structured way like this: The mean ozone concentration in garden A was 3.00.365 pphm (1 s.e., n=10) You write plus or minus, then the unreliability measure, the units (parts per hundred millioninthiscase)then,inbrackets,tellthereaderwhattheunreliabilitymeasureis(inthis caseonestandarderror)andthesizeofthesampleonwhichtheparameter estimatewas VARIANCE 61 based(inthiscase10).Thismayseemratherstilted,unnecessaryeven.Buttheproblemis thatunlessyoudothis,thereaderwillnotknowwhatkindofunreliabilitymeasureyouhave used."
  },
  {
    "input": "How exactly does this  work?Howdoweturntheproportionality( )intheequationaboveintoequality?The answer is by resorting to an appropriate theoretical distribution (as explained below).",
    "target": "Q: What is How exactly does this  work?Howdoweturntheproportionality( )intheequationaboveintoequality?The answer? A: How exactly does this  work?Howdoweturntheproportionality( )intheequationaboveintoequality?The answer is by resorting to an appropriate theoretical distribution (as explained below)."
  },
  {
    "input": "The values of t for 99% are bigger than these (0.005 in each tail): 62 STATISTICS:ANINTRODUCTIONUSINGR qt(.995,9) [1]3.249836 and the value for 99.5% confidence are bigger still (0.0025 in each tail): qt(.9975,9) [1]3.689662 Values of Students t like these appear in the formula for calculating the width of the confidence interval, and their inclusion is the reason why the width of the confidence interval goes up as our degree of confidence is increased.",
    "target": "Q: What is The values of t for 99% are bigger than these (0.005 in each tail): 62 STATISTICS:ANINTRODUCTIONUSINGR qt(.995,9) [1]3.249836 and the value for 99.5% confidence are bigger still (0.0025 in each tail): qt(.9975,9) [1]3.689662 Values of Students t like these appear in the formula for calculating the width of the confidence interval, and their inclusion? A: The values of t for 99% are bigger than these (0.005 in each tail): 62 STATISTICS:ANINTRODUCTIONUSINGR qt(.995,9) [1]3.249836 and the value for 99.5% confidence are bigger still (0.0025 in each tail): qt(.9975,9) [1]3.689662 Values of Students t like these appear in the formula for calculating the width of the confidence interval, and their inclusion is the reason why the width of the confidence interval goes up as our degree of confidence is increased."
  },
  {
    "input": "The other component of the formula, the standard error, is not affected by our choice of confidence level.",
    "target": "Q: What is The other component of the formula, the standard error,? A: The other component of the formula, the standard error, is not affected by our choice of confidence level."
  },
  {
    "input": "All you do is calculate the sample mean lots of times, once for each sampling from your data, then obtain the confidence interval by looking at the extreme highs and lows of the estimated means using a function called quantile to extract the interval you want (e.g.",
    "target": "Q: What is All you do? A: All you do is calculate the sample mean lots of times, once for each sampling from your data, then obtain the confidence interval by looking at the extreme highs and lows of the estimated means using a function called quantile to extract the interval you want (e.g."
  },
  {
    "input": "a 95% interval is specified using c(0.0275, 0.975) to locate the lower and upper bounds).",
    "target": "Q: What is a 95% interval? A: a 95% interval is specified using c(0.0275, 0.975) to locate the lower and upper bounds)."
  },
  {
    "input": "At n=30, the bootstrapped CI based on 1000 simulations was quantile(a,c(0.025,0.975)) 2.5% 97.5% 24.86843 37.6895 (youwillgetslightlydifferentvaluesbecauseoftherandomization).Itisinterestingtosee qffiffiffi qffiffiffiffiffiffiffiffiffiffiffi howthiscompareswiththenormaltheoryconfidenceinterval:1:96 s2 1:96 337:065 n 30 6:5698 implying that a repeat of the sample is likely to have a mean value in the range 24.39885to37.53846.Asyousee,theestimatesfromthebootstrapandnormaltheoryare reassuringly close.",
    "target": "Q: What is At n=30, the bootstrapped CI based on 1000 simulations was quantile(a,c(0.025,0.975)) 2.5% 97.5% 24.86843 37.6895 (youwillgetslightlydifferentvaluesbecauseoftherandomization).Itisinterestingtosee qffiffiffi qffiffiffiffiffiffiffiffiffiffiffi howthiscompareswiththenormaltheoryconfidenceinterval:1:96 s2 1:96 337:065 n 30 6:5698 implying that a repeat of the sample? A: At n=30, the bootstrapped CI based on 1000 simulations was quantile(a,c(0.025,0.975)) 2.5% 97.5% 24.86843 37.6895 (youwillgetslightlydifferentvaluesbecauseoftherandomization).Itisinterestingtosee qffiffiffi qffiffiffiffiffiffiffiffiffiffiffi howthiscompareswiththenormaltheoryconfidenceinterval:1:96 s2 1:96 337:065 n 30 6:5698 implying that a repeat of the sample is likely to have a mean value in the range 24.39885to37.53846.Asyousee,theestimatesfromthebootstrapandnormaltheoryare reassuringly close."
  },
  {
    "input": "I prefer the bootstrapped estimatebecauseitmakesfewerassumptions.If,asinourexample,thedataareskew,then this is reflected in the asymmetry of the confidence intervals above and below the mean (6.7abovethemean,and6.1belowit,atn=30).BothnormalandStudentstassumethat there is no skew, and so their confidence intervals are symmetrical, whatever the data actually show.",
    "target": "Q: What is I prefer the bootstrapped estimatebecauseitmakesfewerassumptions.If,asinourexample,thedataareskew,then this? A: I prefer the bootstrapped estimatebecauseitmakesfewerassumptions.If,asinourexample,thedataareskew,then this is reflected in the asymmetry of the confidence intervals above and below the mean (6.7abovethemean,and6.1belowit,atn=30).BothnormalandStudentstassumethat there is no skew, and so their confidence intervals are symmetrical, whatever the data actually show."
  },
  {
    "input": "This is a picture of what heteroscedasticity looks like.",
    "target": "Q: What is This? A: This is a picture of what heteroscedasticity looks like."
  },
  {
    "input": "The questions we might want to answer are these: (cid:129) what is the mean value?",
    "target": "Q: What is The questions we might want to answer are these: (cid:129) what? A: The questions we might want to answer are these: (cid:129) what is the mean value?"
  },
  {
    "input": "(cid:129) is the mean value significantly different from current expectation or theory?",
    "target": "Q: What is (cid:129)? A: (cid:129) is the mean value significantly different from current expectation or theory?"
  },
  {
    "input": "(cid:129) what is the level of uncertainty associated with our estimate of the mean value?",
    "target": "Q: What is (cid:129) what? A: (cid:129) what is the level of uncertainty associated with our estimate of the mean value?"
  },
  {
    "input": "(cid:129) if data were collected over a period of time, is there evidence for serial correlation?",
    "target": "Q: What is (cid:129) if data were collected over a period of time,? A: (cid:129) if data were collected over a period of time, is there evidence for serial correlation?"
  },
  {
    "input": "It is much better in cases with non- normalityand/oroutlierstouseanon-parametrictechniquesuchasWilcoxonssigned-rank test.",
    "target": "Q: What is It? A: It is much better in cases with non- normalityand/oroutlierstouseanon-parametrictechniquesuchasWilcoxonssigned-rank test."
  },
  {
    "input": "If there is serial correlation in the data, then you need to use time series analysis or mixed effects models.",
    "target": "Q: What is If there? A: If there is serial correlation in the data, then you need to use time series analysis or mixed effects models."
  },
  {
    "input": "DataSummaryintheOne-SampleCase To see what is involved, read the data called y from the file called example.csv: data<-read.csv(\"c:\\\\temp\\\\example.csv\") attach(data) names(data) [1]\"y\" Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.",
    "target": "Q: What is DataSummaryintheOne-SampleCase To see what? A: DataSummaryintheOne-SampleCase To see what is involved, read the data called y from the file called example.csv: data<-read.csv(\"c:\\\\temp\\\\example.csv\") attach(data) names(data) [1]\"y\" Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley."
  },
  {
    "input": "for minimum) and the largest value is 2.984 (labelled Max.",
    "target": "Q: What is for minimum) and the largest value? A: for minimum) and the largest value is 2.984 (labelled Max."
  },
  {
    "input": "There are two measures of central tendency: the median is 2.414 and the arithmeticmeanin2.419.Whatyoumaybeunfamiliarwitharethefigureslabelled1st Qu.",
    "target": "Q: What is There are two measures of central tendency: the median? A: There are two measures of central tendency: the median is 2.414 and the arithmeticmeanin2.419.Whatyoumaybeunfamiliarwitharethefigureslabelled1st Qu."
  },
  {
    "input": "The median is the second quartile by definition (half the data are smaller than themedian).Thethirdquartileisthevalueofthedataabovewhichliethelargest25%of the data (it is sometimes called the 75th percentile, because 75% of the values of y are smaller than this value).",
    "target": "Q: What is The median? A: The median is the second quartile by definition (half the data are smaller than themedian).Thethirdquartileisthevalueofthedataabovewhichliethelargest25%of the data (it is sometimes called the 75th percentile, because 75% of the values of y are smaller than this value)."
  },
  {
    "input": "The graphical equivalent of this summary table is known as a box-and-whisker plot: boxplot(y) Thereisalotofinformationhere.Theboldhorizontalbarinthemiddleoftheboxshows themedianvalueofy.Thetopoftheboxshowsthe75thpercentile,andthebottomofthe boxshowsthe25thpercentile.Theboxasawholeshowswherethemiddle50%ofthedata lie (this is called the interquartile range; we can see that this is between about 2.25 and 2.55).Iftheboxesaboveandbelowthemedianaredifferentsizes,thenthisisindicativeof skewinthedata.Thewhiskersshowthemaximumandminimumvaluesofy(lateronwe shall see what happens when the data contain outliers).",
    "target": "Q: What is The graphical equivalent of this summary table? A: The graphical equivalent of this summary table is known as a box-and-whisker plot: boxplot(y) Thereisalotofinformationhere.Theboldhorizontalbarinthemiddleoftheboxshows themedianvalueofy.Thetopoftheboxshowsthe75thpercentile,andthebottomofthe boxshowsthe25thpercentile.Theboxasawholeshowswherethemiddle50%ofthedata lie (this is called the interquartile range; we can see that this is between about 2.25 and 2.55).Iftheboxesaboveandbelowthemedianaredifferentsizes,thenthisisindicativeof skewinthedata.Thewhiskersshowthemaximumandminimumvaluesofy(lateronwe shall see what happens when the data contain outliers)."
  },
  {
    "input": "68 STATISTICS:ANINTRODUCTIONUSINGR Another sort of plot that we might want to use for a single sample is the histogram: hist(y) Histogramsarefundamentallydifferentfromthegraphsthatwehaveencounteredsofar, becauseinallcasestodatetheresponsevariablehasbeenontheyaxis(theordinate).Witha histogram,theresponsevariableisonthexaxis(theabscissa).Theordinateofahistogram showsthefrequencywithwhichdifferentvaluesoftheresponsewereobserved.Wecansee that rather few values of y were less that 2.0 or greater than 2.8.",
    "target": "Q: What is 68 STATISTICS:ANINTRODUCTIONUSINGR Another sort of plot that we might want to use for a single sample? A: 68 STATISTICS:ANINTRODUCTIONUSINGR Another sort of plot that we might want to use for a single sample is the histogram: hist(y) Histogramsarefundamentallydifferentfromthegraphsthatwehaveencounteredsofar, becauseinallcasestodatetheresponsevariablehasbeenontheyaxis(theordinate).Witha histogram,theresponsevariableisonthexaxis(theabscissa).Theordinateofahistogram showsthefrequencywithwhichdifferentvaluesoftheresponsewereobserved.Wecansee that rather few values of y were less that 2.0 or greater than 2.8."
  },
  {
    "input": "Ourhistogram(above)isclearlynotsymmetricalaboutitsmode(2.5to2.6).Therearesix barsbelowthemodebutonlyfourabovethemode.Datalikethisaresaidtobeskewtothe left because the longer tail is on the left of the distribution.",
    "target": "Q: What is Ourhistogram(above)isclearlynotsymmetricalaboutitsmode(2.5to2.6).Therearesix barsbelowthemodebutonlyfourabovethemode.Datalikethisaresaidtobeskewtothe left because the longer tail? A: Ourhistogram(above)isclearlynotsymmetricalaboutitsmode(2.5to2.6).Therearesix barsbelowthemodebutonlyfourabovethemode.Datalikethisaresaidtobeskewtothe left because the longer tail is on the left of the distribution."
  },
  {
    "input": "Perhapsthemostimportantissueiswhereexactlytodrawthelinesbetween thebars(the binwidthsinthejargon).Forwhole-number(integer)datathisisoftenaneasydecision (wecoulddrawabarofthehistogramforeachoftheintegervaluesofy).Butforcontinuous (real number) data like we have here, that approach is a non-starter.",
    "target": "Q: What is Perhapsthemostimportantissueiswhereexactlytodrawthelinesbetween thebars(the binwidthsinthejargon).Forwhole-number(integer)datathisisoftenaneasydecision (wecoulddrawabarofthehistogramforeachoftheintegervaluesofy).Butforcontinuous (real number) data like we have here, that approach? A: Perhapsthemostimportantissueiswhereexactlytodrawthelinesbetween thebars(the binwidthsinthejargon).Forwhole-number(integer)datathisisoftenaneasydecision (wecoulddrawabarofthehistogramforeachoftheintegervaluesofy).Butforcontinuous (real number) data like we have here, that approach is a non-starter."
  },
  {
    "input": "How many different valuesofydowehaveinourvectorof100numbers?Theappropriatefunctiontoanswer questionslikethisistable:wedonotwanttoseeallthevaluesofy,wejustwanttoknow howmanydifferentvaluesofythereare.Thatistosay,wewanttoknowthelengthofthe table of different y values: length(table(y)) [1]100 Thisshowsusthattherearenorepeatsofanyoftheyvalues,andahistogramofunique values would be completely uninformative (a plot like this is called a rug plot, and has short vertical bars placed at each value of y).",
    "target": "Q: What is How many different valuesofydowehaveinourvectorof100numbers?Theappropriatefunctiontoanswer questionslikethisistable:wedonotwanttoseeallthevaluesofy,wejustwanttoknow howmanydifferentvaluesofythereare.Thatistosay,wewanttoknowthelengthofthe table of different y values: length(table(y)) [1]100 Thisshowsusthattherearenorepeatsofanyoftheyvalues,andahistogramofunique values would be completely uninformative (a plot like this? A: How many different valuesofydowehaveinourvectorof100numbers?Theappropriatefunctiontoanswer questionslikethisistable:wedonotwanttoseeallthevaluesofy,wejustwanttoknow howmanydifferentvaluesofythereare.Thatistosay,wewanttoknowthelengthofthe table of different y values: length(table(y)) [1]100 Thisshowsusthattherearenorepeatsofanyoftheyvalues,andahistogramofunique values would be completely uninformative (a plot like this is called a rug plot, and has short vertical bars placed at each value of y)."
  },
  {
    "input": "plot(range(y),c(0,10),type=\"n\",xlab=\"yvalues\",ylab=\"\") for(iin1:100)lines(c(y[i],y[i]),c(0,1),col=\"blue\") 01 8 6 4 2 0 SINGLESAMPLES 69 2.0 2.2 2.4 2.6 2.8 3.0 y values Let us look more closely to see what R has chosen on our behalf in designing the histogram.Thexaxisislabelledevery0.2units,ineachofwhichtherearetwobars.Sothe chosen bin width is 0.1.",
    "target": "Q: What is plot(range(y),c(0,10),type=\"n\",xlab=\"yvalues\",ylab=\"\") for(iin1:100)lines(c(y[i],y[i]),c(0,1),col=\"blue\") 01 8 6 4 2 0 SINGLESAMPLES 69 2.0 2.2 2.4 2.6 2.8 3.0 y values Let us look more closely to see what R has chosen on our behalf in designing the histogram.Thexaxisislabelledevery0.2units,ineachofwhichtherearetwobars.Sothe chosen bin width? A: plot(range(y),c(0,10),type=\"n\",xlab=\"yvalues\",ylab=\"\") for(iin1:100)lines(c(y[i],y[i]),c(0,1),col=\"blue\") 01 8 6 4 2 0 SINGLESAMPLES 69 2.0 2.2 2.4 2.6 2.8 3.0 y values Let us look more closely to see what R has chosen on our behalf in designing the histogram.Thexaxisislabelledevery0.2units,ineachofwhichtherearetwobars.Sothe chosen bin width is 0.1."
  },
  {
    "input": "Suppose that a is the value of the lower breakandbisthevalueofthehigherbreakforagivenbarofthehistogram.Theconvention aboutwhattodoisindicatedbytheuseofroundbracketsandsquarebrackets:(a,b]or [a,b).Thenumbernexttothesquarebracketisincludedinthebar,whilethenumbernext totheroundbracketisexcludedfromthisbar.Thefirstconvention(a,b]isthedefaultin R,andmeansincludetheright-handendpointb,butnottheleft-handoneainthisbar(in the function definition, this is written as right = TRUE).",
    "target": "Q: What is Suppose that a? A: Suppose that a is the value of the lower breakandbisthevalueofthehigherbreakforagivenbarofthehistogram.Theconvention aboutwhattodoisindicatedbytheuseofroundbracketsandsquarebrackets:(a,b]or [a,b).Thenumbernexttothesquarebracketisincludedinthebar,whilethenumbernext totheroundbracketisexcludedfromthisbar.Thefirstconvention(a,b]isthedefaultin R,andmeansincludetheright-handendpointb,butnottheleft-handoneainthisbar(in the function definition, this is written as right = TRUE)."
  },
  {
    "input": "TheNormalDistribution This famous distribution has a central place is statistical analysis.",
    "target": "Q: What is TheNormalDistribution This famous distribution has a central place? A: TheNormalDistribution This famous distribution has a central place is statistical analysis."
  },
  {
    "input": "You can score 3 by throwing 1 and 2 or 2 and 1 (so the probability of getting 3 is 21/ 36=1/18;thesameasscoring11bygetting5and6or6and5).Themostlikelyscoreis7 becausetherearesomanywaysofgettingthis:1and6,2and5,3and4,4and3,5and2or6 and1).Letussimulate10000playsofthegameandproduceahistogramoftheresults.The possible scores are the 11 numbers from 2 to 12: score<-2:12 The number of ways of getting each score are: ways<-c(1,2,3,4,5,6,5,4,3,2,1) SINGLESAMPLES 71 We can use the rep function to produce a vector of all the 36 possible outcomes: game<-rep(score,ways) game [1] 2 3 3 4 4 4 5 5 5 5 6 6 6 6 6 7 7 7 7 7 7 [22] 8 8 8 8 8 9 9 9 9 10 10 10 11 11 12 Nowwedrawasinglerandomsamplefromthisvectortorepresenttheoutcomeofone throw (this game produced a score of 5): sample(game,1) [1]5 and we record this score ina vector called outcome.",
    "target": "Q: What is You can score 3 by throwing 1 and 2 or 2 and 1 (so the probability of getting 3? A: You can score 3 by throwing 1 and 2 or 2 and 1 (so the probability of getting 3 is 21/ 36=1/18;thesameasscoring11bygetting5and6or6and5).Themostlikelyscoreis7 becausetherearesomanywaysofgettingthis:1and6,2and5,3and4,4and3,5and2or6 and1).Letussimulate10000playsofthegameandproduceahistogramoftheresults.The possible scores are the 11 numbers from 2 to 12: score<-2:12 The number of ways of getting each score are: ways<-c(1,2,3,4,5,6,5,4,3,2,1) SINGLESAMPLES 71 We can use the rep function to produce a vector of all the 36 possible outcomes: game<-rep(score,ways) game [1] 2 3 3 4 4 4 5 5 5 5 6 6 6 6 6 7 7 7 7 7 7 [22] 8 8 8 8 8 9 9 9 9 10 10 10 11 11 12 Nowwedrawasinglerandomsamplefromthisvectortorepresenttheoutcomeofone throw (this game produced a score of 5): sample(game,1) [1]5 and we record this score ina vector called outcome."
  },
  {
    "input": "The game is repeated 10000 times: outcome<-numeric(10000) for(iin1:10000)outcome[i]<-sample(game,1) This is what the distribution of outcomes looks like: hist(outcome,breaks=(1.5:12.5)) Notethetrickofspecifyingthebreakpointstobeoffsetby0.5inordertogetthecorrect labels in the centre of the relevant bars.",
    "target": "Q: What is The game? A: The game is repeated 10000 times: outcome<-numeric(10000) for(iin1:10000)outcome[i]<-sample(game,1) This is what the distribution of outcomes looks like: hist(outcome,breaks=(1.5:12.5)) Notethetrickofspecifyingthebreakpointstobeoffsetby0.5inordertogetthecorrect labels in the centre of the relevant bars."
  },
  {
    "input": "The triangular distribution of scores has become a normal distribution of mean scores, even though we wereaveragingacrossonlythreegames.Todemonstratethegoodnessoffittothenormal distribution, we can overlay the histogram with a smooth probability density function generatedfromanormaldistribution(dnorm)withthesamemeanandstandarddeviationof our actual sample of games: mean(mean.score) [1]6.9821 sd(mean.score) [1]1.366118 To accommodate the top of the smooth density function, we need to make the y axis a littlelonger:ylim=c(0,3000).Togenerateasmoothcurve,weneedaseriesofvaluesfor thexaxisrangingbetween2and12(asaruleofthumb,youneed100orsovaluestomakea smooth-looking curve in R): xv<-seq(2,12,0.1) SINGLESAMPLES 73 Nowcalculatetheheightofthecurve.Thestandardnormalhasanintegralof1.0butour histogram has an integral of 10000 so we calculate the height of the curve like this yv<-10000*dnorm(xv,mean(mean.score),sd(mean.score)) We shall make a few minor embellishments by removing the heading from the plot (main=\"\"), colouring the bars in yellow (col=\"yellow\"): hist(mean.score,breaks=(1.5:12.5),ylim=c(0,3000), col=\"yellow\",main=\"\") and overlaying the normal probability density in red: lines(xv,yv,col=\"red\") As you can see, the fit to the normal distribution is excellent, even though we were averaging across just three throws of the dice.",
    "target": "Q: What is The triangular distribution of scores has become a normal distribution of mean scores, even though we wereaveragingacrossonlythreegames.Todemonstratethegoodnessoffittothenormal distribution, we can overlay the histogram with a smooth probability density function generatedfromanormaldistribution(dnorm)withthesamemeanandstandarddeviationof our actual sample of games: mean(mean.score) [1]6.9821 sd(mean.score) [1]1.366118 To accommodate the top of the smooth density function, we need to make the y axis a littlelonger:ylim=c(0,3000).Togenerateasmoothcurve,weneedaseriesofvaluesfor thexaxisrangingbetween2and12(asaruleofthumb,youneed100orsovaluestomakea smooth-looking curve in R): xv<-seq(2,12,0.1) SINGLESAMPLES 73 Nowcalculatetheheightofthecurve.Thestandardnormalhasanintegralof1.0butour histogram has an integral of 10000 so we calculate the height of the curve like this yv<-10000*dnorm(xv,mean(mean.score),sd(mean.score)) We shall make a few minor embellishments by removing the heading from the plot (main=\"\"), colouring the bars in yellow (col=\"yellow\"): hist(mean.score,breaks=(1.5:12.5),ylim=c(0,3000), col=\"yellow\",main=\"\") and overlaying the normal probability density in red: lines(xv,yv,col=\"red\") As you can see, the fit to the normal distribution? A: The triangular distribution of scores has become a normal distribution of mean scores, even though we wereaveragingacrossonlythreegames.Todemonstratethegoodnessoffittothenormal distribution, we can overlay the histogram with a smooth probability density function generatedfromanormaldistribution(dnorm)withthesamemeanandstandarddeviationof our actual sample of games: mean(mean.score) [1]6.9821 sd(mean.score) [1]1.366118 To accommodate the top of the smooth density function, we need to make the y axis a littlelonger:ylim=c(0,3000).Togenerateasmoothcurve,weneedaseriesofvaluesfor thexaxisrangingbetween2and12(asaruleofthumb,youneed100orsovaluestomakea smooth-looking curve in R): xv<-seq(2,12,0.1) SINGLESAMPLES 73 Nowcalculatetheheightofthecurve.Thestandardnormalhasanintegralof1.0butour histogram has an integral of 10000 so we calculate the height of the curve like this yv<-10000*dnorm(xv,mean(mean.score),sd(mean.score)) We shall make a few minor embellishments by removing the heading from the plot (main=\"\"), colouring the bars in yellow (col=\"yellow\"): hist(mean.score,breaks=(1.5:12.5),ylim=c(0,3000), col=\"yellow\",main=\"\") and overlaying the normal probability density in red: lines(xv,yv,col=\"red\") As you can see, the fit to the normal distribution is excellent, even though we were averaging across just three throws of the dice."
  },
  {
    "input": "What is the area beneath the curve to the left of \u00002?",
    "target": "Q: What is What? A: What is the area beneath the curve to the left of \u00002?"
  },
  {
    "input": "It is obviously a small number, but the curvature makes it hard to estimate the area accurately from the plot.",
    "target": "Q: What is It? A: It is obviously a small number, but the curvature makes it hard to estimate the area accurately from the plot."
  },
  {
    "input": "Because we are dealing with a standard normal (mean=0, sd=1) we need only specify the value of the normal deviate, which is \u00002 in our case: pnorm(-2) [1]0.02275013 SINGLESAMPLES 75 Thistellsusthatjustabitlessthan2.5%ofvalueswillbelowerthan\u00002.Whatabout1 standard deviation below the mean?",
    "target": "Q: What is Because we are dealing with a standard normal (mean=0, sd=1) we need only specify the value of the normal deviate, which? A: Because we are dealing with a standard normal (mean=0, sd=1) we need only specify the value of the normal deviate, which is \u00002 in our case: pnorm(-2) [1]0.02275013 SINGLESAMPLES 75 Thistellsusthatjustabitlessthan2.5%ofvalueswillbelowerthan\u00002.Whatabout1 standard deviation below the mean?"
  },
  {
    "input": "pnorm(-1) [1]0.1586553 In this case, about 16% of random samples will be smaller than 1 standard deviation belowthemean.Whataboutbigvaluesofthenormaldeviate?Thedensityfunctionshowsa maximumof+3.Whatistheprobabilityofgettingasamplefromanormaldistributionthat is more than 3 standard deviations above the mean?",
    "target": "Q: What is pnorm(-1) [1]0.1586553 In this case, about 16% of random samples will be smaller than 1 standard deviation belowthemean.Whataboutbigvaluesofthenormaldeviate?Thedensityfunctionshowsa maximumof+3.Whatistheprobabilityofgettingasamplefromanormaldistributionthat? A: pnorm(-1) [1]0.1586553 In this case, about 16% of random samples will be smaller than 1 standard deviation belowthemean.Whataboutbigvaluesofthenormaldeviate?Thedensityfunctionshowsa maximumof+3.Whatistheprobabilityofgettingasamplefromanormaldistributionthat is more than 3 standard deviations above the mean?"
  },
  {
    "input": "The only point to note here is that pnormgivestheprobabilityofgettingavaluelessthanthevaluespecified(notmore,aswe wanthere).Thetrickissimplytosubtractthevaluegivenbypnormfrom1togettheanswer we want: 1-pnorm(3) [1]0.001349898 This tells us that a value as large as 3 or more is very unlikely indeed: less than 0.2%, in fact.",
    "target": "Q: What is The only point to note here? A: The only point to note here is that pnormgivestheprobabilityofgettingavaluelessthanthevaluespecified(notmore,aswe wanthere).Thetrickissimplytosubtractthevaluegivenbypnormfrom1togettheanswer we want: 1-pnorm(3) [1]0.001349898 This tells us that a value as large as 3 or more is very unlikely indeed: less than 0.2%, in fact."
  },
  {
    "input": "The function we need is called qnorm (quantiles of the normal distribution) and it is used by specifying our two probabilities 0.025 and 0.975 in a vector like this c(0.025,0.975): qnorm(c(0.025,0.975)) [1]-1.959964 1.959964 These are two very important numbers in statistics.",
    "target": "Q: What is The function we need? A: The function we need is called qnorm (quantiles of the normal distribution) and it is used by specifying our two probabilities 0.025 and 0.975 in a vector like this c(0.025,0.975): qnorm(c(0.025,0.975)) [1]-1.959964 1.959964 These are two very important numbers in statistics."
  },
  {
    "input": "Let us shade in these areas under the normal probability density function to see what is involved: 76 STATISTICS:ANINTRODUCTIONUSINGR Inthegreenareabetweenthetwoverticallines,wecanexpect95%ofallrandomsamples tofall;weexpect2.5%ofsamplestobemorethan1.96standarddeviationsbelowthemean (the left-hand red area), and we expect 2.5% of samples to be greater than 1.96 standard deviationsabovethemean(theright-handredarea).Ifwediscoverthatthisisnotthecase, then our sample is not normally distributed.",
    "target": "Q: What is Let us shade in these areas under the normal probability density function to see what? A: Let us shade in these areas under the normal probability density function to see what is involved: 76 STATISTICS:ANINTRODUCTIONUSINGR Inthegreenareabetweenthetwoverticallines,wecanexpect95%ofallrandomsamples tofall;weexpect2.5%ofsamplestobemorethan1.96standarddeviationsbelowthemean (the left-hand red area), and we expect 2.5% of samples to be greater than 1.96 standard deviationsabovethemean(theright-handredarea).Ifwediscoverthatthisisnotthecase, then our sample is not normally distributed."
  },
  {
    "input": "What is the probability that a randomly selected individual will be: (cid:129) shorter than a particular height?",
    "target": "Q: What is What? A: What is the probability that a randomly selected individual will be: (cid:129) shorter than a particular height?"
  },
  {
    "input": "The area under the whole curve is exactly 1; everybody has a height between minus infinityandplusinfinity.True,butnotparticularlyhelpful.Supposewewanttoknowthe probability that one of our people, selected at random from the group, will be less than SINGLESAMPLES 77 160cmtall.Weneedtoconvertthisheightintoavalueofz;thatistosay,weneedtoconvert 160cmintoanumberofstandarddeviationsfromthemean.Whatdoweknowaboutthe standardnormaldistribution?Ithasameanof0andastandarddeviationof1.Sowecan convertanyvaluey,fromadistributionwithmeanyandstandarddeviationstoastandard normal very simply by calculating: y\u0000y z s Soweconvert160cmintoanumberofstandarddeviations.Itislessthanthemeanheight (170cm) so its value will be negative: 160\u0000170 z \u00001:25 8 Nowweneedtofindtheprobabilityofavalueofthestandardnormaltakingavalueof \u00001.25orsmaller.Thisistheareaundertheleft-handtailofthedistribution.Thefunctionwe needforthisispnorm:weprovideitwithavalueofz(or,moregenerally,withaquantile) and it provides us with the probability we want: pnorm(-1.25) [1]0.1056498 So the answer to our first question is just over 10% (the orange shaded area, below).",
    "target": "Q: What is The area under the whole curve? A: The area under the whole curve is exactly 1; everybody has a height between minus infinityandplusinfinity.True,butnotparticularlyhelpful.Supposewewanttoknowthe probability that one of our people, selected at random from the group, will be less than SINGLESAMPLES 77 160cmtall.Weneedtoconvertthisheightintoavalueofz;thatistosay,weneedtoconvert 160cmintoanumberofstandarddeviationsfromthemean.Whatdoweknowaboutthe standardnormaldistribution?Ithasameanof0andastandarddeviationof1.Sowecan convertanyvaluey,fromadistributionwithmeanyandstandarddeviationstoastandard normal very simply by calculating: y\u0000y z s Soweconvert160cmintoanumberofstandarddeviations.Itislessthanthemeanheight (170cm) so its value will be negative: 160\u0000170 z \u00001:25 8 Nowweneedtofindtheprobabilityofavalueofthestandardnormaltakingavalueof \u00001.25orsmaller.Thisistheareaundertheleft-handtailofthedistribution.Thefunctionwe needforthisispnorm:weprovideitwithavalueofz(or,moregenerally,withaquantile) and it provides us with the probability we want: pnorm(-1.25) [1]0.1056498 So the answer to our first question is just over 10% (the orange shaded area, below)."
  },
  {
    "input": "First we convert our value of 185cm into a number of standard deviations: 185\u0000170 z 1:875 8 Then we ask what probability is associated with this, using pnorm: pnorm(1.875) [1]0.9696036 Butthisistheanswertoadifferentquestion.Thisistheprobabilitythatsomeonewillbe lessthan185cmtall(thatiswhatthefunctionpnormhasbeenwrittentoprovide).Allwe need to do is to work out the complement of this: 1-pnorm(1.875) [1]0.03039636 So the answer to the second question is about 3% (the blue shaded area, below).",
    "target": "Q: What is First we convert our value of 185cm into a number of standard deviations: 185\u0000170 z 1:875 8 Then we ask what probability? A: First we convert our value of 185cm into a number of standard deviations: 185\u0000170 z 1:875 8 Then we ask what probability is associated with this, using pnorm: pnorm(1.875) [1]0.9696036 Butthisistheanswertoadifferentquestion.Thisistheprobabilitythatsomeonewillbe lessthan185cmtall(thatiswhatthefunctionpnormhasbeenwrittentoprovide).Allwe need to do is to work out the complement of this: 1-pnorm(1.875) [1]0.03039636 So the answer to the second question is about 3% (the blue shaded area, below)."
  },
  {
    "input": "Thefunctioncalledpolygonisusedforcolouringindifferentshapedareasunderthecurve: toseehowitisused,type?polygon par(mfrow=c(2,2)) ht<-seq(150,190,0.01) pd<-dnorm(ht,170,8) plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") yv<-pd[ht<=160] SINGLESAMPLES 79 xv<-ht[ht<=160] xv<-c(xv,160,150) yv<-c(yv,yv[1],yv[1]) polygon(xv,yv,col=\"orange\") plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") xv<-ht[ht>=185] yv<-pd[ht>=185] xv<-c(xv,190,185) yv<-c(yv,yv[501],yv[501]) polygon(xv,yv,col=\"blue\") plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") xv<-ht[ht>=160&ht<=180] yv<-pd[ht>=160&ht<=180] xv<-c(xv,180,160) yv<-c(yv,pd[1],pd[1]) polygon(xv,yv,col=\"green\") PlotsforTestingNormalityofSingleSamples Thesimplesttestofnormality(andinmanywaysthebest)isthequantilequantileplot;it plotstherankedsamplesfromourdistributionagainstasimilarnumberofrankedquantiles takenfromanormaldistribution.Ifthesampleisnormallydistributedthenthelinewillbe straight.Departuresfromnormalityshowupasvarioussortsofnon-linearity(e.g.S-shapes orbananashapes).Thefunctionsyouneedareqqnormandqqline(quantilequantileplot against a normal distribution): data<-read.csv(\"c:\\\\temp\\\\skewdata.csv\") attach(data) qqnorm(values) qqline(values,lty=2) 80 STATISTICS:ANINTRODUCTIONUSINGR This shows a marked S-shape, indicative of non-normality (as we already know, our distribution is non-normal because it is skew to the left; see p. 68).",
    "target": "Q: What is Thefunctioncalledpolygonisusedforcolouringindifferentshapedareasunderthecurve: toseehowitisused,type?polygon par(mfrow=c(2,2)) ht<-seq(150,190,0.01) pd<-dnorm(ht,170,8) plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") yv<-pd[ht<=160] SINGLESAMPLES 79 xv<-ht[ht<=160] xv<-c(xv,160,150) yv<-c(yv,yv[1],yv[1]) polygon(xv,yv,col=\"orange\") plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") xv<-ht[ht>=185] yv<-pd[ht>=185] xv<-c(xv,190,185) yv<-c(yv,yv[501],yv[501]) polygon(xv,yv,col=\"blue\") plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") xv<-ht[ht>=160&ht<=180] yv<-pd[ht>=160&ht<=180] xv<-c(xv,180,160) yv<-c(yv,pd[1],pd[1]) polygon(xv,yv,col=\"green\") PlotsforTestingNormalityofSingleSamples Thesimplesttestofnormality(andinmanywaysthebest)isthequantilequantileplot;it plotstherankedsamplesfromourdistributionagainstasimilarnumberofrankedquantiles takenfromanormaldistribution.Ifthesampleisnormallydistributedthenthelinewillbe straight.Departuresfromnormalityshowupasvarioussortsofnon-linearity(e.g.S-shapes orbananashapes).Thefunctionsyouneedareqqnormandqqline(quantilequantileplot against a normal distribution): data<-read.csv(\"c:\\\\temp\\\\skewdata.csv\") attach(data) qqnorm(values) qqline(values,lty=2) 80 STATISTICS:ANINTRODUCTIONUSINGR This shows a marked S-shape, indicative of non-normality (as we already know, our distribution? A: Thefunctioncalledpolygonisusedforcolouringindifferentshapedareasunderthecurve: toseehowitisused,type?polygon par(mfrow=c(2,2)) ht<-seq(150,190,0.01) pd<-dnorm(ht,170,8) plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") yv<-pd[ht<=160] SINGLESAMPLES 79 xv<-ht[ht<=160] xv<-c(xv,160,150) yv<-c(yv,yv[1],yv[1]) polygon(xv,yv,col=\"orange\") plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") xv<-ht[ht>=185] yv<-pd[ht>=185] xv<-c(xv,190,185) yv<-c(yv,yv[501],yv[501]) polygon(xv,yv,col=\"blue\") plot(ht,dnorm(ht,170,8),type=\"l\",col=\"brown\", ylab=\"Probabilitydensity\",xlab=\"Height\") xv<-ht[ht>=160&ht<=180] yv<-pd[ht>=160&ht<=180] xv<-c(xv,180,160) yv<-c(yv,pd[1],pd[1]) polygon(xv,yv,col=\"green\") PlotsforTestingNormalityofSingleSamples Thesimplesttestofnormality(andinmanywaysthebest)isthequantilequantileplot;it plotstherankedsamplesfromourdistributionagainstasimilarnumberofrankedquantiles takenfromanormaldistribution.Ifthesampleisnormallydistributedthenthelinewillbe straight.Departuresfromnormalityshowupasvarioussortsofnon-linearity(e.g.S-shapes orbananashapes).Thefunctionsyouneedareqqnormandqqline(quantilequantileplot against a normal distribution): data<-read.csv(\"c:\\\\temp\\\\skewdata.csv\") attach(data) qqnorm(values) qqline(values,lty=2) 80 STATISTICS:ANINTRODUCTIONUSINGR This shows a marked S-shape, indicative of non-normality (as we already know, our distribution is non-normal because it is skew to the left; see p. 68)."
  },
  {
    "input": "The actual speed is 299000kms \u00001 plus the values in our dataframe called light: light<-read.csv(\"c:\\\\temp\\\\light.csv\") attach(light) names(light) [1]\"speed\" hist(speed) We get a summary of the non-parametric descriptors of the sample like this: summary(speed) Min.",
    "target": "Q: What is The actual speed? A: The actual speed is 299000kms \u00001 plus the values in our dataframe called light: light<-read.csv(\"c:\\\\temp\\\\light.csv\") attach(light) names(light) [1]\"speed\" hist(speed) We get a summary of the non-parametric descriptors of the sample like this: summary(speed) Min."
  },
  {
    "input": "650 850 940 909 980 1070 From this, you see at once that the median (940) is substantially bigger than the mean (909),asaconsequenceofthestrongnegativeskewinthedataseeninthehistogram.The interquartilerange,thedifferencebetweenthefirstandthirdquartiles,is980\u0000850=130.",
    "target": "Q: What is 650 850 940 909 980 1070 From this, you see at once that the median (940)? A: 650 850 940 909 980 1070 From this, you see at once that the median (940) is substantially bigger than the mean (909),asaconsequenceofthestrongnegativeskewinthedataseeninthehistogram.The interquartilerange,thedifferencebetweenthefirstandthirdquartiles,is980\u0000850=130."
  },
  {
    "input": "This is useful in the detection of outliers: a good rule of thumb is this an outlier is a value more than 1.5 times the interquartile range above the third quartile, or below the first quartile.",
    "target": "Q: What is This? A: This is useful in the detection of outliers: a good rule of thumb is this an outlier is a value more than 1.5 times the interquartile range above the third quartile, or below the first quartile."
  },
  {
    "input": "(1301.5=195).Inthiscase,therefore,outlierswouldbemeasurementsofspeedthatwere lessthan850\u0000195=655orgreaterthan980+195=1175.Youwillseethatthereareno large outliers in this data set, but one or more small outliers (the minimum is 650).",
    "target": "Q: What is (1301.5=195).Inthiscase,therefore,outlierswouldbemeasurementsofspeedthatwere lessthan850\u0000195=655orgreaterthan980+195=1175.Youwillseethatthereareno large outliers in this data set, but one or more small outliers (the minimum? A: (1301.5=195).Inthiscase,therefore,outlierswouldbemeasurementsofspeedthatwere lessthan850\u0000195=655orgreaterthan980+195=1175.Youwillseethatthereareno large outliers in this data set, but one or more small outliers (the minimum is 650)."
  },
  {
    "input": "The speed of light is significantly less than 990.",
    "target": "Q: What is The speed of light? A: The speed of light is significantly less than 990."
  },
  {
    "input": "Wetake10000randomsampleswithreplacementusingn=100fromthe100valuesof lightandcalculate10000valuesofthemean.Thenweask:whatistheprobabilityofobtaining a mean as large as 990 by inspecting the right-hand tail of the cumulative probability distributionofour10000bootstrappedmeanvalues?Thisisnotashardasitsounds: a<-numeric(10000) for(iin1:10000) a[i]<-mean(sample(speed,replace=T)) hist(a) 82 STATISTICS:ANINTRODUCTIONUSINGR Thetestvalueof990isoffthescaletotheright.Ameanof990isclearlymostunlikely, given the data: max(a) [1]983 Inour10000samplesofthedata,weneverobtainedameanvaluegreaterthan983,sothe probability that the mean is 990 is clearly p<0.0001.",
    "target": "Q: What is Wetake10000randomsampleswithreplacementusingn=100fromthe100valuesof lightandcalculate10000valuesofthemean.Thenweask:whatistheprobabilityofobtaining a mean as large as 990 by inspecting the right-hand tail of the cumulative probability distributionofour10000bootstrappedmeanvalues?Thisisnotashardasitsounds: a<-numeric(10000) for(iin1:10000) a[i]<-mean(sample(speed,replace=T)) hist(a) 82 STATISTICS:ANINTRODUCTIONUSINGR Thetestvalueof990isoffthescaletotheright.Ameanof990isclearlymostunlikely, given the data: max(a) [1]983 Inour10000samplesofthedata,weneverobtainedameanvaluegreaterthan983,sothe probability that the mean? A: Wetake10000randomsampleswithreplacementusingn=100fromthe100valuesof lightandcalculate10000valuesofthemean.Thenweask:whatistheprobabilityofobtaining a mean as large as 990 by inspecting the right-hand tail of the cumulative probability distributionofour10000bootstrappedmeanvalues?Thisisnotashardasitsounds: a<-numeric(10000) for(iin1:10000) a[i]<-mean(sample(speed,replace=T)) hist(a) 82 STATISTICS:ANINTRODUCTIONUSINGR Thetestvalueof990isoffthescaletotheright.Ameanof990isclearlymostunlikely, given the data: max(a) [1]983 Inour10000samplesofthedata,weneverobtainedameanvaluegreaterthan983,sothe probability that the mean is 990 is clearly p<0.0001."
  },
  {
    "input": "StudentstDistribution Students t distribution is used instead of the normal distribution when sample sizes are small (n<30).",
    "target": "Q: What is StudentstDistribution Students t distribution? A: StudentstDistribution Students t distribution is used instead of the normal distribution when sample sizes are small (n<30)."
  },
  {
    "input": "We are going to plot a graph to show how the upper interval (equivalenttothenormals1.96)varieswithsamplesizeinatdistribution.Thisisadeviate, so the appropriate function is qt.",
    "target": "Q: What is We are going to plot a graph to show how the upper interval (equivalenttothenormals1.96)varieswithsamplesizeinatdistribution.Thisisadeviate, so the appropriate function? A: We are going to plot a graph to show how the upper interval (equivalenttothenormals1.96)varieswithsamplesizeinatdistribution.Thisisadeviate, so the appropriate function is qt."
  },
  {
    "input": "We need to supply it with the probability (in this case p=0.975)andthedegreesoffreedom(weshallvarythisfrom1to30toproducethegraph) plot(c(0,30),c(0,10),type=\"n\", xlab=\"Degreesoffreedom\",ylab=\"Studentstvalue\") lines(1:30,qt(0.975,df=1:30),col=\"red\") abline(h=1.96,lty=2,col=\"green\") The importance of using Students t rather than the normal is relatively slight until the degreesoffreedomfallbelowabout10(above whichthecritical value isroughly2),and then it increases dramatically below about 5 degrees of freedom.",
    "target": "Q: What is We need to supply it with the probability (in this case p=0.975)andthedegreesoffreedom(weshallvarythisfrom1to30toproducethegraph) plot(c(0,30),c(0,10),type=\"n\", xlab=\"Degreesoffreedom\",ylab=\"Studentstvalue\") lines(1:30,qt(0.975,df=1:30),col=\"red\") abline(h=1.96,lty=2,col=\"green\") The importance of using Students t rather than the normal? A: We need to supply it with the probability (in this case p=0.975)andthedegreesoffreedom(weshallvarythisfrom1to30toproducethegraph) plot(c(0,30),c(0,10),type=\"n\", xlab=\"Degreesoffreedom\",ylab=\"Studentstvalue\") lines(1:30,qt(0.975,df=1:30),col=\"red\") abline(h=1.96,lty=2,col=\"green\") The importance of using Students t rather than the normal is relatively slight until the degreesoffreedomfallbelowabout10(above whichthecritical value isroughly2),and then it increases dramatically below about 5 degrees of freedom."
  },
  {
    "input": "For samples with more SINGLESAMPLES 83 than30degreesoffreedom,Studentstproducesanasymptoticvalueof1.96,justlikethe normal(thisisthehorizontalgreendottedline).ThegraphdemonstratesthatStudentst=2 is a reasonable rule of thumb; memorizing this will save you lots of time in looking up critical values in later life.",
    "target": "Q: What is For samples with more SINGLESAMPLES 83 than30degreesoffreedom,Studentstproducesanasymptoticvalueof1.96,justlikethe normal(thisisthehorizontalgreendottedline).ThegraphdemonstratesthatStudentst=2? A: For samples with more SINGLESAMPLES 83 than30degreesoffreedom,Studentstproducesanasymptoticvalueof1.96,justlikethe normal(thisisthehorizontalgreendottedline).ThegraphdemonstratesthatStudentst=2 is a reasonable rule of thumb; memorizing this will save you lots of time in looking up critical values in later life."
  },
  {
    "input": "Skew Skew (or skewness) is the dimensionless version of the third moment about the mean P y\u0000y3 m  3 n which is rendered dimensionless by dividing by the cube of the standard deviation of y (because this is also measured in units of y3): (cid:3)pffiffiffiffi(cid:4) 3 s sd y3  s2 3 The skew is then given by m skew  3 1 s 3 Itmeasurestheextenttowhichadistributionhaslong,drawn-outtailsononesideorthe other.Anormaldistributionissymmetricalandhasskew=0.Negativevaluesof mean 1 skewtotheleft(negativeskew)andpositivevaluesmeanskewtotheright.Totestwhether aparticularvalueofskewissignificantlydifferentfrom0(andhencethedistributionfrom whichitwascalculatedissignificantlynon-normal)wedividetheestimateofskewbyits approximate standard error: rffiffiffi 6 SE   1 n ItisstraightforwardtowriteanRfunctiontocalculatethedegreeofskewforanyvector of numbers, x, like this: skew<-function(x){ m3<-sum((x-mean(x))^3)/length(x) s3<-sqrt(var(x))^3 m3/s3 } Notetheuseofthelength(x)functiontoworkoutthesamplesize,n,whateverthesize ofthevectorx.Thelastexpressioninsideafunctionisnotassignedtoavariablename,and is returned as the value of skew(x) when this is executed from the command line.",
    "target": "Q: What is Skew Skew (or skewness)? A: Skew Skew (or skewness) is the dimensionless version of the third moment about the mean P y\u0000y3 m  3 n which is rendered dimensionless by dividing by the cube of the standard deviation of y (because this is also measured in units of y3): (cid:3)pffiffiffiffi(cid:4) 3 s sd y3  s2 3 The skew is then given by m skew  3 1 s 3 Itmeasurestheextenttowhichadistributionhaslong,drawn-outtailsononesideorthe other.Anormaldistributionissymmetricalandhasskew=0.Negativevaluesof mean 1 skewtotheleft(negativeskew)andpositivevaluesmeanskewtotheright.Totestwhether aparticularvalueofskewissignificantlydifferentfrom0(andhencethedistributionfrom whichitwascalculatedissignificantlynon-normal)wedividetheestimateofskewbyits approximate standard error: rffiffiffi 6 SE   1 n ItisstraightforwardtowriteanRfunctiontocalculatethedegreeofskewforanyvector of numbers, x, like this: skew<-function(x){ m3<-sum((x-mean(x))^3)/length(x) s3<-sqrt(var(x))^3 m3/s3 } Notetheuseofthelength(x)functiontoworkoutthesamplesize,n,whateverthesize ofthevectorx.Thelastexpressioninsideafunctionisnotassignedtoavariablename,and is returned as the value of skew(x) when this is executed from the command line."
  },
  {
    "input": "We use the new function skew to quantify the degree of skewness: skew(values) [1]1.318905 Nowweneedtoknowwhetheraskewof1.319issignificantlydiffpereffiffinffiffitffiffiffifffiromzero.Wedo a t test, dividing the observed value of skew by its standard error 6=n: skew(values)/sqrt(6/length(values)) [1]2.949161 Finally,weask:whatistheprobabilityofgettingatvalueof2.949bychancealone,given that we have 28 degrees of freedom, when the skew value really is zero?",
    "target": "Q: What is We use the new function skew to quantify the degree of skewness: skew(values) [1]1.318905 Nowweneedtoknowwhetheraskewof1.319issignificantlydiffpereffiffinffiffitffiffiffifffiromzero.Wedo a t test, dividing the observed value of skew by its standard error 6=n: skew(values)/sqrt(6/length(values)) [1]2.949161 Finally,weask:whatistheprobabilityofgettingatvalueof2.949bychancealone,given that we have 28 degrees of freedom, when the skew value really? A: We use the new function skew to quantify the degree of skewness: skew(values) [1]1.318905 Nowweneedtoknowwhetheraskewof1.319issignificantlydiffpereffiffinffiffitffiffiffifffiromzero.Wedo a t test, dividing the observed value of skew by its standard error 6=n: skew(values)/sqrt(6/length(values)) [1]2.949161 Finally,weask:whatistheprobabilityofgettingatvalueof2.949bychancealone,given that we have 28 degrees of freedom, when the skew value really is zero?"
  },
  {
    "input": "Thenextstepmightbetolookforatransformationthatnormalizesthedatabyreducing theskewness.Onewayofdrawinginthelargervaluesistotakesquareroots,soletustry this to begin with: skew(sqrt(values))/sqrt(6/length(values)) [1]1.474851 86 STATISTICS:ANINTRODUCTIONUSINGR This is not significantly skew.",
    "target": "Q: What is Thenextstepmightbetolookforatransformationthatnormalizesthedatabyreducing theskewness.Onewayofdrawinginthelargervaluesistotakesquareroots,soletustry this to begin with: skew(sqrt(values))/sqrt(6/length(values)) [1]1.474851 86 STATISTICS:ANINTRODUCTIONUSINGR This? A: Thenextstepmightbetolookforatransformationthatnormalizesthedatabyreducing theskewness.Onewayofdrawinginthelargervaluesistotakesquareroots,soletustry this to begin with: skew(sqrt(values))/sqrt(6/length(values)) [1]1.474851 86 STATISTICS:ANINTRODUCTIONUSINGR This is not significantly skew."
  },
  {
    "input": "Kurtosis Thisisameasureofnon-normalitythathastodowiththepeakyness,orflat-toppedness,ofa distribution.Thenormaldistributionisbell-shaped,whereasakurtoticdistributionisother thanbell-shaped.Inparticular,amoreflat-toppeddistributionissaidtobeplatykurtic,anda morepointydistributionissaidtobeleptokurtic.Kurtosisisthedimensionlessversionof the fourth moment about the mean P y\u0000y4 m  4 n whichisrendereddimensionlessbydividingbythesquareofthevarianceofy(becausethis is also measured in units of y4): s var y2  s22 4 Kurtosis is then given by m kurtosis  4\u00003 2 s 4 The minus 3 is included because a normal distribution has m /s =3.",
    "target": "Q: What is Kurtosis Thisisameasureofnon-normalitythathastodowiththepeakyness,orflat-toppedness,ofa distribution.Thenormaldistributionisbell-shaped,whereasakurtoticdistributionisother thanbell-shaped.Inparticular,amoreflat-toppeddistributionissaidtobeplatykurtic,anda morepointydistributionissaidtobeleptokurtic.Kurtosisisthedimensionlessversionof the fourth moment about the mean P y\u0000y4 m  4 n whichisrendereddimensionlessbydividingbythesquareofthevarianceofy(becausethis? A: Kurtosis Thisisameasureofnon-normalitythathastodowiththepeakyness,orflat-toppedness,ofa distribution.Thenormaldistributionisbell-shaped,whereasakurtoticdistributionisother thanbell-shaped.Inparticular,amoreflat-toppeddistributionissaidtobeplatykurtic,anda morepointydistributionissaidtobeleptokurtic.Kurtosisisthedimensionlessversionof the fourth moment about the mean P y\u0000y4 m  4 n whichisrendereddimensionlessbydividingbythesquareofthevarianceofy(becausethis is also measured in units of y4): s var y2  s22 4 Kurtosis is then given by m kurtosis  4\u00003 2 s 4 The minus 3 is included because a normal distribution has m /s =3."
  },
  {
    "input": "The approximate standard error of kurtosis is rffiffiffiffiffi 24 SE   2 n An R function to calculate kurtosis might look like this: kurtosis<-function(x){ m4<-sum((x-mean(x))^4)/length(x) s4<-var(x)^2 m4/s4-3 } For our present data, we find that kurtosis is not significantly different from normal: kurtosis(values) SINGLESAMPLES 87 [1]1.297751 kurtosis(values)/sqrt(24/length(values)) [1]1.450930 because the t value (1.45) is substantially less than the rule of thumb (2.0).",
    "target": "Q: What is The approximate standard error of kurtosis? A: The approximate standard error of kurtosis is rffiffiffiffiffi 24 SE   2 n An R function to calculate kurtosis might look like this: kurtosis<-function(x){ m4<-sum((x-mean(x))^4)/length(x) s4<-var(x)^2 m4/s4-3 } For our present data, we find that kurtosis is not significantly different from normal: kurtosis(values) SINGLESAMPLES 87 [1]1.297751 kurtosis(values)/sqrt(24/length(values)) [1]1.450930 because the t value (1.45) is substantially less than the rule of thumb (2.0)."
  },
  {
    "input": "6 Two Samples There is absolutely no point in carrying out an analysis that is more complicated than it needs tobe.Occamsrazor applies tothechoice ofstatistical model justasstrongly asto anything else: simplest is best.",
    "target": "Q: What is 6 Two Samples There? A: 6 Two Samples There is absolutely no point in carrying out an analysis that is more complicated than it needs tobe.Occamsrazor applies tothechoice ofstatistical model justasstrongly asto anything else: simplest is best."
  },
  {
    "input": "The answer, as always, is to look up the critical value of the varianceratio.Inthiscase,wewantcriticalvaluesofFishersF.TheRfunctionforthisisqf whichstandsforquantilesoftheFdistribution.Forourexampleofozonelevelsinmarket gardens (see Chapter 4) there were 10 replicates in each garden, so there were 10\u00001=9 degreesoffreedomforeachgarden.Incomparingtwogardens,therefore,wehave9d.f.in the numerator and 9 d.f.",
    "target": "Q: What is The answer, as always,? A: The answer, as always, is to look up the critical value of the varianceratio.Inthiscase,wewantcriticalvaluesofFishersF.TheRfunctionforthisisqf whichstandsforquantilesoftheFdistribution.Forourexampleofozonelevelsinmarket gardens (see Chapter 4) there were 10 replicates in each garden, so there were 10\u00001=9 degreesoffreedomforeachgarden.Incomparingtwogardens,therefore,wehave9d.f.in the numerator and 9 d.f."
  },
  {
    "input": "TWOSAMPLES 89 typicallyone-tailed(thetreatmentvarianceisexpectedtobelargerthantheerrorvarianceif themeansaresignificantlydifferent;seep.153),inthiscase,wehadnoexpectationasto which garden was likely to have the higher variance, so we carry out a two-tailed test (p1\u0000=2).Supposeweworkatthetraditional0:05,thenwefindthecriticalvalue of F like this: qf(0.975,9,9) 4.025994 Thismeansthatacalculatedvarianceratiowillneedtobegreaterthanorequalto4.026in orderforustoconcludethatthetwovariancesaresignificantlydifferentat0:05.Tosee thetestinaction,wecancomparethevariancesinozoneconcentrationformarketgardensB and C. f.test.data<-read.csv(\"c:\\\\temp\\\\f.test.data.csv\") attach(f.test.data) names(f.test.data) [1]\"gardenB\"\"gardenC\" First, we compute the two variances: var(gardenB) [1]1.333333 var(gardenC) [1]14.22222 The larger variance is clearly in garden C, so we compute the F ratio like this: F.ratio<-var(gardenC)/var(gardenB) F.ratio [1]10.66667 TheteststatisticshowsusthatthevarianceingardenCismorethan10timesasbigasthe varianceingardenB.ThecriticalvalueofFforthistest(with9d.f.inboththenumerator and the denominator) is 4.026 (see qf, above), so we conclude: since the test statistic is larger than the critical value, we reject the null hypothesis.",
    "target": "Q: What is TWOSAMPLES 89 typicallyone-tailed(thetreatmentvarianceisexpectedtobelargerthantheerrorvarianceif themeansaresignificantlydifferent;seep.153),inthiscase,wehadnoexpectationasto which garden was likely to have the higher variance, so we carry out a two-tailed test (p1\u0000=2).Supposeweworkatthetraditional0:05,thenwefindthecriticalvalue of F like this: qf(0.975,9,9) 4.025994 Thismeansthatacalculatedvarianceratiowillneedtobegreaterthanorequalto4.026in orderforustoconcludethatthetwovariancesaresignificantlydifferentat0:05.Tosee thetestinaction,wecancomparethevariancesinozoneconcentrationformarketgardensB and C. f.test.data<-read.csv(\"c:\\\\temp\\\\f.test.data.csv\") attach(f.test.data) names(f.test.data) [1]\"gardenB\"\"gardenC\" First, we compute the two variances: var(gardenB) [1]1.333333 var(gardenC) [1]14.22222 The larger variance? A: TWOSAMPLES 89 typicallyone-tailed(thetreatmentvarianceisexpectedtobelargerthantheerrorvarianceif themeansaresignificantlydifferent;seep.153),inthiscase,wehadnoexpectationasto which garden was likely to have the higher variance, so we carry out a two-tailed test (p1\u0000=2).Supposeweworkatthetraditional0:05,thenwefindthecriticalvalue of F like this: qf(0.975,9,9) 4.025994 Thismeansthatacalculatedvarianceratiowillneedtobegreaterthanorequalto4.026in orderforustoconcludethatthetwovariancesaresignificantlydifferentat0:05.Tosee thetestinaction,wecancomparethevariancesinozoneconcentrationformarketgardensB and C. f.test.data<-read.csv(\"c:\\\\temp\\\\f.test.data.csv\") attach(f.test.data) names(f.test.data) [1]\"gardenB\"\"gardenC\" First, we compute the two variances: var(gardenB) [1]1.333333 var(gardenC) [1]14.22222 The larger variance is clearly in garden C, so we compute the F ratio like this: F.ratio<-var(gardenC)/var(gardenB) F.ratio [1]10.66667 TheteststatisticshowsusthatthevarianceingardenCismorethan10timesasbigasthe varianceingardenB.ThecriticalvalueofFforthistest(with9d.f.inboththenumerator and the denominator) is 4.026 (see qf, above), so we conclude: since the test statistic is larger than the critical value, we reject the null hypothesis."
  },
  {
    "input": "The null hypothesis was that the two variances were not significantly different, so we acceptthealternativehypothesisthatthetwovariancesaresignificantlydifferent.Infact,it isbetterpracticetopresentthepvalueassociatedwiththecalculatedFratioratherthanjust torejectthenullhypothesis.Todothisweusepfratherthanqf.Wedoubletheresulting probability to allow for the two-tailed nature of the test: 2*(1-pf(F.ratio,9,9)) [1]0.001624199 90 STATISTICS:ANINTRODUCTIONUSINGR sotheprobabilityofobtaininganFratioaslargeasthisorlarger,ifthevarianceswerethe same(asassumedbythenullhypothesis),islessthan0.002.Itisimportanttonotethatthep value is not the probability that the null hypothesis is true (this is a common mistake amongstbeginners).Thenullhypothesisisassumedtobetrueincarryingoutthetest.Keep rereadingthisparagraphuntilyouaresurethatyouunderstandthisdistinctionbetweenwhat p values are and what they are not.",
    "target": "Q: What is The null hypothesis was that the two variances were not significantly different, so we acceptthealternativehypothesisthatthetwovariancesaresignificantlydifferent.Infact,it isbetterpracticetopresentthepvalueassociatedwiththecalculatedFratioratherthanjust torejectthenullhypothesis.Todothisweusepfratherthanqf.Wedoubletheresulting probability to allow for the two-tailed nature of the test: 2*(1-pf(F.ratio,9,9)) [1]0.001624199 90 STATISTICS:ANINTRODUCTIONUSINGR sotheprobabilityofobtaininganFratioaslargeasthisorlarger,ifthevarianceswerethe same(asassumedbythenullhypothesis),islessthan0.002.Itisimportanttonotethatthep value? A: The null hypothesis was that the two variances were not significantly different, so we acceptthealternativehypothesisthatthetwovariancesaresignificantlydifferent.Infact,it isbetterpracticetopresentthepvalueassociatedwiththecalculatedFratioratherthanjust torejectthenullhypothesis.Todothisweusepfratherthanqf.Wedoubletheresulting probability to allow for the two-tailed nature of the test: 2*(1-pf(F.ratio,9,9)) [1]0.001624199 90 STATISTICS:ANINTRODUCTIONUSINGR sotheprobabilityofobtaininganFratioaslargeasthisorlarger,ifthevarianceswerethe same(asassumedbythenullhypothesis),islessthan0.002.Itisimportanttonotethatthep value is not the probability that the null hypothesis is true (this is a common mistake amongstbeginners).Thenullhypothesisisassumedtobetrueincarryingoutthetest.Keep rereadingthisparagraphuntilyouaresurethatyouunderstandthisdistinctionbetweenwhat p values are and what they are not."
  },
  {
    "input": "There is a built-in function called var.test for speeding up the procedure.",
    "target": "Q: What is There? A: There is a built-in function called var.test for speeding up the procedure."
  },
  {
    "input": "detach(f.test.data) ComparingTwoMeans Givenwhatweknowaboutthevariationfromreplicatetoreplicatewithineachsample(the within-sample variance), how likely is it that our two sample means were drawn from populationswiththesameaverage?Ifthisishighlylikely,then weshallsaythatourtwo samplemeansarenotsignificantlydifferent.Ifitisratherunlikely,thenweshallsaythatour sample means are significantly different.",
    "target": "Q: What is detach(f.test.data) ComparingTwoMeans Givenwhatweknowaboutthevariationfromreplicatetoreplicatewithineachsample(the within-sample variance), how likely? A: detach(f.test.data) ComparingTwoMeans Givenwhatweknowaboutthevariationfromreplicatetoreplicatewithineachsample(the within-sample variance), how likely is it that our two sample means were drawn from populationswiththesameaverage?Ifthisishighlylikely,then weshallsaythatourtwo samplemeansarenotsignificantlydifferent.Ifitisratherunlikely,thenweshallsaythatour sample means are significantly different."
  },
  {
    "input": "As with all of the classical tests, we do this by calculatinga test statistic and then asking thequestion: howlikely are we toobtaina test statistic this big (or bigger) if the null hypothesis is true?",
    "target": "Q: What is As with all of the classical tests, we do this by calculatinga test statistic and then asking thequestion: howlikely are we toobtaina test statistic this big (or bigger) if the null hypothesis? A: As with all of the classical tests, we do this by calculatinga test statistic and then asking thequestion: howlikely are we toobtaina test statistic this big (or bigger) if the null hypothesis is true?"
  },
  {
    "input": "The critical value is calculated on the assumption that the null hypothesis is true.",
    "target": "Q: What is The critical value? A: The critical value is calculated on the assumption that the null hypothesis is true."
  },
  {
    "input": "It is a useful feature of R that it has built-in statisticaltablesforalloftheimportantprobabilitydistributions,sothatifweprovideRwith the relevant degrees of freedom, it can tell us the critical value for any particular case.",
    "target": "Q: What is It? A: It is a useful feature of R that it has built-in statisticaltablesforalloftheimportantprobabilitydistributions,sothatifweprovideRwith the relevant degrees of freedom, it can tell us the critical value for any particular case."
  },
  {
    "input": "The test statistic is the number of standard errors by which the two sample means are separated: difference between the two means y \u0000y t   A B SE of the difference SE diff Wealreadyknowthestandarderrorofthemean(seep.60)butwehavenotyetmetthe standard error of the difference between two means.",
    "target": "Q: What is The test statistic? A: The test statistic is the number of standard errors by which the two sample means are separated: difference between the two means y \u0000y t   A B SE of the difference SE diff Wealreadyknowthestandarderrorofthemean(seep.60)butwehavenotyetmetthe standard error of the difference between two means."
  },
  {
    "input": "non- correlated) variables, the variance of a difference is the sum of the separate variances (see Box 6.1).",
    "target": "Q: What is non- correlated) variables, the variance of a difference? A: non- correlated) variables, the variance of a difference is the sum of the separate variances (see Box 6.1)."
  },
  {
    "input": "minus 2 times the covariance of A A B B samplesAandB(seeBox6.2).Butbecause(byassumption)thesamplesfromAand BPare independently drawn they are uncorrelated, the covariance is zero, so 2 y \u0000  y \u0000 0.",
    "target": "Q: What is minus 2 times the covariance of A A B B samplesAandB(seeBox6.2).Butbecause(byassumption)thesamplesfromAand BPare independently drawn they are uncorrelated, the covariance? A: minus 2 times the covariance of A A B B samplesAandB(seeBox6.2).Butbecause(byassumption)thesamplesfromAand BPare independently drawn they are uncorrelated, the covariance is zero, so 2 y \u0000  y \u0000 0."
  },
  {
    "input": "This important result needs to be stated separately: A A B B 2 2 2 y \u0000y A B A B If two samples are independent, the variance of the difference is the sum of the two samplevariances.Thisisnottrue,ofcourse,ifthesamplesarepositivelyornegatively correlated (see p. 108).",
    "target": "Q: What is This important result needs to be stated separately: A A B B 2 2 2 y \u0000y A B A B If two samples are independent, the variance of the difference? A: This important result needs to be stated separately: A A B B 2 2 2 y \u0000y A B A B If two samples are independent, the variance of the difference is the sum of the two samplevariances.Thisisnottrue,ofcourse,ifthesamplesarepositivelyornegatively correlated (see p. 108)."
  },
  {
    "input": "This important result allows us to write down the formula for the standard error of the difference between two sample means: sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi s2 s2 SE  A  B diff n n A B AtthisstagewehaveeverythingweneedtocarryoutStudentsttest.Ournullhypothesis is that the two sample means are the same, and we shall accept this unless the value of Students t is sufficiently large that it is unlikely that such a difference arose by chance alone.",
    "target": "Q: What is This important result allows us to write down the formula for the standard error of the difference between two sample means: sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi s2 s2 SE  A  B diff n n A B AtthisstagewehaveeverythingweneedtocarryoutStudentsttest.Ournullhypothesis? A: This important result allows us to write down the formula for the standard error of the difference between two sample means: sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi s2 s2 SE  A  B diff n n A B AtthisstagewehaveeverythingweneedtocarryoutStudentsttest.Ournullhypothesis is that the two sample means are the same, and we shall accept this unless the value of Students t is sufficiently large that it is unlikely that such a difference arose by chance alone."
  },
  {
    "input": "Anotherwayofthinkingofthisistoreasonthatthecompletesamplesizeas20,andwehave estimatedtwoparametersfromthedata,y andy ,sowehave20\u00002=18d.f.Wetypically A B use5%asthechanceofrejectingthenullhypothesiswhenitistrue(thisistheTypeIerror rate).Becausewedidnotknowinadvancewhichofthetwogardenswasgoingtohavethe higher mean ozone concentration (and we usually do not), this is a two-tailed test, so the critical value of Students t is: qt(0.975,18) [1]2.100922 TWOSAMPLES 93 This means that our test statistic needs to be bigger than 2.1 in order to reject the null hypothesis,andhencetoconcludethatthetwomeansaresignificantlydifferentat0:05.",
    "target": "Q: What is Anotherwayofthinkingofthisistoreasonthatthecompletesamplesizeas20,andwehave estimatedtwoparametersfromthedata,y andy ,sowehave20\u00002=18d.f.Wetypically A B use5%asthechanceofrejectingthenullhypothesiswhenitistrue(thisistheTypeIerror rate).Becausewedidnotknowinadvancewhichofthetwogardenswasgoingtohavethe higher mean ozone concentration (and we usually do not), this? A: Anotherwayofthinkingofthisistoreasonthatthecompletesamplesizeas20,andwehave estimatedtwoparametersfromthedata,y andy ,sowehave20\u00002=18d.f.Wetypically A B use5%asthechanceofrejectingthenullhypothesiswhenitistrue(thisistheTypeIerror rate).Becausewedidnotknowinadvancewhichofthetwogardenswasgoingtohavethe higher mean ozone concentration (and we usually do not), this is a two-tailed test, so the critical value of Students t is: qt(0.975,18) [1]2.100922 TWOSAMPLES 93 This means that our test statistic needs to be bigger than 2.1 in order to reject the null hypothesis,andhencetoconcludethatthetwomeansaresignificantlydifferentat0:05."
  },
  {
    "input": "The data frame is attached like this: t.test.data<-read.csv(\"c:\\\\temp\\\\t.test.data.csv\") attach(t.test.data) names(t.test.data) [1]\"gardenA\"\"gardenB\" A useful graphical test for two samples employs the notches option of boxplot: ozone<-c(gardenA,gardenB) label<-factor(c(rep(\"A\",10),rep(\"B\",10))) boxplot(ozonelabel,notch=T,xlab=\"Garden\", ylab=\"Ozonepphm\",col=\"lightblue\") Because the notches of two plots do not overlap, we conclude that the medians are significantly different at the 5% level.",
    "target": "Q: What is The data frame? A: The data frame is attached like this: t.test.data<-read.csv(\"c:\\\\temp\\\\t.test.data.csv\") attach(t.test.data) names(t.test.data) [1]\"gardenA\"\"gardenB\" A useful graphical test for two samples employs the notches option of boxplot: ozone<-c(gardenA,gardenB) label<-factor(c(rep(\"A\",10),rep(\"B\",10))) boxplot(ozonelabel,notch=T,xlab=\"Garden\", ylab=\"Ozonepphm\",col=\"lightblue\") Because the notches of two plots do not overlap, we conclude that the medians are significantly different at the 5% level."
  },
  {
    "input": "Note that the variability is similar in both gardens (bothintermsoftherangethelengthofthewhiskersandtheinterquartilerangethesize of the boxes).",
    "target": "Q: What is Note that the variability? A: Note that the variability is similar in both gardens (bothintermsoftherangethelengthofthewhiskersandtheinterquartilerangethesize of the boxes)."
  },
  {
    "input": "Tocarryoutattestlong-hand,webeginbycalculatingthevariancesofthetwosamples, s2A and s2B: s2A<-var(gardenA) s2B<-var(gardenB) We need to check that the two variances are not significantly different: s2A/s2B [1]1 94 STATISTICS:ANINTRODUCTIONUSINGR They are identical, which is excellent.",
    "target": "Q: What is Tocarryoutattestlong-hand,webeginbycalculatingthevariancesofthetwosamples, s2A and s2B: s2A<-var(gardenA) s2B<-var(gardenB) We need to check that the two variances are not significantly different: s2A/s2B [1]1 94 STATISTICS:ANINTRODUCTIONUSINGR They are identical, which? A: Tocarryoutattestlong-hand,webeginbycalculatingthevariancesofthetwosamples, s2A and s2B: s2A<-var(gardenA) s2B<-var(gardenB) We need to check that the two variances are not significantly different: s2A/s2B [1]1 94 STATISTICS:ANINTRODUCTIONUSINGR They are identical, which is excellent."
  },
  {
    "input": "Constancy of variance is the most important assumption of the t test.",
    "target": "Q: What is Constancy of variance? A: Constancy of variance is the most important assumption of the t test."
  },
  {
    "input": "In general, the value of the test statistic for Students t is the difference divided by the standard error of the difference.",
    "target": "Q: What is In general, the value of the test statistic for Students t? A: In general, the value of the test statistic for Students t is the difference divided by the standard error of the difference."
  },
  {
    "input": "Inourcase,thenumeratoristhedifferencebetweenthetwomeans(3\u00005=\u00002),andthe denominator is the square root of the sum of the variances (1.333333) divided by their sample sizes (10): (mean(gardenA)-mean(gardenB))/sqrt(s2A/10+s2B/10) Note that in calculating the standard errors we divide by the sample size (10) not thedegreesoffreedom(9);degreesoffreedomwereusedincalculatingthevariances(see p. 53).",
    "target": "Q: What is Inourcase,thenumeratoristhedifferencebetweenthetwomeans(3\u00005=\u00002),andthe denominator? A: Inourcase,thenumeratoristhedifferencebetweenthetwomeans(3\u00005=\u00002),andthe denominator is the square root of the sum of the variances (1.333333) divided by their sample sizes (10): (mean(gardenA)-mean(gardenB))/sqrt(s2A/10+s2B/10) Note that in calculating the standard errors we divide by the sample size (10) not thedegreesoffreedom(9);degreesoffreedomwereusedincalculatingthevariances(see p. 53)."
  },
  {
    "input": "Thenullhypothesiswasthatthetwomeansarenotsignificantlydifferent,sowerejectthis andacceptthealternativehypothesisthatthetwomeansaresignificantlydifferent.Again, ratherthanmerelyrejectingthenullhypothesis,itisbettertostatetheprobabilitythatatest statisticasextremeasthis(ormoreextreme)wouldbeobservedifthenullhypothesiswas true(i.e.themeanvalueswerenotsignificantlydifferent).Forthisweuseptratherthanqt, and 2  pt because we are doing a two-tailed test: 2*pt(-3.872983,18) [1]0.001114540 sop<0.0015.Youwillnotbesurprisedtolearnthatthereisabuilt-infunctiontodoallthe workforus.Itiscalled,helpfully,t.testandisusedsimplybyprovidingthenamesofthe two vectors containing the samples on which the test is to be carried out (gardenA and gardenB in our case).",
    "target": "Q: What is Thenullhypothesiswasthatthetwomeansarenotsignificantlydifferent,sowerejectthis andacceptthealternativehypothesisthatthetwomeansaresignificantlydifferent.Again, ratherthanmerelyrejectingthenullhypothesis,itisbettertostatetheprobabilitythatatest statisticasextremeasthis(ormoreextreme)wouldbeobservedifthenullhypothesiswas true(i.e.themeanvalueswerenotsignificantlydifferent).Forthisweuseptratherthanqt, and 2  pt because we are doing a two-tailed test: 2*pt(-3.872983,18) [1]0.001114540 sop<0.0015.Youwillnotbesurprisedtolearnthatthereisabuilt-infunctiontodoallthe workforus.Itiscalled,helpfully,t.testandisusedsimplybyprovidingthenamesofthe two vectors containing the samples on which the test? A: Thenullhypothesiswasthatthetwomeansarenotsignificantlydifferent,sowerejectthis andacceptthealternativehypothesisthatthetwomeansaresignificantlydifferent.Again, ratherthanmerelyrejectingthenullhypothesis,itisbettertostatetheprobabilitythatatest statisticasextremeasthis(ormoreextreme)wouldbeobservedifthenullhypothesiswas true(i.e.themeanvalueswerenotsignificantlydifferent).Forthisweuseptratherthanqt, and 2  pt because we are doing a two-tailed test: 2*pt(-3.872983,18) [1]0.001114540 sop<0.0015.Youwillnotbesurprisedtolearnthatthereisabuilt-infunctiontodoallthe workforus.Itiscalled,helpfully,t.testandisusedsimplybyprovidingthenamesofthe two vectors containing the samples on which the test is to be carried out (gardenA and gardenB in our case)."
  },
  {
    "input": "WelchTwoSamplet-test data: gardenAandgardenB TWOSAMPLES 95 t=-3.873,df=18,p-value=0.001115 alternativehypothesis:truedifferenceinmeansisnotequalto0 95percentconfidenceinterval: -3.0849115-0.9150885 sampleestimates: meanofxmeanofy 3 5 The result is exactly the same as we obtained long-hand.",
    "target": "Q: What is WelchTwoSamplet-test data: gardenAandgardenB TWOSAMPLES 95 t=-3.873,df=18,p-value=0.001115 alternativehypothesis:truedifferenceinmeansisnotequalto0 95percentconfidenceinterval: -3.0849115-0.9150885 sampleestimates: meanofxmeanofy 3 5 The result? A: WelchTwoSamplet-test data: gardenAandgardenB TWOSAMPLES 95 t=-3.873,df=18,p-value=0.001115 alternativehypothesis:truedifferenceinmeansisnotequalto0 95percentconfidenceinterval: -3.0849115-0.9150885 sampleestimates: meanofxmeanofy 3 5 The result is exactly the same as we obtained long-hand."
  },
  {
    "input": "The value of t is \u00003.873 and sincethesignisirrelevantinattestwerejectthenullhypothesisbecausetheteststatisticis largerthanthecriticalvalueof2.1.Themeanozoneconcentrationissignificantlyhigherin gardenBthaningardenA.Theoutputalsogivesapvalueandaconfidenceinterval.Note that,becausethemeansaresignificantlydifferent,theconfidenceintervalonthedifference does not include zero (in fact, it goesfrom \u00003.085 up to \u00000.915).",
    "target": "Q: What is The value of t? A: The value of t is \u00003.873 and sincethesignisirrelevantinattestwerejectthenullhypothesisbecausetheteststatisticis largerthanthecriticalvalueof2.1.Themeanozoneconcentrationissignificantlyhigherin gardenBthaningardenA.Theoutputalsogivesapvalueandaconfidenceinterval.Note that,becausethemeansaresignificantlydifferent,theconfidenceintervalonthedifference does not include zero (in fact, it goesfrom \u00003.085 up to \u00000.915)."
  },
  {
    "input": "WilcoxonRank-SumTest This is a non-parametric alternative to Students t test, which we could use if the errors werenon-normal.TheWilcoxonrank-sumteststatistic,W,iscalculatedasfollows.Both samplesareputintoasinglearraywiththeirsamplenamesclearlyattached(AandBin this case, as explained below).",
    "target": "Q: What is WilcoxonRank-SumTest This? A: WilcoxonRank-SumTest This is a non-parametric alternative to Students t test, which we could use if the errors werenon-normal.TheWilcoxonrank-sumteststatistic,W,iscalculatedasfollows.Both samplesareputintoasinglearraywiththeirsamplenamesclearlyattached(AandBin this case, as explained below)."
  },
  {
    "input": "Then the aggregate list is sorted, taking care to keep the samplelabelswiththeirrespectivevalues.Arankisthenassignedtoeachvalue,withties gettingtheappropriateaveragerank(two-waytiesget(ranki+(ranki+1))/2,three-way ties get (rank i+(rank i+1)+(rank i+3))/3, and so on).",
    "target": "Q: What is Then the aggregate list? A: Then the aggregate list is sorted, taking care to keep the samplelabelswiththeirrespectivevalues.Arankisthenassignedtoeachvalue,withties gettingtheappropriateaveragerank(two-waytiesget(ranki+(ranki+1))/2,three-way ties get (rank i+(rank i+1)+(rank i+3))/3, and so on)."
  },
  {
    "input": "Finally, the ranks are added up for each of the two samples, and significance is assessed on size of the smaller sum of ranks.",
    "target": "Q: What is Finally, the ranks are added up for each of the two samples, and significance? A: Finally, the ranks are added up for each of the two samples, and significance is assessed on size of the smaller sum of ranks."
  },
  {
    "input": "The non-parametric test is much more TWOSAMPLES 97 appropriate than the t-test when the errors are not normal, and the non-parametric test is about95%aspowerfulwithnormalerrors,andcanbemorepowerfulthanthettestifthe distributionisstronglyskewedbythepresenceofoutliers.Typically,ashere,thettestwill give the lower p value, so the Wilcoxon test is said to be conservative: if a difference is significant under a Wilcoxon test it would be even more significant under a t test.",
    "target": "Q: What is The non-parametric test? A: The non-parametric test is much more TWOSAMPLES 97 appropriate than the t-test when the errors are not normal, and the non-parametric test is about95%aspowerfulwithnormalerrors,andcanbemorepowerfulthanthettestifthe distributionisstronglyskewedbythepresenceofoutliers.Typically,ashere,thettestwill give the lower p value, so the Wilcoxon test is said to be conservative: if a difference is significant under a Wilcoxon test it would be even more significant under a t test."
  },
  {
    "input": "TestsonPairedSamples Sometimes,two-sampledatacomefrompairedobservations.Inthiscase,wemightexpecta correlation between the two measurements, either because they were made on the same individual,orbecausetheyweretakenfromthesamelocation.Youmightrecallthatearlier (Box 6.1) we found that the variance of a difference was the average of y \u0000 2 y \u0000 2\u00002 y \u0000  y \u0000  A A B B A A B B which is the variance of sample A, plus the variance of sample B, minus 2 times the covariance of A and B.",
    "target": "Q: What is TestsonPairedSamples Sometimes,two-sampledatacomefrompairedobservations.Inthiscase,wemightexpecta correlation between the two measurements, either because they were made on the same individual,orbecausetheyweretakenfromthesamelocation.Youmightrecallthatearlier (Box 6.1) we found that the variance of a difference was the average of y \u0000 2 y \u0000 2\u00002 y \u0000  y \u0000  A A B B A A B B which? A: TestsonPairedSamples Sometimes,two-sampledatacomefrompairedobservations.Inthiscase,wemightexpecta correlation between the two measurements, either because they were made on the same individual,orbecausetheyweretakenfromthesamelocation.Youmightrecallthatearlier (Box 6.1) we found that the variance of a difference was the average of y \u0000 2 y \u0000 2\u00002 y \u0000  y \u0000  A A B B A A B B which is the variance of sample A, plus the variance of sample B, minus 2 times the covariance of A and B."
  },
  {
    "input": "When the covariance of A and B is positive, this is a great help becauseitreducesthevarianceofthedifference,whichmakesiteasiertodetectsignificant differences between the means.",
    "target": "Q: What is When the covariance of A and B? A: When the covariance of A and B is positive, this is a great help becauseitreducesthevarianceofthedifference,whichmakesiteasiertodetectsignificant differences between the means."
  },
  {
    "input": "Pairing is not always effective, because the correlation between y and y may be weak.",
    "target": "Q: What is Pairing? A: Pairing is not always effective, because the correlation between y and y may be weak."
  },
  {
    "input": "A B Thefollowingdataareacompositebiodiversityscorebasedonakicksampleofaquatic invertebrates from 16 rivers: streams<-read.csv(\"c:\\\\temp\\\\streams.csv\") attach(streams) names(streams) [1]\"down\"\"up\" The elements are paired because the two samples were taken on the same river, one upstreamandonedownstreamfromthesamesewageoutfall.Ifweignorethefactthatthe samples are paired, it appears that the sewage outfall has no impact on biodiversity score (p=0.6856): t.test(down,up) WelchTwoSamplet-test data: downandup t=-0.4088,df=29.755,p-value=0.6856 alternativehypothesis:truedifferenceinmeansisnotequalto0 95percentconfidenceinterval: -5.248256 3.498256 sampleestimates: meanofxmeanofy 12.500 13.375 However, if we allow that the samples are paired (simply by specifying the option paired=T), the picture is completely different: 98 STATISTICS:ANINTRODUCTIONUSINGR t.test(down,up,paired=T) Pairedt-test data: downandup t=-3.0502,df=15,p-value=0.0081 alternativehypothesis:truedifferenceinmeansisnotequalto0 95percentconfidenceinterval: -1.4864388-0.2635612 sampleestimates: meanofthedifferences -0.875 Now the difference between the means is highly significant (p=0.0081).",
    "target": "Q: What is A B Thefollowingdataareacompositebiodiversityscorebasedonakicksampleofaquatic invertebrates from 16 rivers: streams<-read.csv(\"c:\\\\temp\\\\streams.csv\") attach(streams) names(streams) [1]\"down\"\"up\" The elements are paired because the two samples were taken on the same river, one upstreamandonedownstreamfromthesamesewageoutfall.Ifweignorethefactthatthe samples are paired, it appears that the sewage outfall has no impact on biodiversity score (p=0.6856): t.test(down,up) WelchTwoSamplet-test data: downandup t=-0.4088,df=29.755,p-value=0.6856 alternativehypothesis:truedifferenceinmeansisnotequalto0 95percentconfidenceinterval: -5.248256 3.498256 sampleestimates: meanofxmeanofy 12.500 13.375 However, if we allow that the samples are paired (simply by specifying the option paired=T), the picture? A: A B Thefollowingdataareacompositebiodiversityscorebasedonakicksampleofaquatic invertebrates from 16 rivers: streams<-read.csv(\"c:\\\\temp\\\\streams.csv\") attach(streams) names(streams) [1]\"down\"\"up\" The elements are paired because the two samples were taken on the same river, one upstreamandonedownstreamfromthesamesewageoutfall.Ifweignorethefactthatthe samples are paired, it appears that the sewage outfall has no impact on biodiversity score (p=0.6856): t.test(down,up) WelchTwoSamplet-test data: downandup t=-0.4088,df=29.755,p-value=0.6856 alternativehypothesis:truedifferenceinmeansisnotequalto0 95percentconfidenceinterval: -5.248256 3.498256 sampleestimates: meanofxmeanofy 12.500 13.375 However, if we allow that the samples are paired (simply by specifying the option paired=T), the picture is completely different: 98 STATISTICS:ANINTRODUCTIONUSINGR t.test(down,up,paired=T) Pairedt-test data: downandup t=-3.0502,df=15,p-value=0.0081 alternativehypothesis:truedifferenceinmeansisnotequalto0 95percentconfidenceinterval: -1.4864388-0.2635612 sampleestimates: meanofthedifferences -0.875 Now the difference between the means is highly significant (p=0.0081)."
  },
  {
    "input": "The moral is clear.Ifyoucandoapairedttest,thenyoushouldalwaysdothepairedtest.Itcanneverdo anyharm,andsometimes(ashere)itcandoahugeamountofgood.Ingeneral,ifyouhave information on blocking or spatial correlation (in this case, the fact that the two samples came from the same river), then you should always use it in the analysis.",
    "target": "Q: What is The moral? A: The moral is clear.Ifyoucandoapairedttest,thenyoushouldalwaysdothepairedtest.Itcanneverdo anyharm,andsometimes(ashere)itcandoahugeamountofgood.Ingeneral,ifyouhave information on blocking or spatial correlation (in this case, the fact that the two samples came from the same river), then you should always use it in the analysis."
  },
  {
    "input": "Working with the differences has halved the number of degreesoffreedom(from30to15),butithasmorethancompensatedforthisbyreducing theerrorvariance,becausethereissuchastrongpositivecorrelationbetweeny andy .The A B moral is simple: blocking always helps.",
    "target": "Q: What is Working with the differences has halved the number of degreesoffreedom(from30to15),butithasmorethancompensatedforthisbyreducing theerrorvariance,becausethereissuchastrongpositivecorrelationbetweeny andy .The A B moral? A: Working with the differences has halved the number of degreesoffreedom(from30to15),butithasmorethancompensatedforthisbyreducing theerrorvariance,becausethereissuchastrongpositivecorrelationbetweeny andy .The A B moral is simple: blocking always helps."
  },
  {
    "input": "TheBinomialTest This is one of the simplest of all statistical tests.",
    "target": "Q: What is TheBinomialTest This? A: TheBinomialTest This is one of the simplest of all statistical tests."
  },
  {
    "input": "For example, nine springboard divers were scored as better or worse, having trained under a new regime andundertheconventionalregime(theregimeswereallocatedinarandomizedsequenceto eachathlete:newthenconventional,orconventionalthennew).Diverswerejudgedtwice: onediverwasworseonthenewregime,andeightwerebetter.Whatistheevidencethatthe newregimeproducessignificantlybetterscoresincompetition?Theanswercomesfroma TWOSAMPLES 99 two-tailedbinomialtest.Howlikelyisaresponseof1/9(or8/9ormoreextremethanthis, i.e.0/9or9/9)ifthepopulationsareactuallythesame(i.e.therewasnodifferencebetween thetwotrainingregimes)?Weusebinom.testforthis,specifyingthenumberoffailures (1) and the total sample size (9): binom.test(1,9) This produces the output: Exactbinomialtest data: 1and9 numberofsuccesses=1,numberoftrials=9,p-value=0.03906 alternativehypothesis:trueprobabilityofsuccessisnotequalto0.5 95percentconfidenceinterval: 0.0028091370.482496515 sampleestimates: probabilityofsuccess 0.1111111 From this we would conclude that the new training regime is significantly better than the traditional method, because p<0.05.",
    "target": "Q: What is For example, nine springboard divers were scored as better or worse, having trained under a new regime andundertheconventionalregime(theregimeswereallocatedinarandomizedsequenceto eachathlete:newthenconventional,orconventionalthennew).Diverswerejudgedtwice: onediverwasworseonthenewregime,andeightwerebetter.Whatistheevidencethatthe newregimeproducessignificantlybetterscoresincompetition?Theanswercomesfroma TWOSAMPLES 99 two-tailedbinomialtest.Howlikelyisaresponseof1/9(or8/9ormoreextremethanthis, i.e.0/9or9/9)ifthepopulationsareactuallythesame(i.e.therewasnodifferencebetween thetwotrainingregimes)?Weusebinom.testforthis,specifyingthenumberoffailures (1) and the total sample size (9): binom.test(1,9) This produces the output: Exactbinomialtest data: 1and9 numberofsuccesses=1,numberoftrials=9,p-value=0.03906 alternativehypothesis:trueprobabilityofsuccessisnotequalto0.5 95percentconfidenceinterval: 0.0028091370.482496515 sampleestimates: probabilityofsuccess 0.1111111 From this we would conclude that the new training regime? A: For example, nine springboard divers were scored as better or worse, having trained under a new regime andundertheconventionalregime(theregimeswereallocatedinarandomizedsequenceto eachathlete:newthenconventional,orconventionalthennew).Diverswerejudgedtwice: onediverwasworseonthenewregime,andeightwerebetter.Whatistheevidencethatthe newregimeproducessignificantlybetterscoresincompetition?Theanswercomesfroma TWOSAMPLES 99 two-tailedbinomialtest.Howlikelyisaresponseof1/9(or8/9ormoreextremethanthis, i.e.0/9or9/9)ifthepopulationsareactuallythesame(i.e.therewasnodifferencebetween thetwotrainingregimes)?Weusebinom.testforthis,specifyingthenumberoffailures (1) and the total sample size (9): binom.test(1,9) This produces the output: Exactbinomialtest data: 1and9 numberofsuccesses=1,numberoftrials=9,p-value=0.03906 alternativehypothesis:trueprobabilityofsuccessisnotequalto0.5 95percentconfidenceinterval: 0.0028091370.482496515 sampleestimates: probabilityofsuccess 0.1111111 From this we would conclude that the new training regime is significantly better than the traditional method, because p<0.05."
  },
  {
    "input": "The p value of 0.03906 is the exact probability of obtainingtheobservedresult(1of9)oramoreextremeresult(0of9)ifthetwotraining regimeswerethesame(sotheprobabilityofanyoneofthejudgingoutcomeswas0.5).If thetworegimeshadidenticaleffectsonthejudges,thenascoreofeitherbetterorworse hasaprobabilityof0.5.Soeightsuccesseshasaprobabilityof0.58=0.0039andonefailure has a probability of 0.5.",
    "target": "Q: What is The p value of 0.03906? A: The p value of 0.03906 is the exact probability of obtainingtheobservedresult(1of9)oramoreextremeresult(0of9)ifthetwotraining regimeswerethesame(sotheprobabilityofanyoneofthejudgingoutcomeswas0.5).If thetworegimeshadidenticaleffectsonthejudges,thenascoreofeitherbetterorworse hasaprobabilityof0.5.Soeightsuccesseshasaprobabilityof0.58=0.0039andonefailure has a probability of 0.5."
  },
  {
    "input": "The whole outcome has a probability of 0:00390625\u00020:50:001953125 Note,however,thatthereare9waysofgettingthisresult,sotheprobabilityof1outof 9 is 9\u00020:0019531250:01757812 Thisisnottheanswerwewant,becausethereisamoreextremecasewhenall9ofthejudges thoughtthatthenewregimeproducedimprovedscores.Thereisonlyonewayofobtaining thisoutcome(9successesandnofailures)soithasaprobabilityof0.59=0.001953125.This meansthattheprobabilitytheobservedresultoramoreextremeoutcomeis 0:0019531250:017578120:01953124 Eventhisisnottheanswerwewant,becausethewholeprocessmighthaveworkedinthe oppositedirection(thenewregimemighthaveproducedworsescores(8and1or9and0)so whatweneedisatwo-tailedtest).Theresultwewantisobtainedsimplybydoublingthelast result: 2\u00020:019531240:03906248 whichisthefigureproducedbythebuiltinfunctionbinom.test(above) 100 STATISTICS:ANINTRODUCTIONUSINGR BinomialTeststoCompareTwoProportions Supposethatinyourinstitutiononlyfourfemaleswerepromoted,comparedwith196men.",
    "target": "Q: What is The whole outcome has a probability of 0:00390625\u00020:50:001953125 Note,however,thatthereare9waysofgettingthisresult,sotheprobabilityof1outof 9? A: The whole outcome has a probability of 0:00390625\u00020:50:001953125 Note,however,thatthereare9waysofgettingthisresult,sotheprobabilityof1outof 9 is 9\u00020:0019531250:01757812 Thisisnottheanswerwewant,becausethereisamoreextremecasewhenall9ofthejudges thoughtthatthenewregimeproducedimprovedscores.Thereisonlyonewayofobtaining thisoutcome(9successesandnofailures)soithasaprobabilityof0.59=0.001953125.This meansthattheprobabilitytheobservedresultoramoreextremeoutcomeis 0:0019531250:017578120:01953124 Eventhisisnottheanswerwewant,becausethewholeprocessmighthaveworkedinthe oppositedirection(thenewregimemighthaveproducedworsescores(8and1or9and0)so whatweneedisatwo-tailedtest).Theresultwewantisobtainedsimplybydoublingthelast result: 2\u00020:019531240:03906248 whichisthefigureproducedbythebuiltinfunctionbinom.test(above) 100 STATISTICS:ANINTRODUCTIONUSINGR BinomialTeststoCompareTwoProportions Supposethatinyourinstitutiononlyfourfemaleswerepromoted,comparedwith196men."
  },
  {
    "input": "Thequestionthenarisesastowhethertheapparentpositivediscriminationinfavourof women is statistically significant, or whether this sort of difference could arise through chancealone.ThisiseasyinRusingthebuilt-inbinomialproportionstestprop.testin which we specify two vectors, the first containing the number of successes for females andmalesc(4,196)andsecondcontainingthetotalnumberoffemaleandmalecandidates c(40,3270): prop.test(c(4,196),c(40,3270)) 2-sampletestforequalityofproportionswithcontinuitycorrection data: c(4,196)outofc(40,3270) X-squared=0.5229,df=1,p-value=0.4696 alternativehypothesis:two.sided 95percentconfidenceinterval: -0.06591631 0.14603864 sampleestimates: prop1 prop2 0.100000000.05993884 Warningmessage: Inprop.test(c(4,196),c(40,3270)): Chi-squaredapproximationmaybeincorrect Thereisnoevidenceinfavourofpositivediscrimination(p=0.4696).Aresultlikethis willoccurmorethan45%ofthetimebychancealone.Justthinkwhatwouldhavehappened ifoneofthesuccessfulfemalecandidateshadnotapplied.Thenthesamepromotionsystem wouldhaveproducedafemalesuccessrateof3/39insteadof4/40(7.7%insteadof10%).",
    "target": "Q: What is Thequestionthenarisesastowhethertheapparentpositivediscriminationinfavourof women? A: Thequestionthenarisesastowhethertheapparentpositivediscriminationinfavourof women is statistically significant, or whether this sort of difference could arise through chancealone.ThisiseasyinRusingthebuilt-inbinomialproportionstestprop.testin which we specify two vectors, the first containing the number of successes for females andmalesc(4,196)andsecondcontainingthetotalnumberoffemaleandmalecandidates c(40,3270): prop.test(c(4,196),c(40,3270)) 2-sampletestforequalityofproportionswithcontinuitycorrection data: c(4,196)outofc(40,3270) X-squared=0.5229,df=1,p-value=0.4696 alternativehypothesis:two.sided 95percentconfidenceinterval: -0.06591631 0.14603864 sampleestimates: prop1 prop2 0.100000000.05993884 Warningmessage: Inprop.test(c(4,196),c(40,3270)): Chi-squaredapproximationmaybeincorrect Thereisnoevidenceinfavourofpositivediscrimination(p=0.4696).Aresultlikethis willoccurmorethan45%ofthetimebychancealone.Justthinkwhatwouldhavehappened ifoneofthesuccessfulfemalecandidateshadnotapplied.Thenthesamepromotionsystem wouldhaveproducedafemalesuccessrateof3/39insteadof4/40(7.7%insteadof10%)."
  },
  {
    "input": "The moral is very important: in small samples, small changes have big effects.",
    "target": "Q: What is The moral? A: The moral is very important: in small samples, small changes have big effects."
  },
  {
    "input": "The dictionary definition of contingency is a thing dependent on an uncertain event (OxfordEnglishDictionary).Instatistics,however,thecontingenciesarealltheeventsthat couldpossiblyhappen.Acontingencytableshowsthecountsofhowmanytimeseachof thecontingenciesactuallyhappenedinaparticularsample.Considerthefollowingexample that has to do with the relationship between hair colour and eye colour in white people.",
    "target": "Q: What is The dictionary definition of contingency? A: The dictionary definition of contingency is a thing dependent on an uncertain event (OxfordEnglishDictionary).Instatistics,however,thecontingenciesarealltheeventsthat couldpossiblyhappen.Acontingencytableshowsthecountsofhowmanytimeseachof thecontingenciesactuallyhappenedinaparticularsample.Considerthefollowingexample that has to do with the relationship between hair colour and eye colour in white people."
  },
  {
    "input": "Then we fill in the 22 contingency table like this: Blueeyes Browneyes Fairhair 38 11 Darkhair 14 51 Theseareourobservedfrequencies(orcounts).Thenextstepisveryimportant.Inorder to make any progress in the analysis of these data we need a model which predicts the expectedfrequencies.Whatwouldbeasensiblemodelinacaselikethis?Thereareallsorts ofcomplicatedmodelsthatyoumightselect,butthesimplestmodel(Occamsrazor,orthe principle of parsimony) is that hair colour and eye colour are independent.",
    "target": "Q: What is Then we fill in the 22 contingency table like this: Blueeyes Browneyes Fairhair 38 11 Darkhair 14 51 Theseareourobservedfrequencies(orcounts).Thenextstepisveryimportant.Inorder to make any progress in the analysis of these data we need a model which predicts the expectedfrequencies.Whatwouldbeasensiblemodelinacaselikethis?Thereareallsorts ofcomplicatedmodelsthatyoumightselect,butthesimplestmodel(Occamsrazor,orthe principle of parsimony)? A: Then we fill in the 22 contingency table like this: Blueeyes Browneyes Fairhair 38 11 Darkhair 14 51 Theseareourobservedfrequencies(orcounts).Thenextstepisveryimportant.Inorder to make any progress in the analysis of these data we need a model which predicts the expectedfrequencies.Whatwouldbeasensiblemodelinacaselikethis?Thereareallsorts ofcomplicatedmodelsthatyoumightselect,butthesimplestmodel(Occamsrazor,orthe principle of parsimony) is that hair colour and eye colour are independent."
  },
  {
    "input": "We may not believethatthisisactuallytrue,butthehypothesishasthegreatvirtueofbeingfalsifiable.It is also a very sensible model to choose because it makes it easy to predict the expected frequencies based on the assumption that the model is true.",
    "target": "Q: What is We may not believethatthisisactuallytrue,butthehypothesishasthegreatvirtueofbeingfalsifiable.It? A: We may not believethatthisisactuallytrue,butthehypothesishasthegreatvirtueofbeingfalsifiable.It is also a very sensible model to choose because it makes it easy to predict the expected frequencies based on the assumption that the model is true."
  },
  {
    "input": "Noticethatbecausewehaveonlytwolevelsofhaircolour,thesetwoprobabilitiesaddupto 1((49+65)/114).Whatabouteyecolour?Whatistheprobabilityofselectingsomeoneat randomfromthissamplewithblueeyes?Atotalof52peoplehadblueeyes(38+14)outof the sample of 114, so the probability of blue eyes is52/114 and the probability of brown eyesis62/114.Asbefore,thesesumto1((52+62)/114).Ithelpstoappendthesubtotalsto the margins of the contingency table like this: Blueeyes Browneyes Rowtotals Fairhair 38 11 49 Darkhair 14 51 65 Columntotals 52 62 114 Nowcomestheimportantbit.Wewanttoknowtheexpectedfrequencyofpeoplewithfair hairandblueeyes,tocomparewithourobservedfrequencyof38.Ourmodelsaysthatthetwo areindependent.Thisisessentialinformation,becauseitallowsustocalculatetheexpected probabilityoffairhairandblueeyes.If,andonlyif,thetwotraitsareindependent,thenthe probability of having fair hair and blue eyes is the product of the two probabilities.",
    "target": "Q: What is Noticethatbecausewehaveonlytwolevelsofhaircolour,thesetwoprobabilitiesaddupto 1((49+65)/114).Whatabouteyecolour?Whatistheprobabilityofselectingsomeoneat randomfromthissamplewithblueeyes?Atotalof52peoplehadblueeyes(38+14)outof the sample of 114, so the probability of blue eyes is52/114 and the probability of brown eyesis62/114.Asbefore,thesesumto1((52+62)/114).Ithelpstoappendthesubtotalsto the margins of the contingency table like this: Blueeyes Browneyes Rowtotals Fairhair 38 11 49 Darkhair 14 51 65 Columntotals 52 62 114 Nowcomestheimportantbit.Wewanttoknowtheexpectedfrequencyofpeoplewithfair hairandblueeyes,tocomparewithourobservedfrequencyof38.Ourmodelsaysthatthetwo areindependent.Thisisessentialinformation,becauseitallowsustocalculatetheexpected probabilityoffairhairandblueeyes.If,andonlyif,thetwotraitsareindependent,thenthe probability of having fair hair and blue eyes? A: Noticethatbecausewehaveonlytwolevelsofhaircolour,thesetwoprobabilitiesaddupto 1((49+65)/114).Whatabouteyecolour?Whatistheprobabilityofselectingsomeoneat randomfromthissamplewithblueeyes?Atotalof52peoplehadblueeyes(38+14)outof the sample of 114, so the probability of blue eyes is52/114 and the probability of brown eyesis62/114.Asbefore,thesesumto1((52+62)/114).Ithelpstoappendthesubtotalsto the margins of the contingency table like this: Blueeyes Browneyes Rowtotals Fairhair 38 11 49 Darkhair 14 51 65 Columntotals 52 62 114 Nowcomestheimportantbit.Wewanttoknowtheexpectedfrequencyofpeoplewithfair hairandblueeyes,tocomparewithourobservedfrequencyof38.Ourmodelsaysthatthetwo areindependent.Thisisessentialinformation,becauseitallowsustocalculatetheexpected probabilityoffairhairandblueeyes.If,andonlyif,thetwotraitsareindependent,thenthe probability of having fair hair and blue eyes is the product of the two probabilities."
  },
  {
    "input": "Wecandoexactlyequivalentthingsfortheotherthreecellsofthecontingencytable: Blueeyes Browneyes Rowtotals Fairhair 49 \u0002 52 49 \u0002 62 49 114 114 114 114 Darkhair 65 \u0002 52 65 \u0002 62 65 114 114 114 114 Columntotals 52 62 114 Nowweneedtoknowhowtocalculatetheexpectedfrequency.Itcouldnotbesimpler.It isjusttheprobabilitymultipliedbythetotalsample(n=114).Sotheexpectedfrequencyof blue eyes and fair hair is 49 \u0002 52 \u000211422:35 which is much less than our observed 114 114 frequencyof38.Itisbeginningtolookasifourhypothesisofindependenceofhairandeye colour is false.",
    "target": "Q: What is Wecandoexactlyequivalentthingsfortheotherthreecellsofthecontingencytable: Blueeyes Browneyes Rowtotals Fairhair 49 \u0002 52 49 \u0002 62 49 114 114 114 114 Darkhair 65 \u0002 52 65 \u0002 62 65 114 114 114 114 Columntotals 52 62 114 Nowweneedtoknowhowtocalculatetheexpectedfrequency.Itcouldnotbesimpler.It isjusttheprobabilitymultipliedbythetotalsample(n=114).Sotheexpectedfrequencyof blue eyes and fair hair? A: Wecandoexactlyequivalentthingsfortheotherthreecellsofthecontingencytable: Blueeyes Browneyes Rowtotals Fairhair 49 \u0002 52 49 \u0002 62 49 114 114 114 114 Darkhair 65 \u0002 52 65 \u0002 62 65 114 114 114 114 Columntotals 52 62 114 Nowweneedtoknowhowtocalculatetheexpectedfrequency.Itcouldnotbesimpler.It isjusttheprobabilitymultipliedbythetotalsample(n=114).Sotheexpectedfrequencyof blue eyes and fair hair is 49 \u0002 52 \u000211422:35 which is much less than our observed 114 114 frequencyof38.Itisbeginningtolookasifourhypothesisofindependenceofhairandeye colour is false."
  },
  {
    "input": "It is clear that the observed frequencies and the expected frequencies are different.But insampling, everythingalways varies, sothis isno surprise.",
    "target": "Q: What is It? A: It is clear that the observed frequencies and the expected frequencies are different.But insampling, everythingalways varies, sothis isno surprise."
  },
  {
    "input": "O E (O\u0000E)2 O\u0000E2 E Fairhairandblueeyes 38 22.35 244.92 10.96 Fairhairandbrowneyes 11 26.65 244.92 9.19 Darkhairandblueeyes 14 29.65 244.92 8.26 Darkhairandbrowneyes 51 35.35 244.92 6.93 All we need to do now is to add up the four components of chi-squared to get the test statistic2 35:33.Thequestionnowarises:isthisabigvalueofchi-squaredornot?This isimportant,becauseifitisabiggervalueofchi-squaredthanwewouldexpectbychance, then we should reject the null hypothesis.",
    "target": "Q: What is O E (O\u0000E)2 O\u0000E2 E Fairhairandblueeyes 38 22.35 244.92 10.96 Fairhairandbrowneyes 11 26.65 244.92 9.19 Darkhairandblueeyes 14 29.65 244.92 8.26 Darkhairandbrowneyes 51 35.35 244.92 6.93 All we need to do now? A: O E (O\u0000E)2 O\u0000E2 E Fairhairandblueeyes 38 22.35 244.92 10.96 Fairhairandbrowneyes 11 26.65 244.92 9.19 Darkhairandblueeyes 14 29.65 244.92 8.26 Darkhairandbrowneyes 51 35.35 244.92 6.93 All we need to do now is to add up the four components of chi-squared to get the test statistic2 35:33.Thequestionnowarises:isthisabigvalueofchi-squaredornot?This isimportant,becauseifitisabiggervalueofchi-squaredthanwewouldexpectbychance, then we should reject the null hypothesis."
  },
  {
    "input": "If, on the other hand, it is within the range of values that we would expect by chance alone, then we should accept the null hypothesis.",
    "target": "Q: What is If, on the other hand, it? A: If, on the other hand, it is within the range of values that we would expect by chance alone, then we should accept the null hypothesis."
  },
  {
    "input": "To work out the critical value of chi-squared we need two things: (cid:129) the number of degrees of freedom (cid:129) the degree of certainty with which to work Ingeneral,acontingencytablehasanumberofrows(r)andanumberofcolumns(c),and the degrees of freedom is given by d:f: r\u00001\u0002 c\u00001 Sowehave(2\u00001)(2\u00001)=1degreeoffreedomfora22contingencytable.Youcan seewhythereisonlyonedegreeoffreedombyworkingthroughourexample.Takethefair hair and brown eyesbox (the topright in thetable)and ask how many values this could possiblytake.Thefirstthingtonoteisthatthecountcouldnotbemorethan49,otherwise therowtotalwouldbewrong.Butinprinciple,thenumberinthisboxisfreetobeanyvalue between0and49.Wehaveonedegreeoffreedomforthisbox.Butwhenwehavefixedthis boxtobe11,youwillseethatwehavenofreedomatallforanyoftheotherthreeboxes.",
    "target": "Q: What is To work out the critical value of chi-squared we need two things: (cid:129) the number of degrees of freedom (cid:129) the degree of certainty with which to work Ingeneral,acontingencytablehasanumberofrows(r)andanumberofcolumns(c),and the degrees of freedom? A: To work out the critical value of chi-squared we need two things: (cid:129) the number of degrees of freedom (cid:129) the degree of certainty with which to work Ingeneral,acontingencytablehasanumberofrows(r)andanumberofcolumns(c),and the degrees of freedom is given by d:f: r\u00001\u0002 c\u00001 Sowehave(2\u00001)(2\u00001)=1degreeoffreedomfora22contingencytable.Youcan seewhythereisonlyonedegreeoffreedombyworkingthroughourexample.Takethefair hair and brown eyesbox (the topright in thetable)and ask how many values this could possiblytake.Thefirstthingtonoteisthatthecountcouldnotbemorethan49,otherwise therowtotalwouldbewrong.Butinprinciple,thenumberinthisboxisfreetobeanyvalue between0and49.Wehaveonedegreeoffreedomforthisbox.Butwhenwehavefixedthis boxtobe11,youwillseethatwehavenofreedomatallforanyoftheotherthreeboxes."
  },
  {
    "input": "The more certain we want to be, the larger the value of chi-squared we wouldneedtorejectthenullhypothesis.Itisconventionaltoworkatthe95%level.That isourcertaintylevel,soouruncertaintylevelis100\u000095=5%.Expressedasafraction, this is called alpha (0:05).",
    "target": "Q: What is The more certain we want to be, the larger the value of chi-squared we wouldneedtorejectthenullhypothesis.Itisconventionaltoworkatthe95%level.That isourcertaintylevel,soouruncertaintylevelis100\u000095=5%.Expressedasafraction, this? A: The more certain we want to be, the larger the value of chi-squared we wouldneedtorejectthenullhypothesis.Itisconventionaltoworkatthe95%level.That isourcertaintylevel,soouruncertaintylevelis100\u000095=5%.Expressedasafraction, this is called alpha (0:05)."
  },
  {
    "input": "Technically, alpha is the probability of rejecting the null hypothesiswhenitistrue.ThisiscalledaTypeIerror.ATypeIIerrorisacceptingthe null hypothesis when it is false (see p. 4).",
    "target": "Q: What is Technically, alpha? A: Technically, alpha is the probability of rejecting the null hypothesiswhenitistrue.ThisiscalledaTypeIerror.ATypeIIerrorisacceptingthe null hypothesis when it is false (see p. 4)."
  },
  {
    "input": "Critical values in R are obtained by use of quantiles (q) of the appropriate statistical distribution.Forthechi-squareddistribution,thisfunctioniscalledqchisq.Thefunction has two arguments: the certainty level (1\u0000=0.95), and the degrees of freedom (d.f.=1): qchisq(0.95,1) [1]3.841459 The critical value of chi-squared is 3.841.The logic goes like this: since the calculated valueoftheteststatisticisgreaterthanthecriticalvalue,werejectthenullhypothesis.You should memorize this sentence: put the emphasis on greater and reject.",
    "target": "Q: What is Critical values in R are obtained by use of quantiles (q) of the appropriate statistical distribution.Forthechi-squareddistribution,thisfunctioniscalledqchisq.Thefunction has two arguments: the certainty level (1\u0000=0.95), and the degrees of freedom (d.f.=1): qchisq(0.95,1) [1]3.841459 The critical value of chi-squared? A: Critical values in R are obtained by use of quantiles (q) of the appropriate statistical distribution.Forthechi-squareddistribution,thisfunctioniscalledqchisq.Thefunction has two arguments: the certainty level (1\u0000=0.95), and the degrees of freedom (d.f.=1): qchisq(0.95,1) [1]3.841459 The critical value of chi-squared is 3.841.The logic goes like this: since the calculated valueoftheteststatisticisgreaterthanthecriticalvalue,werejectthenullhypothesis.You should memorize this sentence: put the emphasis on greater and reject."
  },
  {
    "input": "But that is not the end of the story, because we have not establishedthewayinwhichtheyarerelated(e.g.isthecorrelationbetweenthempositiveor negative?",
    "target": "Q: What is But that? A: But that is not the end of the story, because we have not establishedthewayinwhichtheyarerelated(e.g.isthecorrelationbetweenthempositiveor negative?"
  },
  {
    "input": "We start by defining the counts as a 22 matrix like this: count<-matrix(c(38,14,11,51),nrow=2) count [,1][,2] [1,] 38 11 [2,] 14 51 Noticethatyouenterthedatacolumn-wise(notrow-wise)intothematrix.Thenthetest uses the chisq.test function, with the matrix of counts as its only argument: TWOSAMPLES 105 chisq.test(count) PearsonsChi-squaredtestwithYatescontinuitycorrection data: count X-squared=33.112,df=1,p-value=8.7e-09 The calculated value of chi-squared is slightly different from ours, because Yatess correctionhasbeenappliedasthedefault(see?chisq.test).Ifyouswitchthecorrection off (correct=F), you get the exact value we calculated by hand: chisq.test(count,correct=F) PearsonsChi-squaredtest data: count X-squared=35.3338,df=1,p-value=2.778e-09 It makes no difference at all to the interpretation: there is a highly significant positive association between fair hair and blue eyes for this group of people.",
    "target": "Q: What is We start by defining the counts as a 22 matrix like this: count<-matrix(c(38,14,11,51),nrow=2) count [,1][,2] [1,] 38 11 [2,] 14 51 Noticethatyouenterthedatacolumn-wise(notrow-wise)intothematrix.Thenthetest uses the chisq.test function, with the matrix of counts as its only argument: TWOSAMPLES 105 chisq.test(count) PearsonsChi-squaredtestwithYatescontinuitycorrection data: count X-squared=33.112,df=1,p-value=8.7e-09 The calculated value of chi-squared? A: We start by defining the counts as a 22 matrix like this: count<-matrix(c(38,14,11,51),nrow=2) count [,1][,2] [1,] 38 11 [2,] 14 51 Noticethatyouenterthedatacolumn-wise(notrow-wise)intothematrix.Thenthetest uses the chisq.test function, with the matrix of counts as its only argument: TWOSAMPLES 105 chisq.test(count) PearsonsChi-squaredtestwithYatescontinuitycorrection data: count X-squared=33.112,df=1,p-value=8.7e-09 The calculated value of chi-squared is slightly different from ours, because Yatess correctionhasbeenappliedasthedefault(see?chisq.test).Ifyouswitchthecorrection off (correct=F), you get the exact value we calculated by hand: chisq.test(count,correct=F) PearsonsChi-squaredtest data: count X-squared=35.3338,df=1,p-value=2.778e-09 It makes no difference at all to the interpretation: there is a highly significant positive association between fair hair and blue eyes for this group of people."
  },
  {
    "input": "FishersExactTest Thistestisusedfortheanalysisofcontingencytablesbasedonsmallsamplesinwhichone ormoreoftheexpectedfrequenciesarelessthan5.Theindividualcountsarea,b,candd like this: 22table Column1 Column2 Rowtotals Row1 a b a+b Row2 c d c+d Columntotals a+c b+d n The probability of any one particular outcome is given by ab!",
    "target": "Q: What is FishersExactTest Thistestisusedfortheanalysisofcontingencytablesbasedonsmallsamplesinwhichone ormoreoftheexpectedfrequenciesarelessthan5.Theindividualcountsarea,b,candd like this: 22table Column1 Column2 Rowtotals Row1 a b a+b Row2 c d c+d Columntotals a+c b+d n The probability of any one particular outcome? A: FishersExactTest Thistestisusedfortheanalysisofcontingencytablesbasedonsmallsamplesinwhichone ormoreoftheexpectedfrequenciesarelessthan5.Theindividualcountsarea,b,candd like this: 22table Column1 Column2 Rowtotals Row1 a b a+b Row2 c d c+d Columntotals a+c b+d n The probability of any one particular outcome is given by ab!"
  },
  {
    "input": "The response variable is the vector of four counts c(6,4,2,8).",
    "target": "Q: What is The response variable? A: The response variable is the vector of four counts c(6,4,2,8)."
  },
  {
    "input": "TWOSAMPLES 107 There is a built in function called fisher.test, which saves us all this tedious computation.",
    "target": "Q: What is TWOSAMPLES 107 There? A: TWOSAMPLES 107 There is a built in function called fisher.test, which saves us all this tedious computation."
  },
  {
    "input": "It takes as its argument a 22 matrix containing the counts of the four contingencies.Wemakethematrixlikethis(comparewiththealternativemethodofmaking a matrix, above): x<-as.matrix(c(6,4,2,8)) dim(x)<-c(2,2) x [,1] [,2] [1,] 6 2 [2,] 4 8 Then we run the test like this: fisher.test(x) FishersExactTestforCountData data: x p-value=0.1698 alternativehypothesis:trueoddsratioisnotequalto1 95percentconfidenceinterval: 0.602680579.8309210 sampleestimates: oddsratio 5.430473 Thefisher.testcanbeusedwithmatricesmuchbiggerthan22.Alternatively,the functionmaybeprovidedwithtwovectorscontainingfactorlevels,insteadofamatrixof counts,ashere;thissavesyouthetroubleofcountinguphowmanycombinationsofeach factor level there are: table<-read.csv(\"c:\\\\temp\\\\fisher.csv\") attach(table) head(table) treenests 1 A ants 2 B ants 3 A none 4 A ants 5 B none 6 A none The function is invoked by providing the two vector names as arguments: fisher.test(tree,nests) 108 STATISTICS:ANINTRODUCTIONUSINGR CorrelationandCovariance With two continuous variables, x and y, the question naturally arises as to whether their values are correlated with each other (remembering, of course, that correlation does not implycausation).Correlationisdefinedintermsofthevarianceofx,thevarianceofy,and the covariance of x and y (the way the two vary together; the way they co-vary) on the assumption that both variables are normally distributed.",
    "target": "Q: What is It takes as its argument a 22 matrix containing the counts of the four contingencies.Wemakethematrixlikethis(comparewiththealternativemethodofmaking a matrix, above): x<-as.matrix(c(6,4,2,8)) dim(x)<-c(2,2) x [,1] [,2] [1,] 6 2 [2,] 4 8 Then we run the test like this: fisher.test(x) FishersExactTestforCountData data: x p-value=0.1698 alternativehypothesis:trueoddsratioisnotequalto1 95percentconfidenceinterval: 0.602680579.8309210 sampleestimates: oddsratio 5.430473 Thefisher.testcanbeusedwithmatricesmuchbiggerthan22.Alternatively,the functionmaybeprovidedwithtwovectorscontainingfactorlevels,insteadofamatrixof counts,ashere;thissavesyouthetroubleofcountinguphowmanycombinationsofeach factor level there are: table<-read.csv(\"c:\\\\temp\\\\fisher.csv\") attach(table) head(table) treenests 1 A ants 2 B ants 3 A none 4 A ants 5 B none 6 A none The function? A: It takes as its argument a 22 matrix containing the counts of the four contingencies.Wemakethematrixlikethis(comparewiththealternativemethodofmaking a matrix, above): x<-as.matrix(c(6,4,2,8)) dim(x)<-c(2,2) x [,1] [,2] [1,] 6 2 [2,] 4 8 Then we run the test like this: fisher.test(x) FishersExactTestforCountData data: x p-value=0.1698 alternativehypothesis:trueoddsratioisnotequalto1 95percentconfidenceinterval: 0.602680579.8309210 sampleestimates: oddsratio 5.430473 Thefisher.testcanbeusedwithmatricesmuchbiggerthan22.Alternatively,the functionmaybeprovidedwithtwovectorscontainingfactorlevels,insteadofamatrixof counts,ashere;thissavesyouthetroubleofcountinguphowmanycombinationsofeach factor level there are: table<-read.csv(\"c:\\\\temp\\\\fisher.csv\") attach(table) head(table) treenests 1 A ants 2 B ants 3 A none 4 A ants 5 B none 6 A none The function is invoked by providing the two vector names as arguments: fisher.test(tree,nests) 108 STATISTICS:ANINTRODUCTIONUSINGR CorrelationandCovariance With two continuous variables, x and y, the question naturally arises as to whether their values are correlated with each other (remembering, of course, that correlation does not implycausation).Correlationisdefinedintermsofthevarianceofx,thevarianceofy,and the covariance of x and y (the way the two vary together; the way they co-vary) on the assumption that both variables are normally distributed."
  },
  {
    "input": "We have symbols already for thetwovariances,s2 ands2.Wedenotethecovarianceofxandybycov(x,y),afterwhich x y the correlation coefficient r is defined as cov x;y r  qffiffiffiffiffiffiffiffi s2s2 x y Weknowhowtocalculatevariances(p.53),soitremainsonlytoworkoutthevalueof the covariance of x and y. Covariance is defined as the expectation of the vector product xy, which sounds difficult, but is not (Box 6.2).",
    "target": "Q: What is We have symbols already for thetwovariances,s2 ands2.Wedenotethecovarianceofxandybycov(x,y),afterwhich x y the correlation coefficient r? A: We have symbols already for thetwovariances,s2 ands2.Wedenotethecovarianceofxandybycov(x,y),afterwhich x y the correlation coefficient r is defined as cov x;y r  qffiffiffiffiffiffiffiffi s2s2 x y Weknowhowtocalculatevariances(p.53),soitremainsonlytoworkoutthevalueof the covariance of x and y. Covariance is defined as the expectation of the vector product xy, which sounds difficult, but is not (Box 6.2)."
  },
  {
    "input": "The covariance of x and y is the expectation of the product minus the product of the two expectations.",
    "target": "Q: What is The covariance of x and y? A: The covariance of x and y is the expectation of the product minus the product of the two expectations."
  },
  {
    "input": "Correlationandcovariance The correlation coefficient is defined in terms of the covariance of x and y, and the geometric mean of the variances of x and y: cov x;y  x;ypffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi var x\u0002var y The covariance of x and y is defined as the expectation of the vector product: x\u0000x y\u0000y: cov x;yE x\u0000x y\u0000y We start by multiplying through the brackets: x\u0000x y\u0000yxy\u0000xy\u0000xyxy Now applying expectations, and remembering that the expectation of x is x and the expectation of y is y, we get cov x;yE xy\u0000xE y\u0000E xyxyE xy\u0000xy\u0000xyxy TWOSAMPLES 109 Then \u0000xyxy cancels out, leaving \u0000xy which is \u0000E xE y so cov x;yE xy\u0000E xE y Noticethatwhenxandyareuncorrelated,E xyE xE y,sothecovarianceis0in this case.",
    "target": "Q: What is Correlationandcovariance The correlation coefficient? A: Correlationandcovariance The correlation coefficient is defined in terms of the covariance of x and y, and the geometric mean of the variances of x and y: cov x;y  x;ypffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi var x\u0002var y The covariance of x and y is defined as the expectation of the vector product: x\u0000x y\u0000y: cov x;yE x\u0000x y\u0000y We start by multiplying through the brackets: x\u0000x y\u0000yxy\u0000xy\u0000xyxy Now applying expectations, and remembering that the expectation of x is x and the expectation of y is y, we get cov x;yE xy\u0000xE y\u0000E xyxyE xy\u0000xy\u0000xyxy TWOSAMPLES 109 Then \u0000xyxy cancels out, leaving \u0000xy which is \u0000E xE y so cov x;yE xy\u0000E xE y Noticethatwhenxandyareuncorrelated,E xyE xE y,sothecovarianceis0in this case."
  },
  {
    "input": "The corrected sum of products SSXY (see p. 123) is given by P P X x y SSXY  xy\u0000 n so covariance is computed as sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 cov x;ySSXY n\u000012 which provides a shortcut formula for the correlation coefficient SSXY r pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi SSX:SSY becausethedegreesoffreedom(n\u00001)cancelout.ThesignofrtakesthesignofSSXY: positive for positive correlations and negative for negative correlations.",
    "target": "Q: What is The corrected sum of products SSXY (see p. 123)? A: The corrected sum of products SSXY (see p. 123) is given by P P X x y SSXY  xy\u0000 n so covariance is computed as sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 cov x;ySSXY n\u000012 which provides a shortcut formula for the correlation coefficient SSXY r pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi SSX:SSY becausethedegreesoffreedom(n\u00001)cancelout.ThesignofrtakesthesignofSSXY: positive for positive correlations and negative for negative correlations."
  },
  {
    "input": "So now you know the definition of the correlation coefficient: it is the covariance divided by the geometric mean of the two variances.",
    "target": "Q: What is So now you know the definition of the correlation coefficient: it? A: So now you know the definition of the correlation coefficient: it is the covariance divided by the geometric mean of the two variances."
  },
  {
    "input": "Thesedatashowthedepthofthewatertable(mbelowthesurface)inwinterandsummer at nine locations: paired<-read.csv(\"c:\\\\temp\\\\water.table.csv\") attach(paired) names(paired) [1]\"Location\"\"Summer\" \"Winter\" Webeginbyaskingwhetherthereisacorrelationbetweensummerandwinterwatertable depths across locations: TWOSAMPLES 111 cor(Summer,Winter) [1]0.8820102 There is a strong positive correlation.",
    "target": "Q: What is Thesedatashowthedepthofthewatertable(mbelowthesurface)inwinterandsummer at nine locations: paired<-read.csv(\"c:\\\\temp\\\\water.table.csv\") attach(paired) names(paired) [1]\"Location\"\"Summer\" \"Winter\" Webeginbyaskingwhetherthereisacorrelationbetweensummerandwinterwatertable depths across locations: TWOSAMPLES 111 cor(Summer,Winter) [1]0.8820102 There? A: Thesedatashowthedepthofthewatertable(mbelowthesurface)inwinterandsummer at nine locations: paired<-read.csv(\"c:\\\\temp\\\\water.table.csv\") attach(paired) names(paired) [1]\"Location\"\"Summer\" \"Winter\" Webeginbyaskingwhetherthereisacorrelationbetweensummerandwinterwatertable depths across locations: TWOSAMPLES 111 cor(Summer,Winter) [1]0.8820102 There is a strong positive correlation."
  },
  {
    "input": "Not surprisingly, places where the water table is highinsummertendtohaveahighwatertableinwinteraswell.Ifyouwanttodetermine thesignificanceofacorrelation(i.e.thepvalueassociatedwiththecalculatedvalueofr) thenusecor.testratherthancor.Thistesthasnon-parametricoptionsforKendallstau orSpearmansrankdependingonthemethodyouspecify(method=\"k\"ormethod=\"s\"), but the default method is Pearsons product-moment correlation (method=\"p\"): cor.test(Summer,Winter) Pearsonsproduct-momentcorrelation data: SummerandWinter t=4.9521,df=7,p-value=0.001652 alternativehypothesis:truecorrelationisnotequalto0 95percentconfidenceinterval: 0.52599840.9750087 sampleestimates: cor 0.8820102 Thecorrelationishighlysignificant(p=0.00165).Now,letusinvestigatetherelation- shipbetweenthecorrelationcoefficientandthethreevariances:thesummervariance,the winter variance, and the variance of the differences (summerwinter): varS<-var(Summer) varW<-var(Winter) varD<-var(Summer-Winter) The correlation coefficient  is related to these three variances by: 22\u00002  y z y\u0000z 2  y z So, using the values we have just calculated, we get a correlation coefficient of (varS+varW-varD)/(2*sqrt(varS)*sqrt(varW)) [1]0.8820102 whichchecksout.Wecanalsoseewhetherthevarianceofthedifferenceisequaltothesum of the component variances (as we saw for independent variables on p. 97): varD [1]0.01015 112 STATISTICS:ANINTRODUCTIONUSINGR varS+varW [1]0.07821389 No,itisnot.Theywouldbeequalonlyifthetwosampleswereindependent.Infact,we knowthatthetwovariablesarepositivelycorrelated,sothevarianceofthedifferenceshould be less than the sum of the variances by an amount equal to 2\u0002r\u0002s \u0002s : 1 2 varS+varW-2*0.8820102*sqrt(varS)*sqrt(varW) [1]0.01015 Thats more like it.",
    "target": "Q: What is Not surprisingly, places where the water table? A: Not surprisingly, places where the water table is highinsummertendtohaveahighwatertableinwinteraswell.Ifyouwanttodetermine thesignificanceofacorrelation(i.e.thepvalueassociatedwiththecalculatedvalueofr) thenusecor.testratherthancor.Thistesthasnon-parametricoptionsforKendallstau orSpearmansrankdependingonthemethodyouspecify(method=\"k\"ormethod=\"s\"), but the default method is Pearsons product-moment correlation (method=\"p\"): cor.test(Summer,Winter) Pearsonsproduct-momentcorrelation data: SummerandWinter t=4.9521,df=7,p-value=0.001652 alternativehypothesis:truecorrelationisnotequalto0 95percentconfidenceinterval: 0.52599840.9750087 sampleestimates: cor 0.8820102 Thecorrelationishighlysignificant(p=0.00165).Now,letusinvestigatetherelation- shipbetweenthecorrelationcoefficientandthethreevariances:thesummervariance,the winter variance, and the variance of the differences (summerwinter): varS<-var(Summer) varW<-var(Winter) varD<-var(Summer-Winter) The correlation coefficient  is related to these three variances by: 22\u00002  y z y\u0000z 2  y z So, using the values we have just calculated, we get a correlation coefficient of (varS+varW-varD)/(2*sqrt(varS)*sqrt(varW)) [1]0.8820102 whichchecksout.Wecanalsoseewhetherthevarianceofthedifferenceisequaltothesum of the component variances (as we saw for independent variables on p. 97): varD [1]0.01015 112 STATISTICS:ANINTRODUCTIONUSINGR varS+varW [1]0.07821389 No,itisnot.Theywouldbeequalonlyifthetwosampleswereindependent.Infact,we knowthatthetwovariablesarepositivelycorrelated,sothevarianceofthedifferenceshould be less than the sum of the variances by an amount equal to 2\u0002r\u0002s \u0002s : 1 2 varS+varW-2*0.8820102*sqrt(varS)*sqrt(varW) [1]0.01015 Thats more like it."
  },
  {
    "input": "Scale-DependentCorrelations Anothermajordifficultywithcorrelationsisthatscatterplotscangiveahighlymisleading impressionofwhatisgoingon.Themoralofthisexerciseisveryimportant:thingsarenot alwaysastheyseem.Thefollowingdatashowthenumberofspeciesofmammalsinforests of differing productivity: data<-read.csv(\"c:\\\\temp\\\\productivity.csv\") attach(data) names(data) [1]\"productivity\"\"mammals\" \"region\" plot(productivity,mammals,pch=16,col=\"blue\") There is a very clear positive correlation: increasing productivity is associated with increasing species richness.",
    "target": "Q: What is Scale-DependentCorrelations Anothermajordifficultywithcorrelationsisthatscatterplotscangiveahighlymisleading impressionofwhatisgoingon.Themoralofthisexerciseisveryimportant:thingsarenot alwaysastheyseem.Thefollowingdatashowthenumberofspeciesofmammalsinforests of differing productivity: data<-read.csv(\"c:\\\\temp\\\\productivity.csv\") attach(data) names(data) [1]\"productivity\"\"mammals\" \"region\" plot(productivity,mammals,pch=16,col=\"blue\") There? A: Scale-DependentCorrelations Anothermajordifficultywithcorrelationsisthatscatterplotscangiveahighlymisleading impressionofwhatisgoingon.Themoralofthisexerciseisveryimportant:thingsarenot alwaysastheyseem.Thefollowingdatashowthenumberofspeciesofmammalsinforests of differing productivity: data<-read.csv(\"c:\\\\temp\\\\productivity.csv\") attach(data) names(data) [1]\"productivity\"\"mammals\" \"region\" plot(productivity,mammals,pch=16,col=\"blue\") There is a very clear positive correlation: increasing productivity is associated with increasing species richness."
  },
  {
    "input": "The correlation is highly significant: TWOSAMPLES 113 cor.test(productivity,mammals,method=\"spearman\") Spearmansrankcorrelationrho data: productivityandmammals S=6515.754,p-value=5.775e-11 alternativehypothesis:truerhoisnotequalto0 sampleestimates: rho 0.7516389 Warningmessage: Incor.test.default(productivity,mammals,method=\"spearman\"): Cannotcomputeexactp-valuewithties Butwhatifwelookattherelationshipforeachregionseparately,usingadifferentcolour for each region?",
    "target": "Q: What is The correlation? A: The correlation is highly significant: TWOSAMPLES 113 cor.test(productivity,mammals,method=\"spearman\") Spearmansrankcorrelationrho data: productivityandmammals S=6515.754,p-value=5.775e-11 alternativehypothesis:truerhoisnotequalto0 sampleestimates: rho 0.7516389 Warningmessage: Incor.test.default(productivity,mammals,method=\"spearman\"): Cannotcomputeexactp-valuewithties Butwhatifwelookattherelationshipforeachregionseparately,usingadifferentcolour for each region?"
  },
  {
    "input": "In every single case, increasing productivity is associated with reduced mammal speciesrichnesswithineachregion.Thelessonisclear: youneedtobe extremely careful when looking at correlations across different scales.",
    "target": "Q: What is In every single case, increasing productivity? A: In every single case, increasing productivity is associated with reduced mammal speciesrichnesswithineachregion.Thelessonisclear: youneedtobe extremely careful when looking at correlations across different scales."
  },
  {
    "input": "7 Regression Regressionanalysisisthestatisticalmethodyouusewhenboththeresponsevariableand theexplanatoryvariablearecontinuousvariables(i.e.realnumberswithdecimalplaces  thingslikeheights,weights,volumes,ortemperatures).Perhapstheeasiestwayofknowing when regression is the appropriate analysis is to see that a scatterplot is the appropriate graphic(incontrasttoanalysisofvariance,say,whentheappropriateplotwouldhavebeen a box-and-whisker or a bar chart).",
    "target": "Q: What is 7 Regression Regressionanalysisisthestatisticalmethodyouusewhenboththeresponsevariableand theexplanatoryvariablearecontinuousvariables(i.e.realnumberswithdecimalplaces  thingslikeheights,weights,volumes,ortemperatures).Perhapstheeasiestwayofknowing when regression? A: 7 Regression Regressionanalysisisthestatisticalmethodyouusewhenboththeresponsevariableand theexplanatoryvariablearecontinuousvariables(i.e.realnumberswithdecimalplaces  thingslikeheights,weights,volumes,ortemperatures).Perhapstheeasiestwayofknowing when regression is the appropriate analysis is to see that a scatterplot is the appropriate graphic(incontrasttoanalysisofvariance,say,whentheappropriateplotwouldhavebeen a box-and-whisker or a bar chart)."
  },
  {
    "input": "Perhaps the most important thing to learn about regressionisthatmodelchoiceisareallybigdeal.Thesimplestmodelofallisthelinearmodel: yabx The response variable is y, and x is a continuous explanatory variable.",
    "target": "Q: What is Perhaps the most important thing to learn about regressionisthatmodelchoiceisareallybigdeal.Thesimplestmodelofallisthelinearmodel: yabx The response variable? A: Perhaps the most important thing to learn about regressionisthatmodelchoiceisareallybigdeal.Thesimplestmodelofallisthelinearmodel: yabx The response variable is y, and x is a continuous explanatory variable."
  },
  {
    "input": "There are two parameters, a and b: the intercept is a (the value of y when x=0); and the slope is b (the slope,orgradient,isthechangeinydividedbythechangeinxwhichbroughtitabout).The slope is so important that it is worth drawing a picture to make clear what is involved.",
    "target": "Q: What is There are two parameters, a and b: the intercept? A: There are two parameters, a and b: the intercept is a (the value of y when x=0); and the slope is b (the slope,orgradient,isthechangeinydividedbythechangeinxwhichbroughtitabout).The slope is so important that it is worth drawing a picture to make clear what is involved."
  },
  {
    "input": "REGRESSION 115 Thetaskistoworkouttheslopeandinterceptofthisnegativelinearrelationshipbetween theresponsevariableandtheexplanatoryvariable.Itiseasiesttostartwiththeinterceptin thiscase,becausethevalueofx=0appearsonthegraph(itdoesnotalways).Theintercept is simply the value of y when x=0.",
    "target": "Q: What is REGRESSION 115 Thetaskistoworkouttheslopeandinterceptofthisnegativelinearrelationshipbetween theresponsevariableandtheexplanatoryvariable.Itiseasiesttostartwiththeinterceptin thiscase,becausethevalueofx=0appearsonthegraph(itdoesnotalways).Theintercept? A: REGRESSION 115 Thetaskistoworkouttheslopeandinterceptofthisnegativelinearrelationshipbetween theresponsevariableandtheexplanatoryvariable.Itiseasiesttostartwiththeinterceptin thiscase,becausethevalueofx=0appearsonthegraph(itdoesnotalways).Theintercept is simply the value of y when x=0."
  },
  {
    "input": "Estimating the slope is slightly more involved because we need to calculate change iny change inxthat brought it about Inpractice,itisagoodideaforprecisiontoselectalargechangeinx.Letustakeitfrom2to 8.Becausetheslopeofthegraphisnegative,thevalueofyislowerwhenx=8thanitiswhen x=2.Atx=2,wedrawabluelineverticallydownwardsfromtheregressionlinetothevalue ofywhenx=8.Thelengthofthisbluelineisthechangeiny(oftendenotedasdeltay,ory insymbols).Nowwedrawahorizontalbrownlineshowingthechangeinxfrom2to8.The lengthofthisbrownlineisx.Whenx=2wecanreadoffthevalueofy(approximately)from thegraph:itisroughly66.Similarly,whenx=8wecanreadoffthevalueofyas24.",
    "target": "Q: What is Estimating the slope? A: Estimating the slope is slightly more involved because we need to calculate change iny change inxthat brought it about Inpractice,itisagoodideaforprecisiontoselectalargechangeinx.Letustakeitfrom2to 8.Becausetheslopeofthegraphisnegative,thevalueofyislowerwhenx=8thanitiswhen x=2.Atx=2,wedrawabluelineverticallydownwardsfromtheregressionlinetothevalue ofywhenx=8.Thelengthofthisbluelineisthechangeiny(oftendenotedasdeltay,ory insymbols).Nowwedrawahorizontalbrownlineshowingthechangeinxfrom2to8.The lengthofthisbrownlineisx.Whenx=2wecanreadoffthevalueofy(approximately)from thegraph:itisroughly66.Similarly,whenx=8wecanreadoffthevalueofyas24."
  },
  {
    "input": "reg.data<-read.csv(\"c:\\\\temp\\\\tannin.csv\") attach(reg.data) names(reg.data) [1]\"growth\"\"tannin\" plot(tannin,growth,pch=21,bg=\"blue\") REGRESSION 117 This is how we do regression by eye.",
    "target": "Q: What is reg.data<-read.csv(\"c:\\\\temp\\\\tannin.csv\") attach(reg.data) names(reg.data) [1]\"growth\"\"tannin\" plot(tannin,growth,pch=21,bg=\"blue\") REGRESSION 117 This? A: reg.data<-read.csv(\"c:\\\\temp\\\\tannin.csv\") attach(reg.data) names(reg.data) [1]\"growth\"\"tannin\" plot(tannin,growth,pch=21,bg=\"blue\") REGRESSION 117 This is how we do regression by eye."
  },
  {
    "input": "Howdidthexvaluechange?Itincreasedfrom0to8,sothechangeinxis+8(whenworking outregressionsbyeye,itisagoodideatotakeasbigarangeofxvaluesaspossible,sohere wetookthecompleterangeofx).Whatisthevalueofywhenx=0?Itisabout12,sothe interceptisroughlya12.Finally,whatisthevalueofb?Itisthechangeiny(\u000010)divided bythechangeinxwhichbroughtitabout(8),sob\u000010/8=\u00001.25.Soourroughguessat the regression equation is y12:0\u00001:25x Thats all there is to it.",
    "target": "Q: What is Howdidthexvaluechange?Itincreasedfrom0to8,sothechangeinxis+8(whenworking outregressionsbyeye,itisagoodideatotakeasbigarangeofxvaluesaspossible,sohere wetookthecompleterangeofx).Whatisthevalueofywhenx=0?Itisabout12,sothe interceptisroughlya12.Finally,whatisthevalueofb?Itisthechangeiny(\u000010)divided bythechangeinxwhichbroughtitabout(8),sob\u000010/8=\u00001.25.Soourroughguessat the regression equation? A: Howdidthexvaluechange?Itincreasedfrom0to8,sothechangeinxis+8(whenworking outregressionsbyeye,itisagoodideatotakeasbigarangeofxvaluesaspossible,sohere wetookthecompleterangeofx).Whatisthevalueofywhenx=0?Itisabout12,sothe interceptisroughlya12.Finally,whatisthevalueofb?Itisthechangeiny(\u000010)divided bythechangeinxwhichbroughtitabout(8),sob\u000010/8=\u00001.25.Soourroughguessat the regression equation is y12:0\u00001:25x Thats all there is to it."
  },
  {
    "input": "It is easy to find out using the R function lm which stands for linear model(notethatthefirstletterofthefunctionnamelmisalowercaseL,notanumberone).",
    "target": "Q: What is It? A: It is easy to find out using the R function lm which stands for linear model(notethatthefirstletterofthefunctionnamelmisalowercaseL,notanumberone)."
  },
  {
    "input": "AllweneeddoistellRwhichofthevariablesistheresponsevariable(growthinthiscase) and which is the explanatory variable (tannin concentration in the diet).",
    "target": "Q: What is AllweneeddoistellRwhichofthevariablesistheresponsevariable(growthinthiscase) and which? A: AllweneeddoistellRwhichofthevariablesistheresponsevariable(growthinthiscase) and which is the explanatory variable (tannin concentration in the diet)."
  },
  {
    "input": "The response variable goes on the left of the tilde  and the explanatory variable goes on the right, likethis:growthtannin.Thisisreadgrowthismodelledasafunctionoftannin.Now we write: lm(growthtannin) Coefficients: (Intercept) tannin 11.756 -1.217 The two parameters are called Coefficients in R: the intercept is11.756 (compared with out guesstimate of 12), and the slope is \u00001.217 (compared with our guesstimate of \u00001.25).",
    "target": "Q: What is The response variable goes on the left of the tilde  and the explanatory variable goes on the right, likethis:growthtannin.Thisisreadgrowthismodelledasafunctionoftannin.Now we write: lm(growthtannin) Coefficients: (Intercept) tannin 11.756 -1.217 The two parameters are called Coefficients in R: the intercept is11.756 (compared with out guesstimate of 12), and the slope? A: The response variable goes on the left of the tilde  and the explanatory variable goes on the right, likethis:growthtannin.Thisisreadgrowthismodelledasafunctionoftannin.Now we write: lm(growthtannin) Coefficients: (Intercept) tannin 11.756 -1.217 The two parameters are called Coefficients in R: the intercept is11.756 (compared with out guesstimate of 12), and the slope is \u00001.217 (compared with our guesstimate of \u00001.25)."
  },
  {
    "input": "That is to say that, given the data, and having selected a linear model, we want to find the values of the slope and intercept that make the data most likely.",
    "target": "Q: What is That? A: That is to say that, given the data, and having selected a linear model, we want to find the values of the slope and intercept that make the data most likely."
  },
  {
    "input": "Keep rereading this sentence until you understand what it is saying.",
    "target": "Q: What is Keep rereading this sentence until you understand what it? A: Keep rereading this sentence until you understand what it is saying."
  },
  {
    "input": "Theleast-squaresestimateoftheregressionslope,b Thebest-fitslopeisfoundbyrotatingthelineunPtiltheerrorsumofsquares,SSE,is minimized,sowewanttofindtheminimumof y\u0000a\u0000bx2.Westartbyfinding the derivative of SSE with respect to b: X dSSE \u00002 x y\u0000a\u0000bx db Now, multiplying through the bracketed term by x gives: X dSSE \u00002 xy\u0000ax\u0000bx2 db Applysummationtoeachtermseparately,setthederivativetozero,anddivideboth sides by \u00002 to remove the unnecessary constant: X X X xy\u0000 ax\u0000 bx2 0 We cannot solve the equation as it stands because there areP2 unknowns, a and b. HPowever, we know the value of a is y\u0000bx.",
    "target": "Q: What is Theleast-squaresestimateoftheregressionslope,b Thebest-fitslopeisfoundbyrotatingthelineunPtiltheerrorsumofsquares,SSE,is minimized,sowewanttofindtheminimumof y\u0000a\u0000bx2.Westartbyfinding the derivative of SSE with respect to b: X dSSE \u00002 x y\u0000a\u0000bx db Now, multiplying through the bracketed term by x gives: X dSSE \u00002 xy\u0000ax\u0000bx2 db Applysummationtoeachtermseparately,setthederivativetozero,anddivideboth sides by \u00002 to remove the unnecessary constant: X X X xy\u0000 ax\u0000 bx2 0 We cannot solve the equation as it stands because there areP2 unknowns, a and b. HPowever, we know the value of a? A: Theleast-squaresestimateoftheregressionslope,b Thebest-fitslopeisfoundbyrotatingthelineunPtiltheerrorsumofsquares,SSE,is minimized,sowewanttofindtheminimumof y\u0000a\u0000bx2.Westartbyfinding the derivative of SSE with respect to b: X dSSE \u00002 x y\u0000a\u0000bx db Now, multiplying through the bracketed term by x gives: X dSSE \u00002 xy\u0000ax\u0000bx2 db Applysummationtoeachtermseparately,setthederivativetozero,anddivideboth sides by \u00002 to remove the unnecessary constant: X X X xy\u0000 ax\u0000 bx2 0 We cannot solve the equation as it stands because there areP2 unknowns, a and b. HPowever, we know the value of a is y\u0000bx."
  },
  {
    "input": "Also, note that ax can be written as a x, so, replacing a and taking both a and b outside their summations gives: (cid:2)P P (cid:3) X X X y x xy\u0000 \u0000b x\u0000b x2 0 n n P Now multiply out the central bracketed term by x to get P P (cid:4)P (cid:5) X 2 X x y x xy\u0000 b \u0000b x2 0 n n Finally,takethetwotermscontainingbtotheright-handside,andnotetheirchangeof sign: P P (cid:4)P (cid:5) X X 2 x y x xy\u0000 b x2\u0000b n n P (cid:4)P (cid:5) Then divide both sides by x2\u0000 x 2=n to obtain the required estimate b: P P P x y xy\u0000 b (cid:4)P n (cid:5) P x 2 x2\u0000 n Thus, the value of b that minimizes the sum of squares of the departures is given simply by (see Box 7.3 for more details): SSXY b SSX This is the maximum likelihood estimate of the slope of the linear regression.",
    "target": "Q: What is Also, note that ax can be written as a x, so, replacing a and taking both a and b outside their summations gives: (cid:2)P P (cid:3) X X X y x xy\u0000 \u0000b x\u0000b x2 0 n n P Now multiply out the central bracketed term by x to get P P (cid:4)P (cid:5) X 2 X x y x xy\u0000 b \u0000b x2 0 n n Finally,takethetwotermscontainingbtotheright-handside,andnotetheirchangeof sign: P P (cid:4)P (cid:5) X X 2 x y x xy\u0000 b x2\u0000b n n P (cid:4)P (cid:5) Then divide both sides by x2\u0000 x 2=n to obtain the required estimate b: P P P x y xy\u0000 b (cid:4)P n (cid:5) P x 2 x2\u0000 n Thus, the value of b that minimizes the sum of squares of the departures? A: Also, note that ax can be written as a x, so, replacing a and taking both a and b outside their summations gives: (cid:2)P P (cid:3) X X X y x xy\u0000 \u0000b x\u0000b x2 0 n n P Now multiply out the central bracketed term by x to get P P (cid:4)P (cid:5) X 2 X x y x xy\u0000 b \u0000b x2 0 n n Finally,takethetwotermscontainingbtotheright-handside,andnotetheirchangeof sign: P P (cid:4)P (cid:5) X X 2 x y x xy\u0000 b x2\u0000b n n P (cid:4)P (cid:5) Then divide both sides by x2\u0000 x 2=n to obtain the required estimate b: P P P x y xy\u0000 b (cid:4)P n (cid:5) P x 2 x2\u0000 n Thus, the value of b that minimizes the sum of squares of the departures is given simply by (see Box 7.3 for more details): SSXY b SSX This is the maximum likelihood estimate of the slope of the linear regression."
  },
  {
    "input": "The difference between eachdata point and thevalue predicted by the model atthe same value ofxiscalledaresidual.Some residuals arepositive(above theline)andothersare negative(belowtheline).Letusdrawverticallinestoindicatethesizeoftheresiduals.The firstxpointisattannin=0.Theyvaluemeasuredatthispointwasgrowth=12.Butwhat is the growth predicted by the model at tannin=0?",
    "target": "Q: What is The difference between eachdata point and thevalue predicted by the model atthe same value ofxiscalledaresidual.Some residuals arepositive(above theline)andothersare negative(belowtheline).Letusdrawverticallinestoindicatethesizeoftheresiduals.The firstxpointisattannin=0.Theyvaluemeasuredatthispointwasgrowth=12.Butwhat? A: The difference between eachdata point and thevalue predicted by the model atthe same value ofxiscalledaresidual.Some residuals arepositive(above theline)andothersare negative(belowtheline).Letusdrawverticallinestoindicatethesizeoftheresiduals.The firstxpointisattannin=0.Theyvaluemeasuredatthispointwasgrowth=12.Butwhat is the growth predicted by the model at tannin=0?"
  },
  {
    "input": "There is a built-in function called predict to work this out: fitted<-predict(lm(growthtannin)) fitted 1 2 3 4 5 6 7 11.75555610.538889 9.322222 8.105556 6.888889 5.672222 4.455556 8 9 3.2388892.022222 Sothefirstpredictedvalueofgrowthis11.755556whentannin=0.Todrawthefirst residual, both x coordinates will be 0.",
    "target": "Q: What is There? A: There is a built-in function called predict to work this out: fitted<-predict(lm(growthtannin)) fitted 1 2 3 4 5 6 7 11.75555610.538889 9.322222 8.105556 6.888889 5.672222 4.455556 8 9 3.2388892.022222 Sothefirstpredictedvalueofgrowthis11.755556whentannin=0.Todrawthefirst residual, both x coordinates will be 0."
  },
  {
    "input": "Our maximum likelihood model is defined as the model that minimizes the sum of the squares of these residuals.Itisuseful,therefore,towritedownexactlywhatanyoneoftheresiduals,d,is:it is the measured value, y, minus the fitted value called ^y (y hat): d y\u0000^y We can improve on this, because we know that ^y is on the straight line abx, so d y\u0000 abxy\u0000a\u0000bx Theequationincludesa\u0000bxbecauseoftheminussignoutsidethebracket.Nowourbest- fitline,bydefinition,isgivenbythevaluesofaaPndbthatminimizethesumsofthesquares oftheds(PseeBoxP7.1).Note,also,thatjustas y\u0000y0(Box4.1),sothesumofthe residuals d  y\u0000a\u0000bx0 (Box 7.2).",
    "target": "Q: What is Our maximum likelihood model? A: Our maximum likelihood model is defined as the model that minimizes the sum of the squares of these residuals.Itisuseful,therefore,towritedownexactlywhatanyoneoftheresiduals,d,is:it is the measured value, y, minus the fitted value called ^y (y hat): d y\u0000^y We can improve on this, because we know that ^y is on the straight line abx, so d y\u0000 abxy\u0000a\u0000bx Theequationincludesa\u0000bxbecauseoftheminussignoutsidethebracket.Nowourbest- fitline,bydefinition,isgivenbythevaluesofaaPndbthatminimizethesumsofthesquares oftheds(PseeBoxP7.1).Note,also,thatjustas y\u0000y0(Box4.1),sothesumofthe residuals d  y\u0000a\u0000bx0 (Box 7.2)."
  },
  {
    "input": "Thesumoftheresidualsinalinearregressioniszero Eachpointonthegraph,y,islocatedatx.Themodelpredictsavalue^yabxwhere a is the intercept and b is the slope.",
    "target": "Q: What is Thesumoftheresidualsinalinearregressioniszero Eachpointonthegraph,y,islocatedatx.Themodelpredictsavalue^yabxwhere a? A: Thesumoftheresidualsinalinearregressioniszero Eachpointonthegraph,y,islocatedatx.Themodelpredictsavalue^yabxwhere a is the intercept and b is the slope."
  },
  {
    "input": "Each residual is the distance between y and the fitted value ^y at location x and we are interested in the sum of the residuals: X y\u0000a\u0000bx which we shall prove is equal to zero.",
    "target": "Q: What is Each residual? A: Each residual is the distance between y and the fitted value ^y at location x and we are interested in the sum of the residuals: X y\u0000a\u0000bx which we shall prove is equal to zero."
  },
  {
    "input": "REGRESSION 121 Togetanoverviewofwhatisinvolved,itisusefultoplotthesumofthesquaresofthe residualsagainstthevalueoftheparameterwearetryingtoestimate.Letustaketheslopeas ourexample.Weknowonethingforcertainaboutourstraight-linemodel;itwillpassthrough thepointinthecentreofthecloudofdatawhosecoordinatesare(x;y).Thebest-fitlinewill bepivotedaboutthemeanvaluesofxandyandourjobistofindthebestvalueforthisslope theonethatminimizesthesumofsquaresoftheredlinesinthegraphabove.Itshouldbe reasonablyclearthatifourestimateoftheslopeistoosteep,thenthefitwillbepoorandthe sumofsquareswillbelarge.Likewise,ifourestimateoftheslopeistooshallow,thenthefit willbepoorandthesumofsquareswillbelarge.Somewherebetweenthesetwoextremes, therewillbeavaluefortheslopethatminimizesthesumofsquares.Thisisthebest-fitvalue thatwewanttodiscover.Firstweneedtoloopthoroughanappropriaterangeofvalueswhich will include the best-fit value (say \u00001.4<b<\u00001.0) and work out the sum of the squared residuals:letuscallthisquantitysse(youwillseewhylater): (cid:129) change the value of the slope b (cid:129) work out the new intercept ay\u0000bx (cid:129) predict the fitted values of growth for each level of tannin abx (cid:129) work out the residuals y\u0000a\u0000bx P (cid:129) square them and add them up, y\u0000a\u0000bx2 (cid:129) associate this value of sse[i] with the current estimate of the slope b[i] Once this process is complete, we can produce a U-shaped graph with the squared residualsontheyaxisandtheestimateoftheslopeonthexaxis.Nowwefindtheminimum valueofsse(itturnsouttobe20.072)anddrawahorizontaldashedgreenline.Atthepoint wherethisminimumtouchesthegraph,wereaddowntothexaxistofindthebestvalueof the slope (the red arrow).",
    "target": "Q: What is REGRESSION 121 Togetanoverviewofwhatisinvolved,itisusefultoplotthesumofthesquaresofthe residualsagainstthevalueoftheparameterwearetryingtoestimate.Letustaketheslopeas ourexample.Weknowonethingforcertainaboutourstraight-linemodel;itwillpassthrough thepointinthecentreofthecloudofdatawhosecoordinatesare(x;y).Thebest-fitlinewill bepivotedaboutthemeanvaluesofxandyandourjobistofindthebestvalueforthisslope theonethatminimizesthesumofsquaresoftheredlinesinthegraphabove.Itshouldbe reasonablyclearthatifourestimateoftheslopeistoosteep,thenthefitwillbepoorandthe sumofsquareswillbelarge.Likewise,ifourestimateoftheslopeistooshallow,thenthefit willbepoorandthesumofsquareswillbelarge.Somewherebetweenthesetwoextremes, therewillbeavaluefortheslopethatminimizesthesumofsquares.Thisisthebest-fitvalue thatwewanttodiscover.Firstweneedtoloopthoroughanappropriaterangeofvalueswhich will include the best-fit value (say \u00001.4<b<\u00001.0) and work out the sum of the squared residuals:letuscallthisquantitysse(youwillseewhylater): (cid:129) change the value of the slope b (cid:129) work out the new intercept ay\u0000bx (cid:129) predict the fitted values of growth for each level of tannin abx (cid:129) work out the residuals y\u0000a\u0000bx P (cid:129) square them and add them up, y\u0000a\u0000bx2 (cid:129) associate this value of sse[i] with the current estimate of the slope b[i] Once this process? A: REGRESSION 121 Togetanoverviewofwhatisinvolved,itisusefultoplotthesumofthesquaresofthe residualsagainstthevalueoftheparameterwearetryingtoestimate.Letustaketheslopeas ourexample.Weknowonethingforcertainaboutourstraight-linemodel;itwillpassthrough thepointinthecentreofthecloudofdatawhosecoordinatesare(x;y).Thebest-fitlinewill bepivotedaboutthemeanvaluesofxandyandourjobistofindthebestvalueforthisslope theonethatminimizesthesumofsquaresoftheredlinesinthegraphabove.Itshouldbe reasonablyclearthatifourestimateoftheslopeistoosteep,thenthefitwillbepoorandthe sumofsquareswillbelarge.Likewise,ifourestimateoftheslopeistooshallow,thenthefit willbepoorandthesumofsquareswillbelarge.Somewherebetweenthesetwoextremes, therewillbeavaluefortheslopethatminimizesthesumofsquares.Thisisthebest-fitvalue thatwewanttodiscover.Firstweneedtoloopthoroughanappropriaterangeofvalueswhich will include the best-fit value (say \u00001.4<b<\u00001.0) and work out the sum of the squared residuals:letuscallthisquantitysse(youwillseewhylater): (cid:129) change the value of the slope b (cid:129) work out the new intercept ay\u0000bx (cid:129) predict the fitted values of growth for each level of tannin abx (cid:129) work out the residuals y\u0000a\u0000bx P (cid:129) square them and add them up, y\u0000a\u0000bx2 (cid:129) associate this value of sse[i] with the current estimate of the slope b[i] Once this process is complete, we can produce a U-shaped graph with the squared residualsontheyaxisandtheestimateoftheslopeonthexaxis.Nowwefindtheminimum valueofsse(itturnsouttobe20.072)anddrawahorizontaldashedgreenline.Atthepoint wherethisminimumtouchesthegraph,wereaddowntothexaxistofindthebestvalueof the slope (the red arrow)."
  },
  {
    "input": "This is the value (b=\u00001.217) that R provided for us earlier.",
    "target": "Q: What is This? A: This is the value (b=\u00001.217) that R provided for us earlier."
  },
  {
    "input": "122 STATISTICS:ANINTRODUCTIONUSINGR Here is the R code that produces the figure and extracts the best estimate of b: b<- seq(-1.43,-1,0.002) sse<-numeric(length(b)) for(iin1:length(b)){ a<-mean(growth)-b[i]*mean(tannin) residual<-growth-a-b[i]*tannin sse[i]<-sum(residual^2) } plot(b,sse,type=\"l\",ylim=c(19,24)) arrows(-1.216,20.07225,-1.216,19,col=\"red\") abline(h=20.07225,col=\"green\",lty=2) lines(b,sse) b[which(sse==min(sse))] CalculationsInvolvedinLinearRegression P P We want to find the minPimum ofP d2 P  y\u0000 Pa\u0000bx2.",
    "target": "Q: What is 122 STATISTICS:ANINTRODUCTIONUSINGR Here? A: 122 STATISTICS:ANINTRODUCTIONUSINGR Here is the R code that produces the figure and extracts the best estimate of b: b<- seq(-1.43,-1,0.002) sse<-numeric(length(b)) for(iin1:length(b)){ a<-mean(growth)-b[i]*mean(tannin) residual<-growth-a-b[i]*tannin sse[i]<-sum(residual^2) } plot(b,sse,type=\"l\",ylim=c(19,24)) arrows(-1.216,20.07225,-1.216,19,col=\"red\") abline(h=20.07225,col=\"green\",lty=2) lines(b,sse) b[which(sse==min(sse))] CalculationsInvolvedinLinearRegression P P We want to find the minPimum ofP d2 P  y\u0000 Pa\u0000bx2."
  },
  {
    "input": "The sum of products is worked out pointwise, so for our data, it is: tannin [1] 0 1 2 3 4 5 6 7 8 growth [1] 12 10 8 11 6 7 2 3 3 tannin*growth [1] 0 10 16 33 24 35 12 21 24 We have 012=0, plus 110=10, plus 28=16, and so on: sum(tannin*growth) [1]175 Thenextthingistousethefamousfivetoworkoutthreeessentialcorrectedsums:the correctedsumofsquaresofx,thecorrectedsumofsquaresofyandthecorrectedsumof products, xy.",
    "target": "Q: What is The sum of products? A: The sum of products is worked out pointwise, so for our data, it is: tannin [1] 0 1 2 3 4 5 6 7 8 growth [1] 12 10 8 11 6 7 2 3 3 tannin*growth [1] 0 10 16 33 24 35 12 21 24 We have 012=0, plus 110=10, plus 28=16, and so on: sum(tannin*growth) [1]175 Thenextthingistousethefamousfivetoworkoutthreeessentialcorrectedsums:the correctedsumofsquaresofx,thecorrectedsumofsquaresofyandthecorrectedsumof products, xy."
  },
  {
    "input": "Correctedsumsofsquaresandproductsinregression ThetotalsumofsquaresisSSY,thesumofsquaresofxisSSX,andthecorrectedsum of products is SSXY: (cid:4)P (cid:5) X 2 y SSY  y2\u0000 n (cid:4)P (cid:5) X 2 x SSX  x2\u0000 n P P X x y SSXY  xy\u0000 n The explained variation is the regression sum of squares, SSR: SSXY2 SSR SSX The unexplained variation is the error sum of squares, SSE, can be obtained by difference SSE SSY \u0000SSR but SSE is defined as the sum of the squares of the residuals, which is X SSE  y\u0000a\u0000bx2 The correlation coefficient, r, is given by SSXY r pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi SSX\u0002SSY 124 STATISTICS:ANINTRODUCTIONUSINGR The next question is how we use SSX, SSY and SSXY to find the maximum likelihood estimatesoftheparametersandtheirassociatedstandarderrors.Itturnsoutthatthisstepis muchsimplerthanwhathasgonebefore.Themaximumlikelihoodestimateoftheslope,b, that we extracted from the graph (above) is just: SSXY b SSX (the detailed derivation of this is in Box 7.1).",
    "target": "Q: What is Correctedsumsofsquaresandproductsinregression ThetotalsumofsquaresisSSY,thesumofsquaresofxisSSX,andthecorrectedsum of products? A: Correctedsumsofsquaresandproductsinregression ThetotalsumofsquaresisSSY,thesumofsquaresofxisSSX,andthecorrectedsum of products is SSXY: (cid:4)P (cid:5) X 2 y SSY  y2\u0000 n (cid:4)P (cid:5) X 2 x SSX  x2\u0000 n P P X x y SSXY  xy\u0000 n The explained variation is the regression sum of squares, SSR: SSXY2 SSR SSX The unexplained variation is the error sum of squares, SSE, can be obtained by difference SSE SSY \u0000SSR but SSE is defined as the sum of the squares of the residuals, which is X SSE  y\u0000a\u0000bx2 The correlation coefficient, r, is given by SSXY r pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi SSX\u0002SSY 124 STATISTICS:ANINTRODUCTIONUSINGR The next question is how we use SSX, SSY and SSXY to find the maximum likelihood estimatesoftheparametersandtheirassociatedstandarderrors.Itturnsoutthatthisstepis muchsimplerthanwhathasgonebefore.Themaximumlikelihoodestimateoftheslope,b, that we extracted from the graph (above) is just: SSXY b SSX (the detailed derivation of this is in Box 7.1)."
  },
  {
    "input": "Nowthatweknowthevalueoftheslope,wecanuseanypointonthefittedstraightlineto workoutthemaximumlikelihoodestimateoftheintercept,a.Onepartofthedefinitionof the best-fit straight line is that it passes through the point (x;y) determined by the mean valuesofxandy.Sinceweknowthatyabx,itmustbethecasethatyabx,andso P P y x ay\u0000bx \u0000b n n We can work out the parameter values for our example.",
    "target": "Q: What is Nowthatweknowthevalueoftheslope,wecanuseanypointonthefittedstraightlineto workoutthemaximumlikelihoodestimateoftheintercept,a.Onepartofthedefinitionof the best-fit straight line? A: Nowthatweknowthevalueoftheslope,wecanuseanypointonthefittedstraightlineto workoutthemaximumlikelihoodestimateoftheintercept,a.Onepartofthedefinitionof the best-fit straight line is that it passes through the point (x;y) determined by the mean valuesofxandy.Sinceweknowthatyabx,itmustbethecasethatyabx,andso P P y x ay\u0000bx \u0000b n n We can work out the parameter values for our example."
  },
  {
    "input": "To keep things as simple as possible,wecancallthevariablesSSX,SSYandSSXY(notethatRiscasesensitivesothe variable SSX is different from ssx): Box7.4.",
    "target": "Q: What is To keep things as simple as possible,wecancallthevariablesSSX,SSYandSSXY(notethatRiscasesensitivesothe variable SSX? A: To keep things as simple as possible,wecancallthevariablesSSX,SSYandSSXY(notethatRiscasesensitivesothe variable SSX is different from ssx): Box7.4."
  },
  {
    "input": "Theshortcutformulaforthesumofproducts,SSXY SSXYisbasedontheexpectationoftheproduct x\u0000x y\u0000y.Startbymultiplyingout the brackets: x\u0000x y\u0000yxy\u0000xy\u0000yxxy P P P P Now apply the summation, remembering that xyy x and yxx y: X X X xy\u0000y x\u0000x ynxy P P We know that xnx and that yny, so X X xy\u0000nyx\u0000nxynxy xy\u0000nxy P P Now replace the product of the two means by x=n\u0002 y=n P P X x y xy\u0000n n n which, on cancelling the ns, gives the corrected sum of products as P P X x y SSXY  xy\u0000 n REGRESSION 125 SSX<-sum(tannin^2)-sum(tannin)^2/length(tannin) SSX [1]60 SSY<-sum(growth^2)-sum(growth)^2/length(growth) SSY [1]108.8889 SSXY<-sum(tannin*growth)-sum(tannin)*sum(growth)/length(tannin) SSXY [1]-73 That is all we need.",
    "target": "Q: What is Theshortcutformulaforthesumofproducts,SSXY SSXYisbasedontheexpectationoftheproduct x\u0000x y\u0000y.Startbymultiplyingout the brackets: x\u0000x y\u0000yxy\u0000xy\u0000yxxy P P P P Now apply the summation, remembering that xyy x and yxx y: X X X xy\u0000y x\u0000x ynxy P P We know that xnx and that yny, so X X xy\u0000nyx\u0000nxynxy xy\u0000nxy P P Now replace the product of the two means by x=n\u0002 y=n P P X x y xy\u0000n n n which, on cancelling the ns, gives the corrected sum of products as P P X x y SSXY  xy\u0000 n REGRESSION 125 SSX<-sum(tannin^2)-sum(tannin)^2/length(tannin) SSX [1]60 SSY<-sum(growth^2)-sum(growth)^2/length(growth) SSY [1]108.8889 SSXY<-sum(tannin*growth)-sum(tannin)*sum(growth)/length(tannin) SSXY [1]-73 That? A: Theshortcutformulaforthesumofproducts,SSXY SSXYisbasedontheexpectationoftheproduct x\u0000x y\u0000y.Startbymultiplyingout the brackets: x\u0000x y\u0000yxy\u0000xy\u0000yxxy P P P P Now apply the summation, remembering that xyy x and yxx y: X X X xy\u0000y x\u0000x ynxy P P We know that xnx and that yny, so X X xy\u0000nyx\u0000nxynxy xy\u0000nxy P P Now replace the product of the two means by x=n\u0002 y=n P P X x y xy\u0000n n n which, on cancelling the ns, gives the corrected sum of products as P P X x y SSXY  xy\u0000 n REGRESSION 125 SSX<-sum(tannin^2)-sum(tannin)^2/length(tannin) SSX [1]60 SSY<-sum(growth^2)-sum(growth)^2/length(growth) SSY [1]108.8889 SSXY<-sum(tannin*growth)-sum(tannin)*sum(growth)/length(tannin) SSXY [1]-73 That is all we need."
  },
  {
    "input": "So the slope is SSXY \u000073 b  \u00001:2166667 SSX 60 and the intercept is given by P P y x 62 36 a \u0000b:  1:2166667 6:88894:8666711:755556 n n 9 9 Now we can write the maximum likelihood regression equation in full: y11:75556\u00001:216667x This, however, is only half of the story.",
    "target": "Q: What is So the slope? A: So the slope is SSXY \u000073 b  \u00001:2166667 SSX 60 and the intercept is given by P P y x 62 36 a \u0000b:  1:2166667 6:88894:8666711:755556 n n 9 9 Now we can write the maximum likelihood regression equation in full: y11:75556\u00001:216667x This, however, is only half of the story."
  },
  {
    "input": "The variation that is explained by the model is called the regression sum of squares (denoted by SSR), and the unexplained variationiscalledtheerrorsumofsquares(denotedbySSE;thisisthesumofthesquaresof thelengthsoftheredlinesthatwedrewonthescatterplotonp.119).ThenSSY=SSR+SSE (the proof is given in Box 7.5).",
    "target": "Q: What is The variation that? A: The variation that is explained by the model is called the regression sum of squares (denoted by SSR), and the unexplained variationiscalledtheerrorsumofsquares(denotedbySSE;thisisthesumofthesquaresof thelengthsoftheredlinesthatwedrewonthescatterplotonp.119).ThenSSY=SSR+SSE (the proof is given in Box 7.5)."
  },
  {
    "input": "The difference between y and y is the sum of the differences y\u0000^y and ^y\u0000y, as you can see from the figure: Inspection of the figure shows clearly that y\u0000y y\u0000^y ^y\u0000y.",
    "target": "Q: What is The difference between y and y? A: The difference between y and y is the sum of the differences y\u0000^y and ^y\u0000y, as you can see from the figure: Inspection of the figure shows clearly that y\u0000y y\u0000^y ^y\u0000y."
  },
  {
    "input": "It is not obvious, however, that the sums of the squares of these three quantities should be equal.",
    "target": "Q: What is It? A: It is not obvious, however, that the sums of the squares of these three quantities should be equal."
  },
  {
    "input": "First, square the terms on both sides of the equation y\u0000y2  y\u0000^y2 ^y\u0000y2 to get y2\u00002yyy2 y2\u00002y^y^y2^y2\u00002^yyy2 From both sides, subtract y2y2 to leave \u00002yy\u00002y^y2^y2\u00002^yy Apply summation then set to zero: X X X X 0 2yy\u0000 2y^y 2^y2\u0000 2^yy Now group the terms with and without y: X X X X 0 2yy\u0000 2^yy 2^y2\u0000 2y^y REGRESSION 127 Because y is a constant, we can take it outside the summation along with the 2: X X 02y y\u0000^y2 ^y ^y\u0000y P WPealreadyknowthat y\u0000^y0fromBox7.2,soallthatremainsistoproveisthat 2 ^y ^y\u0000y0.",
    "target": "Q: What is First, square the terms on both sides of the equation y\u0000y2  y\u0000^y2 ^y\u0000y2 to get y2\u00002yyy2 y2\u00002y^y^y2^y2\u00002^yyy2 From both sides, subtract y2y2 to leave \u00002yy\u00002y^y2^y2\u00002^yy Apply summation then set to zero: X X X X 0 2yy\u0000 2y^y 2^y2\u0000 2^yy Now group the terms with and without y: X X X X 0 2yy\u0000 2^yy 2^y2\u0000 2y^y REGRESSION 127 Because y? A: First, square the terms on both sides of the equation y\u0000y2  y\u0000^y2 ^y\u0000y2 to get y2\u00002yyy2 y2\u00002y^y^y2^y2\u00002^yyy2 From both sides, subtract y2y2 to leave \u00002yy\u00002y^y2^y2\u00002^yy Apply summation then set to zero: X X X X 0 2yy\u0000 2y^y 2^y2\u0000 2^yy Now group the terms with and without y: X X X X 0 2yy\u0000 2^yy 2^y2\u0000 2y^y REGRESSION 127 Because y is a constant, we can take it outside the summation along with the 2: X X 02y y\u0000^y2 ^y ^y\u0000y P WPealreadyknowthat y\u0000^y0fromBox7.2,soallthatremainsistoproveisthat 2 ^y ^y\u0000y0."
  },
  {
    "input": "First multiply through the bracket to get^y2\u0000y^y, then replace the ^y: yb x\u0000x yb x\u0000x\u0000y yb x\u0000x This is the gory bit.",
    "target": "Q: What is First multiply through the bracket to get^y2\u0000y^y, then replace the ^y: yb x\u0000x yb x\u0000x\u0000y yb x\u0000x This? A: First multiply through the bracket to get^y2\u0000y^y, then replace the ^y: yb x\u0000x yb x\u0000x\u0000y yb x\u0000x This is the gory bit."
  },
  {
    "input": "This is because SSRb:SSXY 128 STATISTICS:ANINTRODUCTIONUSINGR so we can immediately work out SSR\u00001:21667\u0002\u00007388:81667.",
    "target": "Q: What is This? A: This is because SSRb:SSXY 128 STATISTICS:ANINTRODUCTIONUSINGR so we can immediately work out SSR\u00001:21667\u0002\u00007388:81667."
  },
  {
    "input": "And since SSY= SSR+SSE we can get SSE by subtraction: SSE SSY \u0000SSR108:8889\u000088:8166720:07222 These components are now drawn together in what is known as the ANOVA table.",
    "target": "Q: What is And since SSY= SSR+SSE we can get SSE by subtraction: SSE SSY \u0000SSR108:8889\u000088:8166720:07222 These components are now drawn together in what? A: And since SSY= SSR+SSE we can get SSE by subtraction: SSE SSY \u0000SSR108:8889\u000088:8166720:07222 These components are now drawn together in what is known as the ANOVA table."
  },
  {
    "input": "The easiest to deal with is the total sum of squares, bPecause it always has the same formulaforitsdegreesoffreedom.ThedefinitionisSSY  y\u0000y2,andyoucanseethat there is just one parameter estimated from the data: the mean value, y.",
    "target": "Q: What is The easiest to deal with? A: The easiest to deal with is the total sum of squares, bPecause it always has the same formulaforitsdegreesoffreedom.ThedefinitionisSSY  y\u0000y2,andyoucanseethat there is just one parameter estimated from the data: the mean value, y."
  },
  {
    "input": "The next easiest to work out is theerrorsumofsquares.LetuslookatitsformulatosePehowmanyparametersneedtobe estimatedfromthedatabeforewecanworkoutSSE  y\u0000a\u0000bx2.Weneedtoknow thevaluesofbothaandbbeforewecancalculateSSE.Theseareestimatedfromthedata,so thedegreesoffreedomforerroraren\u00002.Thisisimportant,sorereadthelastsentenceifyou donotseeityet.Themostdifficultofthethreeistheregressiondegreesoffreedom,because youneedtothinkabout thisoneinadifferentway.",
    "target": "Q: What is The next easiest to work out? A: The next easiest to work out is theerrorsumofsquares.LetuslookatitsformulatosePehowmanyparametersneedtobe estimatedfromthedatabeforewecanworkoutSSE  y\u0000a\u0000bx2.Weneedtoknow thevaluesofbothaandbbeforewecancalculateSSE.Theseareestimatedfromthedata,so thedegreesoffreedomforerroraren\u00002.Thisisimportant,sorereadthelastsentenceifyou donotseeityet.Themostdifficultofthethreeistheregressiondegreesoffreedom,because youneedtothinkabout thisoneinadifferentway."
  },
  {
    "input": "TocompletetheANOVAtable,weneedtounderstandthefourthcolumn,headedMean squares.Thiscolumncontainsthevariances,onwhichanalysisofvarianceisbased.The key point to recall is that sum of squares variance degrees of freedom Thisis very easy to calculate in thecontext of the ANOVA table, because the relevant sums of squares and degrees of freedom are in adjacent columns.",
    "target": "Q: What is TocompletetheANOVAtable,weneedtounderstandthefourthcolumn,headedMean squares.Thiscolumncontainsthevariances,onwhichanalysisofvarianceisbased.The key point to recall? A: TocompletetheANOVAtable,weneedtounderstandthefourthcolumn,headedMean squares.Thiscolumncontainsthevariances,onwhichanalysisofvarianceisbased.The key point to recall is that sum of squares variance degrees of freedom Thisis very easy to calculate in thecontext of the ANOVA table, because the relevant sums of squares and degrees of freedom are in adjacent columns."
  },
  {
    "input": "Traditionally, one doesnotfillinthebottombox(itwouldbetheoverallvarianceiny,SSY/(n\u00001)).Finally, REGRESSION 129 the ANOVA table is completed by working out the F ratio, which is a ratio between two variances.",
    "target": "Q: What is Traditionally, one doesnotfillinthebottombox(itwouldbetheoverallvarianceiny,SSY/(n\u00001)).Finally, REGRESSION 129 the ANOVA table? A: Traditionally, one doesnotfillinthebottombox(itwouldbetheoverallvarianceiny,SSY/(n\u00001)).Finally, REGRESSION 129 the ANOVA table is completed by working out the F ratio, which is a ratio between two variances."
  },
  {
    "input": "Here is the completed ANOVA table: Source Sumofsquares Degreesoffreedom Meansquares Fratio Regression 88.817 1 88.817 30.974 Error 20.072 7 s2=2.86746 Total 108.889 8 Noticethatthecomponentdegreesoffreedomadduptothetotaldegreesoffreedom(this is always true, in any ANOVA table, and is a good check on your understanding of the designoftheexperiment).ThelastquestionconcernsthemagnitudeoftheFratio=30.974: isitbigenoughtojustifyrejectionofthenullhypothesis?ThecriticalvalueoftheFratiois thevalueofFthatwouldariseduetochancealonewhenthenullhypothesiswastrue,given thatwehave1d.f.inthenumeratorand7d.f.inthedenominator.Wehavetodecideonthe levelofuncertaintythatwearewillingtoputupwith;thetraditionalvalueforworklikethis is5%,soourcertaintyis0.95.NowwecanusequantilesoftheFdistribution,qf,tofindthe critical value: qf(0.95,1,7) [1]5.591448 Because our calculated value of F (the test statistic=30.974) is much larger than the criticalvalue(5.591),wecanbeconfidentinrejectingthenullhypothesis.Perhapsabetter thing to do, rather than working rigidly at the 5% uncertainty level, is to ask what is the probabilityofgettingavalueforFasbigas30.974orlargerifthenullhypothesisistrue.For this we use 1-pf rather than qf: 1-pf(30.974,1,7) [1]0.0008460725 It is very unlikely indeed (p<0.001).",
    "target": "Q: What is Here? A: Here is the completed ANOVA table: Source Sumofsquares Degreesoffreedom Meansquares Fratio Regression 88.817 1 88.817 30.974 Error 20.072 7 s2=2.86746 Total 108.889 8 Noticethatthecomponentdegreesoffreedomadduptothetotaldegreesoffreedom(this is always true, in any ANOVA table, and is a good check on your understanding of the designoftheexperiment).ThelastquestionconcernsthemagnitudeoftheFratio=30.974: isitbigenoughtojustifyrejectionofthenullhypothesis?ThecriticalvalueoftheFratiois thevalueofFthatwouldariseduetochancealonewhenthenullhypothesiswastrue,given thatwehave1d.f.inthenumeratorand7d.f.inthedenominator.Wehavetodecideonthe levelofuncertaintythatwearewillingtoputupwith;thetraditionalvalueforworklikethis is5%,soourcertaintyis0.95.NowwecanusequantilesoftheFdistribution,qf,tofindthe critical value: qf(0.95,1,7) [1]5.591448 Because our calculated value of F (the test statistic=30.974) is much larger than the criticalvalue(5.591),wecanbeconfidentinrejectingthenullhypothesis.Perhapsabetter thing to do, rather than working rigidly at the 5% uncertainty level, is to ask what is the probabilityofgettingavalueforFasbigas30.974orlargerifthenullhypothesisistrue.For this we use 1-pf rather than qf: 1-pf(30.974,1,7) [1]0.0008460725 It is very unlikely indeed (p<0.001)."
  },
  {
    "input": "Thestandarderroroftheregressionslope This quantity is given by rffiffiffiffiffiffiffiffi s2 SE  b SSX The error variance s2 comes from the ANOVA table and is the quantity used in calculatingstandarderrorsandconfidenceintervalsfortheparameters,andincarrying outhypothesistesting.SSXmeasuresthespreadofthexvaluesalongthexaxis.Recall that standard errors are unreliability estimates.",
    "target": "Q: What is Thestandarderroroftheregressionslope This quantity? A: Thestandarderroroftheregressionslope This quantity is given by rffiffiffiffiffiffiffiffi s2 SE  b SSX The error variance s2 comes from the ANOVA table and is the quantity used in calculatingstandarderrorsandconfidenceintervalsfortheparameters,andincarrying outhypothesistesting.SSXmeasuresthespreadofthexvaluesalongthexaxis.Recall that standard errors are unreliability estimates."
  },
  {
    "input": "The difference is that the left-hand graph has all of its x values close to the mean valueofx,whilethegraphontherighthasabroadspanofxvalues.Whichofthese doyouthinkwouldgivethemostreliableestimateoftheslope?Itisprettyclearthat it is the graph on the right, with the wider range of x values.",
    "target": "Q: What is The difference? A: The difference is that the left-hand graph has all of its x values close to the mean valueofx,whilethegraphontherighthasabroadspanofxvalues.Whichofthese doyouthinkwouldgivethemostreliableestimateoftheslope?Itisprettyclearthat it is the graph on the right, with the wider range of x values."
  },
  {
    "input": "What is the purpose of the big square root term?",
    "target": "Q: What is What? A: What is the purpose of the big square root term?"
  },
  {
    "input": "This is there to make sure that the units of the unreliability estimate are the same as the units of the parameter whose unreliability is being assessed.Theerrorvarianceisinunitsofysquared,buttheslopeisinunitsofyper unit change in x.",
    "target": "Q: What is This? A: This is there to make sure that the units of the unreliability estimate are the same as the units of the parameter whose unreliability is being assessed.Theerrorvarianceisinunitsofysquared,buttheslopeisinunitsofyper unit change in x."
  },
  {
    "input": "Thestandarderroroftheintercept This quantity is given by rffiffiffiffiffiffiPffiffiffiffiffiffiffiffiffiffi s2 x2 SE  a n\u0002SSX whichisliketheformulaforthestandarderroroftheslope,butwithtwoadditional terms.",
    "target": "Q: What is Thestandarderroroftheintercept This quantity? A: Thestandarderroroftheintercept This quantity is given by rffiffiffiffiffiffiPffiffiffiffiffiffiffiffiffiffi s2 x2 SE  a n\u0002SSX whichisliketheformulaforthestandarderroroftheslope,butwithtwoadditional terms."
  },
  {
    "input": "Uncertainty declines withPincreasing sample size n. It is less clear why uncertainty should increase with x2.",
    "target": "Q: What is Uncertainty declines withPincreasing sample size n. It? A: Uncertainty declines withPincreasing sample size n. It is less clear why uncertainty should increase with x2."
  },
  {
    "input": "The reason for this is that uncertainty in the estimate of the intercept increases, the further away from the intercept that the meanvalueofxlies.Youcanseethisfromthefollowinggraphsofy=3+2x(solid black lines).",
    "target": "Q: What is The reason for this? A: The reason for this is that uncertainty in the estimate of the intercept increases, the further away from the intercept that the meanvalueofxlies.Youcanseethisfromthefollowinggraphsofy=3+2x(solid black lines)."
  },
  {
    "input": "On the left is a graph with a low value of x2 and on the right an identicalgraph(sameslopeandintercept)butestimatedfromadatasetwithahigher value of x9.",
    "target": "Q: What is On the left? A: On the left is a graph with a low value of x2 and on the right an identicalgraph(sameslopeandintercept)butestimatedfromadatasetwithahigher value of x9."
  },
  {
    "input": "In both cases there is a variation inthe slope from b=1.5 to b=2.5 (dotted blue lines).",
    "target": "Q: What is In both cases there? A: In both cases there is a variation inthe slope from b=1.5 to b=2.5 (dotted blue lines)."
  },
  {
    "input": "In general, the standard error for a predicted value^y is given by: sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi (cid:2) (cid:3) 1 x\u0000x2 SE^y  s2 n  SSX Notethattheformulaforthestandarderroroftheinterceptisjustthespecialcaseof this for x=0.",
    "target": "Q: What is In general, the standard error for a predicted value^y? A: In general, the standard error for a predicted value^y is given by: sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi (cid:2) (cid:3) 1 x\u0000x2 SE^y  s2 n  SSX Notethattheformulaforthestandarderroroftheinterceptisjustthespecialcaseof this for x=0."
  },
  {
    "input": "132 STATISTICS:ANINTRODUCTIONUSINGR The formula for the standard error of the intercept is a little more involved (Box 7.7): rffiffiffiffiffiffiPffiffiffiffiffiffiffiffiffiffi rffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi s2 x2 2:867\u0002204 SE   1:0408 a n\u0002SSX 9\u000260 Nowthatweknowwhereallthenumberscomefrom,wecanrepeattheanalysisinRand see just how straightforward it is.",
    "target": "Q: What is 132 STATISTICS:ANINTRODUCTIONUSINGR The formula for the standard error of the intercept? A: 132 STATISTICS:ANINTRODUCTIONUSINGR The formula for the standard error of the intercept is a little more involved (Box 7.7): rffiffiffiffiffiffiPffiffiffiffiffiffiffiffiffiffi rffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi s2 x2 2:867\u0002204 SE   1:0408 a n\u0002SSX 9\u000260 Nowthatweknowwhereallthenumberscomefrom,wecanrepeattheanalysisinRand see just how straightforward it is."
  },
  {
    "input": "It is good practice to give the statistical model a name: model is as good as any.",
    "target": "Q: What is It? A: It is good practice to give the statistical model a name: model is as good as any."
  },
  {
    "input": "If you want to see the ANOVA table rather than the parameter estimates, then the appropriate function is summary.aov: summary.aov(model) Df SumSq MeanSq Fvalue Pr(>F) tannin 1 88.82 88.82 30.97 0.000846*** Residuals 7 20.07 2.87 Thisshowstheerrorvariance(s2=2.87)alongwithSSR(88.82)andSSE(20.07),and thepvaluewejustcomputedusing1-pf.Ofthetwosortsofsummarytable,summary.",
    "target": "Q: What is If you want to see the ANOVA table rather than the parameter estimates, then the appropriate function? A: If you want to see the ANOVA table rather than the parameter estimates, then the appropriate function is summary.aov: summary.aov(model) Df SumSq MeanSq Fvalue Pr(>F) tannin 1 88.82 88.82 30.97 0.000846*** Residuals 7 20.07 2.87 Thisshowstheerrorvariance(s2=2.87)alongwithSSR(88.82)andSSE(20.07),and thepvaluewejustcomputedusing1-pf.Ofthetwosortsofsummarytable,summary."
  },
  {
    "input": "CanwecombinewhatwehavelearnedaboutSSY,SSRandSSEintoameasureoffitthat hastheseproperties?Ourproposedmetricisthefractionofthetotalvariationinythatis explainedbytheregression.ThetotalvariationisSSYandtheexplainedvariationisSSR, so our measure  let us call it r2  is given by SSR r2  SSY Thisvariesfrom1,whentheregressionexplainsallofthevariationiny(SSR=SSY),to0 when the regression explains none of the variation in y (SSE=SSY).",
    "target": "Q: What is CanwecombinewhatwehavelearnedaboutSSY,SSRandSSEintoameasureoffitthat hastheseproperties?Ourproposedmetricisthefractionofthetotalvariationinythatis explainedbytheregression.ThetotalvariationisSSYandtheexplainedvariationisSSR, so our measure  let us call it r2? A: CanwecombinewhatwehavelearnedaboutSSY,SSRandSSEintoameasureoffitthat hastheseproperties?Ourproposedmetricisthefractionofthetotalvariationinythatis explainedbytheregression.ThetotalvariationisSSYandtheexplainedvariationisSSR, so our measure  let us call it r2  is given by SSR r2  SSY Thisvariesfrom1,whentheregressionexplainsallofthevariationiny(SSR=SSY),to0 when the regression explains none of the variation in y (SSE=SSY)."
  },
  {
    "input": "ModelChecking The final thing you will want to do is to expose the model to critical appraisal.",
    "target": "Q: What is ModelChecking The final thing you will want to do? A: ModelChecking The final thing you will want to do is to expose the model to critical appraisal."
  },
  {
    "input": "The simplest way to do this is with four built-in model-checking plots: par(mfrow=c(2,2)) plot(model) The first graph (top left) shows residuals on the y axis against fitted values on the x axis.Ittakesexperiencetointerprettheseplots,butwhatyoudonotwanttoseeislotsof structure or pattern in the plot.",
    "target": "Q: What is The simplest way to do this? A: The simplest way to do this is with four built-in model-checking plots: par(mfrow=c(2,2)) plot(model) The first graph (top left) shows residuals on the y axis against fitted values on the x axis.Ittakesexperiencetointerprettheseplots,butwhatyoudonotwanttoseeislotsof structure or pattern in the plot."
  },
  {
    "input": "It is a major problem if the scatter increases as the fitted values get bigger; REGRESSION 135 thiswouldshowuplikeawedgeofcheeseonitsside(likethisorlesscommonlylike this;seep.65).Butinourpresentcase,everythingisOKontheconstancyofvariance front.",
    "target": "Q: What is It? A: It is a major problem if the scatter increases as the fitted values get bigger; REGRESSION 135 thiswouldshowuplikeawedgeofcheeseonitsside(likethisorlesscommonlylike this;seep.65).Butinourpresentcase,everythingisOKontheconstancyofvariance front."
  },
  {
    "input": "The next plot (top right) is the normal quantilequantile plot (qqnorm, p. 79) which shouldbeastraightlineiftheerrorsarenormallydistributed.Again,thepresentexample looksfine.IfthepatternwereS-shapedorbanana-shaped,wewouldneedtofitadifferent model to the data.",
    "target": "Q: What is The next plot (top right)? A: The next plot (top right) is the normal quantilequantile plot (qqnorm, p. 79) which shouldbeastraightlineiftheerrorsarenormallydistributed.Again,thepresentexample looksfine.IfthepatternwereS-shapedorbanana-shaped,wewouldneedtofitadifferent model to the data."
  },
  {
    "input": "Thethirdplot(bottomleft)islikethefirst,butonadifferentscale;itshowsthesquareroot ofthestandardizedresiduals(whereallthevaluesarepositive)againstthefittedvalues;if there was a problem, the points would be distributed inside a triangular shape, with the scatteroftheresidualsincreasingasthefittedvaluesincrease.Butthereisnosuchpattern here, which is good.",
    "target": "Q: What is Thethirdplot(bottomleft)islikethefirst,butonadifferentscale;itshowsthesquareroot ofthestandardizedresiduals(whereallthevaluesarepositive)againstthefittedvalues;if there was a problem, the points would be distributed inside a triangular shape, with the scatteroftheresidualsincreasingasthefittedvaluesincrease.Butthereisnosuchpattern here, which? A: Thethirdplot(bottomleft)islikethefirst,butonadifferentscale;itshowsthesquareroot ofthestandardizedresiduals(whereallthevaluesarepositive)againstthefittedvalues;if there was a problem, the points would be distributed inside a triangular shape, with the scatteroftheresidualsincreasingasthefittedvaluesincrease.Butthereisnosuchpattern here, which is good."
  },
  {
    "input": "The fourth and final plot (lower right) is all about highlighting influential points (p.148);thesearethepointsonthegraphthathavethebiggesteffectsontheparameter estimates.",
    "target": "Q: What is The fourth and final plot (lower right)? A: The fourth and final plot (lower right) is all about highlighting influential points (p.148);thesearethepointsonthegraphthathavethebiggesteffectsontheparameter estimates."
  },
  {
    "input": "In my opinion, this information is more clearly displayed in tabular form; try influence.measures(model).",
    "target": "Q: What is In my opinion, this information? A: In my opinion, this information is more clearly displayed in tabular form; try influence.measures(model)."
  },
  {
    "input": "It is the top two graphs of plot(model) that are most important, and you should concentrate on these.",
    "target": "Q: What is It? A: It is the top two graphs of plot(model) that are most important, and you should concentrate on these."
  },
  {
    "input": "The important point is that we always do model-checking; the summary(model) table is not the end of the process of regression analysis.",
    "target": "Q: What is The important point? A: The important point is that we always do model-checking; the summary(model) table is not the end of the process of regression analysis."
  },
  {
    "input": "136 STATISTICS:ANINTRODUCTIONUSINGR It is straightforward to estimate the parameters of such models if the equations can be transformedsothatthey becomelinearintheirparameters.Anexample shouldmake this clear.",
    "target": "Q: What is 136 STATISTICS:ANINTRODUCTIONUSINGR It? A: 136 STATISTICS:ANINTRODUCTIONUSINGR It is straightforward to estimate the parameters of such models if the equations can be transformedsothatthey becomelinearintheirparameters.Anexample shouldmake this clear."
  },
  {
    "input": "Here is a summary of the linear model applied to these data: summary(lm(amounttime)) Coefficients: Estimate Std.Error tvalue Pr(>|t|) (Intercept) 84.5534 5.0277 16.82 <2e-16*** time -2.8272 0.2879 -9.82 9.94e-11*** Residualstandarderror:14.34on29degreesoffreedom MultipleR-squared:0.7688, AdjustedR-squared:0.7608 F-statistic:96.44on1and29DF,p-value:9.939e-11 Themodelexplainsmorethan76%ofthevariationintheresponse(averyhighvalueof r-squared)andthepvalueisvanishinglysmall.Themoralisthatpvaluesandr-squaredare not good measures of model adequacy.",
    "target": "Q: What is Here? A: Here is a summary of the linear model applied to these data: summary(lm(amounttime)) Coefficients: Estimate Std.Error tvalue Pr(>|t|) (Intercept) 84.5534 5.0277 16.82 <2e-16*** time -2.8272 0.2879 -9.82 9.94e-11*** Residualstandarderror:14.34on29degreesoffreedom MultipleR-squared:0.7688, AdjustedR-squared:0.7608 F-statistic:96.44on1and29DF,p-value:9.939e-11 Themodelexplainsmorethan76%ofthevariationintheresponse(averyhighvalueof r-squared)andthepvalueisvanishinglysmall.Themoralisthatpvaluesandr-squaredare not good measures of model adequacy."
  },
  {
    "input": "Let us try taking logs of both sides y ae \u0000bx log ylog a\u0000bx If we replace log(y) by Y and log(a) by A, you can see that we have a linear model: Y A\u0000bx TheinterceptofthislinearmodelisAandtheslopeis\u0000b.Tofitthemodelwehavethe untransformed values of time on the x axis and the log of amount on the y axis: plot(time,log(amount),pch=21,col=\"blue\",bg=\"red\") abline(lm(log(amount)time),col=\"blue\") 138 STATISTICS:ANINTRODUCTIONUSINGR The fit to the model is greatly improved.",
    "target": "Q: What is Let us try taking logs of both sides y ae \u0000bx log ylog a\u0000bx If we replace log(y) by Y and log(a) by A, you can see that we have a linear model: Y A\u0000bx TheinterceptofthislinearmodelisAandtheslopeis\u0000b.Tofitthemodelwehavethe untransformed values of time on the x axis and the log of amount on the y axis: plot(time,log(amount),pch=21,col=\"blue\",bg=\"red\") abline(lm(log(amount)time),col=\"blue\") 138 STATISTICS:ANINTRODUCTIONUSINGR The fit to the model? A: Let us try taking logs of both sides y ae \u0000bx log ylog a\u0000bx If we replace log(y) by Y and log(a) by A, you can see that we have a linear model: Y A\u0000bx TheinterceptofthislinearmodelisAandtheslopeis\u0000b.Tofitthemodelwehavethe untransformed values of time on the x axis and the log of amount on the y axis: plot(time,log(amount),pch=21,col=\"blue\",bg=\"red\") abline(lm(log(amount)time),col=\"blue\") 138 STATISTICS:ANINTRODUCTIONUSINGR The fit to the model is greatly improved."
  },
  {
    "input": "There is a new issue, however, in that the variance appears to increase with time and, as you will recall, non-constant variance is a potentiallyseriousproblem.Letusestimatetheparametervaluesofthisexponentialmodel and then check its assumptions using plot(model).",
    "target": "Q: What is There? A: There is a new issue, however, in that the variance appears to increase with time and, as you will recall, non-constant variance is a potentiallyseriousproblem.Letusestimatetheparametervaluesofthisexponentialmodel and then check its assumptions using plot(model)."
  },
  {
    "input": "model<-lm(log(amount)time) summary(model) Coefficients: Estimate Std.Error tvalue Pr(>|t|) (Intercept) 4.547386 0.100295 45.34 <2e-16*** time -0.068528 0.005743 -11.93 1.04e-12*** Residualstandarderror:0.286on29degreesoffreedom MultipleR-squared:0.8308, AdjustedR-squared:0.825 F-statistic:142.4on1and29DF, p-value:1.038e-12 Theslopeofthestraightlineis\u00000.068528anditsstandarderroris0.005743.Thevalueof r2 is even higher following transformation (83%) and the p value is even lower.",
    "target": "Q: What is model<-lm(log(amount)time) summary(model) Coefficients: Estimate Std.Error tvalue Pr(>|t|) (Intercept) 4.547386 0.100295 45.34 <2e-16*** time -0.068528 0.005743 -11.93 1.04e-12*** Residualstandarderror:0.286on29degreesoffreedom MultipleR-squared:0.8308, AdjustedR-squared:0.825 F-statistic:142.4on1and29DF, p-value:1.038e-12 Theslopeofthestraightlineis\u00000.068528anditsstandarderroris0.005743.Thevalueof r2? A: model<-lm(log(amount)time) summary(model) Coefficients: Estimate Std.Error tvalue Pr(>|t|) (Intercept) 4.547386 0.100295 45.34 <2e-16*** time -0.068528 0.005743 -11.93 1.04e-12*** Residualstandarderror:0.286on29degreesoffreedom MultipleR-squared:0.8308, AdjustedR-squared:0.825 F-statistic:142.4on1and29DF, p-value:1.038e-12 Theslopeofthestraightlineis\u00000.068528anditsstandarderroris0.005743.Thevalueof r2 is even higher following transformation (83%) and the p value is even lower."
  },
  {
    "input": "upper<-4.547386+ 0.100295 lower<-4.547386- 0.100295 Now we return to the original scale of measurement by taking antilogs using exp: exp(upper) [1]104.3427 exp(lower) [1]85.37822 sotheinterceptontheoriginalaxisisbetween85.38and104.34,butthebestestimateforthe intercept is exp(4.547386) [1]94.38536 which means that the interval above the intercept is 9.957 but the interval below it is 9.007.",
    "target": "Q: What is upper<-4.547386+ 0.100295 lower<-4.547386- 0.100295 Now we return to the original scale of measurement by taking antilogs using exp: exp(upper) [1]104.3427 exp(lower) [1]85.37822 sotheinterceptontheoriginalaxisisbetween85.38and104.34,butthebestestimateforthe intercept? A: upper<-4.547386+ 0.100295 lower<-4.547386- 0.100295 Now we return to the original scale of measurement by taking antilogs using exp: exp(upper) [1]104.3427 exp(lower) [1]85.37822 sotheinterceptontheoriginalaxisisbetween85.38and104.34,butthebestestimateforthe intercept is exp(4.547386) [1]94.38536 which means that the interval above the intercept is 9.957 but the interval below it is 9.007."
  },
  {
    "input": "We call the variable xv, to stand for x values: xv<-seq(0,30,0.25) Thisgivesus121values(length(xv)).Weknowtheequationfortheexponentialcurve is94.38536exp(\u00000.068528x),sonowwecancalculatetheyvalues(amounts)associated with each x value: yv<-94.38536*exp(-0.068528*xv) 140 STATISTICS:ANINTRODUCTIONUSINGR Now we use the lines function to add the curve to the scatterplot: lines(xv,yv,col=\"red\") As you can see, our model is a good description of the data for intermediate values of time, but the model is poor at predicting amount for time=0 and for time > 28.",
    "target": "Q: What is We call the variable xv, to stand for x values: xv<-seq(0,30,0.25) Thisgivesus121values(length(xv)).Weknowtheequationfortheexponentialcurve is94.38536exp(\u00000.068528x),sonowwecancalculatetheyvalues(amounts)associated with each x value: yv<-94.38536*exp(-0.068528*xv) 140 STATISTICS:ANINTRODUCTIONUSINGR Now we use the lines function to add the curve to the scatterplot: lines(xv,yv,col=\"red\") As you can see, our model? A: We call the variable xv, to stand for x values: xv<-seq(0,30,0.25) Thisgivesus121values(length(xv)).Weknowtheequationfortheexponentialcurve is94.38536exp(\u00000.068528x),sonowwecancalculatetheyvalues(amounts)associated with each x value: yv<-94.38536*exp(-0.068528*xv) 140 STATISTICS:ANINTRODUCTIONUSINGR Now we use the lines function to add the curve to the scatterplot: lines(xv,yv,col=\"red\") As you can see, our model is a good description of the data for intermediate values of time, but the model is poor at predicting amount for time=0 and for time > 28."
  },
  {
    "input": "One of the simplest ways is to use polynomial regression yabxcx2dx3...",
    "target": "Q: What is One of the simplest ways? A: One of the simplest ways is to use polynomial regression yabxcx2dx3..."
  },
  {
    "input": "Evenifwerestrictourselvestotheinclusionofaquadraticterm,x2,therearemanycurves we can describe, depending upon the signs of the linear and quadratic terms: par(mfrow=c(2,2)) curve(4+2*x-0.1*x^2,0,10,col=\"red\",ylab=\"y\") curve(4+2*x-0.2*x^2,0,10,col=\"red\",ylab=\"y\") curve(12-4*x+0.3*x^2,0,10,col=\"red\",ylab=\"y\") curve(4+0.5*x+0.1*x^2,0,10,col=\"red\",ylab=\"y\") REGRESSION 141 In the top left panel, there is a curve with positive but declining slope, with no hint of a hump (y42x\u00000:1x2).",
    "target": "Q: What is Evenifwerestrictourselvestotheinclusionofaquadraticterm,x2,therearemanycurves we can describe, depending upon the signs of the linear and quadratic terms: par(mfrow=c(2,2)) curve(4+2*x-0.1*x^2,0,10,col=\"red\",ylab=\"y\") curve(4+2*x-0.2*x^2,0,10,col=\"red\",ylab=\"y\") curve(12-4*x+0.3*x^2,0,10,col=\"red\",ylab=\"y\") curve(4+0.5*x+0.1*x^2,0,10,col=\"red\",ylab=\"y\") REGRESSION 141 In the top left panel, there? A: Evenifwerestrictourselvestotheinclusionofaquadraticterm,x2,therearemanycurves we can describe, depending upon the signs of the linear and quadratic terms: par(mfrow=c(2,2)) curve(4+2*x-0.1*x^2,0,10,col=\"red\",ylab=\"y\") curve(4+2*x-0.2*x^2,0,10,col=\"red\",ylab=\"y\") curve(12-4*x+0.3*x^2,0,10,col=\"red\",ylab=\"y\") curve(4+0.5*x+0.1*x^2,0,10,col=\"red\",ylab=\"y\") REGRESSION 141 In the top left panel, there is a curve with positive but declining slope, with no hint of a hump (y42x\u00000:1x2)."
  },
  {
    "input": "It is very importanttounderstandthatthequadraticmodeldescribestherelationshipbetweenyandx; it does not pretend to explain the mechanistic (or causal) relationship between y and x. Wecanusethedecaydataasanexampleofmodelcomparison.Howmuchbetterthana linear model with two parameters (call it model2) is a quadratic with three parameters (model3)?ThefunctionIstandsforasisandallowsyoutousearithmeticoperatorslike caret(^forcalculatingpowers)inamodelformulawherethesamesymbolwouldotherwise meansomethingdifferent(inamodelformula,caretmeanstheorderofinteractiontermsto be fitted).",
    "target": "Q: What is It? A: It is very importanttounderstandthatthequadraticmodeldescribestherelationshipbetweenyandx; it does not pretend to explain the mechanistic (or causal) relationship between y and x. Wecanusethedecaydataasanexampleofmodelcomparison.Howmuchbetterthana linear model with two parameters (call it model2) is a quadratic with three parameters (model3)?ThefunctionIstandsforasisandallowsyoutousearithmeticoperatorslike caret(^forcalculatingpowers)inamodelformulawherethesamesymbolwouldotherwise meansomethingdifferent(inamodelformula,caretmeanstheorderofinteractiontermsto be fitted)."
  },
  {
    "input": "To see how much better the quadratic model is when compared to the simpler linear model we can use AIC (see p. 232) or anova (see p. 172): AIC(model2,model3) df AIC model2 3 257.0016 model3 4 230.4445 The much lower AIC of the quadratic model3 means that it is preferred (see p. 232 for details).Alternatively,ifyoulikepvalues,thencomparisonofthetwomodelsbyanova shows that the curvature is highly significant (p<0.000001): anova(model2,model3) AnalysisofVarianceTable Model1: amounttime Model2: amounttime+I(time^2) Res.Df RSSDfSumofSq F Pr(>F) 1 295960.6 2 282372.6 1 3588.1 42.344 4.727e-07*** Non-LinearRegression Sometimeswehaveamechanisticmodelfortherelationshipbetweenyandx,andwewantto estimate the parameters and standard errors of the parameters of a specific non-linear equationfromdata.Thereareanumberoffrequently-usednon-linearmodelstochoosefrom.",
    "target": "Q: What is To see how much better the quadratic model? A: To see how much better the quadratic model is when compared to the simpler linear model we can use AIC (see p. 232) or anova (see p. 172): AIC(model2,model3) df AIC model2 3 257.0016 model3 4 230.4445 The much lower AIC of the quadratic model3 means that it is preferred (see p. 232 for details).Alternatively,ifyoulikepvalues,thencomparisonofthetwomodelsbyanova shows that the curvature is highly significant (p<0.000001): anova(model2,model3) AnalysisofVarianceTable Model1: amounttime Model2: amounttime+I(time^2) Res.Df RSSDfSumofSq F Pr(>F) 1 295960.6 2 282372.6 1 3588.1 42.344 4.727e-07*** Non-LinearRegression Sometimeswehaveamechanisticmodelfortherelationshipbetweenyandx,andwewantto estimate the parameters and standard errors of the parameters of a specific non-linear equationfromdata.Thereareanumberoffrequently-usednon-linearmodelstochoosefrom."
  },
  {
    "input": "Whatwemeaninthiscasebynon-linearisnotthattherelationshipiscurved(itwascurvedin the caseofpolynomialregressions,butthesewerelinearmodels),butthattherelationship cannotbelinearizedbytransformationoftheresponsevariableortheexplanatoryvariable (orboth).Hereisanexample:itshowsjawbonelengthasafunctionofageindeer.Theory indicates that the relationship is an asymptotic exponential with three parameters: ya\u0000be \u0000cx InR,themaindifferencebetweenlinearmodelsandnon-linearmodelsisthatwehaveto tellRtheexactnatureoftheequationaspartofthemodelformulawhenweusenon-linear modelling.Inplaceoflmwewritenls(thisstandsfornonlinearleastsquares).Thenwe writeya-b*exp(-c*x)tospellouttheprecise nonlinear modelwewantRtofittothe data.TheslightlytediousthingisthatRrequiresustospecifyinitialguessesatthevaluesof theparametersa,bandc(note,however,thatsomecommonnon-linearmodelshaveself- startingversionsinRwhichbypassthisstep).Letusplotthedataandworkoutsensible startingvalues.Italwayshelpsincaseslikethistoevaluatetheequationsbehaviouratthe limits.Thatistosay,thevaluesofywhenx=0andwhenx=infinity.Forx=0,wehave exp(-0)whichis1,and1b=bsoy=a\u0000b.Forx=infinity,wehaveexp(-infinity) which is 0, and 0  b=0 so y=a.",
    "target": "Q: What is Whatwemeaninthiscasebynon-linearisnotthattherelationshipiscurved(itwascurvedin the caseofpolynomialregressions,butthesewerelinearmodels),butthattherelationship cannotbelinearizedbytransformationoftheresponsevariableortheexplanatoryvariable (orboth).Hereisanexample:itshowsjawbonelengthasafunctionofageindeer.Theory indicates that the relationship? A: Whatwemeaninthiscasebynon-linearisnotthattherelationshipiscurved(itwascurvedin the caseofpolynomialregressions,butthesewerelinearmodels),butthattherelationship cannotbelinearizedbytransformationoftheresponsevariableortheexplanatoryvariable (orboth).Hereisanexample:itshowsjawbonelengthasafunctionofageindeer.Theory indicates that the relationship is an asymptotic exponential with three parameters: ya\u0000be \u0000cx InR,themaindifferencebetweenlinearmodelsandnon-linearmodelsisthatwehaveto tellRtheexactnatureoftheequationaspartofthemodelformulawhenweusenon-linear modelling.Inplaceoflmwewritenls(thisstandsfornonlinearleastsquares).Thenwe writeya-b*exp(-c*x)tospellouttheprecise nonlinear modelwewantRtofittothe data.TheslightlytediousthingisthatRrequiresustospecifyinitialguessesatthevaluesof theparametersa,bandc(note,however,thatsomecommonnon-linearmodelshaveself- startingversionsinRwhichbypassthisstep).Letusplotthedataandworkoutsensible startingvalues.Italwayshelpsincaseslikethistoevaluatetheequationsbehaviouratthe limits.Thatistosay,thevaluesofywhenx=0andwhenx=infinity.Forx=0,wehave exp(-0)whichis1,and1b=bsoy=a\u0000b.Forx=infinity,wehaveexp(-infinity) which is 0, and 0  b=0 so y=a."
  },
  {
    "input": "That is to say, the asymptotic value of y is a, REGRESSION 143 and the intercept is a\u0000b.",
    "target": "Q: What is That? A: That is to say, the asymptotic value of y is a, REGRESSION 143 and the intercept is a\u0000b."
  },
  {
    "input": "The easiestway to find SSY is to fit a null model, estimating only the intercept.",
    "target": "Q: What is The easiestway to find SSY? A: The easiestway to find SSY is to fit a null model, estimating only the intercept."
  },
  {
    "input": "In R, the intercept isparameter1andisfittedlikethis:y1.Thesumofsquaresassociatedwiththismodel is SSY: 146 STATISTICS:ANINTRODUCTIONUSINGR null.model<-lm(bone1) summary.aov(null.model) DfSumSqMeanSqFvaluePr(>F) Residuals 53 59008 1113 The key figure to extract from this is the total sum of squares SSY=59008.",
    "target": "Q: What is In R, the intercept isparameter1andisfittedlikethis:y1.Thesumofsquaresassociatedwiththismodel? A: In R, the intercept isparameter1andisfittedlikethis:y1.Thesumofsquaresassociatedwiththismodel is SSY: 146 STATISTICS:ANINTRODUCTIONUSINGR null.model<-lm(bone1) summary.aov(null.model) DfSumSqMeanSqFvaluePr(>F) Residuals 53 59008 1113 The key figure to extract from this is the total sum of squares SSY=59008."
  },
  {
    "input": "Recall that r-squared is SSR/SST expressedasapercentage,andthatSSR=SSY\u0000SSE.Thus,thefractionofthevariancein bone length explained by our model is 100*(59008-8923.72)/59008 [1]84.8771 GeneralizedAdditiveModels Sometimes we can see that the relationship between y and x is non-linear but we do not have any theory or any mechanistic model to suggest a particular functional form (mathematical equation) to describe the relationship.",
    "target": "Q: What is Recall that r-squared? A: Recall that r-squared is SSR/SST expressedasapercentage,andthatSSR=SSY\u0000SSE.Thus,thefractionofthevariancein bone length explained by our model is 100*(59008-8923.72)/59008 [1]84.8771 GeneralizedAdditiveModels Sometimes we can see that the relationship between y and x is non-linear but we do not have any theory or any mechanistic model to suggest a particular functional form (mathematical equation) to describe the relationship."
  },
  {
    "input": "library(mgcv) hump<-read.csv(\"c:\\\\temp\\\\hump.csv\") attach(hump) names(hump) [1]\"y\"\"x\" We start by fitting the generalized additive model as a smoothed function of x, s(x): model<-gam(ys(x)) Then we plot the model, and overlay the scatterplot of data points: plot(model,col=\"blue\") points(x,y-mean(y),pch=21,bg=\"red\") REGRESSION 147 The y axis is labelled s(x,7.45) which is interpreted as saying that the smoothed function of x shown by the solid blue curve (the non-parametric smoother) involves the equivalentof7.45degreesoffreedom(rememberthatastraightlinewoulduse2d.f.",
    "target": "Q: What is library(mgcv) hump<-read.csv(\"c:\\\\temp\\\\hump.csv\") attach(hump) names(hump) [1]\"y\"\"x\" We start by fitting the generalized additive model as a smoothed function of x, s(x): model<-gam(ys(x)) Then we plot the model, and overlay the scatterplot of data points: plot(model,col=\"blue\") points(x,y-mean(y),pch=21,bg=\"red\") REGRESSION 147 The y axis? A: library(mgcv) hump<-read.csv(\"c:\\\\temp\\\\hump.csv\") attach(hump) names(hump) [1]\"y\"\"x\" We start by fitting the generalized additive model as a smoothed function of x, s(x): model<-gam(ys(x)) Then we plot the model, and overlay the scatterplot of data points: plot(model,col=\"blue\") points(x,y-mean(y),pch=21,bg=\"red\") REGRESSION 147 The y axis is labelled s(x,7.45) which is interpreted as saying that the smoothed function of x shown by the solid blue curve (the non-parametric smoother) involves the equivalentof7.45degreesoffreedom(rememberthatastraightlinewoulduse2d.f."
  },
  {
    "input": "The model summary is obtained in the usual way: summary(model) Family:gaussian Linkfunction:identity Formula: ys(x) Parametriccoefficients: Estimate Std.Error tvalue Pr(>|t|) (Intercept) 1.95737 0.03446 56.8 <2e-16*** Approximatesignificanceofsmoothterms: edf Ref.df F p-value s(x) 7.452 8.403 116.9 <2e-16*** R-sq.",
    "target": "Q: What is The model summary? A: The model summary is obtained in the usual way: summary(model) Family:gaussian Linkfunction:identity Formula: ys(x) Parametriccoefficients: Estimate Std.Error tvalue Pr(>|t|) (Intercept) 1.95737 0.03446 56.8 <2e-16*** Approximatesignificanceofsmoothterms: edf Ref.df F p-value s(x) 7.452 8.403 116.9 <2e-16*** R-sq."
  },
  {
    "input": "The intercept (1.95737) is just the mean value of y.",
    "target": "Q: What is The intercept (1.95737)? A: The intercept (1.95737) is just the mean value of y."
  },
  {
    "input": "Note that because of the strong hump in the relationship, a linear model lm(yx) indicatesnosignificantrelationshipbetweenthetwovariables(p=0.346).Thisisanobject lesson in always plotting the data before you come to conclusions from the statistical analysis;inthiscase,ifyouhadstartedwithalinearmodelyouwouldhavethrownoutthe 148 STATISTICS:ANINTRODUCTIONUSINGR babywiththebathwaterbyconcludingthatnothingwashappening.Infact,somethingvery significant is happening but it is producing a humped, rather than a trended relationship.",
    "target": "Q: What is Note that because of the strong hump in the relationship, a linear model lm(yx) indicatesnosignificantrelationshipbetweenthetwovariables(p=0.346).Thisisanobject lesson in always plotting the data before you come to conclusions from the statistical analysis;inthiscase,ifyouhadstartedwithalinearmodelyouwouldhavethrownoutthe 148 STATISTICS:ANINTRODUCTIONUSINGR babywiththebathwaterbyconcludingthatnothingwashappening.Infact,somethingvery significant? A: Note that because of the strong hump in the relationship, a linear model lm(yx) indicatesnosignificantrelationshipbetweenthetwovariables(p=0.346).Thisisanobject lesson in always plotting the data before you come to conclusions from the statistical analysis;inthiscase,ifyouhadstartedwithalinearmodelyouwouldhavethrownoutthe 148 STATISTICS:ANINTRODUCTIONUSINGR babywiththebathwaterbyconcludingthatnothingwashappening.Infact,somethingvery significant is happening but it is producing a humped, rather than a trended relationship."
  },
  {
    "input": "Itisimportanttounderstand,however,thatapointmayappeartobeanoutlierbecauseof misspecification of the model, and not because there is anything wrong with the data.",
    "target": "Q: What is Itisimportanttounderstand,however,thatapointmayappeartobeanoutlierbecauseof misspecification of the model, and not because there? A: Itisimportanttounderstand,however,thatapointmayappeartobeanoutlierbecauseof misspecification of the model, and not because there is anything wrong with the data."
  },
  {
    "input": "Measuresofleverageforagivendatapointyareproportionalto x\u0000x2.Thecommonest measure of leverage is 1 x \u0000x2 h  Pi i n x \u0000x2 i REGRESSION 149 wherethedenominatorisSSX.Agoodruleofthumbisthatapointishighlyinfluentialifits 2p h > ; i n where p is the number of parameters in the model.",
    "target": "Q: What is Measuresofleverageforagivendatapointyareproportionalto x\u0000x2.Thecommonest measure of leverage? A: Measuresofleverageforagivendatapointyareproportionalto x\u0000x2.Thecommonest measure of leverage is 1 x \u0000x2 h  Pi i n x \u0000x2 i REGRESSION 149 wherethedenominatorisSSX.Agoodruleofthumbisthatapointishighlyinfluentialifits 2p h > ; i n where p is the number of parameters in the model."
  },
  {
    "input": "There is a useful function called influence.measures which highlights influential points in a given model reg<-lm(y1x1) influence.measures(reg) Influencemeasuresof lm(formula=y1x1): dfb.1_ dfb.x1 dffit cov.r cook.d hat inf 1 0.687 -0.5287 0.7326 1.529 0.26791 0.348 2 0.382 -0.2036 0.5290 1.155 0.13485 0.196 3 -0.031 0.0165-0.0429 2.199 0.00122 0.196 4 -0.496 0.2645-0.6871 0.815 0.19111 0.196 5 -0.105 -0.1052-0.5156 1.066 0.12472 0.174 6 -3.023 4.1703 4.6251 4.679 7.62791 0.891 * Youcanseepoint#6ishighlightedbyanasterisk,drawingattentiontoitshighinfluence.",
    "target": "Q: What is There? A: There is a useful function called influence.measures which highlights influential points in a given model reg<-lm(y1x1) influence.measures(reg) Influencemeasuresof lm(formula=y1x1): dfb.1_ dfb.x1 dffit cov.r cook.d hat inf 1 0.687 -0.5287 0.7326 1.529 0.26791 0.348 2 0.382 -0.2036 0.5290 1.155 0.13485 0.196 3 -0.031 0.0165-0.0429 2.199 0.00122 0.196 4 -0.496 0.2645-0.6871 0.815 0.19111 0.196 5 -0.105 -0.1052-0.5156 1.066 0.12472 0.174 6 -3.023 4.1703 4.6251 4.679 7.62791 0.891 * Youcanseepoint#6ishighlightedbyanasterisk,drawingattentiontoitshighinfluence."
  },
  {
    "input": "More formally, this is given by: X SSY  y\u0000y2 which should look familiar, because it is the formula used for defining the variance of y (s2=sum of squares/degrees of freedom; see p. 51).",
    "target": "Q: What is More formally, this? A: More formally, this is given by: X SSY  y\u0000y2 which should look familiar, because it is the formula used for defining the variance of y (s2=sum of squares/degrees of freedom; see p. 51)."
  },
  {
    "input": "This next step is the key to understanding how ANOVA works.",
    "target": "Q: What is This next step? A: This next step is the key to understanding how ANOVA works."
  },
  {
    "input": "The analysis is formalized by defining this new sum of squares: it is the sum of the squaresofthedifferencesbetweentheindividualyvaluesandtherelevanttreatmentmean.",
    "target": "Q: What is The analysis? A: The analysis is formalized by defining this new sum of squares: it is the sum of the squaresofthedifferencesbetweentheindividualyvaluesandtherelevanttreatmentmean."
  },
  {
    "input": "So the degrees of freedom associated with SSE is kn\u0000k= k(n\u00001).Anotherwayofseeingthisistosaythattherearenreplicatesineachtreatment,and hence n\u00001 degrees of freedom for error in each treatment (because 1 d.f.",
    "target": "Q: What is So the degrees of freedom associated with SSE? A: So the degrees of freedom associated with SSE is kn\u0000k= k(n\u00001).Anotherwayofseeingthisistosaythattherearenreplicatesineachtreatment,and hence n\u00001 degrees of freedom for error in each treatment (because 1 d.f."
  },
  {
    "input": "Nowwecometotheanalysispartoftheanalysisofvariance.Thetotalsumofsquaresin y,SSY,is broken up(analysed) into components.The unexplained part ofthe variation is calledtheerrorsum ofsquares, SSE.The componentofthevariation thatisexplainedby differences between the treatment means is called the treatment sum of squares, and is traditionally denoted by SSA.",
    "target": "Q: What is Nowwecometotheanalysispartoftheanalysisofvariance.Thetotalsumofsquaresin y,SSY,is broken up(analysed) into components.The unexplained part ofthe variation? A: Nowwecometotheanalysispartoftheanalysisofvariance.Thetotalsumofsquaresin y,SSY,is broken up(analysed) into components.The unexplained part ofthe variation is calledtheerrorsum ofsquares, SSE.The componentofthevariation thatisexplainedby differences between the treatment means is called the treatment sum of squares, and is traditionally denoted by SSA."
  },
  {
    "input": "This is because in two-way ANOVA, with two different categoricalexplanatoryvariables,weshalluseSSBtodenotethesumofsquaresattributable todifferencesbetweenthemeansofthesecondfactor.AndSSCtodenotethesumofsquares attributable to differences between the means of the third factor.",
    "target": "Q: What is This? A: This is because in two-way ANOVA, with two different categoricalexplanatoryvariables,weshalluseSSBtodenotethesumofsquaresattributable todifferencesbetweenthemeansofthesecondfactor.AndSSCtodenotethesumofsquares attributable to differences between the means of the third factor."
  },
  {
    "input": "SSA SSY SSE Typically, we compute all but one of the components, then find the value of the last componentbysubtractionoftheothersfromSSY.WealreadyhaveaformulaforSSE,sowe couldobtainSSAbydifference:SSASSY \u0000SSE.StartingwithSSY,wecalculatethesum of the squares of the differences between the y values and the overall mean: SSY<-sum((ozone-mean(ozone))^2) SSY [1]44 The question now is: How much of this 44 is attributable to differences between the means of gardens A and B (SSA=explained variation) and how much is sampling error ANALYSIS OFVARIANCE 155 (SSE=unexplained variation)?",
    "target": "Q: What is SSA SSY SSE Typically, we compute all but one of the components, then find the value of the last componentbysubtractionoftheothersfromSSY.WealreadyhaveaformulaforSSE,sowe couldobtainSSAbydifference:SSASSY \u0000SSE.StartingwithSSY,wecalculatethesum of the squares of the differences between the y values and the overall mean: SSY<-sum((ozone-mean(ozone))^2) SSY [1]44 The question now is: How much of this 44? A: SSA SSY SSE Typically, we compute all but one of the components, then find the value of the last componentbysubtractionoftheothersfromSSY.WealreadyhaveaformulaforSSE,sowe couldobtainSSAbydifference:SSASSY \u0000SSE.StartingwithSSY,wecalculatethesum of the squares of the differences between the y values and the overall mean: SSY<-sum((ozone-mean(ozone))^2) SSY [1]44 The question now is: How much of this 44 is attributable to differences between the means of gardens A and B (SSA=explained variation) and how much is sampling error ANALYSIS OFVARIANCE 155 (SSE=unexplained variation)?"
  },
  {
    "input": "We have a formula defining SSE; it is the sum of the squaresoftheresidualscalculatedseparatelyforeachgarden, usingtheappropriatemean value.",
    "target": "Q: What is We have a formula defining SSE; it? A: We have a formula defining SSE; it is the sum of the squaresoftheresidualscalculatedseparatelyforeachgarden, usingtheappropriatemean value."
  },
  {
    "input": "For garden A, we get sum((ozone[garden==\"A\"]-mean(ozone[garden==\"A\"]))^2) [1]12 and for garden B sum((ozone[garden==\"B\"]-mean(ozone[garden==\"B\"]))^2) [1]12 sotheerrorsumofsquaresisthetotalofthesecomponentsSSE=12+12=24.Finally,we can obtain the treatment sum of squares, SSA, by difference: SSA44\u00002420 At this point, we can fill in the ANOVA table (see p. 128): Source Sumofsquares Degreesoffreedom Meansquare Fratio Garden 20.0 1 20.0 15.0 Error 24.0 18 s2=1.3333 Total 44.0 19 WeneedtotestwhetheranFratioof15.0islargeorsmall.Todothiswecompareitwith thecriticalvalueofFfromquantilesoftheFdistribution,qf.Wehave1degreeoffreedom inthe numerator, and18degrees offreedom inthedenominator,and we want towork at 95% certainty (0:05): qf(0.95,1,18) [1]4.413873 Thecalculatedvalueoftheteststatisticof15.0ismuchgreaterthanthecriticalvalueof F=4.41,sowecanrejectthenullhypothesis(equalityofthemeans)andacceptthealternative hypothesis(thetwomeansaresignificantlydifferent).Weusedaone-tailedFtest(0.95rather than0.975intheqffunction)becauseweareonlyinterestedinthecasewherethetreatment varianceislargerelativetotheerrorvariance.Butthisapproachisratherold-fashioned;the modern view is to calculate the effect size (the difference between the means is 2.0 pphm ozone)andtostatetheprobabilitythatsuchadifferencewouldarisebychancealonewhenthe nullhypothesiswastrueandthemeanozoneconcentrationwasthesameinthetwogardens.",
    "target": "Q: What is For garden A, we get sum((ozone[garden==\"A\"]-mean(ozone[garden==\"A\"]))^2) [1]12 and for garden B sum((ozone[garden==\"B\"]-mean(ozone[garden==\"B\"]))^2) [1]12 sotheerrorsumofsquaresisthetotalofthesecomponentsSSE=12+12=24.Finally,we can obtain the treatment sum of squares, SSA, by difference: SSA44\u00002420 At this point, we can fill in the ANOVA table (see p. 128): Source Sumofsquares Degreesoffreedom Meansquare Fratio Garden 20.0 1 20.0 15.0 Error 24.0 18 s2=1.3333 Total 44.0 19 WeneedtotestwhetheranFratioof15.0islargeorsmall.Todothiswecompareitwith thecriticalvalueofFfromquantilesoftheFdistribution,qf.Wehave1degreeoffreedom inthe numerator, and18degrees offreedom inthedenominator,and we want towork at 95% certainty (0:05): qf(0.95,1,18) [1]4.413873 Thecalculatedvalueoftheteststatisticof15.0ismuchgreaterthanthecriticalvalueof F=4.41,sowecanrejectthenullhypothesis(equalityofthemeans)andacceptthealternative hypothesis(thetwomeansaresignificantlydifferent).Weusedaone-tailedFtest(0.95rather than0.975intheqffunction)becauseweareonlyinterestedinthecasewherethetreatment varianceislargerelativetotheerrorvariance.Butthisapproachisratherold-fashioned;the modern view? A: For garden A, we get sum((ozone[garden==\"A\"]-mean(ozone[garden==\"A\"]))^2) [1]12 and for garden B sum((ozone[garden==\"B\"]-mean(ozone[garden==\"B\"]))^2) [1]12 sotheerrorsumofsquaresisthetotalofthesecomponentsSSE=12+12=24.Finally,we can obtain the treatment sum of squares, SSA, by difference: SSA44\u00002420 At this point, we can fill in the ANOVA table (see p. 128): Source Sumofsquares Degreesoffreedom Meansquare Fratio Garden 20.0 1 20.0 15.0 Error 24.0 18 s2=1.3333 Total 44.0 19 WeneedtotestwhetheranFratioof15.0islargeorsmall.Todothiswecompareitwith thecriticalvalueofFfromquantilesoftheFdistribution,qf.Wehave1degreeoffreedom inthe numerator, and18degrees offreedom inthedenominator,and we want towork at 95% certainty (0:05): qf(0.95,1,18) [1]4.413873 Thecalculatedvalueoftheteststatisticof15.0ismuchgreaterthanthecriticalvalueof F=4.41,sowecanrejectthenullhypothesis(equalityofthemeans)andacceptthealternative hypothesis(thetwomeansaresignificantlydifferent).Weusedaone-tailedFtest(0.95rather than0.975intheqffunction)becauseweareonlyinterestedinthecasewherethetreatment varianceislargerelativetotheerrorvariance.Butthisapproachisratherold-fashioned;the modern view is to calculate the effect size (the difference between the means is 2.0 pphm ozone)andtostatetheprobabilitythatsuchadifferencewouldarisebychancealonewhenthe nullhypothesiswastrueandthemeanozoneconcentrationwasthesameinthetwogardens."
  },
  {
    "input": "ForthisweusecumulativeprobabilitiesoftheFdistribution,ratherthanquantiles,likethis: 1-pf(15.0,1,18) [1]0.001114539 156 STATISTICS:ANINTRODUCTIONUSINGR Sotheprobabilityofobtainingdataasextremeasours(ormoreextreme)ifthetwomeans really were the same is roughly 0.1%.",
    "target": "Q: What is ForthisweusecumulativeprobabilitiesoftheFdistribution,ratherthanquantiles,likethis: 1-pf(15.0,1,18) [1]0.001114539 156 STATISTICS:ANINTRODUCTIONUSINGR Sotheprobabilityofobtainingdataasextremeasours(ormoreextreme)ifthetwomeans really were the same? A: ForthisweusecumulativeprobabilitiesoftheFdistribution,ratherthanquantiles,likethis: 1-pf(15.0,1,18) [1]0.001114539 156 STATISTICS:ANINTRODUCTIONUSINGR Sotheprobabilityofobtainingdataasextremeasours(ormoreextreme)ifthetwomeans really were the same is roughly 0.1%."
  },
  {
    "input": "Here is the whole analysis in R in a single line: summary(aov(ozonegarden)) DfSum Sq MeanSq F value Pr(>F) garden 1 20 20.000 15 0.00111** Residuals 18 24 1.333 Thefirstcolumnshowsthesourcesofvariation(SSAandSSE,respectively);notethatR leavesoffthebottomrowthatweincludedfortotalvariation,SSY.Thenextcolumnshows degrees of freedom: there aretwo levelsofgarden (A and B)so there is2\u00001=1 d.f.for garden,andthereare10replicatespergarden,so10\u00001=9d.f.pergardenandtwogardens, so error d.f.=29=18.",
    "target": "Q: What is Here? A: Here is the whole analysis in R in a single line: summary(aov(ozonegarden)) DfSum Sq MeanSq F value Pr(>F) garden 1 20 20.000 15 0.00111** Residuals 18 24 1.333 Thefirstcolumnshowsthesourcesofvariation(SSAandSSE,respectively);notethatR leavesoffthebottomrowthatweincludedfortotalvariation,SSY.Thenextcolumnshows degrees of freedom: there aretwo levelsofgarden (A and B)so there is2\u00001=1 d.f.for garden,andthereare10replicatespergarden,so10\u00001=9d.f.pergardenandtwogardens, so error d.f.=29=18."
  },
  {
    "input": "The next column shows the sums of squares: SSA=20 and SSE=24.Thefourthcolumngivesthemeansquares(sumsofsquaresdividedbydegreesof freedom); the treatment mean square is 20.0 and the error variance, s2 (synonym of the residualmeansquare)is24/18=1.3333.TheFratiois20/1.333=15,andtheprobability thatthis(oraresultmoreextremethanthis)wouldarisebychancealoneifthetwomeans really were the same is 0.001115 (just as we calculated long-hand, above).",
    "target": "Q: What is The next column shows the sums of squares: SSA=20 and SSE=24.Thefourthcolumngivesthemeansquares(sumsofsquaresdividedbydegreesof freedom); the treatment mean square? A: The next column shows the sums of squares: SSA=20 and SSE=24.Thefourthcolumngivesthemeansquares(sumsofsquaresdividedbydegreesof freedom); the treatment mean square is 20.0 and the error variance, s2 (synonym of the residualmeansquare)is24/18=1.3333.TheFratiois20/1.333=15,andtheprobability thatthis(oraresultmoreextremethanthis)wouldarisebychancealoneifthetwomeans really were the same is 0.001115 (just as we calculated long-hand, above)."
  },
  {
    "input": "plot(aov(ozonegarden)) ANALYSIS OFVARIANCE 157 Thefirstplotshowsthatthevariancesareidenticalinthetwotreatments(thisisexactly whatwewanttosee).Thesecondplotshowsareasonablystraightlinerelationshiponthe normal quantilequantile plot (especially since, in this example, the y values are whole numbers), so we can be confident that non-normality of errors is not a major problem.",
    "target": "Q: What is plot(aov(ozonegarden)) ANALYSIS OFVARIANCE 157 Thefirstplotshowsthatthevariancesareidenticalinthetwotreatments(thisisexactly whatwewanttosee).Thesecondplotshowsareasonablystraightlinerelationshiponthe normal quantilequantile plot (especially since, in this example, the y values are whole numbers), so we can be confident that non-normality of errors? A: plot(aov(ozonegarden)) ANALYSIS OFVARIANCE 157 Thefirstplotshowsthatthevariancesareidenticalinthetwotreatments(thisisexactly whatwewanttosee).Thesecondplotshowsareasonablystraightlinerelationshiponthe normal quantilequantile plot (especially since, in this example, the y values are whole numbers), so we can be confident that non-normality of errors is not a major problem."
  },
  {
    "input": "The shortcut formula for SSA (Box 8.1) is then: 2 P (cid:2)P (cid:3) T2 y 2 SSA i \u0000 n kn We should confirm that this really does give SSA: 302502 802 3400 6400 SSA \u0000  \u0000 340\u000032020 10 2\u000210 10 20 whichchecksout.Inallsortsofanalysisofvariance,thekeypointtorealizeisthatthesum of the subtotals squared is always divided by the number of numbers that were added togethertogeteachsubtotal.Thatsoundscomplicated,buttheideaissimple.Inourcasewe 158 STATISTICS:ANINTRODUCTIONUSINGR squaredthesubtotalsT andT andaddedtheresultstogether.Wedividedby10because 1 2 T and T were each the sum of 10 numbers.",
    "target": "Q: What is The shortcut formula for SSA (Box 8.1)? A: The shortcut formula for SSA (Box 8.1) is then: 2 P (cid:2)P (cid:3) T2 y 2 SSA i \u0000 n kn We should confirm that this really does give SSA: 302502 802 3400 6400 SSA \u0000  \u0000 340\u000032020 10 2\u000210 10 20 whichchecksout.Inallsortsofanalysisofvariance,thekeypointtorealizeisthatthesum of the subtotals squared is always divided by the number of numbers that were added togethertogeteachsubtotal.Thatsoundscomplicated,buttheideaissimple.Inourcasewe 158 STATISTICS:ANINTRODUCTIONUSINGR squaredthesubtotalsT andT andaddedtheresultstogether.Wedividedby10because 1 2 T and T were each the sum of 10 numbers."
  },
  {
    "input": "Correctedsumsofsquaresinone-wayANOVA Thetotalsumofsquares,SSY,isdefinedasthesumofthesquaresofthedifferences between all the data points, y, and the overall mean, y: Xk X SSY  y\u0000y2 i1 P where the inner Pmeans the sum over the n replicates within each of the k factor levelsandtheouter meansaddthesubtotalsacrosseachofthefactorlevels.The errorsumofsquares,SSE,isthesumofthesquaresofthedifferencesbetweenthedata points, y, and their individual treatment means, y: i Xk X SSE  y\u0000y2 i i1 The treatment sum of squares, SSA, is the sum of the squares of the differences between the individual treatment means, y, and the overall mean, y: i Xk Xn Xk SSA y \u0000y2 n y \u0000y2 i i i1 j1 i1 Squaring the bracketed term and applying summation gives X X y2\u00002y y k:y 2 i i Now replace y by T =n (where T is our conventional name for the k individual i i Pi treatment totals) and replace y by y=k:n to get P P P P P k T2 y k T y y i1 i \u00002 i1 ik n2 n:k:n k:n:k:n P P P Notethat k T  j n y sotheright-handpositiveandnegativetermsboth i1(cid:2)P i (cid:3) i1 j1 ij have the form y 2=k:n2.",
    "target": "Q: What is Correctedsumsofsquaresinone-wayANOVA Thetotalsumofsquares,SSY,isdefinedasthesumofthesquaresofthedifferences between all the data points, y, and the overall mean, y: Xk X SSY  y\u0000y2 i1 P where the inner Pmeans the sum over the n replicates within each of the k factor levelsandtheouter meansaddthesubtotalsacrosseachofthefactorlevels.The errorsumofsquares,SSE,isthesumofthesquaresofthedifferencesbetweenthedata points, y, and their individual treatment means, y: i Xk X SSE  y\u0000y2 i i1 The treatment sum of squares, SSA,? A: Correctedsumsofsquaresinone-wayANOVA Thetotalsumofsquares,SSY,isdefinedasthesumofthesquaresofthedifferences between all the data points, y, and the overall mean, y: Xk X SSY  y\u0000y2 i1 P where the inner Pmeans the sum over the n replicates within each of the k factor levelsandtheouter meansaddthesubtotalsacrosseachofthefactorlevels.The errorsumofsquares,SSE,isthesumofthesquaresofthedifferencesbetweenthedata points, y, and their individual treatment means, y: i Xk X SSE  y\u0000y2 i i1 The treatment sum of squares, SSA, is the sum of the squares of the differences between the individual treatment means, y, and the overall mean, y: i Xk Xn Xk SSA y \u0000y2 n y \u0000y2 i i i1 j1 i1 Squaring the bracketed term and applying summation gives X X y2\u00002y y k:y 2 i i Now replace y by T =n (where T is our conventional name for the k individual i i Pi treatment totals) and replace y by y=k:n to get P P P P P k T2 y k T y y i1 i \u00002 i1 ik n2 n:k:n k:n:k:n P P P Notethat k T  j n y sotheright-handpositiveandnegativetermsboth i1(cid:2)P i (cid:3) i1 j1 ij have the form y 2=k:n2."
  },
  {
    "input": "ANALYSIS OFVARIANCE 159 EffectSizes Sofarwehaveconcentratedonhypothesistesting,usingsummary.aov.Itisusuallymore informativetoinvestigatetheeffectsofthedifferentfactorlevels,usingsummary.lmlikethis: summary.lm(aov(ozonegarden)) Itwaseasytointerpretthiskindofoutputinthecontextofaregression,wheretherows representparametersthatareintuitive:namely,theinterceptandtheslope.Inthecontextof analysisofvariance,ittakesafairbitofpracticebeforethemeaningofthiskindofoutput is transparent.",
    "target": "Q: What is ANALYSIS OFVARIANCE 159 EffectSizes Sofarwehaveconcentratedonhypothesistesting,usingsummary.aov.Itisusuallymore informativetoinvestigatetheeffectsofthedifferentfactorlevels,usingsummary.lmlikethis: summary.lm(aov(ozonegarden)) Itwaseasytointerpretthiskindofoutputinthecontextofaregression,wheretherows representparametersthatareintuitive:namely,theinterceptandtheslope.Inthecontextof analysisofvariance,ittakesafairbitofpracticebeforethemeaningofthiskindofoutput? A: ANALYSIS OFVARIANCE 159 EffectSizes Sofarwehaveconcentratedonhypothesistesting,usingsummary.aov.Itisusuallymore informativetoinvestigatetheeffectsofthedifferentfactorlevels,usingsummary.lmlikethis: summary.lm(aov(ozonegarden)) Itwaseasytointerpretthiskindofoutputinthecontextofaregression,wheretherows representparametersthatareintuitive:namely,theinterceptandtheslope.Inthecontextof analysisofvariance,ittakesafairbitofpracticebeforethemeaningofthiskindofoutput is transparent."
  },
  {
    "input": "Tounderstandtheanswerstothesequestions,weneedtoknowhowtheequationforthe explanatory variables is structured when the explanatory variable, as here, is categorical.",
    "target": "Q: What is Tounderstandtheanswerstothesequestions,weneedtoknowhowtheequationforthe explanatory variables? A: Tounderstandtheanswerstothesequestions,weneedtoknowhowtheequationforthe explanatory variables is structured when the explanatory variable, as here, is categorical."
  },
  {
    "input": "To recap, the linear regression model is written as lm(yx) which R interprets as the two-parameter linear equation yabx inwhichthevaluesoftheparametersaandbaretobeestimatedfromthedata.Butwhat about ouranalysisofvariance?",
    "target": "Q: What is To recap, the linear regression model? A: To recap, the linear regression model is written as lm(yx) which R interprets as the two-parameter linear equation yabx inwhichthevaluesoftheparametersaandbaretobeestimatedfromthedata.Butwhat about ouranalysisofvariance?"
  },
  {
    "input": "The aov model is exactly analogous to the regression model aov(yx) But what is the associated equation?",
    "target": "Q: What is The aov model? A: The aov model is exactly analogous to the regression model aov(yx) But what is the associated equation?"
  },
  {
    "input": "1 4 With a categorical explanatory variable, the levels are all coded as 0 except for the level associated with the y value in question, which is coded as 1.",
    "target": "Q: What is 1 4 With a categorical explanatory variable, the levels are all coded as 0 except for the level associated with the y value in question, which? A: 1 4 With a categorical explanatory variable, the levels are all coded as 0 except for the level associated with the y value in question, which is coded as 1."
  },
  {
    "input": "The simplest interpretationofthethree-parametercasethatwehavedealtwithsofaristhattheintercept a is the overall mean from the experiment: mean(ozone) [1]4 Then,ifaistheoverallmean,soa+bmustbethemeanforgardenAanda+cmustbe themeanforgardenB(seetheequationsabove).Ifthatistrue,thenbmustbethedifference betweenthemeanofgardenAandtheoverallmean.Andcmustbethedifferencebetween the mean of garden B and the overall mean.",
    "target": "Q: What is The simplest interpretationofthethree-parametercasethatwehavedealtwithsofaristhattheintercept a? A: The simplest interpretationofthethree-parametercasethatwehavedealtwithsofaristhattheintercept a is the overall mean from the experiment: mean(ozone) [1]4 Then,ifaistheoverallmean,soa+bmustbethemeanforgardenAanda+cmustbe themeanforgardenB(seetheequationsabove).Ifthatistrue,thenbmustbethedifference betweenthemeanofgardenAandtheoverallmean.Andcmustbethedifferencebetween the mean of garden B and the overall mean."
  },
  {
    "input": "Thus, the intercept is a mean, and the other parameters are differences between means.",
    "target": "Q: What is Thus, the intercept? A: Thus, the intercept is a mean, and the other parameters are differences between means."
  },
  {
    "input": "But it suffers from the fact that there is a redundant parameter.",
    "target": "Q: What is But it suffers from the fact that there? A: But it suffers from the fact that there is a redundant parameter."
  },
  {
    "input": "The experiment produces only two means (one for each garden), and so there is no point in having three parameterstorepresenttheoutputoftheexperiment.Oneofthethreeparametersissaidto bealiased(seep.16).Therearelotsofwaysroundthisdilemma,asexplainedindetailin Chapter12oncontrasts.HereweadopttheconventionthatisusedasthedefaultinRusing so-calledtreatmentcontrasts.Underthisconvention,wedispensewiththeoverallmean,a.",
    "target": "Q: What is The experiment produces only two means (one for each garden), and so there? A: The experiment produces only two means (one for each garden), and so there is no point in having three parameterstorepresenttheoutputoftheexperiment.Oneofthethreeparametersissaidto bealiased(seep.16).Therearelotsofwaysroundthisdilemma,asexplainedindetailin Chapter12oncontrasts.HereweadopttheconventionthatisusedasthedefaultinRusing so-calledtreatmentcontrasts.Underthisconvention,wedispensewiththeoverallmean,a."
  },
  {
    "input": "Error t value Pr(>|t|) (Intercept) 3.0000 0.3651 8.216 1.67e-07*** gardenB 2.0000 0.5164 3.873 0.00111** The intercept is 3.0 which is the mean for garden A (because the factor level A comes beforelevelBinthealphabet).TheestimateforgardenBis2.0.Thistellsusthatthemean ozoneconcentrationingardenBis2pphmgreaterthaningardenA(greaterbecausethereis 162 STATISTICS:ANINTRODUCTIONUSINGR nominussign).WewouldcomputethemeanforgardenBas3.0+2.0=5.0.Inpractice, we would not obtain the means like this, but by using tapply, instead: tapply(ozone,garden,mean) A B 3 5 There is more about these issues in Chapter 12.",
    "target": "Q: What is Error t value Pr(>|t|) (Intercept) 3.0000 0.3651 8.216 1.67e-07*** gardenB 2.0000 0.5164 3.873 0.00111** The intercept? A: Error t value Pr(>|t|) (Intercept) 3.0000 0.3651 8.216 1.67e-07*** gardenB 2.0000 0.5164 3.873 0.00111** The intercept is 3.0 which is the mean for garden A (because the factor level A comes beforelevelBinthealphabet).TheestimateforgardenBis2.0.Thistellsusthatthemean ozoneconcentrationingardenBis2pphmgreaterthaningardenA(greaterbecausethereis 162 STATISTICS:ANINTRODUCTIONUSINGR nominussign).WewouldcomputethemeanforgardenBas3.0+2.0=5.0.Inpractice, we would not obtain the means like this, but by using tapply, instead: tapply(ozone,garden,mean) A B 3 5 There is more about these issues in Chapter 12."
  },
  {
    "input": "PlotsforInterpretingOne-WayANOVA There are two traditional ways of plotting the results of ANOVA: (cid:129) box-and-whisker plots (cid:129) barplots with error bars Here is an example to compare the two approaches.",
    "target": "Q: What is PlotsforInterpretingOne-WayANOVA There are two traditional ways of plotting the results of ANOVA: (cid:129) box-and-whisker plots (cid:129) barplots with error bars Here? A: PlotsforInterpretingOne-WayANOVA There are two traditional ways of plotting the results of ANOVA: (cid:129) box-and-whisker plots (cid:129) barplots with error bars Here is an example to compare the two approaches."
  },
  {
    "input": "We have an experiment on plant competition where the response variable is biomass and we have one factor with five levels.",
    "target": "Q: What is We have an experiment on plant competition where the response variable? A: We have an experiment on plant competition where the response variable is biomass and we have one factor with five levels."
  },
  {
    "input": "The factor is called clipping and the levels are control (i.e.",
    "target": "Q: What is The factor? A: The factor is called clipping and the levels are control (i.e."
  },
  {
    "input": "unclipped), two intensities of shoot pruning and two intensities of root pruning: comp<-read.csv(\"c:\\\\temp\\\\competition.csv\") attach(comp) names(comp) [1]\"biomass\" \"clipping\" This is what the data look like: plot(clipping,biomass,xlab=\"Competitiontreatment\", ylab=\"Biomass\",col=\"lightgrey\") ANALYSIS OFVARIANCE 163 The box-and-whisker plot is good at showing the nature of the variation within each treatment,andalsowhetherthereisskewwithineachtreatment(e.g.forthecontrolplots, there is a wider range of values between the median and upper quartile than between the lower quartile and median).",
    "target": "Q: What is unclipped), two intensities of shoot pruning and two intensities of root pruning: comp<-read.csv(\"c:\\\\temp\\\\competition.csv\") attach(comp) names(comp) [1]\"biomass\" \"clipping\" This? A: unclipped), two intensities of shoot pruning and two intensities of root pruning: comp<-read.csv(\"c:\\\\temp\\\\competition.csv\") attach(comp) names(comp) [1]\"biomass\" \"clipping\" This is what the data look like: plot(clipping,biomass,xlab=\"Competitiontreatment\", ylab=\"Biomass\",col=\"lightgrey\") ANALYSIS OFVARIANCE 163 The box-and-whisker plot is good at showing the nature of the variation within each treatment,andalsowhetherthereisskewwithineachtreatment(e.g.forthecontrolplots, there is a wider range of values between the median and upper quartile than between the lower quartile and median)."
  },
  {
    "input": "Here is the one-way ANOVA: model<-aov(biomassclipping) summary(model) Df SumSq MeanSq Fvalue Pr(>F) clipping 4 85356 21339 4.3015 0.008752** Residuals 25 124020 4961 FromtheANOVAtablewecanseethatthepoolederrorvariances2=4961.Nowwe needtoknowhowmanynumberswereusedinthecalculationofeachofthefivemeans: table(clipping) clipping control n25 n50 r10 r5 6 6 6 6 6 Therewasequalreplication(whichmakeslpifeffiffiffieffiffiaffiffisffiffiier)p,affinffiffidffiffiffiffieffiffiaffifficffiffihffiffi meanwasbasedonsix replicates,sothestandarderrorofthemeanis s2=n 4961=628:75.Weshalldraw an error bar up 28.75 from each mean and down by the same distance, so we need five values, one for each bar, each of 28.75: se<-rep(28.75,5) Now we can use the new function to add the error bars to the plot: error.bars(heights,se) ANALYSIS OFVARIANCE 165 Wedonotgetthesamefeelforthedistributionofthevalueswithineachtreatmentaswas obtainedbythebox-and-whiskerplot,butwecancertainlyseeclearlywhichmeansarenot significantlydifferent.If,ashere,weuse1standarderrorasthelengthoftheerrorbars, thenwhenthebarsoverlapthisimpliesthatthetwomeansarenotsignificantlydifferent.",
    "target": "Q: What is Here? A: Here is the one-way ANOVA: model<-aov(biomassclipping) summary(model) Df SumSq MeanSq Fvalue Pr(>F) clipping 4 85356 21339 4.3015 0.008752** Residuals 25 124020 4961 FromtheANOVAtablewecanseethatthepoolederrorvariances2=4961.Nowwe needtoknowhowmanynumberswereusedinthecalculationofeachofthefivemeans: table(clipping) clipping control n25 n50 r10 r5 6 6 6 6 6 Therewasequalreplication(whichmakeslpifeffiffiffieffiffiaffiffisffiffiier)p,affinffiffidffiffiffiffieffiffiaffifficffiffihffiffi meanwasbasedonsix replicates,sothestandarderrorofthemeanis s2=n 4961=628:75.Weshalldraw an error bar up 28.75 from each mean and down by the same distance, so we need five values, one for each bar, each of 28.75: se<-rep(28.75,5) Now we can use the new function to add the error bars to the plot: error.bars(heights,se) ANALYSIS OFVARIANCE 165 Wedonotgetthesamefeelforthedistributionofthevalueswithineachtreatmentaswas obtainedbythebox-and-whiskerplot,butwecancertainlyseeclearlywhichmeansarenot significantlydifferent.If,ashere,weuse1standarderrorasthelengthoftheerrorbars, thenwhenthebarsoverlapthisimpliesthatthetwomeansarenotsignificantlydifferent."
  },
  {
    "input": "An alternative graphical method is to use 95% confidence intervals for the lengths of thebars,ratherthanstandarderrorsofmeans.Thisiseasytodo:wemultiplyourstandard errors by Students t, qt(.975,5)=2.570582, to get the lengths of the confidence intervals: ci<-se*qt(.975,5) barplot(heights,col=\"green\",ylim=c(0,700), ylab=\"meanbiomass\",xlab=\"competitiontreatment\") error.bars(heights,ci) 166 STATISTICS:ANINTRODUCTIONUSINGR Now,alloftheerrorbarsoverlap,implyingvisuallythattherearenosignificantdifferences betweenthemeans.Butweknowthatthisisnottruefromouranalysisofvariance,inwhich werejectedthenullhypothesisthatallthemeanswerethesameatp=0.00875.Ifitwerethe casethatthebarsdidnotoverlapwhenweareusingconfidenceintervals(ashere),thenthat wouldimplythatthemeansdifferedbymorethan4standarderrors,andthisismuchgreater thanthedifferencerequiredforsignificance.Sothisisnotperfecteither.Withstandarderrors wecouldbesurethatthemeanswerenotsignificantlydifferentwhenthebarsdidoverlap.",
    "target": "Q: What is An alternative graphical method? A: An alternative graphical method is to use 95% confidence intervals for the lengths of thebars,ratherthanstandarderrorsofmeans.Thisiseasytodo:wemultiplyourstandard errors by Students t, qt(.975,5)=2.570582, to get the lengths of the confidence intervals: ci<-se*qt(.975,5) barplot(heights,col=\"green\",ylim=c(0,700), ylab=\"meanbiomass\",xlab=\"competitiontreatment\") error.bars(heights,ci) 166 STATISTICS:ANINTRODUCTIONUSINGR Now,alloftheerrorbarsoverlap,implyingvisuallythattherearenosignificantdifferences betweenthemeans.Butweknowthatthisisnottruefromouranalysisofvariance,inwhich werejectedthenullhypothesisthatallthemeanswerethesameatp=0.00875.Ifitwerethe casethatthebarsdidnotoverlapwhenweareusingconfidenceintervals(ashere),thenthat wouldimplythatthemeansdifferedbymorethan4standarderrors,andthisismuchgreater thanthedifferencerequiredforsignificance.Sothisisnotperfecteither.Withstandarderrors wecouldbesurethatthemeanswerenotsignificantlydifferentwhenthebarsdidoverlap."
  },
  {
    "input": "The answer is yes, we can, if we use LSD bars (LSD stands for least significant difference).",
    "target": "Q: What is The answer? A: The answer is yes, we can, if we use LSD bars (LSD stands for least significant difference)."
  },
  {
    "input": "Let us revisit the formula for Students t test: a difference t  standard error of the difference Wesaythatthedifferenceissignificantwhent>2(bytheruleofthumb,ort>qt(0.975,df) ifwewanttobemoreprecise).Wecanrearrangethisformulatofindthesmallestdifference thatwewouldregardasbeingsignificant.Wecancallthistheleastsignificantdifference: LSD=qt(0.975,df)standard error of a difference \u00192\u0002SE diff In our present example this is qt(0.975,10)*sqrt(2*4961/6) [1]90.60794 ANALYSIS OFVARIANCE 167 becauseadifferenceisbasedon122=10degreesoffreedom.Whatwearesayingisthat thetwomeanswouldbesignificantlydifferentiftheydifferedby90.61ormore.Howcan weshowthisgraphically?Wewantoverlappingbarstoindicateadifferencelessthan90.61, andnon-overlappingbarstorepresentadifferencegreaterthan90.61.Withabitofthought youwillrealizethatweneedtodrawbarsthatareLSD/2inlength,upanddownfromeach mean.",
    "target": "Q: What is Let us revisit the formula for Students t test: a difference t  standard error of the difference Wesaythatthedifferenceissignificantwhent>2(bytheruleofthumb,ort>qt(0.975,df) ifwewanttobemoreprecise).Wecanrearrangethisformulatofindthesmallestdifference thatwewouldregardasbeingsignificant.Wecancallthistheleastsignificantdifference: LSD=qt(0.975,df)standard error of a difference \u00192\u0002SE diff In our present example this? A: Let us revisit the formula for Students t test: a difference t  standard error of the difference Wesaythatthedifferenceissignificantwhent>2(bytheruleofthumb,ort>qt(0.975,df) ifwewanttobemoreprecise).Wecanrearrangethisformulatofindthesmallestdifference thatwewouldregardasbeingsignificant.Wecancallthistheleastsignificantdifference: LSD=qt(0.975,df)standard error of a difference \u00192\u0002SE diff In our present example this is qt(0.975,10)*sqrt(2*4961/6) [1]90.60794 ANALYSIS OFVARIANCE 167 becauseadifferenceisbasedon122=10degreesoffreedom.Whatwearesayingisthat thetwomeanswouldbesignificantlydifferentiftheydifferedby90.61ormore.Howcan weshowthisgraphically?Wewantoverlappingbarstoindicateadifferencelessthan90.61, andnon-overlappingbarstorepresentadifferencegreaterthan90.61.Withabitofthought youwillrealizethatweneedtodrawbarsthatareLSD/2inlength,upanddownfromeach mean."
  },
  {
    "input": "The control biomass is significantly lower than any of the four treatments, but none of the four treatments is significantlydifferentfromanyother.Thestatisticalanalysisofthiscontrastisexplainedin detailinChapter12.Sadly,mostjournaleditorsinsistonerrorbarsof1standarderror.Itis truethattherearecomplicatingissuestodowithLSDbars(notleastthevexedquestionof multiplecomparisons;seep.17),butatleastLSD/2barsdowhatwasintendedbytheerror plot (i.e.",
    "target": "Q: What is The control biomass? A: The control biomass is significantly lower than any of the four treatments, but none of the four treatments is significantlydifferentfromanyother.Thestatisticalanalysisofthiscontrastisexplainedin detailinChapter12.Sadly,mostjournaleditorsinsistonerrorbarsof1standarderror.Itis truethattherearecomplicatingissuestodowithLSDbars(notleastthevexedquestionof multiplecomparisons;seep.17),butatleastLSD/2barsdowhatwasintendedbytheerror plot (i.e."
  },
  {
    "input": "The model is gaindiet+supplement+diet:supplement but this can be simplified using the asterisk notation like this: model<-aov(gaindiet*supplement) summary(model) Df SumSq MeanSq Fvalue Pr(>F) diet 2 287.17 143.59 83.52 3.00e-14*** supplement 3 91.88 30.63 17.82 2.95e-07*** diet:supplement 6 3.41 0.57 0.33 0.917 Residuals 36 61.89 1.72 The ANOVA table shows that there is no hint of any interaction between the two explanatoryvariables(p=0.9166);evidentlytheeffectsofdietandsupplementareadditive butbotharehighlysignificant.Nowthatweknowthattheerrorvariances2=1.72wecan add the error bars to the plot.",
    "target": "Q: What is The model? A: The model is gaindiet+supplement+diet:supplement but this can be simplified using the asterisk notation like this: model<-aov(gaindiet*supplement) summary(model) Df SumSq MeanSq Fvalue Pr(>F) diet 2 287.17 143.59 83.52 3.00e-14*** supplement 3 91.88 30.63 17.82 2.95e-07*** diet:supplement 6 3.41 0.57 0.33 0.917 Residuals 36 61.89 1.72 The ANOVA table shows that there is no hint of any interaction between the two explanatoryvariables(p=0.9166);evidentlytheeffectsofdietandsupplementareadditive butbotharehighlysignificant.Nowthatweknowthattheerrorvariances2=1.72wecan add the error bars to the plot."
  },
  {
    "input": "Dont forget to add the legend: legend(locator(1),labels,gray(shade)) ANALYSIS OFVARIANCE 171 The disadvantage of the ANOVA table is that it does not show us the effect sizes, and doesnotallowustoworkouthowmanylevelsofeachofthetwofactorsaresignificantly different.Asapreliminarytomodelsimplification,summary.lmisoftenmoreusefulthan summary.aov: summary.lm(model) Coefficients: Estimate Std.Error t value Pr(>|t|) (Intercept) 26.3485 0.6556 40.191 <2e-16*** dietoats -3.0501 0.9271 -3.2900.002248** dietwheat -6.7094 0.9271 -7.2371.61e-08*** supplementcontrol -3.0518 0.9271 -3.2920.002237** supplementsupergain -3.8824 0.9271 -4.1870.000174*** supplementsupersupp -0.7732 0.9271 -0.8340.409816 dietoats:supplementcontrol 0.2471 1.3112 0.1880.851571 dietwheat:supplementcontrol 0.8183 1.3112 0.6240.536512 dietoats:supplementsupergain 0.2470 1.3112 0.1880.851652 dietwheat:supplementsupergain 1.2557 1.3112 0.9580.344601 dietoats:supplementsupersupp -0.6650 1.3112 -0.5070.615135 dietwheat:supplementsupersupp 0.8024 1.3112 0.6120.544381 Residualstandarderror:1.311on36degreesoffreedom MultipleR-squared: 0.8607, AdjustedR-squared: 0.8182 F-statistic:20.22on11and36DF, p-value:3.295e-12 172 STATISTICS:ANINTRODUCTIONUSINGR Thisisarathercomplexmodel,becausethereare12estimatedparameters(thenumberof rowsinthetable):6maineffectsand6interactions.Theoutputre-emphasizesthatnoneof the interaction terms is significant, but it suggests that the minimal adequate model will require five parameters:an intercept,a difference duetooats,a difference duetowheat, a difference due to control and difference due to supergain (these are the five rows with significancestars).Thisdrawsattentiontothemainshortcomingofusingtreatmentcontrasts asthedefault.Ifyoulookcarefullyatthetable,youwillseethattheeffectsizesoftwoofthe supplements, control and supergain, are not significantly different from one another.",
    "target": "Q: What is Dont forget to add the legend: legend(locator(1),labels,gray(shade)) ANALYSIS OFVARIANCE 171 The disadvantage of the ANOVA table? A: Dont forget to add the legend: legend(locator(1),labels,gray(shade)) ANALYSIS OFVARIANCE 171 The disadvantage of the ANOVA table is that it does not show us the effect sizes, and doesnotallowustoworkouthowmanylevelsofeachofthetwofactorsaresignificantly different.Asapreliminarytomodelsimplification,summary.lmisoftenmoreusefulthan summary.aov: summary.lm(model) Coefficients: Estimate Std.Error t value Pr(>|t|) (Intercept) 26.3485 0.6556 40.191 <2e-16*** dietoats -3.0501 0.9271 -3.2900.002248** dietwheat -6.7094 0.9271 -7.2371.61e-08*** supplementcontrol -3.0518 0.9271 -3.2920.002237** supplementsupergain -3.8824 0.9271 -4.1870.000174*** supplementsupersupp -0.7732 0.9271 -0.8340.409816 dietoats:supplementcontrol 0.2471 1.3112 0.1880.851571 dietwheat:supplementcontrol 0.8183 1.3112 0.6240.536512 dietoats:supplementsupergain 0.2470 1.3112 0.1880.851652 dietwheat:supplementsupergain 1.2557 1.3112 0.9580.344601 dietoats:supplementsupersupp -0.6650 1.3112 -0.5070.615135 dietwheat:supplementsupersupp 0.8024 1.3112 0.6120.544381 Residualstandarderror:1.311on36degreesoffreedom MultipleR-squared: 0.8607, AdjustedR-squared: 0.8182 F-statistic:20.22on11and36DF, p-value:3.295e-12 172 STATISTICS:ANINTRODUCTIONUSINGR Thisisarathercomplexmodel,becausethereare12estimatedparameters(thenumberof rowsinthetable):6maineffectsand6interactions.Theoutputre-emphasizesthatnoneof the interaction terms is significant, but it suggests that the minimal adequate model will require five parameters:an intercept,a difference duetooats,a difference duetowheat, a difference due to control and difference due to supergain (these are the five rows with significancestars).Thisdrawsattentiontothemainshortcomingofusingtreatmentcontrasts asthedefault.Ifyoulookcarefullyatthetable,youwillseethattheeffectsizesoftwoofthe supplements, control and supergain, are not significantly different from one another."
  },
  {
    "input": "Supersupp is not obviously different from the agrimore (\u00000.727 with standard error 0.509).",
    "target": "Q: What is Supersupp? A: Supersupp is not obviously different from the agrimore (\u00000.727 with standard error 0.509)."
  },
  {
    "input": "Nor is supergain obviously different from the unsup- plementedcontrolanimals(3.38\u00002.70=0.68).Weshalltryanewtwo-levelfactortoreplace thefour-levelsupplement,andseeifthissignificantlyreducesthemodelsexplanatorypower.",
    "target": "Q: What is Nor? A: Nor is supergain obviously different from the unsup- plementedcontrolanimals(3.38\u00002.70=0.68).Weshalltryanewtwo-levelfactortoreplace thefour-levelsupplement,andseeifthissignificantlyreducesthemodelsexplanatorypower."
  },
  {
    "input": "This is theminimal adequate model:now all of the parameters are significantly different from zero and from one another: summary(model2) Coefficients: Estimate Std.",
    "target": "Q: What is This? A: This is theminimal adequate model:now all of the parameters are significantly different from zero and from one another: summary(model2) Coefficients: Estimate Std."
  },
  {
    "input": "Pseudoreplication:NestedDesignsandSplitPlots Themodel-fittingfunctionsaov,lmeandlmerhavethefacilitytodealwithcomplicated errorstructures.Detailedanalysisofthesetopicsisbeyondthescopeofthisbook(seeTheR Book (Crawley, 2013) for worked examples), but it is important that you can recognize them, and hence avoid the pitfalls of pseudoreplication.",
    "target": "Q: What is Pseudoreplication:NestedDesignsandSplitPlots Themodel-fittingfunctionsaov,lmeandlmerhavethefacilitytodealwithcomplicated errorstructures.Detailedanalysisofthesetopicsisbeyondthescopeofthisbook(seeTheR Book (Crawley, 2013) for worked examples), but it? A: Pseudoreplication:NestedDesignsandSplitPlots Themodel-fittingfunctionsaov,lmeandlmerhavethefacilitytodealwithcomplicated errorstructures.Detailedanalysisofthesetopicsisbeyondthescopeofthisbook(seeTheR Book (Crawley, 2013) for worked examples), but it is important that you can recognize them, and hence avoid the pitfalls of pseudoreplication."
  },
  {
    "input": "Note that the non-significant main effect for density (p=0.053) does not mean that density is unimportant, because densityappearsinasignificantinteractionwithirrigation(thedensitytermscancelout when averaged over the two irrigation treatments; see below).",
    "target": "Q: What is Note that the non-significant main effect for density (p=0.053) does not mean that density? A: Note that the non-significant main effect for density (p=0.053) does not mean that density is unimportant, because densityappearsinasignificantinteractionwithirrigation(thedensitytermscancelout when averaged over the two irrigation treatments; see below)."
  },
  {
    "input": "The best way to understand the two significant interaction terms is to plot them using interac- tion.plot like this: interaction.plot(fertilizer,irrigation,yield) Irrigation increases yield proportionately more on the P-fertilized plots than on the N-fertilized plots.",
    "target": "Q: What is The best way to understand the two significant interaction terms? A: The best way to understand the two significant interaction terms is to plot them using interac- tion.plot like this: interaction.plot(fertilizer,irrigation,yield) Irrigation increases yield proportionately more on the P-fertilized plots than on the N-fertilized plots."
  },
  {
    "input": "The irrigation:density interaction is more complicated: interaction.plot(density,irrigation,yield) 176 STATISTICS:ANINTRODUCTIONUSINGR Ontheirrigatedplots,yieldisminimalonthelow-densityplots,butoncontrolplotsyield is minimal on the high-density plots.",
    "target": "Q: What is The irrigation:density interaction? A: The irrigation:density interaction is more complicated: interaction.plot(density,irrigation,yield) 176 STATISTICS:ANINTRODUCTIONUSINGR Ontheirrigatedplots,yieldisminimalonthelow-densityplots,butoncontrolplotsyield is minimal on the high-density plots."
  },
  {
    "input": "RandomEffectsandNestedDesigns Mixedeffectsmodelsaresocalledbecausetheexplanatoryvariablesareamixtureoffixed effects and random effects: (cid:129) fixed effects influence only the mean of y (cid:129) random effects influence only the variance of y A random effect should be thought of as coming from a population of effects: the existence of this population is an extra assumption.",
    "target": "Q: What is RandomEffectsandNestedDesigns Mixedeffectsmodelsaresocalledbecausetheexplanatoryvariablesareamixtureoffixed effects and random effects: (cid:129) fixed effects influence only the mean of y (cid:129) random effects influence only the variance of y A random effect should be thought of as coming from a population of effects: the existence of this population? A: RandomEffectsandNestedDesigns Mixedeffectsmodelsaresocalledbecausetheexplanatoryvariablesareamixtureoffixed effects and random effects: (cid:129) fixed effects influence only the mean of y (cid:129) random effects influence only the variance of y A random effect should be thought of as coming from a population of effects: the existence of this population is an extra assumption."
  },
  {
    "input": "Mixed effects models are particularly useful in cases where there is temporal pseudoreplication (repeated measure- ments) and/or spatial pseudoreplication (e.g.",
    "target": "Q: What is Mixed effects models are particularly useful in cases where there? A: Mixed effects models are particularly useful in cases where there is temporal pseudoreplication (repeated measure- ments) and/or spatial pseudoreplication (e.g."
  },
  {
    "input": "It is difficult without lots of experience to know when to use categorical explanatory variables as fixed effects and when as random effects.",
    "target": "Q: What is It? A: It is difficult without lots of experience to know when to use categorical explanatory variables as fixed effects and when as random effects."
  },
  {
    "input": "Yes means fixed effects in a split-plot design (see p. 173) 178 STATISTICS:ANINTRODUCTIONUSINGR (cid:129) Isitahierarchicalobservationalstudy?Yesmeansrandomeffects,perhapsinavariance components analysis (see p. 183) (cid:129) When your model contains both fixed and random effects, use mixed effects models (cid:129) If your model structure is linear, use linear mixed effects, lmer (cid:129) Otherwise, specify the model equation and use non-linear mixed effects, nlme RemovingthePseudoreplication The extreme response to pseudoreplication in a data set is simply to eliminate it.",
    "target": "Q: What is Yes means fixed effects in a split-plot design (see p. 173) 178 STATISTICS:ANINTRODUCTIONUSINGR (cid:129) Isitahierarchicalobservationalstudy?Yesmeansrandomeffects,perhapsinavariance components analysis (see p. 183) (cid:129) When your model contains both fixed and random effects, use mixed effects models (cid:129) If your model structure? A: Yes means fixed effects in a split-plot design (see p. 173) 178 STATISTICS:ANINTRODUCTIONUSINGR (cid:129) Isitahierarchicalobservationalstudy?Yesmeansrandomeffects,perhapsinavariance components analysis (see p. 183) (cid:129) When your model contains both fixed and random effects, use mixed effects models (cid:129) If your model structure is linear, use linear mixed effects, lmer (cid:129) Otherwise, specify the model equation and use non-linear mixed effects, nlme RemovingthePseudoreplication The extreme response to pseudoreplication in a data set is simply to eliminate it."
  },
  {
    "input": "differences in growth rates between successive times) (cid:129) inferences made with each of the separate analyses are not independent, and it is not always clear how they should be combined AnalysisofLongitudinalData The key feature of longitudinal data is that the same individuals are measured repeatedly through time.",
    "target": "Q: What is differences in growth rates between successive times) (cid:129) inferences made with each of the separate analyses are not independent, and it? A: differences in growth rates between successive times) (cid:129) inferences made with each of the separate analyses are not independent, and it is not always clear how they should be combined AnalysisofLongitudinalData The key feature of longitudinal data is that the same individuals are measured repeatedly through time."
  },
  {
    "input": "The advantage of longitudinal studies is that they are capable of separating age effects from cohort effects; these are inextricably confounded in cross-sectional studies.",
    "target": "Q: What is The advantage of longitudinal studies? A: The advantage of longitudinal studies is that they are capable of separating age effects from cohort effects; these are inextricably confounded in cross-sectional studies."
  },
  {
    "input": "This is particularly important when differences between years mean that cohorts originating at different times experience different conditions, so that individuals of the same age in differentcohortswouldbeexpectedtodiffer.Therearetwoextremecasesinlongitudinal studies: (cid:129) a few measurements on a large number of individuals (cid:129) a large number of measurements on a few individuals In thefirst case it isdifficult tofitan accuratemodelfor changewithinindividuals,but treatmenteffectsarelikelytobetestedeffectively.Inthesecondcase,itispossibletogetan accuratemodelofthewayindividualschangethoughtime,butthereislesspowerfortesting thesignificanceoftreatmenteffects,especiallyifvariationfromindividualtoindividualis large.Inthefirstcase,lessattentionwillbepaidtoestimatingthecorrelationstructure,whilein thesecondcasethecovariancemodelwillbetheprincipalfocusofattention.Theaimsare: ANALYSIS OFVARIANCE 179 (cid:129) to estimate the average time course of a process (cid:129) tocharacterizethedegreeofheterogeneityfromindividualtoindividualintherateofthe process (cid:129) identify the factors associated with both of these, including possible cohort effects Theresponseisnottheindividualmeasurement,butthesequenceofmeasurementsonan individualsubject.Thisenablesustodistinguishbetweenageeffectsandyeareffects(see Diggle, Liang and Zeger, 1994, for details).",
    "target": "Q: What is This? A: This is particularly important when differences between years mean that cohorts originating at different times experience different conditions, so that individuals of the same age in differentcohortswouldbeexpectedtodiffer.Therearetwoextremecasesinlongitudinal studies: (cid:129) a few measurements on a large number of individuals (cid:129) a large number of measurements on a few individuals In thefirst case it isdifficult tofitan accuratemodelfor changewithinindividuals,but treatmenteffectsarelikelytobetestedeffectively.Inthesecondcase,itispossibletogetan accuratemodelofthewayindividualschangethoughtime,butthereislesspowerfortesting thesignificanceoftreatmenteffects,especiallyifvariationfromindividualtoindividualis large.Inthefirstcase,lessattentionwillbepaidtoestimatingthecorrelationstructure,whilein thesecondcasethecovariancemodelwillbetheprincipalfocusofattention.Theaimsare: ANALYSIS OFVARIANCE 179 (cid:129) to estimate the average time course of a process (cid:129) tocharacterizethedegreeofheterogeneityfromindividualtoindividualintherateofthe process (cid:129) identify the factors associated with both of these, including possible cohort effects Theresponseisnottheindividualmeasurement,butthesequenceofmeasurementsonan individualsubject.Thisenablesustodistinguishbetweenageeffectsandyeareffects(see Diggle, Liang and Zeger, 1994, for details)."
  },
  {
    "input": "The technique is weak when the values of the explanatory variables change through time.",
    "target": "Q: What is The technique? A: The technique is weak when the values of the explanatory variables change through time."
  },
  {
    "input": "Derived variable analysis makes most sense when it is based on the parameters of scientifically interpretable non-linear models from each time sequence.",
    "target": "Q: What is Derived variable analysis makes most sense when it? A: Derived variable analysis makes most sense when it is based on the parameters of scientifically interpretable non-linear models from each time sequence."
  },
  {
    "input": "The factor levels are numbers, so we need to declare the explanatory variables to be categorical before we begin: Treatment<-factor(Treatment) Rat<-factor(Rat) Liver<-factor(Liver) Here is the analysis done the wrong way: model<-aov(GlycogenTreatment) summary(model) Df SumSq Mean Sq F value Pr(>F) Treatment 2 1558 778.8 14.5 3.03e-05*** Residuals 33 1773 53.7 Treatmenthasahighlysignificanteffectonliverglycogencontent(p=0.00003).Wrong!",
    "target": "Q: What is The factor levels are numbers, so we need to declare the explanatory variables to be categorical before we begin: Treatment<-factor(Treatment) Rat<-factor(Rat) Liver<-factor(Liver) Here? A: The factor levels are numbers, so we need to declare the explanatory variables to be categorical before we begin: Treatment<-factor(Treatment) Rat<-factor(Rat) Liver<-factor(Liver) Here is the analysis done the wrong way: model<-aov(GlycogenTreatment) summary(model) Df SumSq Mean Sq F value Pr(>F) Treatment 2 1558 778.8 14.5 3.03e-05*** Residuals 33 1773 53.7 Treatmenthasahighlysignificanteffectonliverglycogencontent(p=0.00003).Wrong!"
  },
  {
    "input": "Here is the analysis of variance done properly, averaging away the pseudoreplication.",
    "target": "Q: What is Here? A: Here is the analysis of variance done properly, averaging away the pseudoreplication."
  },
  {
    "input": "Sumsofsquaresinhierarchicaldesigns The trick to understanding these sums of squares is to appreciate that with nested categorical explanatory variables (random effects) the correction factor, which is (cid:2)P (cid:3) subtracted from the sum of squared subtotals, is not the conventional y 2=kn.",
    "target": "Q: What is Sumsofsquaresinhierarchicaldesigns The trick to understanding these sums of squares? A: Sumsofsquaresinhierarchicaldesigns The trick to understanding these sums of squares is to appreciate that with nested categorical explanatory variables (random effects) the correction factor, which is (cid:2)P (cid:3) subtracted from the sum of squared subtotals, is not the conventional y 2=kn."
  },
  {
    "input": "Instead, the correction factor is the uncorrected sum of squared subtotals from the levelinthehierarchyimmediatelyabovethelevelinquestion.Thisisveryhardtosee without lots of practice.",
    "target": "Q: What is Instead, the correction factor? A: Instead, the correction factor is the uncorrected sum of squared subtotals from the levelinthehierarchyimmediatelyabovethelevelinquestion.Thisisveryhardtosee without lots of practice."
  },
  {
    "input": "In this case, in the formula for SSA, above, has n=12 andkn=36.Weneedtocalculatesumsofsquaresforratswithintreatments,SS , Rats liver bits within rats within treatments, SS , and preparations within liver bits Liverbits within rats within treatments, SS : Preparations P P R2 C2 SS  \u0000 Rats 6 12 P P L2 R2 SS  \u0000 Liverbits 2 6 P P y2 L2 SS  \u0000 Preparations 1 2 The correction factor at any level is the uncorrected sum of squares from the level above.",
    "target": "Q: What is In this case, in the formula for SSA, above, has n=12 andkn=36.Weneedtocalculatesumsofsquaresforratswithintreatments,SS , Rats liver bits within rats within treatments, SS , and preparations within liver bits Liverbits within rats within treatments, SS : Preparations P P R2 C2 SS  \u0000 Rats 6 12 P P L2 R2 SS  \u0000 Liverbits 2 6 P P y2 L2 SS  \u0000 Preparations 1 2 The correction factor at any level? A: In this case, in the formula for SSA, above, has n=12 andkn=36.Weneedtocalculatesumsofsquaresforratswithintreatments,SS , Rats liver bits within rats within treatments, SS , and preparations within liver bits Liverbits within rats within treatments, SS : Preparations P P R2 C2 SS  \u0000 Rats 6 12 P P L2 R2 SS  \u0000 Liverbits 2 6 P P y2 L2 SS  \u0000 Preparations 1 2 The correction factor at any level is the uncorrected sum of squares from the level above."
  },
  {
    "input": "We add the three components together to do this: vc<-c(21.17,14.165,36.065) 100*vc/sum(vc) [1]29.6498619.8389450.51120 Thats all there is to it.",
    "target": "Q: What is We add the three components together to do this: vc<-c(21.17,14.165,36.065) 100*vc/sum(vc) [1]29.6498619.8389450.51120 Thats all there? A: We add the three components together to do this: vc<-c(21.17,14.165,36.065) 100*vc/sum(vc) [1]29.6498619.8389450.51120 Thats all there is to it."
  },
  {
    "input": "Analysisofcovarianceisdoneinthefamiliarway:itisjustthattheexplanatoryvariablesare amixtureofcontinuousandcategoricalvariables.Westartbyfittingthemostcomplicated model,withdifferentslopesandinterceptsforthegrazedandungrazedplants.Forthis,weuse theasteriskoperator: model<-lm(FruitRoot*Grazing) Animportantthingtorealizeaboutanalysisofcovarianceisthatordermatters.Lookat the regression sum of squares in the ANOVA table when we fit root first: summary.aov(model) Df Sum Sq Mean Sq F value Pr(>F) Root 1 16795 16795 359.968 <2e-16*** Grazing 1 5264 5264 112.832 1.21e-12*** 188 STATISTICS:ANINTRODUCTIONUSINGR Root:Grazing 1 5 5 0.103 0.75 Residuals 36 1680 47 and when we fit root second: model<-lm(FruitGrazing*Root) summary.aov(model) Grazing 1 2910 2910 62.380 2.26e-09*** Root 1 19149 19149 410.420 <2e-16*** Grazing:Root 1 5 5 0.103 0.75 Residuals 36 1680 47 Inbothcases,theerrorsumofsquares(1680)andtheinteractionsumofsquares(5)are the same, but the regression sum of squares (labelled Root) is much greater when root is fittedtothemodelaftergrazing(19,149),thanwhenitisfittedfirst(16,795).Thisisbecause thedataforANCOVAaretypicallynon-orthogonal.Remember,withnon-orthogonaldata, order matters (Box 9.1).",
    "target": "Q: What is Analysisofcovarianceisdoneinthefamiliarway:itisjustthattheexplanatoryvariablesare amixtureofcontinuousandcategoricalvariables.Westartbyfittingthemostcomplicated model,withdifferentslopesandinterceptsforthegrazedandungrazedplants.Forthis,weuse theasteriskoperator: model<-lm(FruitRoot*Grazing) Animportantthingtorealizeaboutanalysisofcovarianceisthatordermatters.Lookat the regression sum of squares in the ANOVA table when we fit root first: summary.aov(model) Df Sum Sq Mean Sq F value Pr(>F) Root 1 16795 16795 359.968 <2e-16*** Grazing 1 5264 5264 112.832 1.21e-12*** 188 STATISTICS:ANINTRODUCTIONUSINGR Root:Grazing 1 5 5 0.103 0.75 Residuals 36 1680 47 and when we fit root second: model<-lm(FruitGrazing*Root) summary.aov(model) Grazing 1 2910 2910 62.380 2.26e-09*** Root 1 19149 19149 410.420 <2e-16*** Grazing:Root 1 5 5 0.103 0.75 Residuals 36 1680 47 Inbothcases,theerrorsumofsquares(1680)andtheinteractionsumofsquares(5)are the same, but the regression sum of squares (labelled Root)? A: Analysisofcovarianceisdoneinthefamiliarway:itisjustthattheexplanatoryvariablesare amixtureofcontinuousandcategoricalvariables.Westartbyfittingthemostcomplicated model,withdifferentslopesandinterceptsforthegrazedandungrazedplants.Forthis,weuse theasteriskoperator: model<-lm(FruitRoot*Grazing) Animportantthingtorealizeaboutanalysisofcovarianceisthatordermatters.Lookat the regression sum of squares in the ANOVA table when we fit root first: summary.aov(model) Df Sum Sq Mean Sq F value Pr(>F) Root 1 16795 16795 359.968 <2e-16*** Grazing 1 5264 5264 112.832 1.21e-12*** 188 STATISTICS:ANINTRODUCTIONUSINGR Root:Grazing 1 5 5 0.103 0.75 Residuals 36 1680 47 and when we fit root second: model<-lm(FruitGrazing*Root) summary.aov(model) Grazing 1 2910 2910 62.380 2.26e-09*** Root 1 19149 19149 410.420 <2e-16*** Grazing:Root 1 5 5 0.103 0.75 Residuals 36 1680 47 Inbothcases,theerrorsumofsquares(1680)andtheinteractionsumofsquares(5)are the same, but the regression sum of squares (labelled Root) is much greater when root is fittedtothemodelaftergrazing(19,149),thanwhenitisfittedfirst(16,795).Thisisbecause thedataforANCOVAaretypicallynon-orthogonal.Remember,withnon-orthogonaldata, order matters (Box 9.1)."
  },
  {
    "input": "Now we can compute SSE by difference: SSE SSY \u0000SSA\u0000SSR\u0000SSR diff ANALYSIS OFCOVARIANCE 189 But SSE is defined for the k levels in which the regressions were computed as Xk X SSE  y\u0000a \u0000bx2 i i i1 Of course, both methods give the same answer.",
    "target": "Q: What is Now we can compute SSE by difference: SSE SSY \u0000SSA\u0000SSR\u0000SSR diff ANALYSIS OFCOVARIANCE 189 But SSE? A: Now we can compute SSE by difference: SSE SSY \u0000SSA\u0000SSR\u0000SSR diff ANALYSIS OFCOVARIANCE 189 But SSE is defined for the k levels in which the regressions were computed as Xk X SSE  y\u0000a \u0000bx2 i i i1 Of course, both methods give the same answer."
  },
  {
    "input": "The hard thing about ANCOVA is understanding what the parameterestimatesmean.Startingatthetop,thefirstrow,aslabelled,containsanintercept.",
    "target": "Q: What is The hard thing about ANCOVA? A: The hard thing about ANCOVA is understanding what the parameterestimatesmean.Startingatthetop,thefirstrow,aslabelled,containsanintercept."
  },
  {
    "input": "It is the intercept for the graph of seed production against initial rootstock size for the 190 STATISTICS:ANINTRODUCTIONUSINGR grazingtreatmentwhosefactorlevelcomesfirstinthealphabet.Toseewhichonethisis,we can use levels: levels(Grazing) [1]\"Grazed\" \"Ungrazed\" Sotheinterceptisforthegrazedplants.Thesecondrow,labelledGrazingUngrazed, is a difference between two intercepts.",
    "target": "Q: What is It? A: It is the intercept for the graph of seed production against initial rootstock size for the 190 STATISTICS:ANINTRODUCTIONUSINGR grazingtreatmentwhosefactorlevelcomesfirstinthealphabet.Toseewhichonethisis,we can use levels: levels(Grazing) [1]\"Grazed\" \"Ungrazed\" Sotheinterceptisforthegrazedplants.Thesecondrow,labelledGrazingUngrazed, is a difference between two intercepts."
  },
  {
    "input": "Thethirdrow,labelledRoot,isaslope:itisthegradientofthegraphofseedproduction against initial rootstock size, and it is the same for both grazed and ungrazed plants.",
    "target": "Q: What is Thethirdrow,labelledRoot,isaslope:itisthegradientofthegraphofseedproduction against initial rootstock size, and it? A: Thethirdrow,labelledRoot,isaslope:itisthegradientofthegraphofseedproduction against initial rootstock size, and it is the same for both grazed and ungrazed plants."
  },
  {
    "input": "plants at 7 mm initial root diameter), it is clear that the ungrazed plants (green symbols) producedmoreseedthanthegrazedplants(36.103more,tobeprecise).Thiswillbecome clearer when we fit the lines predicted by model2: abline(-127.829,23.56,col=\"blue\") abline(-127.829+36.103,23.56,col=\"blue\") Thisexampleshowsthegreatstrengthofanalysisofcovariance.Bycontrollingforinitial plantsize,wehavecompletelyreversedtheinterpretation.Thenavefirstimpressionwas that grazing increased seed production: tapply(Fruit,Grazing,mean) Grazed Ungrazed 67.9405 50.8805 andthiswassignificantifwewererashenoughtofitgrazingonitsown(p=0.0268aswe saw earlier).",
    "target": "Q: What is plants at 7 mm initial root diameter), it? A: plants at 7 mm initial root diameter), it is clear that the ungrazed plants (green symbols) producedmoreseedthanthegrazedplants(36.103more,tobeprecise).Thiswillbecome clearer when we fit the lines predicted by model2: abline(-127.829,23.56,col=\"blue\") abline(-127.829+36.103,23.56,col=\"blue\") Thisexampleshowsthegreatstrengthofanalysisofcovariance.Bycontrollingforinitial plantsize,wehavecompletelyreversedtheinterpretation.Thenavefirstimpressionwas that grazing increased seed production: tapply(Fruit,Grazing,mean) Grazed Ungrazed 67.9405 50.8805 andthiswassignificantifwewererashenoughtofitgrazingonitsown(p=0.0268aswe saw earlier)."
  },
  {
    "input": "In many applications, multiple regression is the most difficult of all the statistical models to do well.",
    "target": "Q: What is In many applications, multiple regression? A: In many applications, multiple regression is the most difficult of all the statistical models to do well."
  },
  {
    "input": "It is as well to remember the following truths about models: (cid:129) all models are wrong (cid:129) some models are better than others (cid:129) the correct model can never be known with certainty (cid:129) the simpler the model, the better it is Fitting models to data is the central function of R. The process is essentially one of exploration;therearenofixedrulesandnoabsolutes.Theobjectistodetermineaminimal adequate model from the large set of potential models that might be used to describe the given set of data.",
    "target": "Q: What is It? A: It is as well to remember the following truths about models: (cid:129) all models are wrong (cid:129) some models are better than others (cid:129) the correct model can never be known with certainty (cid:129) the simpler the model, the better it is Fitting models to data is the central function of R. The process is essentially one of exploration;therearenofixedrulesandnoabsolutes.Theobjectistodetermineaminimal adequate model from the large set of potential models that might be used to describe the given set of data."
  },
  {
    "input": "However, it is impossible to maximize a models realism, generality and holism simulta- neously,andtheprincipleofparsimony(Occamsrazor;seep.8)isavitaltoolinhelpingto chooseonemodeloveranother.Thus,wewouldonlyincludeanexplanatoryvariableina modelifitsignificantlyimprovedthefitofamodel.Justbecausewewenttothetroubleof measuringsomething,thatdoesnotmeanwehavetohaveitinourmodel.Parsimonysays that, other things being equal, we prefer: (cid:129) a model with n  1 parameters to a model with n parameters (cid:129) a model with k  1 explanatory variables to a model with k explanatory variables (cid:129) a linear model to a model which is curved (cid:129) a model without a hump to a model with a hump (cid:129) a model without interactions to a model containing interactions between variables Other considerations include a preference for models containing explanatory variables thatareeasytomeasureovervariablesthataredifficultorexpensivetomeasure.Also,we prefer models that are based on a sound mechanistic understanding of the process over purely empirical functions.",
    "target": "Q: What is However, it? A: However, it is impossible to maximize a models realism, generality and holism simulta- neously,andtheprincipleofparsimony(Occamsrazor;seep.8)isavitaltoolinhelpingto chooseonemodeloveranother.Thus,wewouldonlyincludeanexplanatoryvariableina modelifitsignificantlyimprovedthefitofamodel.Justbecausewewenttothetroubleof measuringsomething,thatdoesnotmeanwehavetohaveitinourmodel.Parsimonysays that, other things being equal, we prefer: (cid:129) a model with n  1 parameters to a model with n parameters (cid:129) a model with k  1 explanatory variables to a model with k explanatory variables (cid:129) a linear model to a model which is curved (cid:129) a model without a hump to a model with a hump (cid:129) a model without interactions to a model containing interactions between variables Other considerations include a preference for models containing explanatory variables thatareeasytomeasureovervariablesthataredifficultorexpensivetomeasure.Also,we prefer models that are based on a sound mechanistic understanding of the process over purely empirical functions."
  },
  {
    "input": "Ifnoneoftheparametersissignificant,thenthe minimaladequatemodelisthenullmodel Caveats Model simplification is an important process, but it should not be taken to extremes.",
    "target": "Q: What is Ifnoneoftheparametersissignificant,thenthe minimaladequatemodelisthenullmodel Caveats Model simplification? A: Ifnoneoftheparametersissignificant,thenthe minimaladequatemodelisthenullmodel Caveats Model simplification is an important process, but it should not be taken to extremes."
  },
  {
    "input": "Similarly, it may be preferable to say that the odds of infection increase 10-fold under a given treatment, ratherthantosaythatthelogitsincreaseby2.321(withoutmodelsimplificationthisis equivalent to saying that there is a 10.186-fold increase in the odds).",
    "target": "Q: What is Similarly, it may be preferable to say that the odds of infection increase 10-fold under a given treatment, ratherthantosaythatthelogitsincreaseby2.321(withoutmodelsimplificationthisis equivalent to saying that there? A: Similarly, it may be preferable to say that the odds of infection increase 10-fold under a given treatment, ratherthantosaythatthelogitsincreaseby2.321(withoutmodelsimplificationthisis equivalent to saying that there is a 10.186-fold increase in the odds)."
  },
  {
    "input": "It would be absurd, however, to fix on an estimate of 6 rather than 6.1 just because 6 is a whole number.",
    "target": "Q: What is It would be absurd, however, to fix on an estimate of 6 rather than 6.1 just because 6? A: It would be absurd, however, to fix on an estimate of 6 rather than 6.1 just because 6 is a whole number."
  },
  {
    "input": "This is not a problem in orthogonal designs, because sums of squares can be unequivocally attributed to each factor and interaction term.",
    "target": "Q: What is This? A: This is not a problem in orthogonal designs, because sums of squares can be unequivocally attributed to each factor and interaction term."
  },
  {
    "input": "But as soon as there are missing values or unequal weights, then it is impossible to tell how the parameter estimates and standard errors of the significant terms would have been altered if the non-significant terms had been deleted.",
    "target": "Q: What is But as soon as there are missing values or unequal weights, then it? A: But as soon as there are missing values or unequal weights, then it is impossible to tell how the parameter estimates and standard errors of the significant terms would have been altered if the non-significant terms had been deleted."
  },
  {
    "input": "The best practice is this (cid:129) say whether your data are orthogonal or not (cid:129) present a minimal adequate model (cid:129) givealistofthenon-significanttermsthatwereomitted,andthedeviancechangesthat resulted from their deletion Readers can then judge for themselves the relative magnitude of the non-significant factors, and the importance of correlations between the explanatory variables.",
    "target": "Q: What is The best practice? A: The best practice is this (cid:129) say whether your data are orthogonal or not (cid:129) present a minimal adequate model (cid:129) givealistofthenon-significanttermsthatwereomitted,andthedeviancechangesthat resulted from their deletion Readers can then judge for themselves the relative magnitude of the non-significant factors, and the importance of correlations between the explanatory variables."
  },
  {
    "input": "How is ozone concentration related to wind speed, air temperature and the intensity of solar radiation?",
    "target": "Q: What is How? A: How is ozone concentration related to wind speed, air temperature and the intensity of solar radiation?"
  },
  {
    "input": "ozone.pollution<-read.csv(\"c:\\\\temp\\\\ozone.data.csv\") attach(ozone.pollution) names(ozone.pollution) [1]\"rad\" \"temp\" \"wind\" \"ozone\" Inmultipleregression,itisalways agoodideatousethepairsfunctiontolookatall the correlations: pairs(ozone.pollution,panel=panel.smooth) 198 STATISTICS:ANINTRODUCTIONUSINGR The response variable, ozone concentration, is shown on the y axis of the bottom row of panels: there is a strong negative relationship with wind speed, a positive correlation with temperature and a rather unclear, but possibly humped relationship with radiation.",
    "target": "Q: What is ozone.pollution<-read.csv(\"c:\\\\temp\\\\ozone.data.csv\") attach(ozone.pollution) names(ozone.pollution) [1]\"rad\" \"temp\" \"wind\" \"ozone\" Inmultipleregression,itisalways agoodideatousethepairsfunctiontolookatall the correlations: pairs(ozone.pollution,panel=panel.smooth) 198 STATISTICS:ANINTRODUCTIONUSINGR The response variable, ozone concentration,? A: ozone.pollution<-read.csv(\"c:\\\\temp\\\\ozone.data.csv\") attach(ozone.pollution) names(ozone.pollution) [1]\"rad\" \"temp\" \"wind\" \"ozone\" Inmultipleregression,itisalways agoodideatousethepairsfunctiontolookatall the correlations: pairs(ozone.pollution,panel=panel.smooth) 198 STATISTICS:ANINTRODUCTIONUSINGR The response variable, ozone concentration, is shown on the y axis of the bottom row of panels: there is a strong negative relationship with wind speed, a positive correlation with temperature and a rather unclear, but possibly humped relationship with radiation."
  },
  {
    "input": "The older tree function is better for this graphical search for interactions than is the more modern rpart (but the latter is much better for statistical modelling): par(mfrow=c(1,1)) library(tree) model<-tree(ozone.,data=ozone.pollution) plot(model) text(model) 200 STATISTICS:ANINTRODUCTIONUSINGR This shows that temperature is far and away the most important factor affecting ozone concentration(thelongerthebranchesinthetree,thegreaterthedevianceexplained).Wind speedisimportantatbothhighandlowtemperatures,withstillairbeingassociatedwithhigher meanozonelevels(thefiguresattheendsofthebranchesaremeanozoneconcentrations).",
    "target": "Q: What is The older tree function? A: The older tree function is better for this graphical search for interactions than is the more modern rpart (but the latter is much better for statistical modelling): par(mfrow=c(1,1)) library(tree) model<-tree(ozone.,data=ozone.pollution) plot(model) text(model) 200 STATISTICS:ANINTRODUCTIONUSINGR This shows that temperature is far and away the most important factor affecting ozone concentration(thelongerthebranchesinthetree,thegreaterthedevianceexplained).Wind speedisimportantatbothhighandlowtemperatures,withstillairbeingassociatedwithhigher meanozonelevels(thefiguresattheendsofthebranchesaremeanozoneconcentrations)."
  },
  {
    "input": "In model4, theleast significant quadratic term is for rad, so we delete this: model5<-update(model4,.-I(rad^2)) summary(model5) This deletion hasrendered the temp:rad interactioninsignificant, andcausedthemain effectofradiationtobecomeinsignificant.Weshouldtryremovingthetemp:radinteraction: model6<-update(model5,.-temp:rad) summary(model6) Coefficients: Estimate Std.Error t value Pr (>|t|) (Intercept)291.16758 100.87723 2.886 0.00473** temp -6.33955 2.71627 -2.334 0.02150* wind -13.39674 2.29623 -5.834 6.05e-08*** rad 0.06586 0.02005 3.285 0.00139** I(temp^2) 0.05102 0.01774 2.876 0.00488** I(wind^2) 0.46464 0.10060 4.619 1.10e-05*** Residual standarderror: 18.25 on 105 degreesof freedom Multiple R-Squared:0.713, AdjustedR-squared:0.6994 F-statistic: 52.18 on5 and 105 DF, p-value: 0 Nowwearemakingprogress.Allthetermsinmodel6aresignificant.Atthisstage,we should check the assumptions, using plot(model6): 202 STATISTICS:ANINTRODUCTIONUSINGR Thereisaclearpatternofvarianceincreasingwiththemeanofthefittedvalues.Thisis badnews(heteroscedasticity).Also,thenormalityplotisdistinctlycurved;again,thisisbad news.Letustrytransformationoftheresponsevariable.Therearenozerosintheresponse, so a log transformation is worth trying.",
    "target": "Q: What is In model4, theleast significant quadratic term? A: In model4, theleast significant quadratic term is for rad, so we delete this: model5<-update(model4,.-I(rad^2)) summary(model5) This deletion hasrendered the temp:rad interactioninsignificant, andcausedthemain effectofradiationtobecomeinsignificant.Weshouldtryremovingthetemp:radinteraction: model6<-update(model5,.-temp:rad) summary(model6) Coefficients: Estimate Std.Error t value Pr (>|t|) (Intercept)291.16758 100.87723 2.886 0.00473** temp -6.33955 2.71627 -2.334 0.02150* wind -13.39674 2.29623 -5.834 6.05e-08*** rad 0.06586 0.02005 3.285 0.00139** I(temp^2) 0.05102 0.01774 2.876 0.00488** I(wind^2) 0.46464 0.10060 4.619 1.10e-05*** Residual standarderror: 18.25 on 105 degreesof freedom Multiple R-Squared:0.713, AdjustedR-squared:0.6994 F-statistic: 52.18 on5 and 105 DF, p-value: 0 Nowwearemakingprogress.Allthetermsinmodel6aresignificant.Atthisstage,we should check the assumptions, using plot(model6): 202 STATISTICS:ANINTRODUCTIONUSINGR Thereisaclearpatternofvarianceincreasingwiththemeanofthefittedvalues.Thisis badnews(heteroscedasticity).Also,thenormalityplotisdistinctlycurved;again,thisisbad news.Letustrytransformationoftheresponsevariable.Therearenozerosintheresponse, so a log transformation is worth trying."
  },
  {
    "input": "This is another air pollution dataframe, but the response variable thistimeissulphurdioxideconcentration.Therearesixcontinuousexplanatoryvariables: pollute<-read.csv(\"c:\\\\temp\\\\sulphur.dioxide.csv\") attach(pollute) names(pollute) [1]\"Pollution\" \"Temp\" \"Industry\" \"Population\"\"Wind\" [6]\"Rain\" \"Wet.days\" Here are the 36 paired scatterplots: pairs(pollute,panel=panel.smooth) 204 STATISTICS:ANINTRODUCTIONUSINGR This time, let us begin with the tree model rather than the generalized additive model.",
    "target": "Q: What is This? A: This is another air pollution dataframe, but the response variable thistimeissulphurdioxideconcentration.Therearesixcontinuousexplanatoryvariables: pollute<-read.csv(\"c:\\\\temp\\\\sulphur.dioxide.csv\") attach(pollute) names(pollute) [1]\"Pollution\" \"Temp\" \"Industry\" \"Population\"\"Wind\" [6]\"Rain\" \"Wet.days\" Here are the 36 paired scatterplots: pairs(pollute,panel=panel.smooth) 204 STATISTICS:ANINTRODUCTIONUSINGR This time, let us begin with the tree model rather than the generalized additive model."
  },
  {
    "input": "par(mfrow=c(1,1)) library(tree) model<-tree(Pollution.,data=pollute) plot(model) text(model) MULTIPLE REGRESSION 205 Thistreemodelismuchmorecomplicatedthanwesawinthepreviousozoneexample.It is interpreted as follows.",
    "target": "Q: What is par(mfrow=c(1,1)) library(tree) model<-tree(Pollution.,data=pollute) plot(model) text(model) MULTIPLE REGRESSION 205 Thistreemodelismuchmorecomplicatedthanwesawinthepreviousozoneexample.It? A: par(mfrow=c(1,1)) library(tree) model<-tree(Pollution.,data=pollute) plot(model) text(model) MULTIPLE REGRESSION 205 Thistreemodelismuchmorecomplicatedthanwesawinthepreviousozoneexample.It is interpreted as follows."
  },
  {
    "input": "The most important explanatory variable is industry, and the thresholdvalueseparatinglowandhighvaluesofindustryis748.Therighthandbranchof thetreeindicatesthemeanvalueofairpollutionforhighlevelsofindustry(67.00).Thefact thatthislimbisunbranchedmeansthatnoothervariablesexplainasignificantamountofthe variationinpollutionlevelsforhighvaluesofindustry.Theleft-handlimbdoesnotshow themeanvaluesofpollutionforlowvaluesofindustry,becausethereareothersignificant explanatory variables.",
    "target": "Q: What is The most important explanatory variable? A: The most important explanatory variable is industry, and the thresholdvalueseparatinglowandhighvaluesofindustryis748.Therighthandbranchof thetreeindicatesthemeanvalueofairpollutionforhighlevelsofindustry(67.00).Thefact thatthislimbisunbranchedmeansthatnoothervariablesexplainasignificantamountofthe variationinpollutionlevelsforhighvaluesofindustry.Theleft-handlimbdoesnotshow themeanvaluesofpollutionforlowvaluesofindustry,becausethereareothersignificant explanatory variables."
  },
  {
    "input": "For high values of population, the number of wet days is significant.",
    "target": "Q: What is For high values of population, the number of wet days? A: For high values of population, the number of wet days is significant."
  },
  {
    "input": "Low numbersofwetdays(<108)havemeanpollutionlevelsof12.00,whiletemperaturehasa significant impact on pollution forplaces where thenumber of wet days is large.",
    "target": "Q: What is Low numbersofwetdays(<108)havemeanpollutionlevelsof12.00,whiletemperaturehasa significant impact on pollution forplaces where thenumber of wet days? A: Low numbersofwetdays(<108)havemeanpollutionlevelsof12.00,whiletemperaturehasa significant impact on pollution forplaces where thenumber of wet days is large."
  },
  {
    "input": "The virtues of tree-based models are numerous: (cid:129) they are easy to appreciate and to describe to other people (cid:129) the most important variables stand out (cid:129) interactions are clearly displayed (cid:129) non-linear effects are captured effectively (cid:129) the complexity of the behaviour of the explanatory variables is plain to see 206 STATISTICS:ANINTRODUCTIONUSINGR Weconcludethattheinteractionstructureishighlycomplex.Weshallneedtocarryout the linear modelling with considerable care.",
    "target": "Q: What is The virtues of tree-based models are numerous: (cid:129) they are easy to appreciate and to describe to other people (cid:129) the most important variables stand out (cid:129) interactions are clearly displayed (cid:129) non-linear effects are captured effectively (cid:129) the complexity of the behaviour of the explanatory variables? A: The virtues of tree-based models are numerous: (cid:129) they are easy to appreciate and to describe to other people (cid:129) the most important variables stand out (cid:129) interactions are clearly displayed (cid:129) non-linear effects are captured effectively (cid:129) the complexity of the behaviour of the explanatory variables is plain to see 206 STATISTICS:ANINTRODUCTIONUSINGR Weconcludethattheinteractionstructureishighlycomplex.Weshallneedtocarryout the linear modelling with considerable care."
  },
  {
    "input": "Ofcoursetherecanbenohardandfastanswertothis,butperhapstherearesomeuseful rulesofthumb?Toputthequestionanotherway,giventhatIhave41datapoints,howmany parameters is it reasonable to expect that I shall be able to estimate from the data?",
    "target": "Q: What is Ofcoursetherecanbenohardandfastanswertothis,butperhapstherearesomeuseful rulesofthumb?Toputthequestionanotherway,giventhatIhave41datapoints,howmany parameters? A: Ofcoursetherecanbenohardandfastanswertothis,butperhapstherearesomeuseful rulesofthumb?Toputthequestionanotherway,giventhatIhave41datapoints,howmany parameters is it reasonable to expect that I shall be able to estimate from the data?"
  },
  {
    "input": "This is a problem because: (cid:129) maineffectscanappeartobenon-significantwhenarelationshipisstronglycurved(see p.148)soifIdontfitaquadraticterm(usingupanextraparameterintheprocess)I shant be confident about leaving a variable out of the model (cid:129) main effects can appear to be non-significant when interactions between two or more variables are pronounced (see p. 174) (cid:129) interactionscanonlybeinvestigatedwhenvariablesappeartogetherinthesamemodel (interactionsbetweentwocontinuousexplanatoryvariablesaretypicallyincludedinthe modelasafunctionoftheproductofthetwovariables,andbothmaineffectsshouldbe included when we do this) Theidealstrategyistofitamaximalmodelcontainingalltheexplanatoryvariables,with curvature terms for each variable, along with all possible interactions terms, and then simplify this maximal model, perhaps using step to speed up the initial stages.",
    "target": "Q: What is This? A: This is a problem because: (cid:129) maineffectscanappeartobenon-significantwhenarelationshipisstronglycurved(see p.148)soifIdontfitaquadraticterm(usingupanextraparameterintheprocess)I shant be confident about leaving a variable out of the model (cid:129) main effects can appear to be non-significant when interactions between two or more variables are pronounced (see p. 174) (cid:129) interactionscanonlybeinvestigatedwhenvariablesappeartogetherinthesamemodel (interactionsbetweentwocontinuousexplanatoryvariablesaretypicallyincludedinthe modelasafunctionoftheproductofthetwovariables,andbothmaineffectsshouldbe included when we do this) Theidealstrategyistofitamaximalmodelcontainingalltheexplanatoryvariables,with curvature terms for each variable, along with all possible interactions terms, and then simplify this maximal model, perhaps using step to speed up the initial stages."
  },
  {
    "input": "In our present example, however, this is completely out of the question.",
    "target": "Q: What is In our present example, however, this? A: In our present example, however, this is completely out of the question."
  },
  {
    "input": "MULTIPLE REGRESSION 207 Inthisexample,itisimpossibletofitallcombinationsofvariablessimultaneouslyand hence it is highly likely that important interaction terms could be overlooked.",
    "target": "Q: What is MULTIPLE REGRESSION 207 Inthisexample,itisimpossibletofitallcombinationsofvariablessimultaneouslyand hence it? A: MULTIPLE REGRESSION 207 Inthisexample,itisimpossibletofitallcombinationsofvariablessimultaneouslyand hence it is highly likely that important interaction terms could be overlooked."
  },
  {
    "input": "Notice that step has removed the linear terms for Temp and Wind; normally when we have a quadratic term we retain the linear term even if its slope is not significantlydifferentfromzero.Letusremovethenon-significanttermsfrommodel2to see what happens: model3<-update(model2,.-Rain-I(Wind^2)) summary(model3) Coefficients: EstimateStd.",
    "target": "Q: What is Notice that step has removed the linear terms for Temp and Wind; normally when we have a quadratic term we retain the linear term even if its slope? A: Notice that step has removed the linear terms for Temp and Wind; normally when we have a quadratic term we retain the linear term even if its slope is not significantlydifferentfromzero.Letusremovethenon-significanttermsfrommodel2to see what happens: model3<-update(model2,.-Rain-I(Wind^2)) summary(model3) Coefficients: EstimateStd."
  },
  {
    "input": "We shall needtoreturntothis.Themodelsupportsoutinterpretationoftheinitialtreemodel:amain effect for industry is very important, as is a main effect for population.",
    "target": "Q: What is We shall needtoreturntothis.Themodelsupportsoutinterpretationoftheinitialtreemodel:amain effect for industry? A: We shall needtoreturntothis.Themodelsupportsoutinterpretationoftheinitialtreemodel:amain effect for industry is very important, as is a main effect for population."
  },
  {
    "input": "One approach is to fit the interaction terms in randomly selected sets.",
    "target": "Q: What is One approach? A: One approach is to fit the interaction terms in randomly selected sets."
  },
  {
    "input": "Timeforacheckonthebehaviourofthemodel: Therearetwosignificanttwo-wayinteractions,Wind:RainandWind:Wet.days.Itis time to check the assumptions: plot(model9) MULTIPLE REGRESSION 211 ThevarianceisOK,butthereisastrongindicationofnon-normalityoferrors.Butwhat aboutthehigher-orderinteractions?Onewaytoproceedistospecifytheinteractionlevel using^3inthemodelformula,butifwedothis,wellrunoutofdegreesoffreedomstraight away.Asensibleoptionistofitthree-waytermsforthevariablesthatalreadyappearintwo- way interactions: in our case, that is just one term, Wind:Rain:Wet.days: model10<-update(model9,.+Wind:Rain:Wet.days) summary(model10) Coefficients: Estimate Std.",
    "target": "Q: What is Timeforacheckonthebehaviourofthemodel: Therearetwosignificanttwo-wayinteractions,Wind:RainandWind:Wet.days.Itis time to check the assumptions: plot(model9) MULTIPLE REGRESSION 211 ThevarianceisOK,butthereisastrongindicationofnon-normalityoferrors.Butwhat aboutthehigher-orderinteractions?Onewaytoproceedistospecifytheinteractionlevel using^3inthemodelformula,butifwedothis,wellrunoutofdegreesoffreedomstraight away.Asensibleoptionistofitthree-waytermsforthevariablesthatalreadyappearintwo- way interactions: in our case, that? A: Timeforacheckonthebehaviourofthemodel: Therearetwosignificanttwo-wayinteractions,Wind:RainandWind:Wet.days.Itis time to check the assumptions: plot(model9) MULTIPLE REGRESSION 211 ThevarianceisOK,butthereisastrongindicationofnon-normalityoferrors.Butwhat aboutthehigher-orderinteractions?Onewaytoproceedistospecifytheinteractionlevel using^3inthemodelformula,butifwedothis,wellrunoutofdegreesoffreedomstraight away.Asensibleoptionistofitthree-waytermsforthevariablesthatalreadyappearintwo- way interactions: in our case, that is just one term, Wind:Rain:Wet.days: model10<-update(model9,.+Wind:Rain:Wet.days) summary(model10) Coefficients: Estimate Std."
  },
  {
    "input": "Error t value Pr (>|t|) (Intercept) 278.464474 68.041497 4.093 0.000282 *** Temp -2.710981 0.618472 -4.383 0.000125 *** Industry 0.064988 0.012264 5.299 9.11e-06 *** Population -0.039430 0.011976 -3.293 0.002485 ** Wind -7.519344 8.151943 -0.922 0.363444 Rain -6.760530 1.792173 -3.772 0.000685 *** Wet.days 1.266742 0.517850 2.446 0.020311 * Wind:Rain 0.631457 0.243866 2.589 0.014516 * Wind:Wet.days -0.230452 0.069843 -3.300 0.002440 ** Wind:Rain:Wet.days 0.002497 0.001214 2.056 0.048247 * Residualstandarderror:11.2on31degreesoffreedom MultipleR-squared: 0.8236, AdjustedR-squared: 0.7724 F-statistic:16.09on9and31DF, p-value:2.231e-09 Thereisindeedamarginallysignificantthree-wayinteraction.Youshouldconfirmthat there is no place for quadratic terms for either Temp or Wind in this model.",
    "target": "Q: What is Error t value Pr (>|t|) (Intercept) 278.464474 68.041497 4.093 0.000282 *** Temp -2.710981 0.618472 -4.383 0.000125 *** Industry 0.064988 0.012264 5.299 9.11e-06 *** Population -0.039430 0.011976 -3.293 0.002485 ** Wind -7.519344 8.151943 -0.922 0.363444 Rain -6.760530 1.792173 -3.772 0.000685 *** Wet.days 1.266742 0.517850 2.446 0.020311 * Wind:Rain 0.631457 0.243866 2.589 0.014516 * Wind:Wet.days -0.230452 0.069843 -3.300 0.002440 ** Wind:Rain:Wet.days 0.002497 0.001214 2.056 0.048247 * Residualstandarderror:11.2on31degreesoffreedom MultipleR-squared: 0.8236, AdjustedR-squared: 0.7724 F-statistic:16.09on9and31DF, p-value:2.231e-09 Thereisindeedamarginallysignificantthree-wayinteraction.Youshouldconfirmthat there? A: Error t value Pr (>|t|) (Intercept) 278.464474 68.041497 4.093 0.000282 *** Temp -2.710981 0.618472 -4.383 0.000125 *** Industry 0.064988 0.012264 5.299 9.11e-06 *** Population -0.039430 0.011976 -3.293 0.002485 ** Wind -7.519344 8.151943 -0.922 0.363444 Rain -6.760530 1.792173 -3.772 0.000685 *** Wet.days 1.266742 0.517850 2.446 0.020311 * Wind:Rain 0.631457 0.243866 2.589 0.014516 * Wind:Wet.days -0.230452 0.069843 -3.300 0.002440 ** Wind:Rain:Wet.days 0.002497 0.001214 2.056 0.048247 * Residualstandarderror:11.2on31degreesoffreedom MultipleR-squared: 0.8236, AdjustedR-squared: 0.7724 F-statistic:16.09on9and31DF, p-value:2.231e-09 Thereisindeedamarginallysignificantthree-wayinteraction.Youshouldconfirmthat there is no place for quadratic terms for either Temp or Wind in this model."
  },
  {
    "input": "The reason why the output is hard to understandisthatitisbasedoncontrasts,andtheideaofcontrastsmaybeunfamiliartoyou.",
    "target": "Q: What is The reason why the output? A: The reason why the output is hard to understandisthatitisbasedoncontrasts,andtheideaofcontrastsmaybeunfamiliartoyou."
  },
  {
    "input": "The key point is that you should only do contrastsaftertheANOVAhasestablishedthattherereallyaresignificantdifferencestobe investigated.Itisbadpracticetocarryoutteststocomparethelargestmeanwiththesmallest mean, if the ANOVA fails to reject the null hypothesis (tempting though this may be).",
    "target": "Q: What is The key point? A: The key point is that you should only do contrastsaftertheANOVAhasestablishedthattherereallyaresignificantdifferencestobe investigated.Itisbadpracticetocarryoutteststocomparethelargestmeanwiththesmallest mean, if the ANOVA fails to reject the null hypothesis (tempting though this may be)."
  },
  {
    "input": "There are two important points to understand about contrasts: (cid:129) there is a huge number of possible contrasts (cid:129) there are only k\u00001 orthogonal contrasts where k is the number of factor levels.",
    "target": "Q: What is There are two important points to understand about contrasts: (cid:129) there? A: There are two important points to understand about contrasts: (cid:129) there is a huge number of possible contrasts (cid:129) there are only k\u00001 orthogonal contrasts where k is the number of factor levels."
  },
  {
    "input": "So the two contrastsavs.bandavs.cimplicitlycontrastbvs.c.Thismeansthatifwehavecarriedout the two contrasts a vs. b and a vs. c then the third contrast b vs. c is not an orthogonal contrastbecauseyouhavealreadycarrieditout,implicitly.Whichparticularcontrastsare orthogonaldependsverymuchonyourchoiceofthefirstcontrasttomake.Supposethere weregoodreasonsforcomparing{a,b,c,e}vs.d.Forexample,dmightbetheplaceboand theotherfourmightbedifferentkindsofdrugtreatment,sowemakethisourfirstcontrast.",
    "target": "Q: What is So the two contrastsavs.bandavs.cimplicitlycontrastbvs.c.Thismeansthatifwehavecarriedout the two contrasts a vs. b and a vs. c then the third contrast b vs. c? A: So the two contrastsavs.bandavs.cimplicitlycontrastbvs.c.Thismeansthatifwehavecarriedout the two contrasts a vs. b and a vs. c then the third contrast b vs. c is not an orthogonal contrastbecauseyouhavealreadycarrieditout,implicitly.Whichparticularcontrastsare orthogonaldependsverymuchonyourchoiceofthefirstcontrasttomake.Supposethere weregoodreasonsforcomparing{a,b,c,e}vs.d.Forexample,dmightbetheplaceboand theotherfourmightbedifferentkindsofdrugtreatment,sowemakethisourfirstcontrast."
  },
  {
    "input": "Inour example,comparingfourmeanswithonemean,anaturalchoiceofcoefficientswouldbe 214 STATISTICS:ANINTRODUCTIONUSINGR \u00001foreachof{a,b,c,e}and+4ford.Alternatively,wecouldhaveselected+0.25foreach of {a, b, c, e} and \u00001 for d. factorlevel: a b c d e contrast1coefficients,c: -1 -1 -1 4 -1 Suppose the second contrast is to compare {a, b} with {c, e}.",
    "target": "Q: What is Inour example,comparingfourmeanswithonemean,anaturalchoiceofcoefficientswouldbe 214 STATISTICS:ANINTRODUCTIONUSINGR \u00001foreachof{a,b,c,e}and+4ford.Alternatively,wecouldhaveselected+0.25foreach of {a, b, c, e} and \u00001 for d. factorlevel: a b c d e contrast1coefficients,c: -1 -1 -1 4 -1 Suppose the second contrast? A: Inour example,comparingfourmeanswithonemean,anaturalchoiceofcoefficientswouldbe 214 STATISTICS:ANINTRODUCTIONUSINGR \u00001foreachof{a,b,c,e}and+4ford.Alternatively,wecouldhaveselected+0.25foreach of {a, b, c, e} and \u00001 for d. factorlevel: a b c d e contrast1coefficients,c: -1 -1 -1 4 -1 Suppose the second contrast is to compare {a, b} with {c, e}."
  },
  {
    "input": "Becausewehadassociatedourexplanatoryvariableclippingwithasetofuser-defined contrasts(above),weneedtoremovethesetogetbacktothedefaulttreatmentcontrasts,like this: contrasts(clipping)<-NULL Now let us refit the model and work out exactly what the summary is showing: model3<-lm(biomassclipping) summary(model3) Coefficients: EstimateStd.",
    "target": "Q: What is Becausewehadassociatedourexplanatoryvariableclippingwithasetofuser-defined contrasts(above),weneedtoremovethesetogetbacktothedefaulttreatmentcontrasts,like this: contrasts(clipping)<-NULL Now let us refit the model and work out exactly what the summary? A: Becausewehadassociatedourexplanatoryvariableclippingwithasetofuser-defined contrasts(above),weneedtoremovethesetogetbacktothedefaulttreatmentcontrasts,like this: contrasts(clipping)<-NULL Now let us refit the model and work out exactly what the summary is showing: model3<-lm(biomassclipping) summary(model3) Coefficients: EstimateStd."
  },
  {
    "input": "Nextweneedtothinkwhattheindividualrowsaretellingus.Letusworkouttheoverall mean value of biomass, to see if that appears anywhere in the table: mean(biomass) [1]561.8 No.Icantseethatnumberanywhere.Whataboutthefivetreatmentmeans?Weobtain these with the extremely useful tapply function: tapply(biomass,clipping,mean) control n25 n50 r10 r5 465.1667 553.3333 569.3333 610.6667 610.5000 Thisisinteresting.Thefirsttreatmentmean(465.1667forthecontrols)doesappear(inrow number1),butnoneoftheothermeansareshown.Sowhatdoesthevalueof88.17inrow 218 STATISTICS:ANINTRODUCTIONUSINGR number2ofthetablemean?Inspectionoftheindividualtreatmentmeansletsyouseethat 88.17 is the difference between the mean biomass for n25 and the mean biomass for the control.Whatabout104.17?Thatturnsouttobethedifferencebetweenthemeanbiomass for n50 and the mean biomass for the control.",
    "target": "Q: What is Nextweneedtothinkwhattheindividualrowsaretellingus.Letusworkouttheoverall mean value of biomass, to see if that appears anywhere in the table: mean(biomass) [1]561.8 No.Icantseethatnumberanywhere.Whataboutthefivetreatmentmeans?Weobtain these with the extremely useful tapply function: tapply(biomass,clipping,mean) control n25 n50 r10 r5 465.1667 553.3333 569.3333 610.6667 610.5000 Thisisinteresting.Thefirsttreatmentmean(465.1667forthecontrols)doesappear(inrow number1),butnoneoftheothermeansareshown.Sowhatdoesthevalueof88.17inrow 218 STATISTICS:ANINTRODUCTIONUSINGR number2ofthetablemean?Inspectionoftheindividualtreatmentmeansletsyouseethat 88.17? A: Nextweneedtothinkwhattheindividualrowsaretellingus.Letusworkouttheoverall mean value of biomass, to see if that appears anywhere in the table: mean(biomass) [1]561.8 No.Icantseethatnumberanywhere.Whataboutthefivetreatmentmeans?Weobtain these with the extremely useful tapply function: tapply(biomass,clipping,mean) control n25 n50 r10 r5 465.1667 553.3333 569.3333 610.6667 610.5000 Thisisinteresting.Thefirsttreatmentmean(465.1667forthecontrols)doesappear(inrow number1),butnoneoftheothermeansareshown.Sowhatdoesthevalueof88.17inrow 218 STATISTICS:ANINTRODUCTIONUSINGR number2ofthetablemean?Inspectionoftheindividualtreatmentmeansletsyouseethat 88.17 is the difference between the mean biomass for n25 and the mean biomass for the control.Whatabout104.17?Thatturnsouttobethedifferencebetweenthemeanbiomass for n50 and the mean biomass for the control."
  },
  {
    "input": "The summary.lm table is designed to make this process as simple as possible.Itdoesthisbyshowingusjustonemean(theintercept)andalltheotherrowsare differencesbetweenmeans.Likewise,inthesecondcolumn,itshowsusjustonestandard error (the standard error of the mean for the intercept  the control treatment in this example), and all ofthe other values arethe standard error of the difference between two means.Sothevalueinthetoprow(28.75)ismuchsmallerthanthevaluesintheotherfour rows (40.66) because the standard error of a mean is rffiffiffiffi s2 n while the standard error of the difference between two means is sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi rffiffiffiffiffiffiffi s2 s2 2s2 A  B  n n n A B Youcanseethatforthesimplestcasewhere(ashere)wehaveequalreplicationinallfive factorlevels(n=6)andweintendtopusffieffiffithepoolederrorvariances2,thestandarderrorof thedifferencebetweentwomeansis 21:414 timesthesizeofthestandarderrorofa mean.",
    "target": "Q: What is The summary.lm table? A: The summary.lm table is designed to make this process as simple as possible.Itdoesthisbyshowingusjustonemean(theintercept)andalltheotherrowsare differencesbetweenmeans.Likewise,inthesecondcolumn,itshowsusjustonestandard error (the standard error of the mean for the intercept  the control treatment in this example), and all ofthe other values arethe standard error of the difference between two means.Sothevalueinthetoprow(28.75)ismuchsmallerthanthevaluesintheotherfour rows (40.66) because the standard error of a mean is rffiffiffiffi s2 n while the standard error of the difference between two means is sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi rffiffiffiffiffiffiffi s2 s2 2s2 A  B  n n n A B Youcanseethatforthesimplestcasewhere(ashere)wehaveequalreplicationinallfive factorlevels(n=6)andweintendtopusffieffiffithepoolederrorvariances2,thestandarderrorof thedifferencebetweentwomeansis 21:414 timesthesizeofthestandarderrorofa mean."
  },
  {
    "input": "The trick is to use levels gets to change the names of the appropriate factor levels.",
    "target": "Q: What is The trick? A: The trick is to use levels gets to change the names of the appropriate factor levels."
  },
  {
    "input": "Now we have the minimal adequate model: summary(model6) Coefficients: EstimateStd.Error tvalue Pr (>|t|) (Intercept) 465.2 28.8 16.152 1.01e-15 *** clip4pruned 120.8 32.2 3.751 0.000815 *** Residualstandarderror:70.54on28degreesoffreedom MultipleR-squared: 0.3345, AdjustedR-squared: 0.3107 F-statistic:14.07on1and28DF, p-value:0.0008149 Ithasjusttwoparameters:themeanforthecontrols(465.2)andthedifferencebetween the control mean and the four other treatment means (465.2+120.8=586.0): 222 STATISTICS:ANINTRODUCTIONUSINGR tapply(biomass,clip4,mean) control pruned 465.1667585.9583 We know from the p value=0.000815 (above) that these two means are significantly different, but just to show how it is done, we can make a final model7 that has no explanatoryvariableatall(itfitsonlytheoverallmean).Thisisachievedbywritingy1in the model formula (parameter 1, you will recall, is the intercept in R): model7<-lm(biomass1) anova(model6,model7) AnalysisofVarianceTable Model1:biomassclip4 Model2:biomass1 Res.Df RSSDfSumofSq F Pr(>F) 1 28139342 2 29209377-1 -7003514.0730.0008149*** Notethatthepvalueisexactlythesameasinmodel6.ThepvaluesinRarecalculated such that they avoid the need for this final step in model simplification: they are p on deletion values.",
    "target": "Q: What is Now we have the minimal adequate model: summary(model6) Coefficients: EstimateStd.Error tvalue Pr (>|t|) (Intercept) 465.2 28.8 16.152 1.01e-15 *** clip4pruned 120.8 32.2 3.751 0.000815 *** Residualstandarderror:70.54on28degreesoffreedom MultipleR-squared: 0.3345, AdjustedR-squared: 0.3107 F-statistic:14.07on1and28DF, p-value:0.0008149 Ithasjusttwoparameters:themeanforthecontrols(465.2)andthedifferencebetween the control mean and the four other treatment means (465.2+120.8=586.0): 222 STATISTICS:ANINTRODUCTIONUSINGR tapply(biomass,clip4,mean) control pruned 465.1667585.9583 We know from the p value=0.000815 (above) that these two means are significantly different, but just to show how it? A: Now we have the minimal adequate model: summary(model6) Coefficients: EstimateStd.Error tvalue Pr (>|t|) (Intercept) 465.2 28.8 16.152 1.01e-15 *** clip4pruned 120.8 32.2 3.751 0.000815 *** Residualstandarderror:70.54on28degreesoffreedom MultipleR-squared: 0.3345, AdjustedR-squared: 0.3107 F-statistic:14.07on1and28DF, p-value:0.0008149 Ithasjusttwoparameters:themeanforthecontrols(465.2)andthedifferencebetween the control mean and the four other treatment means (465.2+120.8=586.0): 222 STATISTICS:ANINTRODUCTIONUSINGR tapply(biomass,clip4,mean) control pruned 465.1667585.9583 We know from the p value=0.000815 (above) that these two means are significantly different, but just to show how it is done, we can make a final model7 that has no explanatoryvariableatall(itfitsonlytheoverallmean).Thisisachievedbywritingy1in the model formula (parameter 1, you will recall, is the intercept in R): model7<-lm(biomass1) anova(model6,model7) AnalysisofVarianceTable Model1:biomassclip4 Model2:biomass1 Res.Df RSSDfSumofSq F Pr(>F) 1 28139342 2 29209377-1 -7003514.0730.0008149*** Notethatthepvalueisexactlythesameasinmodel6.ThepvaluesinRarecalculated such that they avoid the need for this final step in model simplification: they are p on deletion values."
  },
  {
    "input": "It is very useful to know which of the contrasts contributes most to SSA, and to work this out, we compute a set of contrast sums of squares SSC as follows: (cid:3) (cid:4) PcT 2 i i n SSC  i Pc2 i n i or, when all the sample sizes are the same for each factor level: (cid:3) (cid:4) 1P 2 n ciTi SSC  X 1 c2 n i The significance of a contrast is judged in the usual way by carrying out an F test to compare the contrast variance with the pooled error variance, s2 from the ANOVA table.",
    "target": "Q: What is It? A: It is very useful to know which of the contrasts contributes most to SSA, and to work this out, we compute a set of contrast sums of squares SSC as follows: (cid:3) (cid:4) PcT 2 i i n SSC  i Pc2 i n i or, when all the sample sizes are the same for each factor level: (cid:3) (cid:4) 1P 2 n ciTi SSC  X 1 c2 n i The significance of a contrast is judged in the usual way by carrying out an F test to compare the contrast variance with the pooled error variance, s2 from the ANOVA table."
  },
  {
    "input": "CONTRASTS 223 Sinceallcontrastshaveasingledegreeoffreedom,thecontrastvarianceisequaltoSSC,so the F test is just SSC F  s2 The contrast is significant (i.e.",
    "target": "Q: What is CONTRASTS 223 Sinceallcontrastshaveasingledegreeoffreedom,thecontrastvarianceisequaltoSSC,so the F test? A: CONTRASTS 223 Sinceallcontrastshaveasingledegreeoffreedom,thecontrastvarianceisequaltoSSC,so the F test is just SSC F  s2 The contrast is significant (i.e."
  },
  {
    "input": "Whenwedidfactor-levelreductionintheprevioussection,weobtainedthefollowingsums ofsquaresforthefourorthogonalcontraststhatwecarriedout(thefigurescomefromthe model-comparison ANOVA tables, above): Contrast Sumofsquares Controlvs.therest 70035 Shootpruningvs.rootpruning 14553 Intensevs.lightshootpruning 768 Deepvs.shallowrootpruning 0.083 Total What we shall do now iscarry outthe calculations involved in the a priori contrasts in ordertodemonstratethattheresultsareidentical.Thenewquantitiesweneedtocompute thecontrastsumsofsquaresSSCarethetreatmenttotals,T ,intheformulaaboveandthe i contrastcoefficientsc.Itisworthwritingdownthecontrastcoefficientsagain(seep.214, i above): Contrast Contrastcoefficients Controlvs.therest 4 \u00001 \u00001 \u00001 \u00001 Shootpruningvs.rootpruning 0 1 1 \u00001 \u00001 Intensevs.lightshootpruning 0 1 \u00001 0 0 Deepvs.shallowrootpruning 0 0 0 1 \u00001 We use tapply to obtain the five treatment totals: tapply(biomass,clipping,sum) control n25 n50 r10 r5 2791 3320 3416 3664 3663 224 STATISTICS:ANINTRODUCTIONUSINGR Forthefirstcontrast,thecontrastcoefficientsare4,\u00001,\u00001,\u00001,and\u00001.Theformulafor SSC is therefore (cid:5) (cid:6) 1 2 \u0002f 4\u00022791 \u00001\u00023320 \u00001\u00023416 \u00001\u00023664 \u00001\u00023663g 6 (cid:7) (cid:8) 1 \u0002 42 \u000012 \u000012 \u000012 \u000012 6 (cid:3) (cid:4) 1 2 \u0002\u00002899 6 233450   70035 20 3:33333 6 In the second contrast, the coefficient for the control is zero, so we leave this out: (cid:5) (cid:6) (cid:3) (cid:4) 1 2 1 2 \u0002f 1\u00023320 1\u00023416 \u00001\u00023664 \u00001\u00023663g \u0002\u0000591 6 6  (cid:7) (cid:8) 1 4 \u0002 \u000012 \u000012 \u000012 \u000012 6 6 9702:25  14553 0:666666 The third contrast ignores the root treatments, so: (cid:5) (cid:6) (cid:3) (cid:4) 1 2 1 2 \u0002f 1\u00023320 \u00001\u00023416g \u0002\u000096 6 6 256 1 (cid:7) (cid:8)  2  0:33333 768 \u0002 \u000012 12 6 6 Finally, we compare the two root pruning treatments: (cid:5) (cid:6) (cid:3) (cid:4) 1 2 1 2 \u0002f 1\u00023664 \u00001\u00023663g \u00021 6 6 0:027778 1 (cid:7) (cid:8)  2  0:333333 0:083333 \u0002 \u000012 12 6 6 Youwillseethatthesesumsofsquaresareexactlythesameasweobtainedbystepwise factor-levelreduction.Sodontletanyonetellyouthattheseproceduresaredifferentorthat one of them is superior to the other.",
    "target": "Q: What is Whenwedidfactor-levelreductionintheprevioussection,weobtainedthefollowingsums ofsquaresforthefourorthogonalcontraststhatwecarriedout(thefigurescomefromthe model-comparison ANOVA tables, above): Contrast Sumofsquares Controlvs.therest 70035 Shootpruningvs.rootpruning 14553 Intensevs.lightshootpruning 768 Deepvs.shallowrootpruning 0.083 Total What we shall do now iscarry outthe calculations involved in the a priori contrasts in ordertodemonstratethattheresultsareidentical.Thenewquantitiesweneedtocompute thecontrastsumsofsquaresSSCarethetreatmenttotals,T ,intheformulaaboveandthe i contrastcoefficientsc.Itisworthwritingdownthecontrastcoefficientsagain(seep.214, i above): Contrast Contrastcoefficients Controlvs.therest 4 \u00001 \u00001 \u00001 \u00001 Shootpruningvs.rootpruning 0 1 1 \u00001 \u00001 Intensevs.lightshootpruning 0 1 \u00001 0 0 Deepvs.shallowrootpruning 0 0 0 1 \u00001 We use tapply to obtain the five treatment totals: tapply(biomass,clipping,sum) control n25 n50 r10 r5 2791 3320 3416 3664 3663 224 STATISTICS:ANINTRODUCTIONUSINGR Forthefirstcontrast,thecontrastcoefficientsare4,\u00001,\u00001,\u00001,and\u00001.Theformulafor SSC? A: Whenwedidfactor-levelreductionintheprevioussection,weobtainedthefollowingsums ofsquaresforthefourorthogonalcontraststhatwecarriedout(thefigurescomefromthe model-comparison ANOVA tables, above): Contrast Sumofsquares Controlvs.therest 70035 Shootpruningvs.rootpruning 14553 Intensevs.lightshootpruning 768 Deepvs.shallowrootpruning 0.083 Total What we shall do now iscarry outthe calculations involved in the a priori contrasts in ordertodemonstratethattheresultsareidentical.Thenewquantitiesweneedtocompute thecontrastsumsofsquaresSSCarethetreatmenttotals,T ,intheformulaaboveandthe i contrastcoefficientsc.Itisworthwritingdownthecontrastcoefficientsagain(seep.214, i above): Contrast Contrastcoefficients Controlvs.therest 4 \u00001 \u00001 \u00001 \u00001 Shootpruningvs.rootpruning 0 1 1 \u00001 \u00001 Intensevs.lightshootpruning 0 1 \u00001 0 0 Deepvs.shallowrootpruning 0 0 0 1 \u00001 We use tapply to obtain the five treatment totals: tapply(biomass,clipping,sum) control n25 n50 r10 r5 2791 3320 3416 3664 3663 224 STATISTICS:ANINTRODUCTIONUSINGR Forthefirstcontrast,thecontrastcoefficientsare4,\u00001,\u00001,\u00001,and\u00001.Theformulafor SSC is therefore (cid:5) (cid:6) 1 2 \u0002f 4\u00022791 \u00001\u00023320 \u00001\u00023416 \u00001\u00023664 \u00001\u00023663g 6 (cid:7) (cid:8) 1 \u0002 42 \u000012 \u000012 \u000012 \u000012 6 (cid:3) (cid:4) 1 2 \u0002\u00002899 6 233450   70035 20 3:33333 6 In the second contrast, the coefficient for the control is zero, so we leave this out: (cid:5) (cid:6) (cid:3) (cid:4) 1 2 1 2 \u0002f 1\u00023320 1\u00023416 \u00001\u00023664 \u00001\u00023663g \u0002\u0000591 6 6  (cid:7) (cid:8) 1 4 \u0002 \u000012 \u000012 \u000012 \u000012 6 6 9702:25  14553 0:666666 The third contrast ignores the root treatments, so: (cid:5) (cid:6) (cid:3) (cid:4) 1 2 1 2 \u0002f 1\u00023320 \u00001\u00023416g \u0002\u000096 6 6 256 1 (cid:7) (cid:8)  2  0:33333 768 \u0002 \u000012 12 6 6 Finally, we compare the two root pruning treatments: (cid:5) (cid:6) (cid:3) (cid:4) 1 2 1 2 \u0002f 1\u00023664 \u00001\u00023663g \u00021 6 6 0:027778 1 (cid:7) (cid:8)  2  0:333333 0:083333 \u0002 \u000012 12 6 6 Youwillseethatthesesumsofsquaresareexactlythesameasweobtainedbystepwise factor-levelreduction.Sodontletanyonetellyouthattheseproceduresaredifferentorthat one of them is superior to the other."
  },
  {
    "input": "We made several important assumptions about the behaviour of the response variable, and it is worth reiterating those assumptions here, ranked in order of importance: (cid:129) random sampling (cid:129) constant variance (cid:129) normal errors (cid:129) independent errors (cid:129) additive effects Sofar,whenwefoundthatoneormoreoftheassumptionswaswrong,ourtypicalresort wastransformationoftheresponsevariable,coupledperhapswithtransformationofoneor more of the explanatory variables.",
    "target": "Q: What is We made several important assumptions about the behaviour of the response variable, and it? A: We made several important assumptions about the behaviour of the response variable, and it is worth reiterating those assumptions here, ranked in order of importance: (cid:129) random sampling (cid:129) constant variance (cid:129) normal errors (cid:129) independent errors (cid:129) additive effects Sofar,whenwefoundthatoneormoreoftheassumptionswaswrong,ourtypicalresort wastransformationoftheresponsevariable,coupledperhapswithtransformationofoneor more of the explanatory variables."
  },
  {
    "input": "It is worth noting, however, that they still assume random Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.",
    "target": "Q: What is It? A: It is worth noting, however, that they still assume random Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley."
  },
  {
    "input": "This can be rather macabre, as when the response variable is the number of people dying in a medical trail.",
    "target": "Q: What is This can be rather macabre, as when the response variable? A: This can be rather macabre, as when the response variable is the number of people dying in a medical trail."
  },
  {
    "input": "Examples of proportion data based on counts are: Successes Failures dead alive female male diseased healthy occupied unoccupied pollinated notpollinated adult juvenile Youcanseethatthiskindofproportiondatabasedoncountsarisesinagreatmanykinds ofcircumstances.Letusthinkaboutthevarianceofproportiondatalikethis.Ifthesuccess rateis100%thenalltheindividualsarealikeandthevarianceiszero.Again,ifthesuccess rate is zero, then all the individuals are alike and the variance is zero.",
    "target": "Q: What is Examples of proportion data based on counts are: Successes Failures dead alive female male diseased healthy occupied unoccupied pollinated notpollinated adult juvenile Youcanseethatthiskindofproportiondatabasedoncountsarisesinagreatmanykinds ofcircumstances.Letusthinkaboutthevarianceofproportiondatalikethis.Ifthesuccess rateis100%thenalltheindividualsarealikeandthevarianceiszero.Again,ifthesuccess rate? A: Examples of proportion data based on counts are: Successes Failures dead alive female male diseased healthy occupied unoccupied pollinated notpollinated adult juvenile Youcanseethatthiskindofproportiondatabasedoncountsarisesinagreatmanykinds ofcircumstances.Letusthinkaboutthevarianceofproportiondatalikethis.Ifthesuccess rateis100%thenalltheindividualsarealikeandthevarianceiszero.Again,ifthesuccess rate is zero, then all the individuals are alike and the variance is zero."
  },
  {
    "input": "If, however, the success rate is intermediate (50%, say) then some of the individuals are in one class and someareintheother,sovarianceishigh.Thismeansthatunlikecountdata(above)where the variance increased monotonically with the mean, for proportion data the variance isa humpedfunctionofthemean.Thebinomialdistributionisanimportantexampleofthekind ofdistributionusedintheanalysisofproportiondata:ifpistheprobabilityofsuccessandn is the number of trails, then the mean number of successes is np and the variance in the numberofsuccessesisnp(1\u0000p).Asyoucansee,thevarianceis0whenp=1and0when p=0, reaching a peak when p=0.5.",
    "target": "Q: What is If, however, the success rate? A: If, however, the success rate is intermediate (50%, say) then some of the individuals are in one class and someareintheother,sovarianceishigh.Thismeansthatunlikecountdata(above)where the variance increased monotonically with the mean, for proportion data the variance isa humpedfunctionofthemean.Thebinomialdistributionisanimportantexampleofthekind ofdistributionusedintheanalysisofproportiondata:ifpistheprobabilityofsuccessandn is the number of trails, then the mean number of successes is np and the variance in the numberofsuccessesisnp(1\u0000p).Asyoucansee,thevarianceis0whenp=1and0when p=0, reaching a peak when p=0.5."
  },
  {
    "input": "IntroductiontoGeneralizedLinearModels A generalized linear model has three important properties: (cid:129) the error structure (cid:129) the linear predictor (cid:129) the link function OTHERRESPONSE VARIABLES 229 Thesearealllikely tobeunfamiliar concepts.The ideasbehindthem arestraightforward, however, and it is worth learning what each of the concepts involves.",
    "target": "Q: What is IntroductiontoGeneralizedLinearModels A generalized linear model has three important properties: (cid:129) the error structure (cid:129) the linear predictor (cid:129) the link function OTHERRESPONSE VARIABLES 229 Thesearealllikely tobeunfamiliar concepts.The ideasbehindthem arestraightforward, however, and it? A: IntroductiontoGeneralizedLinearModels A generalized linear model has three important properties: (cid:129) the error structure (cid:129) the linear predictor (cid:129) the link function OTHERRESPONSE VARIABLES 229 Thesearealllikely tobeunfamiliar concepts.The ideasbehindthem arestraightforward, however, and it is worth learning what each of the concepts involves."
  },
  {
    "input": "Or glm(yz,family=binomial) which means that the response is binary, and the model has binomial errors.",
    "target": "Q: What is Or glm(yz,family=binomial) which means that the response? A: Or glm(yz,family=binomial) which means that the response is binary, and the model has binomial errors."
  },
  {
    "input": "TheLinearPredictor Thestructureofthemodelrelateseachobservedyvaluetoapredictedvalue.Thepredicted value is obtained by transformation of the value emerging from the linear predictor.",
    "target": "Q: What is TheLinearPredictor Thestructureofthemodelrelateseachobservedyvaluetoapredictedvalue.Thepredicted value? A: TheLinearPredictor Thestructureofthemodelrelateseachobservedyvaluetoapredictedvalue.Thepredicted value is obtained by transformation of the value emerging from the linear predictor."
  },
  {
    "input": "The linearpredictor,(eta),isalinearsumoftheeffectsofoneormoreexplanatoryvariables,x j, and is what we see when we ask for summary.lm: Xp   x  i ib j j1 230 STATISTICS:ANINTRODUCTIONUSINGR where the xs are the values of the p different explanatory variables, and the s are the (usually) unknown parameters to be estimated from the data.",
    "target": "Q: What is The linearpredictor,(eta),isalinearsumoftheeffectsofoneormoreexplanatoryvariables,x j, and? A: The linearpredictor,(eta),isalinearsumoftheeffectsofoneormoreexplanatoryvariables,x j, and is what we see when we ask for summary.lm: Xp   x  i ib j j1 230 STATISTICS:ANINTRODUCTIONUSINGR where the xs are the values of the p different explanatory variables, and the s are the (usually) unknown parameters to be estimated from the data."
  },
  {
    "input": "The right-hand side of the equation is called the linear structure.",
    "target": "Q: What is The right-hand side of the equation? A: The right-hand side of the equation is called the linear structure."
  },
  {
    "input": "Up to now, we have used variance to measurethelackoffitbetweenthemodelandthedata:thiswasthesumofthesquaresofthe differencPebetweentheresponsevariableyandthefittedvaluespredictedbythemodel^y, whichis y\u0000^y2dividedbythedegreesoffreedom.Nowifyoufitadifferentmodel,say Plog(y)=a+bx, then obviously the variance is completely different because it is based on log y\u0000^y2.",
    "target": "Q: What is Up to now, we have used variance to measurethelackoffitbetweenthemodelandthedata:thiswasthesumofthesquaresofthe differencPebetweentheresponsevariableyandthefittedvaluespredictedbythemodel^y, whichis y\u0000^y2dividedbythedegreesoffreedom.Nowifyoufitadifferentmodel,say Plog(y)=a+bx, then obviously the variance? A: Up to now, we have used variance to measurethelackoffitbetweenthemodelandthedata:thiswasthesumofthesquaresofthe differencPebetweentheresponsevariableyandthefittedvaluespredictedbythemodel^y, whichis y\u0000^y2dividedbythedegreesoffreedom.Nowifyoufitadifferentmodel,say Plog(y)=a+bx, then obviously the variance is completely different because it is based on log y\u0000^y2."
  },
  {
    "input": "This makes model comparison difficult, because there is no common currencyformeasuringthefitofthetwomodelstothedata.InaGLM,however,wealways compare y and^y on the same scale on which the response was measured (as a count, for instance,orasaproportionbasedontwocounts).Thismakesmodelcomparisonmuchmore straightforward.",
    "target": "Q: What is This makes model comparison difficult, because there? A: This makes model comparison difficult, because there is no common currencyformeasuringthefitofthetwomodelstothedata.InaGLM,however,wealways compare y and^y on the same scale on which the response was measured (as a count, for instance,orasaproportionbasedontwocounts).Thismakesmodelcomparisonmuchmore straightforward."
  },
  {
    "input": "AGeneralMeasureofVariability The difference is that the measure of lack of fit of the model to the data depends on the context.Wegiveitthenew,moregeneralnameofdeviance.Thetechnicaldefinitionwont mean much to you at this stage, but here it is: deviance\u00002\u0002loglikelihood where the log likelihood depends on the model given the data.",
    "target": "Q: What is AGeneralMeasureofVariability The difference? A: AGeneralMeasureofVariability The difference is that the measure of lack of fit of the model to the data depends on the context.Wegiveitthenew,moregeneralnameofdeviance.Thetechnicaldefinitionwont mean much to you at this stage, but here it is: deviance\u00002\u0002loglikelihood where the log likelihood depends on the model given the data."
  },
  {
    "input": "We dont need to unlearn anything, because deviance is the same as variance when we have constant variance and OTHERRESPONSE VARIABLES 231 normalerrors(asinlinearregression,ANOVAorANCOVA).Butforcountdatawedoneeda differentmeasureoflackoffit(itisbasedonylog y=^yratherthanon y\u0000^y2)andweneeda different definition of deviance for proportion data, and so on.",
    "target": "Q: What is We dont need to unlearn anything, because deviance? A: We dont need to unlearn anything, because deviance is the same as variance when we have constant variance and OTHERRESPONSE VARIABLES 231 normalerrors(asinlinearregression,ANOVAorANCOVA).Butforcountdatawedoneeda differentmeasureoflackoffit(itisbasedonylog y=^yratherthanon y\u0000^y2)andweneeda different definition of deviance for proportion data, and so on."
  },
  {
    "input": "But the point is that for measuringthefit,wecompareyand^y onthesameoriginaluntransformedscale.Weshall discussthevariousdeviancemeasuresinmoredetailinthefollowingchapters,butsothatyou cancomparethemwithoneanother,herearethemainmeasuresoflackoffitside-by-side: Model Deviance Error Link X linear y\u0000 ^y2 Gaussian identity (cid:2) (cid:3) loglinear X y Poisson log 2 ylog ^y (cid:2) (cid:3) (cid:2) (cid:3) logistic X y n\u0000y binomial logit 2 ylog  n\u0000ylog ^y n\u0000^y (cid:2) (cid:3) gamma X y\u0000^y y gamma reciprocal 2 \u0000log y ^y Todeterminethefitofagivenmodel,aGLMevaluatesthelinearpredictorforeachvalue of the response variable, then back-transforms the predicted value to compare it with the observed value of y.",
    "target": "Q: What is But the point? A: But the point is that for measuringthefit,wecompareyand^y onthesameoriginaluntransformedscale.Weshall discussthevariousdeviancemeasuresinmoredetailinthefollowingchapters,butsothatyou cancomparethemwithoneanother,herearethemainmeasuresoflackoffitside-by-side: Model Deviance Error Link X linear y\u0000 ^y2 Gaussian identity (cid:2) (cid:3) loglinear X y Poisson log 2 ylog ^y (cid:2) (cid:3) (cid:2) (cid:3) logistic X y n\u0000y binomial logit 2 ylog  n\u0000ylog ^y n\u0000^y (cid:2) (cid:3) gamma X y\u0000^y y gamma reciprocal 2 \u0000log y ^y Todeterminethefitofagivenmodel,aGLMevaluatesthelinearpredictorforeachvalue of the response variable, then back-transforms the predicted value to compare it with the observed value of y."
  },
  {
    "input": "The parameters are then adjusted, and the model refitted on the transformedscaleinaniterativeprocedureuntilthefit stopsimproving.Itwilltake some time before you understand what is going on here, and why it is so revolutionary.",
    "target": "Q: What is The parameters are then adjusted, and the model refitted on the transformedscaleinaniterativeprocedureuntilthefit stopsimproving.Itwilltake some time before you understand what? A: The parameters are then adjusted, and the model refitted on the transformedscaleinaniterativeprocedureuntilthefit stopsimproving.Itwilltake some time before you understand what is going on here, and why it is so revolutionary."
  },
  {
    "input": "The transformation to be employed is specified in the link function.",
    "target": "Q: What is The transformation to be employed? A: The transformation to be employed is specified in the link function."
  },
  {
    "input": "The fitted value is computedbyapplyingthereciprocalofthelinkfunction,inordertogetbacktotheoriginal scaleofmeasurementoftheresponsevariable.Thus,withaloglink,thefittedvalueisthe antilogofthelinearpredictor,andwiththereciprocallink,thefittedvalueisthereciprocal of the linear predictor.",
    "target": "Q: What is The fitted value? A: The fitted value is computedbyapplyingthereciprocalofthelinkfunction,inordertogetbacktotheoriginal scaleofmeasurementoftheresponsevariable.Thus,withaloglink,thefittedvalueisthe antilogofthelinearpredictor,andwiththereciprocallink,thefittedvalueisthereciprocal of the linear predictor."
  },
  {
    "input": "The valueofisobtainedbytransformingthevalueofybythelinkfunction,andthepredicted value of y is obtained by applying the inverse link function to .",
    "target": "Q: What is The valueofisobtainedbytransformingthevalueofybythelinkfunction,andthepredicted value of y? A: The valueofisobtainedbytransformingthevalueofybythelinkfunction,andthepredicted value of y is obtained by applying the inverse link function to ."
  },
  {
    "input": "An important criterion in the choice of link function is to ensure that the fitted values stay within reasonable bounds.",
    "target": "Q: What is An important criterion in the choice of link function? A: An important criterion in the choice of link function is to ensure that the fitted values stay within reasonable bounds."
  },
  {
    "input": "The total deviance is the same in each case, and we can investigate the conse- quences of altering our assumptions about precisely how a given change in the linear predictorbringsaboutaresponseinthefittedvalueofy.Themostappropriatelinkfunction is the one which produces the minimum residual deviance.",
    "target": "Q: What is The total deviance? A: The total deviance is the same in each case, and we can investigate the conse- quences of altering our assumptions about precisely how a given change in the linear predictorbringsaboutaresponseinthefittedvalueofy.Themostappropriatelinkfunction is the one which produces the minimum residual deviance."
  },
  {
    "input": "CanonicalLinkFunctions The canonical link functions are the default options employed when a particular error structure is specified in the family directive in the model formula.",
    "target": "Q: What is CanonicalLinkFunctions The canonical link functions are the default options employed when a particular error structure? A: CanonicalLinkFunctions The canonical link functions are the default options employed when a particular error structure is specified in the family directive in the model formula."
  },
  {
    "input": "Omission of a link directive means that the following settings are used: Error Canonicallink gaussian identity poisson log binomial logit Gamma reciprocal You should try to memorize these canonical links and to understand why each is appropriatetoitsassociatederrordistribution.Notethatonlygammaerrorshaveacapital initial letter in R. Choosing between using a link function (e.g.",
    "target": "Q: What is Omission of a link directive means that the following settings are used: Error Canonicallink gaussian identity poisson log binomial logit Gamma reciprocal You should try to memorize these canonical links and to understand why each? A: Omission of a link directive means that the following settings are used: Error Canonicallink gaussian identity poisson log binomial logit Gamma reciprocal You should try to memorize these canonical links and to understand why each is appropriatetoitsassociatederrordistribution.Notethatonlygammaerrorshaveacapital initial letter in R. Choosing between using a link function (e.g."
  },
  {
    "input": "And using a generalized linear model instead of a linear model is often the best solution.",
    "target": "Q: What is And using a generalized linear model instead of a linear model? A: And using a generalized linear model instead of a linear model is often the best solution."
  },
  {
    "input": "Learning about deviance, link functions and linear predictors is a small price to pay.",
    "target": "Q: What is Learning about deviance, link functions and linear predictors? A: Learning about deviance, link functions and linear predictors is a small price to pay."
  },
  {
    "input": "This is the basisofautomatedmodelsimplificationusingstep.AICusesapenaltyof2perparameter, so for a given model AIC is calculated as AICdeviance2p andwhenthedeviancegoesdownbylessthan2,theninclusionoftheextraparameterisnot justified.",
    "target": "Q: What is This? A: This is the basisofautomatedmodelsimplificationusingstep.AICusesapenaltyof2perparameter, so for a given model AIC is calculated as AICdeviance2p andwhenthedeviancegoesdownbylessthan2,theninclusionoftheextraparameterisnot justified."
  },
  {
    "input": "13 Count Data Up to this point, the response variables have all been continuous measurements such as weights,heights,lengths,temperaturesandgrowthrates.Agreatdealofthedatacollectedby scientists, medical statisticians and economists, however, is in the form of counts (whole numbersorintegers).Thenumberofindividualswhodied,thenumberoffirmsgoingbankrupt, thenumberofdaysoffrost,thenumberofredbloodcellsonamicroscopeslide,orthenumber ofcratersinasectoroflunarlandscapeareallpotentiallyinterestingvariablesforstudy.With count data, the number 0 often appears as a value of the response variable (consider, for example,whata0wouldmeaninthecontextoftheexamplesjustlisted).Inthischapterwedeal withdataonfrequencies,wherewecounthowmanytimessomethinghappened,butwehave nowayofknowinghowoftenitdidnothappen(e.g.lighteningstrikes,bankruptcies,deaths, births).Thisisincontrastwithcountdataonproportions,whereweknowthenumberdoinga particularthing,butalsothenumbernotdoingthatthing(e.g.theproportiondying,sexratiosat birth,proportionsofdifferentgroupsrespondingtoaquestionnaire).",
    "target": "Q: What is 13 Count Data Up to this point, the response variables have all been continuous measurements such as weights,heights,lengths,temperaturesandgrowthrates.Agreatdealofthedatacollectedby scientists, medical statisticians and economists, however,? A: 13 Count Data Up to this point, the response variables have all been continuous measurements such as weights,heights,lengths,temperaturesandgrowthrates.Agreatdealofthedatacollectedby scientists, medical statisticians and economists, however, is in the form of counts (whole numbersorintegers).Thenumberofindividualswhodied,thenumberoffirmsgoingbankrupt, thenumberofdaysoffrost,thenumberofredbloodcellsonamicroscopeslide,orthenumber ofcratersinasectoroflunarlandscapeareallpotentiallyinterestingvariablesforstudy.With count data, the number 0 often appears as a value of the response variable (consider, for example,whata0wouldmeaninthecontextoftheexamplesjustlisted).Inthischapterwedeal withdataonfrequencies,wherewecounthowmanytimessomethinghappened,butwehave nowayofknowinghowoftenitdidnothappen(e.g.lighteningstrikes,bankruptcies,deaths, births).Thisisincontrastwithcountdataonproportions,whereweknowthenumberdoinga particularthing,butalsothenumbernotdoingthatthing(e.g.theproportiondying,sexratiosat birth,proportionsofdifferentgroupsrespondingtoaquestionnaire)."
  },
  {
    "input": "Straightforward linear regression methods (assuming constant variance and normal errors) are not appropriate for count data for four main reasons: (cid:129) the linear model might lead to the prediction of negative counts (cid:129) the variance of the response variable is likely to increase with the mean (cid:129) the errors will not be normally distributed (cid:129) zeros are difficult to handle in transformations InR,countdataarehandledveryelegantlyinageneralizedlinearmodelbyspecifying family=poissonwhichusesPoissonerrorsandtheloglink(seeChapter12).Theloglink ensuresthatallthefittedvaluesarepositive,whilethePoissonerrorstakeaccountofthefact that the data are integer and have variances that are equal to their means.",
    "target": "Q: What is Straightforward linear regression methods (assuming constant variance and normal errors) are not appropriate for count data for four main reasons: (cid:129) the linear model might lead to the prediction of negative counts (cid:129) the variance of the response variable? A: Straightforward linear regression methods (assuming constant variance and normal errors) are not appropriate for count data for four main reasons: (cid:129) the linear model might lead to the prediction of negative counts (cid:129) the variance of the response variable is likely to increase with the mean (cid:129) the errors will not be normally distributed (cid:129) zeros are difficult to handle in transformations InR,countdataarehandledveryelegantlyinageneralizedlinearmodelbyspecifying family=poissonwhichusesPoissonerrorsandtheloglink(seeChapter12).Theloglink ensuresthatallthefittedvaluesarepositive,whilethePoissonerrorstakeaccountofthefact that the data are integer and have variances that are equal to their means."
  },
  {
    "input": "The question is whether or not proximity to the reactor affects the number of cancer cases.",
    "target": "Q: What is The question? A: The question is whether or not proximity to the reactor affects the number of cancer cases."
  },
  {
    "input": "It is assumed that this is the same as the residual degrees of freedom.",
    "target": "Q: What is It? A: It is assumed that this is the same as the residual degrees of freedom."
  },
  {
    "input": "The fact that residual deviance is larger than residual degrees of freedom indicates that we have overdispersion (extra, unexplained variation in the response).",
    "target": "Q: What is The fact that residual deviance? A: The fact that residual deviance is larger than residual degrees of freedom indicates that we have overdispersion (extra, unexplained variation in the response)."
  },
  {
    "input": "We compen- sate for the overdispersion by refitting the model using quasipoisson rather than Poisson errors: model2<-glm(CancersDistance,quasipoisson) summary(model2) Coefficients: Estimate Std.Error t value Pr(>|t|) (Intercept) 0.186865 0.235364 0.794 0.429 Distance -0.006138 0.004573 -1.342 0.183 (Dispersionparameterforquasipoissonfamilytakentobe1.555271) Nulldeviance:149.48 on93 degreesoffreedom Residualdeviance:146.64 on92 degreesoffreedom AIC:NA NumberofFisherScoringiterations:5 Compensating for the overdispersion has increased the p value to 0.183, so there is no compellingevidence tosupport theexistence of atrend incancer incidence with distance fromthenuclearplant.Todrawthefittedmodelthroughthedata,youneedtounderstand that the GLM with Poisson errors uses the log link, so the parameter estimates and the predictions from the model (the linear predictor above) are in logs, and need to be antilogged before the (non-significant) fitted line is drawn.",
    "target": "Q: What is We compen- sate for the overdispersion by refitting the model using quasipoisson rather than Poisson errors: model2<-glm(CancersDistance,quasipoisson) summary(model2) Coefficients: Estimate Std.Error t value Pr(>|t|) (Intercept) 0.186865 0.235364 0.794 0.429 Distance -0.006138 0.004573 -1.342 0.183 (Dispersionparameterforquasipoissonfamilytakentobe1.555271) Nulldeviance:149.48 on93 degreesoffreedom Residualdeviance:146.64 on92 degreesoffreedom AIC:NA NumberofFisherScoringiterations:5 Compensating for the overdispersion has increased the p value to 0.183, so there? A: We compen- sate for the overdispersion by refitting the model using quasipoisson rather than Poisson errors: model2<-glm(CancersDistance,quasipoisson) summary(model2) Coefficients: Estimate Std.Error t value Pr(>|t|) (Intercept) 0.186865 0.235364 0.794 0.429 Distance -0.006138 0.004573 -1.342 0.183 (Dispersionparameterforquasipoissonfamilytakentobe1.555271) Nulldeviance:149.48 on93 degreesoffreedom Residualdeviance:146.64 on92 degreesoffreedom AIC:NA NumberofFisherScoringiterations:5 Compensating for the overdispersion has increased the p value to 0.183, so there is no compellingevidence tosupport theexistence of atrend incancer incidence with distance fromthenuclearplant.Todrawthefittedmodelthroughthedata,youneedtounderstand that the GLM with Poisson errors uses the log link, so the parameter estimates and the predictions from the model (the linear predictor above) are in logs, and need to be antilogged before the (non-significant) fitted line is drawn."
  },
  {
    "input": "The intercept is 0.186865 and the slope is \u00000.006138 (see above), so the linear predictor gives yv as: yv<-0.186865-0.006138*xv Theimportantpointtobearinmindisthatyvisonalogarithmicscale.Wewanttoploty (the raw numbers of cases, not their logarithms) so we need to take antilogs (to back- transform, in other words): y<-exp(yv) lines(xv,y,col=\"red\") COUNT DATA 237 The red fitted line is curved, but very shallow.",
    "target": "Q: What is The intercept? A: The intercept is 0.186865 and the slope is \u00000.006138 (see above), so the linear predictor gives yv as: yv<-0.186865-0.006138*xv Theimportantpointtobearinmindisthatyvisonalogarithmicscale.Wewanttoploty (the raw numbers of cases, not their logarithms) so we need to take antilogs (to back- transform, in other words): y<-exp(yv) lines(xv,y,col=\"red\") COUNT DATA 237 The red fitted line is curved, but very shallow."
  },
  {
    "input": "Once youhaveunderstoodtherelationshipbetween thelinearpredictor,thelink function and the predicted count data, y, you can speed up the curve drawing with the predict function, because you can specify type=\"response\", which carries out the back- transformation automatically: y<-predict(model2,list(Distance=xv),type=\"response\") lines(xv,y,col=\"red\") AnalysisofDeviancewithCountData The response variable is a count of infected blood cells per mm2 on microscope slides prepared from randomly selected individuals.",
    "target": "Q: What is Once youhaveunderstoodtherelationshipbetween thelinearpredictor,thelink function and the predicted count data, y, you can speed up the curve drawing with the predict function, because you can specify type=\"response\", which carries out the back- transformation automatically: y<-predict(model2,list(Distance=xv),type=\"response\") lines(xv,y,col=\"red\") AnalysisofDeviancewithCountData The response variable? A: Once youhaveunderstoodtherelationshipbetween thelinearpredictor,thelink function and the predicted count data, y, you can speed up the curve drawing with the predict function, because you can specify type=\"response\", which carries out the back- transformation automatically: y<-predict(model2,list(Distance=xv),type=\"response\") lines(xv,y,col=\"red\") AnalysisofDeviancewithCountData The response variable is a count of infected blood cells per mm2 on microscope slides prepared from randomly selected individuals."
  },
  {
    "input": "We need to test whether any of these differences are significant and to assess whether there are any interactions between the explanatory variables: model1<-glm(cellssmoker*sex*age*weight,poisson) summary(model1) You should scroll down to the bottom of the (voluminous) output to find the residual deviance and residual degrees of freedom, because we need to test for overdispersion: Nulldeviance:1052.95 on510 degreesoffreedom Residualdeviance: 736.33 on477 degreesoffreedom AIC:1318 NumberofFisherScoringiterations:6 COUNT DATA 239 The residual deviance (736.33) is much greater than the residual degrees of freedom (477), indicating substantial overdispersion, so before interpreting any of the effects, we should refit the model using quasipoisson errors: model2<-glm(cellssmoker*sex*age*weight,quasipoisson) summary(model2) Coefficients:(2notdefinedbecauseofsingularities) EstimateStd.",
    "target": "Q: What is We need to test whether any of these differences are significant and to assess whether there are any interactions between the explanatory variables: model1<-glm(cellssmoker*sex*age*weight,poisson) summary(model1) You should scroll down to the bottom of the (voluminous) output to find the residual deviance and residual degrees of freedom, because we need to test for overdispersion: Nulldeviance:1052.95 on510 degreesoffreedom Residualdeviance: 736.33 on477 degreesoffreedom AIC:1318 NumberofFisherScoringiterations:6 COUNT DATA 239 The residual deviance (736.33)? A: We need to test whether any of these differences are significant and to assess whether there are any interactions between the explanatory variables: model1<-glm(cellssmoker*sex*age*weight,poisson) summary(model1) You should scroll down to the bottom of the (voluminous) output to find the residual deviance and residual degrees of freedom, because we need to test for overdispersion: Nulldeviance:1052.95 on510 degreesoffreedom Residualdeviance: 736.33 on477 degreesoffreedom AIC:1318 NumberofFisherScoringiterations:6 COUNT DATA 239 The residual deviance (736.33) is much greater than the residual degrees of freedom (477), indicating substantial overdispersion, so before interpreting any of the effects, we should refit the model using quasipoisson errors: model2<-glm(cellssmoker*sex*age*weight,quasipoisson) summary(model2) Coefficients:(2notdefinedbecauseofsingularities) EstimateStd."
  },
  {
    "input": "sexmale:ageyoung:weightobese 2.5346 1.9488 1.301 0.1940 sexmale:ageold:weightover -1.0641 1.9650 -0.542 0.5884 sexmale:ageyoung:weightover -1.1087 2.1234 -0.522 0.6018 smokerTRUE:sexmale:ageold:weightobese -1.6169 3.0561 -0.529 0.5970 smokerTRUE:sexmale:ageyoung:weightobese NA NA NA NA smokerTRUE:sexmale:ageold:weightover NA NA NA NA smokerTRUE:sexmale:ageyoung:weightover 2.4160 2.6846 0.900 0.3686 (Dispersionparameterforquasipoissonfamilytakentobe1.854815) Nulldeviance:1052.95 on510 degreesoffreedom Residualdeviance: 736.33 on477 degreesoffreedom AIC:NA NumberofFisherScoringiterations:6 The first thing to notice is that there are NA (missing value) symbols in the table of the linear predictor (the message reads Coefficients: (2 not defined because of singularities).",
    "target": "Q: What is sexmale:ageyoung:weightobese 2.5346 1.9488 1.301 0.1940 sexmale:ageold:weightover -1.0641 1.9650 -0.542 0.5884 sexmale:ageyoung:weightover -1.1087 2.1234 -0.522 0.6018 smokerTRUE:sexmale:ageold:weightobese -1.6169 3.0561 -0.529 0.5970 smokerTRUE:sexmale:ageyoung:weightobese NA NA NA NA smokerTRUE:sexmale:ageold:weightover NA NA NA NA smokerTRUE:sexmale:ageyoung:weightover 2.4160 2.6846 0.900 0.3686 (Dispersionparameterforquasipoissonfamilytakentobe1.854815) Nulldeviance:1052.95 on510 degreesoffreedom Residualdeviance: 736.33 on477 degreesoffreedom AIC:NA NumberofFisherScoringiterations:6 The first thing to notice? A: sexmale:ageyoung:weightobese 2.5346 1.9488 1.301 0.1940 sexmale:ageold:weightover -1.0641 1.9650 -0.542 0.5884 sexmale:ageyoung:weightover -1.1087 2.1234 -0.522 0.6018 smokerTRUE:sexmale:ageold:weightobese -1.6169 3.0561 -0.529 0.5970 smokerTRUE:sexmale:ageyoung:weightobese NA NA NA NA smokerTRUE:sexmale:ageold:weightover NA NA NA NA smokerTRUE:sexmale:ageyoung:weightover 2.4160 2.6846 0.900 0.3686 (Dispersionparameterforquasipoissonfamilytakentobe1.854815) Nulldeviance:1052.95 on510 degreesoffreedom Residualdeviance: 736.33 on477 degreesoffreedom AIC:NA NumberofFisherScoringiterations:6 The first thing to notice is that there are NA (missing value) symbols in the table of the linear predictor (the message reads Coefficients: (2 not defined because of singularities)."
  },
  {
    "input": "This is the first example we have met of aliasing 240 STATISTICS:ANINTRODUCTIONUSINGR (p. 16): these symbols indicate that there are no data in the dataframe from which to estimate two of the terms in the four-way interaction between smoking, sex, age and weight.",
    "target": "Q: What is This? A: This is the first example we have met of aliasing 240 STATISTICS:ANINTRODUCTIONUSINGR (p. 16): these symbols indicate that there are no data in the dataframe from which to estimate two of the terms in the four-way interaction between smoking, sex, age and weight."
  },
  {
    "input": "With a complicated model like this, it is a good idea to speed up the early stages of model simplification by using the step function, but this is not available with quasipoisson errors, so we need to work through the analysis long- land.Startbyremovingthealiasedfour-wayinteraction,thentryremovingwhatlooks (fromthepvalues)tobetheleastsignificantthree-wayinteraction,thesexageweight interaction: model3<-update(model2,.-smoker:sex:age:weight) model4<-update(model3,.-sex:age:weight) anova(model4,model3,test=\"F\") AnalysisofDevianceTable Resid.Df Resid.",
    "target": "Q: What is With a complicated model like this, it? A: With a complicated model like this, it is a good idea to speed up the early stages of model simplification by using the step function, but this is not available with quasipoisson errors, so we need to work through the analysis long- land.Startbyremovingthealiasedfour-wayinteraction,thentryremovingwhatlooks (fromthepvalues)tobetheleastsignificantthree-wayinteraction,thesexageweight interaction: model3<-update(model2,.-smoker:sex:age:weight) model4<-update(model3,.-sex:age:weight) anova(model4,model3,test=\"F\") AnalysisofDevianceTable Resid.Df Resid."
  },
  {
    "input": "model7<-update(model6,.-smoker:sex:weight) anova(model7,model6,test=\"F\") That is the last of the three-way interactions, so we can start removing the two-way interactions, starting, as usual, with the least significant: model8<-update(model7,.-smoker:age) anova(model8,model7,test=\"F\") Not significant.",
    "target": "Q: What is model7<-update(model6,.-smoker:sex:weight) anova(model7,model6,test=\"F\") That? A: model7<-update(model6,.-smoker:sex:weight) anova(model7,model6,test=\"F\") That is the last of the three-way interactions, so we can start removing the two-way interactions, starting, as usual, with the least significant: model8<-update(model7,.-smoker:age) anova(model8,model7,test=\"F\") Not significant."
  },
  {
    "input": "The biggest main effect (to judge by the p values) is smoking.",
    "target": "Q: What is The biggest main effect (to judge by the p values)? A: The biggest main effect (to judge by the p values) is smoking."
  },
  {
    "input": "The significance of the intercept is not interestinginthiscase(itjustsaysthemeannumberofcellsfornon-smoking,middle-aged females of normal weight is greater than zero  but it is a count, so it would be).",
    "target": "Q: What is The significance of the intercept? A: The significance of the intercept is not interestinginthiscase(itjustsaysthemeannumberofcellsfornon-smoking,middle-aged females of normal weight is greater than zero  but it is a count, so it would be)."
  },
  {
    "input": "The relationship between smoking and weight is shown like this: barplot(tapply(cells,list(smoker,weight),mean),beside=T) ThisisOK,butthebarsarenotinthebestorder;theobesecategoryshouldbeontheright of the figure.",
    "target": "Q: What is The relationship between smoking and weight? A: The relationship between smoking and weight is shown like this: barplot(tapply(cells,list(smoker,weight),mean),beside=T) ThisisOK,butthebarsarenotinthebestorder;theobesecategoryshouldbeontheright of the figure."
  },
  {
    "input": "It is inevitable that we shall fail to measure a number of factors that have an importantinfluenceonthebehaviourofthesysteminquestion.Thatslife,andgiventhat we make every effort to note the important factors, theres little we can do about it.",
    "target": "Q: What is It? A: It is inevitable that we shall fail to measure a number of factors that have an importantinfluenceonthebehaviourofthesysteminquestion.Thatslife,andgiventhat we make every effort to note the important factors, theres little we can do about it."
  },
  {
    "input": "induced<-read.csv(\"c:\\\\temp\\\\induced.csv\") attach(induced) names(induced) [1]\"Tree\" \"Aphid\" \"Caterpillar\" \"Count\" Webeginbyfittingwhatisknownasasaturatedmodel.Thisisacuriousthing,whichhasas manyparametersastherearevaluesoftheresponsevariable.Thefitofthemodelisperfect,so therearenoresidualdegreesoffreedomandnoresidualdeviance.Thereasonwhywefita saturated model is that it is always the best place to start modelling complex contingency tables.",
    "target": "Q: What is induced<-read.csv(\"c:\\\\temp\\\\induced.csv\") attach(induced) names(induced) [1]\"Tree\" \"Aphid\" \"Caterpillar\" \"Count\" Webeginbyfittingwhatisknownasasaturatedmodel.Thisisacuriousthing,whichhasas manyparametersastherearevaluesoftheresponsevariable.Thefitofthemodelisperfect,so therearenoresidualdegreesoffreedomandnoresidualdeviance.Thereasonwhywefita saturated model? A: induced<-read.csv(\"c:\\\\temp\\\\induced.csv\") attach(induced) names(induced) [1]\"Tree\" \"Aphid\" \"Caterpillar\" \"Count\" Webeginbyfittingwhatisknownasasaturatedmodel.Thisisacuriousthing,whichhasas manyparametersastherearevaluesoftheresponsevariable.Thefitofthemodelisperfect,so therearenoresidualdegreesoffreedomandnoresidualdeviance.Thereasonwhywefita saturated model is that it is always the best place to start modelling complex contingency tables."
  },
  {
    "input": "If we fit the saturated model, then there is no risk that we inadvertently leave out importantinteractionsbetweentheso-callednuisancevariables.Thesearetheparameters thatneedtobeinthemodeltoensurethatthemarginaltotalsareproperlyconstrained.",
    "target": "Q: What is If we fit the saturated model, then there? A: If we fit the saturated model, then there is no risk that we inadvertently leave out importantinteractionsbetweentheso-callednuisancevariables.Thesearetheparameters thatneedtobeinthemodeltoensurethatthemarginaltotalsareproperlyconstrained."
  },
  {
    "input": "model<-glm(CountTree*Aphid*Caterpillar,family=poisson) COUNT DATA 245 The asterisk notation ensures that the saturated model is fitted, because all of the main effectsandtwo-wayinteractionsarefitted,alongwiththethree-way,treeaphidcaterpillar interaction.Themodel fitinvolvestheestimation of222=8parameters,andexactly matches the eight values of the response variable, Count.",
    "target": "Q: What is model<-glm(CountTree*Aphid*Caterpillar,family=poisson) COUNT DATA 245 The asterisk notation ensures that the saturated model? A: model<-glm(CountTree*Aphid*Caterpillar,family=poisson) COUNT DATA 245 The asterisk notation ensures that the saturated model is fitted, because all of the main effectsandtwo-wayinteractionsarefitted,alongwiththethree-way,treeaphidcaterpillar interaction.Themodel fitinvolvestheestimation of222=8parameters,andexactly matches the eight values of the response variable, Count."
  },
  {
    "input": "There is no point looking at the saturated model in any detail, because the reams of information it contains are all superfluous.Thefirstrealstepinthemodellingistouseupdatetoremovethethree-way interaction from the saturated model, and then to use anova to test whether the three- way interaction is significant or not: model2<-update(model, .-Tree:Aphid:Caterpillar) Thepunctuationhereisveryimportant(itiscomma,tilde,dot,minus)andnotetheuseof colons rather than asterisks to denote interaction terms rather than main effects plus interaction terms.",
    "target": "Q: What is There? A: There is no point looking at the saturated model in any detail, because the reams of information it contains are all superfluous.Thefirstrealstepinthemodellingistouseupdatetoremovethethree-way interaction from the saturated model, and then to use anova to test whether the three- way interaction is significant or not: model2<-update(model, .-Tree:Aphid:Caterpillar) Thepunctuationhereisveryimportant(itiscomma,tilde,dot,minus)andnotetheuseof colons rather than asterisks to denote interaction terms rather than main effects plus interaction terms."
  },
  {
    "input": "We might proceed like this: wrong<-glm(CountAphid*Caterpillar,family=poisson) wrong1<-update(wrong,.-Aphid:Caterpillar) anova(wrong,wrong1,test=\"Chi\") AnalysisofDevianceTable Model1:CountAphid*Caterpillar Model2:CountAphid+Caterpillar Resid.Df Resid.Dev Df Deviance Pr(>Chi) 1 4 550.19 2 5 556.85 -1 -6.6594 0.009864** The aphidcaterpillar interaction is highly significant (p<0.01) providing strong evi- denceforinduceddefences.Wrong!Byfailingtoincludethetreevariableinthemodelwe haveomittedanimportantexplanatoryvariable.Asitturnsout,andweshouldreallyhave determined by more thorough preliminary analysis, the trees differ enormously in their average levels of leaf holing: tapply(Count,list(Tree,Caterpillar),sum) holed not Tree1 58 2896 Tree2 176 1975 TheproportionofleaveswithholesonTree1was58/(58+2896)=0.0196,butonTree2 was176/(176+1975)=0.0818.Tree2hasmorethanfourtimestheproportionofitsleaves holedbycaterpillars.Ifwehadbeenpayingmoreattentionwhenwedidthemodellingthe wrongway,weshouldhavenoticedthatthemodelcontainingonlyaphidandcaterpillarhad massive overdispersion, and this should have alerted us that all was not well.",
    "target": "Q: What is We might proceed like this: wrong<-glm(CountAphid*Caterpillar,family=poisson) wrong1<-update(wrong,.-Aphid:Caterpillar) anova(wrong,wrong1,test=\"Chi\") AnalysisofDevianceTable Model1:CountAphid*Caterpillar Model2:CountAphid+Caterpillar Resid.Df Resid.Dev Df Deviance Pr(>Chi) 1 4 550.19 2 5 556.85 -1 -6.6594 0.009864** The aphidcaterpillar interaction? A: We might proceed like this: wrong<-glm(CountAphid*Caterpillar,family=poisson) wrong1<-update(wrong,.-Aphid:Caterpillar) anova(wrong,wrong1,test=\"Chi\") AnalysisofDevianceTable Model1:CountAphid*Caterpillar Model2:CountAphid+Caterpillar Resid.Df Resid.Dev Df Deviance Pr(>Chi) 1 4 550.19 2 5 556.85 -1 -6.6594 0.009864** The aphidcaterpillar interaction is highly significant (p<0.01) providing strong evi- denceforinduceddefences.Wrong!Byfailingtoincludethetreevariableinthemodelwe haveomittedanimportantexplanatoryvariable.Asitturnsout,andweshouldreallyhave determined by more thorough preliminary analysis, the trees differ enormously in their average levels of leaf holing: tapply(Count,list(Tree,Caterpillar),sum) holed not Tree1 58 2896 Tree2 176 1975 TheproportionofleaveswithholesonTree1was58/(58+2896)=0.0196,butonTree2 was176/(176+1975)=0.0818.Tree2hasmorethanfourtimestheproportionofitsleaves holedbycaterpillars.Ifwehadbeenpayingmoreattentionwhenwedidthemodellingthe wrongway,weshouldhavenoticedthatthemodelcontainingonlyaphidandcaterpillarhad massive overdispersion, and this should have alerted us that all was not well."
  },
  {
    "input": "COUNT DATA 247 AnalysisofCovariancewithCountData In this example the response is a count of the number of plant species on plots that have different biomass (a continuous explanatory variable) and different soil pH (a categorical variable with three levels: high, mid and low): species<-read.csv(\"c:\\\\temp\\\\species.csv\") attach(species) names(species) [1]\"pH\" \"Biomass\"\"Species\" We start by plotting the data, using different colours for each of the three pH classes: plot(Biomass,Species,pch=21,bg=(1+as.numeric(pH))) Nowwefitastraightforwardanalysisofcovarianceanduseablinetodrawlinesofthe appropriate colour through the scatterplot: model<-lm(SpeciesBiomass*pH) summary(model) Call: lm(formula=SpeciesBiomass*pH) Residuals: Min 1Q Median 3Q Max -9.290 -2.554 -0.124 2.208 15.677 Coefficients: Estimate Std.",
    "target": "Q: What is COUNT DATA 247 AnalysisofCovariancewithCountData In this example the response? A: COUNT DATA 247 AnalysisofCovariancewithCountData In this example the response is a count of the number of plant species on plots that have different biomass (a continuous explanatory variable) and different soil pH (a categorical variable with three levels: high, mid and low): species<-read.csv(\"c:\\\\temp\\\\species.csv\") attach(species) names(species) [1]\"pH\" \"Biomass\"\"Species\" We start by plotting the data, using different colours for each of the three pH classes: plot(Biomass,Species,pch=21,bg=(1+as.numeric(pH))) Nowwefitastraightforwardanalysisofcovarianceanduseablinetodrawlinesofthe appropriate colour through the scatterplot: model<-lm(SpeciesBiomass*pH) summary(model) Call: lm(formula=SpeciesBiomass*pH) Residuals: Min 1Q Median 3Q Max -9.290 -2.554 -0.124 2.208 15.677 Coefficients: Estimate Std."
  },
  {
    "input": "But how much, exactly?Well,thatdependsonourmodeloftheprocess.Perhapsthesimplestmodelisthat absolutely nothing is going on, and that every single bankruptcy case is absolutely independent of every other.",
    "target": "Q: What is But how much, exactly?Well,thatdependsonourmodeloftheprocess.Perhapsthesimplestmodelisthat absolutely nothing? A: But how much, exactly?Well,thatdependsonourmodeloftheprocess.Perhapsthesimplestmodelisthat absolutely nothing is going on, and that every single bankruptcy case is absolutely independent of every other."
  },
  {
    "input": "That leads to the prediction that the numbers of cases per district will follow a Poisson process: a distribution in which the variance is equal to the mean (Box 13.1).",
    "target": "Q: What is That leads to the prediction that the numbers of cases per district will follow a Poisson process: a distribution in which the variance? A: That leads to the prediction that the numbers of cases per district will follow a Poisson process: a distribution in which the variance is equal to the mean (Box 13.1)."
  },
  {
    "input": "ThePoissondistribution The Poisson distribution is widely used for the description of count data.",
    "target": "Q: What is ThePoissondistribution The Poisson distribution? A: ThePoissondistribution The Poisson distribution is widely used for the description of count data."
  },
  {
    "input": "The Poisson is a one-parameter distribution, specified entirely by the mean.",
    "target": "Q: What is The Poisson? A: The Poisson is a one-parameter distribution, specified entirely by the mean."
  },
  {
    "input": "The variance is identical to the mean (), so the variancemean ratio is equal to 1.",
    "target": "Q: What is The variance? A: The variance is identical to the mean (), so the variancemean ratio is equal to 1."
  },
  {
    "input": "The probability of observing a count of x is given by e\u0000x P x x!",
    "target": "Q: What is The probability of observing a count of x? A: The probability of observing a count of x is given by e\u0000x P x x!"
  },
  {
    "input": "The R function that does this is called table: frequencies<-table(cases) frequencies cases 0 1 2 3 4 5 6 7 8 9 10 34 14 10 7 4 5 2 1 1 1 1 There were no cases at all in 34 districts, but one district had 10 cases.",
    "target": "Q: What is The R function that does this? A: The R function that does this is called table: frequencies<-table(cases) frequencies cases 0 1 2 3 4 5 6 7 8 9 10 34 14 10 7 4 5 2 1 1 1 1 There were no cases at all in 34 districts, but one district had 10 cases."
  },
  {
    "input": "A good way to proceedistocompareourdistribution(calledfrequencies)withthedistributionthatwould be observed if the data really did come from a Poisson distribution as postulated by our model.WecanusetheRfunctiondpoistocomputetheprobabilitydensityofeachofthe 11 frequencies from 0 to10 (we multiply theprobability producedbydpois by thetotal sampleof80toobtainthepredictedfrequencies).Weneedtocalculatethemeannumberof cases per district  this is the Poisson distributions only parameter: mean(cases) [1]1.775 The plan is to draw two distributions side by side, so we set up the plotting region: windows(7,4) par(mfrow=c(1,2)) Now we plot the observed frequencies in the left-hand panel: barplot(frequencies,ylab=\"Frequency\",xlab=\"Cases\", col=\"red\",main=\"observed\") and the predicted, Poisson frequencies in the right-hand panel: barplot(dpois(0:10,1.775)*80,names=as.character(0:10), ylab=\"Frequency\",xlab=\"Cases\",col=\"blue\",main=\"expected\") 252 STATISTICS:ANINTRODUCTIONUSINGR Thedistributionsareverydifferent:themodeoftheobserveddataiszero,butthemodeof thePoissonwiththesamemeanis1;theobserveddatacontainedexamplesof8,9and10 cases,butthesewouldbehighlyunlikelyunderaPoissonprocess.Wewouldsaythatthe observed data are highly aggregated; they have a variancemean ratio of nearly 3 (the Poisson, of course, has a variancemean ratio of 1): var(cases)/mean(cases) [1] 2.99483 So, if the data are not Poisson distributed, how are they distributed?",
    "target": "Q: What is A good way to proceedistocompareourdistribution(calledfrequencies)withthedistributionthatwould be observed if the data really did come from a Poisson distribution as postulated by our model.WecanusetheRfunctiondpoistocomputetheprobabilitydensityofeachofthe 11 frequencies from 0 to10 (we multiply theprobability producedbydpois by thetotal sampleof80toobtainthepredictedfrequencies).Weneedtocalculatethemeannumberof cases per district  this? A: A good way to proceedistocompareourdistribution(calledfrequencies)withthedistributionthatwould be observed if the data really did come from a Poisson distribution as postulated by our model.WecanusetheRfunctiondpoistocomputetheprobabilitydensityofeachofthe 11 frequencies from 0 to10 (we multiply theprobability producedbydpois by thetotal sampleof80toobtainthepredictedfrequencies).Weneedtocalculatethemeannumberof cases per district  this is the Poisson distributions only parameter: mean(cases) [1]1.775 The plan is to draw two distributions side by side, so we set up the plotting region: windows(7,4) par(mfrow=c(1,2)) Now we plot the observed frequencies in the left-hand panel: barplot(frequencies,ylab=\"Frequency\",xlab=\"Cases\", col=\"red\",main=\"observed\") and the predicted, Poisson frequencies in the right-hand panel: barplot(dpois(0:10,1.775)*80,names=as.character(0:10), ylab=\"Frequency\",xlab=\"Cases\",col=\"blue\",main=\"expected\") 252 STATISTICS:ANINTRODUCTIONUSINGR Thedistributionsareverydifferent:themodeoftheobserveddataiszero,butthemodeof thePoissonwiththesamemeanis1;theobserveddatacontainedexamplesof8,9and10 cases,butthesewouldbehighlyunlikelyunderaPoissonprocess.Wewouldsaythatthe observed data are highly aggregated; they have a variancemean ratio of nearly 3 (the Poisson, of course, has a variancemean ratio of 1): var(cases)/mean(cases) [1] 2.99483 So, if the data are not Poisson distributed, how are they distributed?"
  },
  {
    "input": "A good candidate distribution where the variancemean ratio is this big (c. 3.0) is the negative binomial distribution (Box 13.2).",
    "target": "Q: What is A good candidate distribution where the variancemean ratio? A: A good candidate distribution where the variancemean ratio is this big (c. 3.0) is the negative binomial distribution (Box 13.2)."
  },
  {
    "input": "Thenegativebinomialdistribution This discrete, two-parameter distribution is useful for describing the distribution of count data, where the variance is often much greater than the mean.",
    "target": "Q: What is Thenegativebinomialdistribution This discrete, two-parameter distribution? A: Thenegativebinomialdistribution This discrete, two-parameter distribution is useful for describing the distribution of count data, where the variance is often much greater than the mean."
  },
  {
    "input": "The density function is (cid:4) (cid:5) (cid:1)  (cid:3) \u0000k kx  x p x 1 k x!",
    "target": "Q: What is The density function? A: The density function is (cid:4) (cid:5) (cid:1)  (cid:3) \u0000k kx  x p x 1 k x!"
  },
  {
    "input": "The zero term is found by setting x = 0 and simplifying: (cid:1) (cid:3)  \u0000k p 0 1 k Successive terms in the distribution can be computed iteratively from (cid:4) (cid:5)(cid:4) (cid:5) kx\u00001  p xp x\u00001 x k An initial estimate of the value of k can be obtained from the sample mean and variance: 2 k \u0019 s2\u0000 Sincekcannotbenegative,itisclearthatthenegativebinomialdistributionshouldnot befittedtodatawherethevarianceislessthanthemean(useabinomialdistribution instead).",
    "target": "Q: What is The zero term? A: The zero term is found by setting x = 0 and simplifying: (cid:1) (cid:3)  \u0000k p 0 1 k Successive terms in the distribution can be computed iteratively from (cid:4) (cid:5)(cid:4) (cid:5) kx\u00001  p xp x\u00001 x k An initial estimate of the value of k can be obtained from the sample mean and variance: 2 k \u0019 s2\u0000 Sincekcannotbenegative,itisclearthatthenegativebinomialdistributionshouldnot befittedtodatawherethevarianceislessthanthemean(useabinomialdistribution instead)."
  },
  {
    "input": "The precise maximum likelihood estimate of k is found numerically, by COUNT DATA 253 iteratingprogressivelymorefine-tunedvaluesofkuntiltheleft-andright-handsides of the following equation are equal: (cid:4) (cid:5) (cid:1)  (cid:3) Xmax A nln 1  x k kx x0 where the vector A(x) contains the total frequency of values greater than x.",
    "target": "Q: What is The precise maximum likelihood estimate of k? A: The precise maximum likelihood estimate of k is found numerically, by COUNT DATA 253 iteratingprogressivelymorefine-tunedvaluesofkuntiltheleft-andright-handsides of the following equation are equal: (cid:4) (cid:5) (cid:1)  (cid:3) Xmax A nln 1  x k kx x0 where the vector A(x) contains the total frequency of values greater than x."
  },
  {
    "input": "The density function for the negative binomial distribution is dnbinomand it has three arguments: thefrequency for which we wanttheprobability(inourcase0to10),theclumpingparameter(size=0.8898),andthe meannumberofcases(mu=1.775);wemultiplybythetotalnumberofcases(80)toobtain the expected frequencies expected<-dnbinom(0:10,size=0.8898,mu=1.775)*80 254 STATISTICS:ANINTRODUCTIONUSINGR The plan istodrawasinglefigureinwhichtheobservedandexpectedfrequenciesare drawnsidebyside.Thetrickistoproduceanewvector(calledboth)whichistwiceaslong as the observed and expected frequency vectors (211=22).",
    "target": "Q: What is The density function for the negative binomial distribution? A: The density function for the negative binomial distribution is dnbinomand it has three arguments: thefrequency for which we wanttheprobability(inourcase0to10),theclumpingparameter(size=0.8898),andthe meannumberofcases(mu=1.775);wemultiplybythetotalnumberofcases(80)toobtain the expected frequencies expected<-dnbinom(0:10,size=0.8898,mu=1.775)*80 254 STATISTICS:ANINTRODUCTIONUSINGR The plan istodrawasinglefigureinwhichtheobservedandexpectedfrequenciesare drawnsidebyside.Thetrickistoproduceanewvector(calledboth)whichistwiceaslong as the observed and expected frequency vectors (211=22)."
  },
  {
    "input": "Then we put the observed frequencies in the odd numbered elements (using modulo 2 to calculate the values of the subscripts), and the expected frequencies in the even numbered elements: both<-numeric(22) both[1:22%%2!=0]<-frequencies both[1:22%%2==0]<-expected On the x axis, we intend to label only every other bar: labels<-character(22) labels[1:22%%2==0]<-as.character(0:10) Nowwecanproducethebarplot,usinglightgreyfortheobservedfrequenciesanddark grey for the negative binomial frequencies: barplot(both,col=rep(c(\"lightgray\",\"darkgray\"),11),names=labels, ylab=\"Frequency\",xlab=\"Cases\") Weneedtoaddalegendtoshowwhatthetwocoloursofthebarsmean.Youcanlocate the legend by trial and error, or by left-clicking mouse when the cursor is in the correct position, using the locator(1) function (see p. 169): legend(locator(1),c(\"Observed\",\"Expected\"), fill=c(\"lightgray\",\"darkgray\")) COUNT DATA 255 ThefittothenegativebinomialdistributionismuchbetterthanitwaswiththePoisson distribution,especiallyintheright-handtail.Buttheobserveddatahavetoomanyzerosand toofewonestoberepresentedperfectlybyanegativebinomialdistribution.Ifyouwantto quantifythelackoffitbetweenPtheobservedandexpectedfrequencydistributions,youcan calculatePearsonschisquare O\u0000E2=Ebasedonthenumberofcomparisonsthathave expected frequency greater than 4. expected [1]30.1449097 17.8665264 11.2450066 7.2150606 4.6734866 3.0443588 [7] 1.9905765 1.3050321 0.8572962 0.5640455 0.3715655 Ifweaccumulatetherightmostsixfrequencies,thenallthevaluesofexpectedwillbe bigger than 4.",
    "target": "Q: What is Then we put the observed frequencies in the odd numbered elements (using modulo 2 to calculate the values of the subscripts), and the expected frequencies in the even numbered elements: both<-numeric(22) both[1:22%%2!=0]<-frequencies both[1:22%%2==0]<-expected On the x axis, we intend to label only every other bar: labels<-character(22) labels[1:22%%2==0]<-as.character(0:10) Nowwecanproducethebarplot,usinglightgreyfortheobservedfrequenciesanddark grey for the negative binomial frequencies: barplot(both,col=rep(c(\"lightgray\",\"darkgray\"),11),names=labels, ylab=\"Frequency\",xlab=\"Cases\") Weneedtoaddalegendtoshowwhatthetwocoloursofthebarsmean.Youcanlocate the legend by trial and error, or by left-clicking mouse when the cursor? A: Then we put the observed frequencies in the odd numbered elements (using modulo 2 to calculate the values of the subscripts), and the expected frequencies in the even numbered elements: both<-numeric(22) both[1:22%%2!=0]<-frequencies both[1:22%%2==0]<-expected On the x axis, we intend to label only every other bar: labels<-character(22) labels[1:22%%2==0]<-as.character(0:10) Nowwecanproducethebarplot,usinglightgreyfortheobservedfrequenciesanddark grey for the negative binomial frequencies: barplot(both,col=rep(c(\"lightgray\",\"darkgray\"),11),names=labels, ylab=\"Frequency\",xlab=\"Cases\") Weneedtoaddalegendtoshowwhatthetwocoloursofthebarsmean.Youcanlocate the legend by trial and error, or by left-clicking mouse when the cursor is in the correct position, using the locator(1) function (see p. 169): legend(locator(1),c(\"Observed\",\"Expected\"), fill=c(\"lightgray\",\"darkgray\")) COUNT DATA 255 ThefittothenegativebinomialdistributionismuchbetterthanitwaswiththePoisson distribution,especiallyintheright-handtail.Buttheobserveddatahavetoomanyzerosand toofewonestoberepresentedperfectlybyanegativebinomialdistribution.Ifyouwantto quantifythelackoffitbetweenPtheobservedandexpectedfrequencydistributions,youcan calculatePearsonschisquare O\u0000E2=Ebasedonthenumberofcomparisonsthathave expected frequency greater than 4. expected [1]30.1449097 17.8665264 11.2450066 7.2150606 4.6734866 3.0443588 [7] 1.9905765 1.3050321 0.8572962 0.5640455 0.3715655 Ifweaccumulatetherightmostsixfrequencies,thenallthevaluesofexpectedwillbe bigger than 4."
  },
  {
    "input": "The degrees of freedom are then given by the number of comparisons (6) minus the number of parameters estimated from the data (2 in our case; the mean and k) minus 1 for contingency (because the total frequency must add up to 80), so there are 3 degreesoffreedom.Weuselevelsgetstoreducethelengthsoftheobservedandexpected vectors, creating an upper interval called 5+ for 5 or more: cs<-factor(0:10) levels(cs)[6:11]<-\"5+\" levels(cs) [1]\"0\" \"1\" \"2\" \"3\" \"4\" \"5+\" Nowmakethetwoshortervectorsofandef(for observedandexpectedfrequencies): ef<-as.vector(tapply(expected,cs,sum)) of<-as.vector(tapply(frequencies,cs,sum)) Finally, we can compute the chi-squared value measuring the difference between the observedandexpectedfrequencydistributions,anduse1-pchisqtoworkoutthepvalue: sum((of-ef)^2/ef) [1]2.581842 1-pchisq(2.581842,3) [1]0.4606818 We conclude that a negative binomial description of these data is reasonable (the observed and expected distributions are not significantly different; p=0.46).",
    "target": "Q: What is The degrees of freedom are then given by the number of comparisons (6) minus the number of parameters estimated from the data (2 in our case; the mean and k) minus 1 for contingency (because the total frequency must add up to 80), so there are 3 degreesoffreedom.Weuselevelsgetstoreducethelengthsoftheobservedandexpected vectors, creating an upper interval called 5+ for 5 or more: cs<-factor(0:10) levels(cs)[6:11]<-\"5+\" levels(cs) [1]\"0\" \"1\" \"2\" \"3\" \"4\" \"5+\" Nowmakethetwoshortervectorsofandef(for observedandexpectedfrequencies): ef<-as.vector(tapply(expected,cs,sum)) of<-as.vector(tapply(frequencies,cs,sum)) Finally, we can compute the chi-squared value measuring the difference between the observedandexpectedfrequencydistributions,anduse1-pchisqtoworkoutthepvalue: sum((of-ef)^2/ef) [1]2.581842 1-pchisq(2.581842,3) [1]0.4606818 We conclude that a negative binomial description of these data? A: The degrees of freedom are then given by the number of comparisons (6) minus the number of parameters estimated from the data (2 in our case; the mean and k) minus 1 for contingency (because the total frequency must add up to 80), so there are 3 degreesoffreedom.Weuselevelsgetstoreducethelengthsoftheobservedandexpected vectors, creating an upper interval called 5+ for 5 or more: cs<-factor(0:10) levels(cs)[6:11]<-\"5+\" levels(cs) [1]\"0\" \"1\" \"2\" \"3\" \"4\" \"5+\" Nowmakethetwoshortervectorsofandef(for observedandexpectedfrequencies): ef<-as.vector(tapply(expected,cs,sum)) of<-as.vector(tapply(frequencies,cs,sum)) Finally, we can compute the chi-squared value measuring the difference between the observedandexpectedfrequencydistributions,anduse1-pchisqtoworkoutthepvalue: sum((of-ef)^2/ef) [1]2.581842 1-pchisq(2.581842,3) [1]0.4606818 We conclude that a negative binomial description of these data is reasonable (the observed and expected distributions are not significantly different; p=0.46)."
  },
  {
    "input": "The only complication is that whereaswithPoissonerrorswecouldsimplysayfamily=poisson,withbinomialerrors we must specifythe number offailures as well as thenumbersof successesby creatinga two-vectorresponsevariable.Todothiswebindtogethertwovectorsusingcbindintoa single object, y, comprising the numbers of successes and the number of failures.",
    "target": "Q: What is The only complication? A: The only complication is that whereaswithPoissonerrorswecouldsimplysayfamily=poisson,withbinomialerrors we must specifythe number offailures as well as thenumbersof successesby creatinga two-vectorresponsevariable.Todothiswebindtogethertwovectorsusingcbindintoa single object, y, comprising the numbers of successes and the number of failures."
  },
  {
    "input": "The binomial denominator, n, is the total sample, and number.of.failures<-binomial.denominatornumber.of.successes y<-cbind(number.of.successes,number.of.failures) Theold-fashionedwayofmodellingthissortofdatawastousethepercentagemortality as the response variable.",
    "target": "Q: What is The binomial denominator, n,? A: The binomial denominator, n, is the total sample, and number.of.failures<-binomial.denominatornumber.of.successes y<-cbind(number.of.successes,number.of.failures) Theold-fashionedwayofmodellingthissortofdatawastousethepercentagemortality as the response variable."
  },
  {
    "input": "There are four problems with this: (cid:129) the errors are not normally distributed (cid:129) the variance is not constant Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.",
    "target": "Q: What is There are four problems with this: (cid:129) the errors are not normally distributed (cid:129) the variance? A: There are four problems with this: (cid:129) the errors are not normally distributed (cid:129) the variance is not constant Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley."
  },
  {
    "input": "PROPORTION DATA 257 (cid:129) the response is bounded (by 100 above and by 0 below) (cid:129) by calculating the percentage, we lose information of the size of the sample, n, from which the proportion was estimated ByusingaGLM,wecauseRtocarryoutaweightedregression,withthesamplesizes,n, asweights,andemployingthelogitlinkfunctiontoensurelinearity.Therearesomekindsof proportiondata,suchaspercentagecover,thatarebestanalysedusingconventionallinear models(withnormalerrorsandconstantvariance)followinpgffiaffiffiffirfficffiffiffisffiiffiffinffiffieffiffiffiffitransformation.Insuch cases,theresponsevariable,measuredinradians,issin \u00001 0:01\u0002p,wherepispercentage cover.",
    "target": "Q: What is PROPORTION DATA 257 (cid:129) the response? A: PROPORTION DATA 257 (cid:129) the response is bounded (by 100 above and by 0 below) (cid:129) by calculating the percentage, we lose information of the size of the sample, n, from which the proportion was estimated ByusingaGLM,wecauseRtocarryoutaweightedregression,withthesamplesizes,n, asweights,andemployingthelogitlinkfunctiontoensurelinearity.Therearesomekindsof proportiondata,suchaspercentagecover,thatarebestanalysedusingconventionallinear models(withnormalerrorsandconstantvariance)followinpgffiaffiffiffirfficffiffiffisffiiffiffinffiffieffiffiffiffitransformation.Insuch cases,theresponsevariable,measuredinradians,issin \u00001 0:01\u0002p,wherepispercentage cover."
  },
  {
    "input": "If, however, the response variable takes the form of a percentage change in some continuousmeasurement(suchasthepercentagechangeinweightonreceivingaparticular diet), then rather than arcsine transform the data, it is usually better treated by either (cid:129) analysisofcovariance(seeChapter9),usingfinalweightastheresponsevariableand initial weight as a covariate, or (cid:129) by specifying the response variable as a relative growth rate, measured as log(final weight/initial weight) both of which can then be analysed with normal errors without further transformation.",
    "target": "Q: What is If, however, the response variable takes the form of a percentage change in some continuousmeasurement(suchasthepercentagechangeinweightonreceivingaparticular diet), then rather than arcsine transform the data, it? A: If, however, the response variable takes the form of a percentage change in some continuousmeasurement(suchasthepercentagechangeinweightonreceivingaparticular diet), then rather than arcsine transform the data, it is usually better treated by either (cid:129) analysisofcovariance(seeChapter9),usingfinalweightastheresponsevariableand initial weight as a covariate, or (cid:129) by specifying the response variable as a relative growth rate, measured as log(final weight/initial weight) both of which can then be analysed with normal errors without further transformation."
  },
  {
    "input": "For proportion data, the mean success rate is given by: total number of successes mean proportion total number of attempts Weneedtogobackandlookatthecountsonwhichourfourproportionswerecalculated.It turnsouttheywere1outof5,1outof6,2outof10and53outof100,respectively.The total number of successes was 1+1+2+53=57 and the total number of attempts was 5+6+10+100=121.",
    "target": "Q: What is For proportion data, the mean success rate? A: For proportion data, the mean success rate is given by: total number of successes mean proportion total number of attempts Weneedtogobackandlookatthecountsonwhichourfourproportionswerecalculated.It turnsouttheywere1outof5,1outof6,2outof10and53outof100,respectively.The total number of successes was 1+1+2+53=57 and the total number of attempts was 5+6+10+100=121."
  },
  {
    "input": "This is nearly double the answer we got by doing it the wrong way.",
    "target": "Q: What is This? A: This is nearly double the answer we got by doing it the wrong way."
  },
  {
    "input": "The arcsine transformationtookcareoftheerrordistribution,whiletheprobittransformationwasused 258 STATISTICS:ANINTRODUCTIONUSINGR tolinearizetherelationshipbetweenpercentagemortalityandlogdoseinabioassay.There isnothingwrongwiththesetransformations,andtheyareavailablewithinR,butasimpler approach is often preferable, and is likely to produce a model that is easier to interpret.",
    "target": "Q: What is The arcsine transformationtookcareoftheerrordistribution,whiletheprobittransformationwasused 258 STATISTICS:ANINTRODUCTIONUSINGR tolinearizetherelationshipbetweenpercentagemortalityandlogdoseinabioassay.There isnothingwrongwiththesetransformations,andtheyareavailablewithinR,butasimpler approach? A: The arcsine transformationtookcareoftheerrordistribution,whiletheprobittransformationwasused 258 STATISTICS:ANINTRODUCTIONUSINGR tolinearizetherelationshipbetweenpercentagemortalityandlogdoseinabioassay.There isnothingwrongwiththesetransformations,andtheyareavailablewithinR,butasimpler approach is often preferable, and is likely to produce a model that is easier to interpret."
  },
  {
    "input": "The major difficulty with modelling proportion data is that the responses are strictly bounded.Thereisnowaythatthepercentagedyingcanbegreaterthan100%orlessthan 0%.Butifweusesimpletechniquessuchaslinearregressionoranalysisofcovariance,then the fitted model could quite easily predict negative values or values greater than 100%, especiallyifthevariancewashighandmanyofthedatawerecloseto0orcloseto100%.",
    "target": "Q: What is The major difficulty with modelling proportion data? A: The major difficulty with modelling proportion data is that the responses are strictly bounded.Thereisnowaythatthepercentagedyingcanbegreaterthan100%orlessthan 0%.Butifweusesimpletechniquessuchaslinearregressionoranalysisofcovariance,then the fitted model could quite easily predict negative values or values greater than 100%, especiallyifthevariancewashighandmanyofthedatawerecloseto0orcloseto100%."
  },
  {
    "input": "The third variable is the size of the sample, n,fromwhichpwasestimated(itisthebinomialdenominator,andthestatisticiansnumber of attempts).",
    "target": "Q: What is The third variable? A: The third variable is the size of the sample, n,fromwhichpwasestimated(itisthebinomialdenominator,andthestatisticiansnumber of attempts)."
  },
  {
    "input": "An important point about the binomial distribution is that the variance is not constant.",
    "target": "Q: What is An important point about the binomial distribution? A: An important point about the binomial distribution is that the variance is not constant."
  },
  {
    "input": "Remember that for the Poisson,thevarianceisequaltothemean,s2 np.Now,aspgetssmaller,soqgetscloser and closer to 1, so the variance of the binomial converges to the mean: s2 npq\u0019np q\u00191 Odds The logistic model for p as a function of x looks like this: eabx p 1eabx andtherearenoprizesforrealizingthatthemodelisnotlinear.Whenconfrontedwithanew equation like this, it is a good idea to work out its behaviour at the limits.",
    "target": "Q: What is Remember that for the Poisson,thevarianceisequaltothemean,s2 np.Now,aspgetssmaller,soqgetscloser and closer to 1, so the variance of the binomial converges to the mean: s2 npq\u0019np q\u00191 Odds The logistic model for p as a function of x looks like this: eabx p 1eabx andtherearenoprizesforrealizingthatthemodelisnotlinear.Whenconfrontedwithanew equation like this, it? A: Remember that for the Poisson,thevarianceisequaltothemean,s2 np.Now,aspgetssmaller,soqgetscloser and closer to 1, so the variance of the binomial converges to the mean: s2 npq\u0019np q\u00191 Odds The logistic model for p as a function of x looks like this: eabx p 1eabx andtherearenoprizesforrealizingthatthemodelisnotlinear.Whenconfrontedwithanew equation like this, it is a good idea to work out its behaviour at the limits."
  },
  {
    "input": "When x=0 then p=exp(a)/(1+exp(a)), so this is the intercept.",
    "target": "Q: What is When x=0 then p=exp(a)/(1+exp(a)), so this? A: When x=0 then p=exp(a)/(1+exp(a)), so this is the intercept."
  },
  {
    "input": "There are two results thatyoushouldmemorize:exp(  )= andexp(\u0000 )=1/exp(  )=0.Sowhenxisavery largepositivenumber,wehavep=1andwhenxisaverylargenegativenumber,wehave p=0/1=0, so the model is strictly bounded.",
    "target": "Q: What is There are two results thatyoushouldmemorize:exp(  )= andexp(\u0000 )=1/exp(  )=0.Sowhenxisavery largepositivenumber,wehavep=1andwhenxisaverylargenegativenumber,wehave p=0/1=0, so the model? A: There are two results thatyoushouldmemorize:exp(  )= andexp(\u0000 )=1/exp(  )=0.Sowhenxisavery largepositivenumber,wehavep=1andwhenxisaverylargenegativenumber,wehave p=0/1=0, so the model is strictly bounded."
  },
  {
    "input": "In symbols, this is the difference betweenthescientiststatingtheprobabilityp,andthebookmakerstatingtheoddsp/q.Now if we take the odds p/q and substitute this into the formula for the logistic, we get: (cid:3) (cid:4) p eabx eabx \u00001  1\u0000 q 1eabx 1eabx which looks awful.",
    "target": "Q: What is In symbols, this? A: In symbols, this is the difference betweenthescientiststatingtheprobabilityp,andthebookmakerstatingtheoddsp/q.Now if we take the odds p/q and substitute this into the formula for the logistic, we get: (cid:3) (cid:4) p eabx eabx \u00001  1\u0000 q 1eabx 1eabx which looks awful."
  },
  {
    "input": "The only difference is that we assess the significanceoftermsonthebasisofchi-squared;thisistheincreaseinscaleddeviancethat results from removal of a term from the current model.",
    "target": "Q: What is The only difference? A: The only difference is that we assess the significanceoftermsonthebasisofchi-squared;thisistheincreaseinscaleddeviancethat results from removal of a term from the current model."
  },
  {
    "input": "Most worrisome is the fact that the degree to which the approximation is satisfactory isitselfunknown.Thismeansthatconsiderablecaremustbeexercisedinthe interpretation of tests of hypotheses on parameters, especially when the parameters are marginallysignificantorwhentheyexplainaverysmallfractionofthetotaldeviance.With binomial or Poisson errors we cannot hope to provide exact p values for our tests of hypotheses.",
    "target": "Q: What is Most worrisome? A: Most worrisome is the fact that the degree to which the approximation is satisfactory isitselfunknown.Thismeansthatconsiderablecaremustbeexercisedinthe interpretation of tests of hypotheses on parameters, especially when the parameters are marginallysignificantorwhentheyexplainaverysmallfractionofthetotaldeviance.With binomial or Poisson errors we cannot hope to provide exact p values for our tests of hypotheses."
  },
  {
    "input": "One simple solution is to assume that the variance is not npq but npqs, where s is an unknownscaleparameter(s>1).Weobtainanestimateofthescaleparameterbydividing thePearsonchi-squaredbythedegreesoffreedom,andusethisestimateofstocomparethe resultingscaleddeviances.Toaccomplishthis,weusefamily=quasibinomialrather than family = binomial when there is overdispersion.",
    "target": "Q: What is One simple solution? A: One simple solution is to assume that the variance is not npq but npqs, where s is an unknownscaleparameter(s>1).Weobtainanestimateofthescaleparameterbydividing thePearsonchi-squaredbythedegreesoffreedom,andusethisestimateofstocomparethe resultingscaleddeviances.Toaccomplishthis,weusefamily=quasibinomialrather than family = binomial when there is overdispersion."
  },
  {
    "input": "The model is specified like this: model<-glm(ydensity,binomial) PROPORTION DATA 263 Thissaysthattheobjectcalledmodelgetsageneralizedlinearmodelinwhichy(the sex ratio) is modelled as a function of a single continuous explanatory variable called density, using an error distribution from the family = binomial.",
    "target": "Q: What is The model? A: The model is specified like this: model<-glm(ydensity,binomial) PROPORTION DATA 263 Thissaysthattheobjectcalledmodelgetsageneralizedlinearmodelinwhichy(the sex ratio) is modelled as a function of a single continuous explanatory variable called density, using an error distribution from the family = binomial."
  },
  {
    "input": "The first parameter is the intercept and the second is the slope of the graph of sex ratio against populationdensity.Theslopeishighlysignificantlysteeperthanzero(proportionatelymore malesathigherpopulationdensity;p=6.8110 \u000012).Wecanseeiflogtransformationof the explanatory variable reduces the residual deviance below 22.091: model<-glm(ylog(density),binomial) summary(model) Coefficients: Estimate Std.",
    "target": "Q: What is The first parameter? A: The first parameter is the intercept and the second is the slope of the graph of sex ratio against populationdensity.Theslopeishighlysignificantlysteeperthanzero(proportionatelymore malesathigherpopulationdensity;p=6.8110 \u000012).Wecanseeiflogtransformationof the explanatory variable reduces the residual deviance below 22.091: model<-glm(ylog(density),binomial) summary(model) Coefficients: Estimate Std."
  },
  {
    "input": "It means that there is extra, unexplained variation, over and above the binomialvarianceassumedbythemodelspecification.Inthemodelwithlog(density)there isnoevidenceofoverdispersion(residualdeviance=5.67on6d.f.",
    "target": "Q: What is It means that there? A: It means that there is extra, unexplained variation, over and above the binomialvarianceassumedbythemodelspecification.Inthemodelwithlog(density)there isnoevidenceofoverdispersion(residualdeviance=5.67on6d.f."
  },
  {
    "input": "As you will see, there is no worryingly great pattern in the residuals against the fitted values, and the normal plot is reasonably linear.",
    "target": "Q: What is As you will see, there? A: As you will see, there is no worryingly great pattern in the residuals against the fitted values, and the normal plot is reasonably linear."
  },
  {
    "input": "Point number 4 is highly influential (it has a big value of Cooks distance),andpointnumber8hashighleverage(butthemodelisstillsignificantwiththis point omitted).",
    "target": "Q: What is Point number 4? A: Point number 4 is highly influential (it has a big value of Cooks distance),andpointnumber8hashighleverage(butthemodelisstillsignificantwiththis point omitted)."
  },
  {
    "input": "We conclude that the proportion of animals that are males increases significantlywithincreasingdensity,andthatthelogisticmodelislinearizedbylogarithmic transformationoftheexplanatoryvariable(populationdensity).Wefinishbydrawingthe fitted line though the scatterplot: xv<-seq(0,6,0.1) plot(log(density),p,ylab=\"Proportionmale\",pch=21,bg=\"blue\") lines(xv,predict(model,list(density=exp(xv)), type=\"response\"),col=\"brown\") Notetheuseoftype=\"response\"toback-transformfromthelogitscaletotheS-shaped proportion scale and exp(xv) to back-transform the logs from the x axis to densities as requiredbythemodelformula(wheretheyarethenloggedagain).Asyoucansee,themodel isverypoorfitforthelowestdensityandforlog(density)3,butareasonablygoodfitover therestoftherange.Isuspectthatthereplicationwassimplytoolowatverylowpopulation densitiestogetanaccurateestimateofthesexratio.Ofcourse,weshouldnotdiscountthe possibility that the data point is not an outlier, but rather that the model is wrong.",
    "target": "Q: What is We conclude that the proportion of animals that are males increases significantlywithincreasingdensity,andthatthelogisticmodelislinearizedbylogarithmic transformationoftheexplanatoryvariable(populationdensity).Wefinishbydrawingthe fitted line though the scatterplot: xv<-seq(0,6,0.1) plot(log(density),p,ylab=\"Proportionmale\",pch=21,bg=\"blue\") lines(xv,predict(model,list(density=exp(xv)), type=\"response\"),col=\"brown\") Notetheuseoftype=\"response\"toback-transformfromthelogitscaletotheS-shaped proportion scale and exp(xv) to back-transform the logs from the x axis to densities as requiredbythemodelformula(wheretheyarethenloggedagain).Asyoucansee,themodel isverypoorfitforthelowestdensityandforlog(density)3,butareasonablygoodfitover therestoftherange.Isuspectthatthereplicationwassimplytoolowatverylowpopulation densitiestogetanaccurateestimateofthesexratio.Ofcourse,weshouldnotdiscountthe possibility that the data point? A: We conclude that the proportion of animals that are males increases significantlywithincreasingdensity,andthatthelogisticmodelislinearizedbylogarithmic transformationoftheexplanatoryvariable(populationdensity).Wefinishbydrawingthe fitted line though the scatterplot: xv<-seq(0,6,0.1) plot(log(density),p,ylab=\"Proportionmale\",pch=21,bg=\"blue\") lines(xv,predict(model,list(density=exp(xv)), type=\"response\"),col=\"brown\") Notetheuseoftype=\"response\"toback-transformfromthelogitscaletotheS-shaped proportion scale and exp(xv) to back-transform the logs from the x axis to densities as requiredbythemodelformula(wheretheyarethenloggedagain).Asyoucansee,themodel isverypoorfitforthelowestdensityandforlog(density)3,butareasonablygoodfitover therestoftherange.Isuspectthatthereplicationwassimplytoolowatverylowpopulation densitiestogetanaccurateestimateofthesexratio.Ofcourse,weshouldnotdiscountthe possibility that the data point is not an outlier, but rather that the model is wrong."
  },
  {
    "input": "PROPORTION DATA 265 germination<-read.csv(\"c:\\\\temp\\\\germination.csv\") attach(germination) names(germination) [1]\"count\" \"sample\" \"Orobanche\"\"extract\" The response variable count is the number of seeds that germinated out of a batch of size=sample.Sothenumberthatdidntgerminateissamplecount,andweconstruct the response vector like this: y<-cbind(count,sample-count) Each of the categorical explanatory variables has two levels: levels(Orobanche) [1]\"a73\"\"a75\" levels(extract) [1]\"bean\" \"cucumber\" WewanttotestthehypothesisthatthereisnointeractionbetweenOrobanchegenotype (a73ora75)andplant extract (beanorcucumber)onthegermination rateoftheseeds.",
    "target": "Q: What is PROPORTION DATA 265 germination<-read.csv(\"c:\\\\temp\\\\germination.csv\") attach(germination) names(germination) [1]\"count\" \"sample\" \"Orobanche\"\"extract\" The response variable count? A: PROPORTION DATA 265 germination<-read.csv(\"c:\\\\temp\\\\germination.csv\") attach(germination) names(germination) [1]\"count\" \"sample\" \"Orobanche\"\"extract\" The response variable count is the number of seeds that germinated out of a batch of size=sample.Sothenumberthatdidntgerminateissamplecount,andweconstruct the response vector like this: y<-cbind(count,sample-count) Each of the categorical explanatory variables has two levels: levels(Orobanche) [1]\"a73\"\"a75\" levels(extract) [1]\"bean\" \"cucumber\" WewanttotestthehypothesisthatthereisnointeractionbetweenOrobanchegenotype (a73ora75)andplant extract (beanorcucumber)onthegermination rateoftheseeds."
  },
  {
    "input": "This requires a factorial analysis using the asterisk * operator like this: model<-glm(yOrobanche*extract,binomial) summary(model) Coefficients: Estimate Std.Error z value Pr (>|z|) (Intercept) -0.4122 0.1842 -2.238 0.0252 * Orobanchea75 -0.1459 0.2232 -0.654 0.5132 extractcucumber 0.5401 0.2498 2.162 0.0306 * Orobanchea75:extractcucumber 0.7781 0.3064 2.539 0.0111 * (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:98.719 on20 degreesoffreedom Residualdeviance:33.278 on17 degreesoffreedom AIC:117.87 NumberofFisherScoringiterations:4 Atfirstglance,itlooksasifthereisahighlysignificantinteraction(p=0.0111).Butwe needtocheckthatthemodelissound.Thefirstthingistocheckforisoverdispersion.The residual deviance is 33.278 on 17 d.f.",
    "target": "Q: What is This requires a factorial analysis using the asterisk * operator like this: model<-glm(yOrobanche*extract,binomial) summary(model) Coefficients: Estimate Std.Error z value Pr (>|z|) (Intercept) -0.4122 0.1842 -2.238 0.0252 * Orobanchea75 -0.1459 0.2232 -0.654 0.5132 extractcucumber 0.5401 0.2498 2.162 0.0306 * Orobanchea75:extractcucumber 0.7781 0.3064 2.539 0.0111 * (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:98.719 on20 degreesoffreedom Residualdeviance:33.278 on17 degreesoffreedom AIC:117.87 NumberofFisherScoringiterations:4 Atfirstglance,itlooksasifthereisahighlysignificantinteraction(p=0.0111).Butwe needtocheckthatthemodelissound.Thefirstthingistocheckforisoverdispersion.The residual deviance? A: This requires a factorial analysis using the asterisk * operator like this: model<-glm(yOrobanche*extract,binomial) summary(model) Coefficients: Estimate Std.Error z value Pr (>|z|) (Intercept) -0.4122 0.1842 -2.238 0.0252 * Orobanchea75 -0.1459 0.2232 -0.654 0.5132 extractcucumber 0.5401 0.2498 2.162 0.0306 * Orobanchea75:extractcucumber 0.7781 0.3064 2.539 0.0111 * (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:98.719 on20 degreesoffreedom Residualdeviance:33.278 on17 degreesoffreedom AIC:117.87 NumberofFisherScoringiterations:4 Atfirstglance,itlooksasifthereisahighlysignificantinteraction(p=0.0111).Butwe needtocheckthatthemodelissound.Thefirstthingistocheckforisoverdispersion.The residual deviance is 33.278 on 17 d.f."
  },
  {
    "input": "so the model is quite badly overdispersed: 33.279/17 [1]1.957588 266 STATISTICS:ANINTRODUCTIONUSINGR The overdispersion factor is almost 2.",
    "target": "Q: What is so the model? A: so the model is quite badly overdispersed: 33.279/17 [1]1.957588 266 STATISTICS:ANINTRODUCTIONUSINGR The overdispersion factor is almost 2."
  },
  {
    "input": "The simplest way to take this into account is to use what is called an empirical scale parameter to reflect the fact that the errors are not binomial as we assumed, but were larger than this (overdispersed) by a factor of 1.9576.",
    "target": "Q: What is The simplest way to take this into account? A: The simplest way to take this into account is to use what is called an empirical scale parameter to reflect the fact that the errors are not binomial as we assumed, but were larger than this (overdispersed) by a factor of 1.9576."
  },
  {
    "input": "Now you see that the interaction is not significant (p=0.081).",
    "target": "Q: What is Now you see that the interaction? A: Now you see that the interaction is not significant (p=0.081)."
  },
  {
    "input": "There is no compelling evidence that different genotypes of Orobanche respond differently to the two plant extracts.",
    "target": "Q: What is There? A: There is no compelling evidence that different genotypes of Orobanche respond differently to the two plant extracts."
  },
  {
    "input": "The next step is to see if any further model simplification is possible: anova(model2,test=\"F\") AnalysisofDevianceTable Model:quasibinomial,link:logit Response:y Termsaddedsequentially(firsttolast) Df Deviance Resid.",
    "target": "Q: What is The next step? A: The next step is to see if any further model simplification is possible: anova(model2,test=\"F\") AnalysisofDevianceTable Model:quasibinomial,link:logit Response:y Termsaddedsequentially(firsttolast) Df Deviance Resid."
  },
  {
    "input": "Dev F Pr(>F) NULL 20 98.719 Orobanche 1 2.544 19 96.175 1.1954 0.2887 extract 1 56.489 18 39.686 26.5412 6.692e-05 *** PROPORTION DATA 267 There is a highly significant difference between the two plant extracts on germination rate,butitisnotobviousthatweneedtokeepOrobanchegenotypeinthemodel.Wetry removing it: model3<-update(model2,.-Orobanche) anova(model2,model3,test=\"F\") AnalysisofDevianceTable Model1:yOrobanche+extract Model2:yextract Resid.",
    "target": "Q: What is Dev F Pr(>F) NULL 20 98.719 Orobanche 1 2.544 19 96.175 1.1954 0.2887 extract 1 56.489 18 39.686 26.5412 6.692e-05 *** PROPORTION DATA 267 There? A: Dev F Pr(>F) NULL 20 98.719 Orobanche 1 2.544 19 96.175 1.1954 0.2887 extract 1 56.489 18 39.686 26.5412 6.692e-05 *** PROPORTION DATA 267 There is a highly significant difference between the two plant extracts on germination rate,butitisnotobviousthatweneedtokeepOrobanchegenotypeinthemodel.Wetry removing it: model3<-update(model2,.-Orobanche) anova(model2,model3,test=\"F\") AnalysisofDevianceTable Model1:yOrobanche+extract Model2:yextract Resid."
  },
  {
    "input": "Dev Df Deviance FPr(>F) 1 18 39.686 2 19 42.751 -1 -3.065 1.44010.2457 ThereisnojustificationforretainingOrobancheinthemodel.Sotheminimaladequate model contains just two parameters: coef(model3) (Intercept)extractcucumber -0.5121761 1.0574031 What,exactly,dothesetwonumbersmean?Rememberthatthecoefficientsarefromthe linearpredictor.Theyareonthetransformedscale,sobecauseweareusingquasi-binomial errors,theyareinlogits(ln(p/(1\u0000p)).Toturnthemintothegerminationratesforthetwo plantextractsrequiresalittlecalculation.Togofromalogitxtoaproportionp,youneedto do the following sum: 1 p 1 1 ex So our first x value is \u00000.5122 and we calculate 1/(1+1/(exp(-0.5122))) [1]0.3746779 Thissaysthatthemeangerminationrateoftheseedswiththefirstplantextractwas37%.",
    "target": "Q: What is Dev Df Deviance FPr(>F) 1 18 39.686 2 19 42.751 -1 -3.065 1.44010.2457 ThereisnojustificationforretainingOrobancheinthemodel.Sotheminimaladequate model contains just two parameters: coef(model3) (Intercept)extractcucumber -0.5121761 1.0574031 What,exactly,dothesetwonumbersmean?Rememberthatthecoefficientsarefromthe linearpredictor.Theyareonthetransformedscale,sobecauseweareusingquasi-binomial errors,theyareinlogits(ln(p/(1\u0000p)).Toturnthemintothegerminationratesforthetwo plantextractsrequiresalittlecalculation.Togofromalogitxtoaproportionp,youneedto do the following sum: 1 p 1 1 ex So our first x value? A: Dev Df Deviance FPr(>F) 1 18 39.686 2 19 42.751 -1 -3.065 1.44010.2457 ThereisnojustificationforretainingOrobancheinthemodel.Sotheminimaladequate model contains just two parameters: coef(model3) (Intercept)extractcucumber -0.5121761 1.0574031 What,exactly,dothesetwonumbersmean?Rememberthatthecoefficientsarefromthe linearpredictor.Theyareonthetransformedscale,sobecauseweareusingquasi-binomial errors,theyareinlogits(ln(p/(1\u0000p)).Toturnthemintothegerminationratesforthetwo plantextractsrequiresalittlecalculation.Togofromalogitxtoaproportionp,youneedto do the following sum: 1 p 1 1 ex So our first x value is \u00000.5122 and we calculate 1/(1+1/(exp(-0.5122))) [1]0.3746779 Thissaysthatthemeangerminationrateoftheseedswiththefirstplantextractwas37%."
  },
  {
    "input": "The correct way to average proportion data is to add up the total counts for the different levels of abstract, and only then to turn them into proportions: tapply(count,extract,sum) bean cucumber 148 276 Thismeansthat148seedsgerminatedwithbeanextractand276withcucumber.Buthow many seeds were involved in each case?",
    "target": "Q: What is The correct way to average proportion data? A: The correct way to average proportion data is to add up the total counts for the different levels of abstract, and only then to turn them into proportions: tapply(count,extract,sum) bean cucumber 148 276 Thismeansthat148seedsgerminatedwithbeanextractand276withcucumber.Buthow many seeds were involved in each case?"
  },
  {
    "input": "The countoffloweringindividualsformstheresponsevariable.ThisisanANCOVA because wehavebothcontinuous(dose)andcategorical(variety)explanatoryvariables.Weuse logistic regression because the response variable is a count (flowered) that can be expressed as a proportion (flowered/number).",
    "target": "Q: What is The countoffloweringindividualsformstheresponsevariable.ThisisanANCOVA because wehavebothcontinuous(dose)andcategorical(variety)explanatoryvariables.Weuse logistic regression because the response variable? A: The countoffloweringindividualsformstheresponsevariable.ThisisanANCOVA because wehavebothcontinuous(dose)andcategorical(variety)explanatoryvariables.Weuse logistic regression because the response variable is a count (flowered) that can be expressed as a proportion (flowered/number)."
  },
  {
    "input": "Letusdrawthefivefittedcurvesthroughthescatterplot.Thevaluesonthedoseaxisneed to go from 0 to 30: xv<-seq(0,32,0.25) length(xv) [1]129 This means we shall need to provide the predict function with 129 repeats of each factor level in turn: yv<-predict(model3,list(dose=xv,variety=rep(\"A\",129)), type=\"response\") lines(xv,yv,col=\"red\") yv<-predict(model3,list(dose=xv,variety=rep(\"B\",129)), type=\"response\") lines(xv,yv,col=\"blue\") yv<-predict(model3,list(dose=xv,variety=rep(\"C\",129)), type=\"response\") lines(xv,yv,col=\"gray\") yv<-predict(model3,list(dose=xv,variety=rep(\"D\",129)), type=\"response\") lines(xv,yv,col=\"green\") yv<-predict(model3,list(dose=xv,variety=rep(\"E\",129)), type=\"response\") lines(xv,yv,col=\"yellow\") 272 STATISTICS:ANINTRODUCTIONUSINGR Asyoucansee,themodelisareasonablefitfortwoofthevarieties(AandE,represented red circles and yellow down-triangles, respectively), not bad for one variety (C, grey diamonds)butverypoorfortwoofthem:B(bluesquares)andD(greenup-triangles).For several varieties, the model overestimates the proportion flowering at zero dose, and for variety B there seems to be some inhibition of flowering at the highest dose because the graph falls from 90% flowering at dose 16 to just 50% at dose 32 (the fitted model is assumedtobeasymptotic).VarietyDappearstobeasymptotingatlessthan80%flowering.",
    "target": "Q: What is Letusdrawthefivefittedcurvesthroughthescatterplot.Thevaluesonthedoseaxisneed to go from 0 to 30: xv<-seq(0,32,0.25) length(xv) [1]129 This means we shall need to provide the predict function with 129 repeats of each factor level in turn: yv<-predict(model3,list(dose=xv,variety=rep(\"A\",129)), type=\"response\") lines(xv,yv,col=\"red\") yv<-predict(model3,list(dose=xv,variety=rep(\"B\",129)), type=\"response\") lines(xv,yv,col=\"blue\") yv<-predict(model3,list(dose=xv,variety=rep(\"C\",129)), type=\"response\") lines(xv,yv,col=\"gray\") yv<-predict(model3,list(dose=xv,variety=rep(\"D\",129)), type=\"response\") lines(xv,yv,col=\"green\") yv<-predict(model3,list(dose=xv,variety=rep(\"E\",129)), type=\"response\") lines(xv,yv,col=\"yellow\") 272 STATISTICS:ANINTRODUCTIONUSINGR Asyoucansee,themodelisareasonablefitfortwoofthevarieties(AandE,represented red circles and yellow down-triangles, respectively), not bad for one variety (C, grey diamonds)butverypoorfortwoofthem:B(bluesquares)andD(greenup-triangles).For several varieties, the model overestimates the proportion flowering at zero dose, and for variety B there seems to be some inhibition of flowering at the highest dose because the graph falls from 90% flowering at dose 16 to just 50% at dose 32 (the fitted model? A: Letusdrawthefivefittedcurvesthroughthescatterplot.Thevaluesonthedoseaxisneed to go from 0 to 30: xv<-seq(0,32,0.25) length(xv) [1]129 This means we shall need to provide the predict function with 129 repeats of each factor level in turn: yv<-predict(model3,list(dose=xv,variety=rep(\"A\",129)), type=\"response\") lines(xv,yv,col=\"red\") yv<-predict(model3,list(dose=xv,variety=rep(\"B\",129)), type=\"response\") lines(xv,yv,col=\"blue\") yv<-predict(model3,list(dose=xv,variety=rep(\"C\",129)), type=\"response\") lines(xv,yv,col=\"gray\") yv<-predict(model3,list(dose=xv,variety=rep(\"D\",129)), type=\"response\") lines(xv,yv,col=\"green\") yv<-predict(model3,list(dose=xv,variety=rep(\"E\",129)), type=\"response\") lines(xv,yv,col=\"yellow\") 272 STATISTICS:ANINTRODUCTIONUSINGR Asyoucansee,themodelisareasonablefitfortwoofthevarieties(AandE,represented red circles and yellow down-triangles, respectively), not bad for one variety (C, grey diamonds)butverypoorfortwoofthem:B(bluesquares)andD(greenup-triangles).For several varieties, the model overestimates the proportion flowering at zero dose, and for variety B there seems to be some inhibition of flowering at the highest dose because the graph falls from 90% flowering at dose 16 to just 50% at dose 32 (the fitted model is assumedtobeasymptotic).VarietyDappearstobeasymptotingatlessthan80%flowering."
  },
  {
    "input": "15 Binary Response Variable Manystatisticalproblemsinvolvebinaryresponsevariables.Forexample,weoftenclassify thingsasdeadoralive,occupiedorempty,healthyordiseased,maleorfemale,literateor illiterate, mature or immature, solvent or insolvent, employed or unemployed, and it is interestingtounderstandthefactorsthatareassociatedwithanindividualbeinginoneclass ortheother.Inastudyofcompanyinsolvency,forinstance,thedatawouldconsistofalist of measurements made on the insolvent companies (their age, size, turnover, location, management experience, workforce training, and so on) and a similar list for the solvent companies.Thequestionthenbecomeswhich,ifany,oftheexplanatoryvariablesincrease the probability of an individual company being insolvent.",
    "target": "Q: What is 15 Binary Response Variable Manystatisticalproblemsinvolvebinaryresponsevariables.Forexample,weoftenclassify thingsasdeadoralive,occupiedorempty,healthyordiseased,maleorfemale,literateor illiterate, mature or immature, solvent or insolvent, employed or unemployed, and it? A: 15 Binary Response Variable Manystatisticalproblemsinvolvebinaryresponsevariables.Forexample,weoftenclassify thingsasdeadoralive,occupiedorempty,healthyordiseased,maleorfemale,literateor illiterate, mature or immature, solvent or insolvent, employed or unemployed, and it is interestingtounderstandthefactorsthatareassociatedwithanindividualbeinginoneclass ortheother.Inastudyofcompanyinsolvency,forinstance,thedatawouldconsistofalist of measurements made on the insolvent companies (their age, size, turnover, location, management experience, workforce training, and so on) and a similar list for the solvent companies.Thequestionthenbecomeswhich,ifany,oftheexplanatoryvariablesincrease the probability of an individual company being insolvent."
  },
  {
    "input": "ThewaythatRtreatsbinarydataistoassumethatthevaluesoftheresponsecomefroma binomialtrialwithsamplesize1.Iftheprobabilitythatanindividualisdeadisp,thenthe probabilityofobtainingy(whereyiseitherdeadoralive,0or1)isgivenbyanabbreviated form of the binomial distribution with n=1, known as the Bernoulli distribution: P ypy 1\u0000p1\u0000y The random variable y has a mean of p and a variance of p(1\u0000p), and the object is to determinehowtheexplanatoryvariablesinfluencethevalueofp.Thetricktousingbinary responsevariableseffectivelyistoknowwhenitisworthusingthem,andwhenitisbetterto lump the successes and failures togetherand analyse the total counts of dead individuals, occupied patches, insolvent firms or whatever.",
    "target": "Q: What is ThewaythatRtreatsbinarydataistoassumethatthevaluesoftheresponsecomefroma binomialtrialwithsamplesize1.Iftheprobabilitythatanindividualisdeadisp,thenthe probabilityofobtainingy(whereyiseitherdeadoralive,0or1)isgivenbyanabbreviated form of the binomial distribution with n=1, known as the Bernoulli distribution: P ypy 1\u0000p1\u0000y The random variable y has a mean of p and a variance of p(1\u0000p), and the object? A: ThewaythatRtreatsbinarydataistoassumethatthevaluesoftheresponsecomefroma binomialtrialwithsamplesize1.Iftheprobabilitythatanindividualisdeadisp,thenthe probabilityofobtainingy(whereyiseitherdeadoralive,0or1)isgivenbyanabbreviated form of the binomial distribution with n=1, known as the Bernoulli distribution: P ypy 1\u0000p1\u0000y The random variable y has a mean of p and a variance of p(1\u0000p), and the object is to determinehowtheexplanatoryvariablesinfluencethevalueofp.Thetricktousingbinary responsevariableseffectivelyistoknowwhenitisworthusingthem,andwhenitisbetterto lump the successes and failures togetherand analyse the total counts of dead individuals, occupied patches, insolvent firms or whatever."
  },
  {
    "input": "The question you need to ask yourself is whetherornotyouhaveuniquevaluesofoneormoreexplanatoryvariablesforeachand everyindividualcase.Iftheanswerisyes,thenanalysiswithabinaryresponsevariableis likelytobefruitful.Iftheanswerisno,thenthereisnothingtobegained,andyoushould reduceyourdatabyaggregatingthecountstotheresolutionatwhicheachcountdoeshavea Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.",
    "target": "Q: What is The question you need to ask yourself? A: The question you need to ask yourself is whetherornotyouhaveuniquevaluesofoneormoreexplanatoryvariablesforeachand everyindividualcase.Iftheanswerisyes,thenanalysiswithabinaryresponsevariableis likelytobefruitful.Iftheanswerisno,thenthereisnothingtobegained,andyoushould reduceyourdatabyaggregatingthecountstotheresolutionatwhicheachcountdoeshavea Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley."
  },
  {
    "input": "In this case there is nothing to be gained fromanalysisusingabinaryresponsevariablebecausenoneoftheindividualsinthestudy haveuniquevaluesofanyoftheexplanatoryvariables.Itmightbeworthwhileifyouhad eachindividualsbodyweight,forexample;thenyoucouldaskwhether,whenyoucontrol forsexandregion,heavypeoplearemorelikelytobeunemployedthanlightpeople.Inthe absence of unique values for any explanatory variables, there are two useful options: (cid:129) analysethedataasacontingencytableusingPoissonerrors,withthecountofthetotal number of individuals in each of the eight contingencies (222) as the response variable (see Chapter 13) in a dataframe with just eight rows (cid:129) decide which of your explanatory variables is the key (perhaps you are interested in genderdifferences),thenexpressthedataasproportions(thenumberofmalesandthe numberoffemales)andrecodethebinaryresponseasacountofatwo-levelfactorthe analysisisnowofproportiondata(e.g.theproportionofallindividualsthatarefemale) using binomial errors (see Chapter 14) If you do have unique measurements of one or more explanatory variables for each individual,thesearelikelytobecontinuousvariablessuchasbodyweight,income,medical history, distance to the nuclear reprocessing plant, geographic isolation, and so on.",
    "target": "Q: What is In this case there? A: In this case there is nothing to be gained fromanalysisusingabinaryresponsevariablebecausenoneoftheindividualsinthestudy haveuniquevaluesofanyoftheexplanatoryvariables.Itmightbeworthwhileifyouhad eachindividualsbodyweight,forexample;thenyoucouldaskwhether,whenyoucontrol forsexandregion,heavypeoplearemorelikelytobeunemployedthanlightpeople.Inthe absence of unique values for any explanatory variables, there are two useful options: (cid:129) analysethedataasacontingencytableusingPoissonerrors,withthecountofthetotal number of individuals in each of the eight contingencies (222) as the response variable (see Chapter 13) in a dataframe with just eight rows (cid:129) decide which of your explanatory variables is the key (perhaps you are interested in genderdifferences),thenexpressthedataasproportions(thenumberofmalesandthe numberoffemales)andrecodethebinaryresponseasacountofatwo-levelfactorthe analysisisnowofproportiondata(e.g.theproportionofallindividualsthatarefemale) using binomial errors (see Chapter 14) If you do have unique measurements of one or more explanatory variables for each individual,thesearelikelytobecontinuousvariablessuchasbodyweight,income,medical history, distance to the nuclear reprocessing plant, geographic isolation, and so on."
  },
  {
    "input": "Inordertocarryoutmodellingonabinaryresponsevariablewetakethefollowingsteps: (cid:129) createasinglevectorcontaining0sand1s(oroneoftwofactorlevels)astheresponse variable (cid:129) use glm with family=binomial (cid:129) you can change the link function from the default logit to complementary log-log (cid:129) fit the model in the usual way (cid:129) testsignificancebydeletionoftermsfromthemaximalmodel,andcomparethechange in deviance with chi-squared (cid:129) note that there is no such thing as overdispersion with a binary response variable, and hence no need to change to using quasi-binomial when the residual deviance is large (cid:129) plot(model)israrelyinformativewithbinaryresponsevariables,somodelchecking is more than usually challenging Thechoiceoflinkfunctionisgenerallymadebytryingbothlinksandselectingthelink thatgivesthelowestdeviance.Thelogitlinkthatweusedearlierissymmetricinpandq,but the complementary log-log link is asymmetric.",
    "target": "Q: What is Inordertocarryoutmodellingonabinaryresponsevariablewetakethefollowingsteps: (cid:129) createasinglevectorcontaining0sand1s(oroneoftwofactorlevels)astheresponse variable (cid:129) use glm with family=binomial (cid:129) you can change the link function from the default logit to complementary log-log (cid:129) fit the model in the usual way (cid:129) testsignificancebydeletionoftermsfromthemaximalmodel,andcomparethechange in deviance with chi-squared (cid:129) note that there? A: Inordertocarryoutmodellingonabinaryresponsevariablewetakethefollowingsteps: (cid:129) createasinglevectorcontaining0sand1s(oroneoftwofactorlevels)astheresponse variable (cid:129) use glm with family=binomial (cid:129) you can change the link function from the default logit to complementary log-log (cid:129) fit the model in the usual way (cid:129) testsignificancebydeletionoftermsfromthemaximalmodel,andcomparethechange in deviance with chi-squared (cid:129) note that there is no such thing as overdispersion with a binary response variable, and hence no need to change to using quasi-binomial when the residual deviance is large (cid:129) plot(model)israrelyinformativewithbinaryresponsevariables,somodelchecking is more than usually challenging Thechoiceoflinkfunctionisgenerallymadebytryingbothlinksandselectingthelink thatgivesthelowestdeviance.Thelogitlinkthatweusedearlierissymmetricinpandq,but the complementary log-log link is asymmetric."
  },
  {
    "input": "BINARY RESPONSEVARIABLE 275 IncidenceFunctions In this example, the response variable is called incidence; a value of 1 means that an islandwasoccupiedbyaparticularspeciesofbird,and0meansthatthebirddidnotbreed there.",
    "target": "Q: What is BINARY RESPONSEVARIABLE 275 IncidenceFunctions In this example, the response variable? A: BINARY RESPONSEVARIABLE 275 IncidenceFunctions In this example, the response variable is called incidence; a value of 1 means that an islandwasoccupiedbyaparticularspeciesofbird,and0meansthatthebirddidnotbreed there."
  },
  {
    "input": "island<-read.csv(\"c:\\\\temp\\\\isolation.csv\") attach(island) names(island) [1]\"incidence\"\"area\" \"isolation\" There are two continuous explanatory variables, so the appropriate analysis is multiple regression.Theresponseisbinary,soweshalldologisticregressionwithbinomialerrors.",
    "target": "Q: What is island<-read.csv(\"c:\\\\temp\\\\isolation.csv\") attach(island) names(island) [1]\"incidence\"\"area\" \"isolation\" There are two continuous explanatory variables, so the appropriate analysis? A: island<-read.csv(\"c:\\\\temp\\\\isolation.csv\") attach(island) names(island) [1]\"incidence\"\"area\" \"isolation\" There are two continuous explanatory variables, so the appropriate analysis is multiple regression.Theresponseisbinary,soweshalldologisticregressionwithbinomialerrors."
  },
  {
    "input": "Df Resid.Dev Df Deviance Pr(>Chi) 1 46 28.252 2 47 28.402 -1 -0.15043 0.6981 The simpler model is notsignificantly worse, so we accept this for thetime being, and inspect the parameter estimates and standard errors: summary(model2) Coefficients: Estimate Std.Error zvalue Pr(>|z|) (Intercept) 6.6417 2.9218 2.273 0.02302* area 0.5807 0.2478 2.344 0.01909* isolation -1.3719 0.4769 -2.877 0.00401** (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:68.029 on49 degreesoffreedom Residualdeviance:28.402 on47 degreesoffreedom AIC:34.402 NumberofFisherScoringiterations:6 276 STATISTICS:ANINTRODUCTIONUSINGR Theestimatesandtheirstandarderrorsareinlogits.Areahasasignificantpositiveeffect (largerislandsaremorelikelytobeoccupied),butisolationhasaverystrongnegativeeffect (isolatedislandsaremuchlesslikelytobeoccupied).Thisistheminimaladequatemodel.",
    "target": "Q: What is Df Resid.Dev Df Deviance Pr(>Chi) 1 46 28.252 2 47 28.402 -1 -0.15043 0.6981 The simpler model? A: Df Resid.Dev Df Deviance Pr(>Chi) 1 46 28.252 2 47 28.402 -1 -0.15043 0.6981 The simpler model is notsignificantly worse, so we accept this for thetime being, and inspect the parameter estimates and standard errors: summary(model2) Coefficients: Estimate Std.Error zvalue Pr(>|z|) (Intercept) 6.6417 2.9218 2.273 0.02302* area 0.5807 0.2478 2.344 0.01909* isolation -1.3719 0.4769 -2.877 0.00401** (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:68.029 on49 degreesoffreedom Residualdeviance:28.402 on47 degreesoffreedom AIC:34.402 NumberofFisherScoringiterations:6 276 STATISTICS:ANINTRODUCTIONUSINGR Theestimatesandtheirstandarderrorsareinlogits.Areahasasignificantpositiveeffect (largerislandsaremorelikelytobeoccupied),butisolationhasaverystrongnegativeeffect (isolatedislandsaremuchlesslikelytobeoccupied).Thisistheminimaladequatemodel."
  },
  {
    "input": "Weshouldplotthefittedmodelthroughthescatterplotofthedata.Itismucheasiertodothis for each variable separately, like this: windows(7,4) par(mfrow=c(1,2)) xv<-seq(0,9,0.01) modela<-glm(incidencearea,binomial) modeli<-glm(incidenceisolation,binomial) yv<-predict(modela,list(area=xv),type=\"response\") plot(area,incidence,pch=21,bg=\"yellow\") lines(xv,yv,col=\"blue\") xv2<-seq(0,10,0.1) yv2<-predict(modeli,list(isolation=xv2),type=\"response\") plot(isolation,incidence,pch=21,bg=\"yellow\") lines(xv2,yv2,col=\"red\") This is well and good, but it is very difficult to know how good the fit of the model is whenthedataareshownonlyas0sor1s.Itissensibletocomputeoneormoreintermediate probabilities from the data, and to show these empirical estimates (ideally with their standard errors) on the plot in order to judge whether the fitted line is a reasonable description of the data.",
    "target": "Q: What is Weshouldplotthefittedmodelthroughthescatterplotofthedata.Itismucheasiertodothis for each variable separately, like this: windows(7,4) par(mfrow=c(1,2)) xv<-seq(0,9,0.01) modela<-glm(incidencearea,binomial) modeli<-glm(incidenceisolation,binomial) yv<-predict(modela,list(area=xv),type=\"response\") plot(area,incidence,pch=21,bg=\"yellow\") lines(xv,yv,col=\"blue\") xv2<-seq(0,10,0.1) yv2<-predict(modeli,list(isolation=xv2),type=\"response\") plot(isolation,incidence,pch=21,bg=\"yellow\") lines(xv2,yv2,col=\"red\") This? A: Weshouldplotthefittedmodelthroughthescatterplotofthedata.Itismucheasiertodothis for each variable separately, like this: windows(7,4) par(mfrow=c(1,2)) xv<-seq(0,9,0.01) modela<-glm(incidencearea,binomial) modeli<-glm(incidenceisolation,binomial) yv<-predict(modela,list(area=xv),type=\"response\") plot(area,incidence,pch=21,bg=\"yellow\") lines(xv,yv,col=\"blue\") xv2<-seq(0,10,0.1) yv2<-predict(modeli,list(isolation=xv2),type=\"response\") plot(isolation,incidence,pch=21,bg=\"yellow\") lines(xv2,yv2,col=\"red\") This is well and good, but it is very difficult to know how good the fit of the model is whenthedataareshownonlyas0sor1s.Itissensibletocomputeoneormoreintermediate probabilities from the data, and to show these empirical estimates (ideally with their standard errors) on the plot in order to judge whether the fitted line is a reasonable description of the data."
  },
  {
    "input": "Now count the total number of islands in each interval using table: table(ac) ac (0.144,3.19] (3.19,6.23] (6.23,9.28] 21 15 14 table(ic) ic (2.02,4.54](4.54,7.06](7.06,9.58] 12 25 13 Theprobabilityofoccupationisgivenbydividingthenumberofsuccessesbythenumber of cases: tapply(incidence,ac,sum)/table(ac) (0.144,3.19] (3.19,6.23] (6.23,9.28] 0.3333333 0.5333333 1.0000000 tapply(incidence,ic,sum)/table(ic) (2.02,4.54](4.54,7.06](7.06,9.58] 1.00 0.68 0.00 pffiffiffiffiffiffiffiffiffiffi Theideaistoplotthesemeanproportionsandtheirstandarderrors( pq=n)alongwith theregressionlinefromthemodeltoseehowclosethemodelgetstothethreecalculated proportions: xv<-seq(0,9,0.01) yv<-predict(modela,list(area=xv),type=\"response\") plot(area,incidence,pch=21,bg=\"yellow\") lines(xv,yv,col=\"blue\") 278 STATISTICS:ANINTRODUCTIONUSINGR d<-(max(area)-min(area))/3 left<-min(area)+d/2 mid<-left+d right<-mid+d xva<-c(left,mid,right) pa<-as.vector(tapply(incidence,ac,sum)/table(ac)) se<-sqrt(pa*(1-pa)/table(ac)) xv<-seq(0,9,0.01) yv<-predict(modela,list(area=xv),type=\"response\") lines(xv,yv,col=\"blue\") points(xva,pa,pch=16,col=\"red\") for(iin1:3)lines(c(xva[i],xva[i]), c(pa[i]+se[i],pa[i]-se[i]),col=\"red\") xv2<-seq(0,10,0.1) yv2<-predict(modeli,list(isolation=xv2),type=\"response\") plot(isolation,incidence,pch=21,bg=\"yellow\") lines(xv2,yv2,col=\"red\") d<-(max(isolation)-min(isolation))/3 left<-min(isolation)+d/2 mid<-left+d right<-mid+d xvi<-c(left,mid,right) pi<-as.vector(tapply(incidence,ic,sum)/table(ic)) se<-sqrt(pi*(1-pi)/table(ic)) points(xvi,pi,pch=16,col=\"blue\") for(iin1:3)lines(c(xvi[i],xvi[i]), c(pi[i]+se[i],pi[i]-se[i]),col=\"blue\") Youcanseeatoncethatthefitfortheright-handgraphofincidenceagainstisolationis excellent;thelogisticisaverygoodmodelforthesedata.Incontrast,thefitfortheleft-hand graph of incidence against area is poor; at low values of area the model (blue line) underestimatestheobserveddata(redpointwitherrorbars)whileforintermediatevaluesof area the model (blue line) overestimates the observed data (red point with error bars).",
    "target": "Q: What is Now count the total number of islands in each interval using table: table(ac) ac (0.144,3.19] (3.19,6.23] (6.23,9.28] 21 15 14 table(ic) ic (2.02,4.54](4.54,7.06](7.06,9.58] 12 25 13 Theprobabilityofoccupationisgivenbydividingthenumberofsuccessesbythenumber of cases: tapply(incidence,ac,sum)/table(ac) (0.144,3.19] (3.19,6.23] (6.23,9.28] 0.3333333 0.5333333 1.0000000 tapply(incidence,ic,sum)/table(ic) (2.02,4.54](4.54,7.06](7.06,9.58] 1.00 0.68 0.00 pffiffiffiffiffiffiffiffiffiffi Theideaistoplotthesemeanproportionsandtheirstandarderrors( pq=n)alongwith theregressionlinefromthemodeltoseehowclosethemodelgetstothethreecalculated proportions: xv<-seq(0,9,0.01) yv<-predict(modela,list(area=xv),type=\"response\") plot(area,incidence,pch=21,bg=\"yellow\") lines(xv,yv,col=\"blue\") 278 STATISTICS:ANINTRODUCTIONUSINGR d<-(max(area)-min(area))/3 left<-min(area)+d/2 mid<-left+d right<-mid+d xva<-c(left,mid,right) pa<-as.vector(tapply(incidence,ac,sum)/table(ac)) se<-sqrt(pa*(1-pa)/table(ac)) xv<-seq(0,9,0.01) yv<-predict(modela,list(area=xv),type=\"response\") lines(xv,yv,col=\"blue\") points(xva,pa,pch=16,col=\"red\") for(iin1:3)lines(c(xva[i],xva[i]), c(pa[i]+se[i],pa[i]-se[i]),col=\"red\") xv2<-seq(0,10,0.1) yv2<-predict(modeli,list(isolation=xv2),type=\"response\") plot(isolation,incidence,pch=21,bg=\"yellow\") lines(xv2,yv2,col=\"red\") d<-(max(isolation)-min(isolation))/3 left<-min(isolation)+d/2 mid<-left+d right<-mid+d xvi<-c(left,mid,right) pi<-as.vector(tapply(incidence,ic,sum)/table(ic)) se<-sqrt(pi*(1-pi)/table(ic)) points(xvi,pi,pch=16,col=\"blue\") for(iin1:3)lines(c(xvi[i],xvi[i]), c(pi[i]+se[i],pi[i]-se[i]),col=\"blue\") Youcanseeatoncethatthefitfortheright-handgraphofincidenceagainstisolationis excellent;thelogisticisaverygoodmodelforthesedata.Incontrast,thefitfortheleft-hand graph of incidence against area? A: Now count the total number of islands in each interval using table: table(ac) ac (0.144,3.19] (3.19,6.23] (6.23,9.28] 21 15 14 table(ic) ic (2.02,4.54](4.54,7.06](7.06,9.58] 12 25 13 Theprobabilityofoccupationisgivenbydividingthenumberofsuccessesbythenumber of cases: tapply(incidence,ac,sum)/table(ac) (0.144,3.19] (3.19,6.23] (6.23,9.28] 0.3333333 0.5333333 1.0000000 tapply(incidence,ic,sum)/table(ic) (2.02,4.54](4.54,7.06](7.06,9.58] 1.00 0.68 0.00 pffiffiffiffiffiffiffiffiffiffi Theideaistoplotthesemeanproportionsandtheirstandarderrors( pq=n)alongwith theregressionlinefromthemodeltoseehowclosethemodelgetstothethreecalculated proportions: xv<-seq(0,9,0.01) yv<-predict(modela,list(area=xv),type=\"response\") plot(area,incidence,pch=21,bg=\"yellow\") lines(xv,yv,col=\"blue\") 278 STATISTICS:ANINTRODUCTIONUSINGR d<-(max(area)-min(area))/3 left<-min(area)+d/2 mid<-left+d right<-mid+d xva<-c(left,mid,right) pa<-as.vector(tapply(incidence,ac,sum)/table(ac)) se<-sqrt(pa*(1-pa)/table(ac)) xv<-seq(0,9,0.01) yv<-predict(modela,list(area=xv),type=\"response\") lines(xv,yv,col=\"blue\") points(xva,pa,pch=16,col=\"red\") for(iin1:3)lines(c(xva[i],xva[i]), c(pa[i]+se[i],pa[i]-se[i]),col=\"red\") xv2<-seq(0,10,0.1) yv2<-predict(modeli,list(isolation=xv2),type=\"response\") plot(isolation,incidence,pch=21,bg=\"yellow\") lines(xv2,yv2,col=\"red\") d<-(max(isolation)-min(isolation))/3 left<-min(isolation)+d/2 mid<-left+d right<-mid+d xvi<-c(left,mid,right) pi<-as.vector(tapply(incidence,ic,sum)/table(ic)) se<-sqrt(pi*(1-pi)/table(ic)) points(xvi,pi,pch=16,col=\"blue\") for(iin1:3)lines(c(xvi[i],xvi[i]), c(pi[i]+se[i],pi[i]-se[i]),col=\"blue\") Youcanseeatoncethatthefitfortheright-handgraphofincidenceagainstisolationis excellent;thelogisticisaverygoodmodelforthesedata.Incontrast,thefitfortheleft-hand graph of incidence against area is poor; at low values of area the model (blue line) underestimatestheobserveddata(redpointwitherrorbars)whileforintermediatevaluesof area the model (blue line) overestimates the observed data (red point with error bars)."
  },
  {
    "input": "280 STATISTICS:ANINTRODUCTIONUSINGR Wegetdowntobusiness,asusual,byfittingamaximalmodelwithdifferentslopesfor each level of the categorical variable: model<-glm(infectedage*weight*sex,family=binomial) summary(model) Coefficients: Estimate Std.Error zvalue Pr(>|z|) (Intercept) -0.109124 1.375388 -0.079 0.937 age 0.024128 0.020874 1.156 0.248 weight -0.074156 0.147678 -0.502 0.616 sexmale -5.969109 4.278066 -1.395 0.163 age:weight -0.001977 0.002006 -0.985 0.325 age:sexmale 0.038086 0.041325 0.922 0.357 weight:sexmale 0.213830 0.343265 0.623 0.533 age:weight:sexmale -0.001651 0.003419 -0.483 0.629 (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:83.234 on80 degreesoffreedom Residualdeviance:55.706 on73 degreesoffreedom AIC:71.706 NumberofFisherScoringiterations:6 Itcertainlydoesnotlookasifanyofthehigh-orderinteractionsaresignificant.Insteadof usingupdateandanovaformodelsimplification,wecanusesteptocomputetheAICfor each term in turn: model2<-step(model) Start: AIC=71.71 First, it tests whether the three-way interaction is required: Df Deviance AIC -age:weight:sex 1 55.943 69.943 <none> 55.706 71.706 Step: AIC=69.94 ThiscausesareductioninAICofjust71.769.9=1.8andhenceisnotsignificant,sothe three-way interaction is eliminated.",
    "target": "Q: What is 280 STATISTICS:ANINTRODUCTIONUSINGR Wegetdowntobusiness,asusual,byfittingamaximalmodelwithdifferentslopesfor each level of the categorical variable: model<-glm(infectedage*weight*sex,family=binomial) summary(model) Coefficients: Estimate Std.Error zvalue Pr(>|z|) (Intercept) -0.109124 1.375388 -0.079 0.937 age 0.024128 0.020874 1.156 0.248 weight -0.074156 0.147678 -0.502 0.616 sexmale -5.969109 4.278066 -1.395 0.163 age:weight -0.001977 0.002006 -0.985 0.325 age:sexmale 0.038086 0.041325 0.922 0.357 weight:sexmale 0.213830 0.343265 0.623 0.533 age:weight:sexmale -0.001651 0.003419 -0.483 0.629 (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:83.234 on80 degreesoffreedom Residualdeviance:55.706 on73 degreesoffreedom AIC:71.706 NumberofFisherScoringiterations:6 Itcertainlydoesnotlookasifanyofthehigh-orderinteractionsaresignificant.Insteadof usingupdateandanovaformodelsimplification,wecanusesteptocomputetheAICfor each term in turn: model2<-step(model) Start: AIC=71.71 First, it tests whether the three-way interaction? A: 280 STATISTICS:ANINTRODUCTIONUSINGR Wegetdowntobusiness,asusual,byfittingamaximalmodelwithdifferentslopesfor each level of the categorical variable: model<-glm(infectedage*weight*sex,family=binomial) summary(model) Coefficients: Estimate Std.Error zvalue Pr(>|z|) (Intercept) -0.109124 1.375388 -0.079 0.937 age 0.024128 0.020874 1.156 0.248 weight -0.074156 0.147678 -0.502 0.616 sexmale -5.969109 4.278066 -1.395 0.163 age:weight -0.001977 0.002006 -0.985 0.325 age:sexmale 0.038086 0.041325 0.922 0.357 weight:sexmale 0.213830 0.343265 0.623 0.533 age:weight:sexmale -0.001651 0.003419 -0.483 0.629 (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:83.234 on80 degreesoffreedom Residualdeviance:55.706 on73 degreesoffreedom AIC:71.706 NumberofFisherScoringiterations:6 Itcertainlydoesnotlookasifanyofthehigh-orderinteractionsaresignificant.Insteadof usingupdateandanovaformodelsimplification,wecanusesteptocomputetheAICfor each term in turn: model2<-step(model) Start: AIC=71.71 First, it tests whether the three-way interaction is required: Df Deviance AIC -age:weight:sex 1 55.943 69.943 <none> 55.706 71.706 Step: AIC=69.94 ThiscausesareductioninAICofjust71.769.9=1.8andhenceisnotsignificant,sothe three-way interaction is eliminated."
  },
  {
    "input": "So there is no really persuasive evidence of an ageweight term (p=0.096).",
    "target": "Q: What is So there? A: So there is no really persuasive evidence of an ageweight term (p=0.096)."
  },
  {
    "input": "weight -0.227912 0.068599 -3.322 0.000893 *** sexmale -1.543444 0.685681 -2.251 0.024388 * (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:83.234 on80 degreesoffreedom Residualdeviance:59.859 on77 degreesoffreedom AIC:67.859 NumberofFisherScoringiterations:5 Weight is highly significant, as we expected from the initial boxplot, sex is quite significant,andageismarginally significant.Itisworth establishingwhether thereisany evidenceofnon-linearityintheresponseofinfectiontoweightorage.Wemightbeginby fitting quadratic terms for the two continuous explanatory variables: model6<-glm(infectedage+weight+sex+I(weight^2)+I(age^2), family=binomial) summary(model6) Coefficients: Estimate Std.Error zvalue Pr(>|z|) (Intercept) -3.4475839 1.7978359 -1.918 0.0552 .",
    "target": "Q: What is weight -0.227912 0.068599 -3.322 0.000893 *** sexmale -1.543444 0.685681 -2.251 0.024388 * (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:83.234 on80 degreesoffreedom Residualdeviance:59.859 on77 degreesoffreedom AIC:67.859 NumberofFisherScoringiterations:5 Weight? A: weight -0.227912 0.068599 -3.322 0.000893 *** sexmale -1.543444 0.685681 -2.251 0.024388 * (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:83.234 on80 degreesoffreedom Residualdeviance:59.859 on77 degreesoffreedom AIC:67.859 NumberofFisherScoringiterations:5 Weight is highly significant, as we expected from the initial boxplot, sex is quite significant,andageismarginally significant.Itisworth establishingwhether thereisany evidenceofnon-linearityintheresponseofinfectiontoweightorage.Wemightbeginby fitting quadratic terms for the two continuous explanatory variables: model6<-glm(infectedage+weight+sex+I(weight^2)+I(age^2), family=binomial) summary(model6) Coefficients: Estimate Std.Error zvalue Pr(>|z|) (Intercept) -3.4475839 1.7978359 -1.918 0.0552 ."
  },
  {
    "input": "It is worth looking at these non-linearitiesinmoredetail,toseeifwecandobetterwithotherkindsofmodels(e.g.non- parametric smoothers, piecewise linear models or step functions).",
    "target": "Q: What is It? A: It is worth looking at these non-linearitiesinmoredetail,toseeifwecandobetterwithotherkindsofmodels(e.g.non- parametric smoothers, piecewise linear models or step functions)."
  },
  {
    "input": "A good start is often a generalized additive model when we have continuous covariates: BINARY RESPONSEVARIABLE 283 library(mgcv) model7<-gam(infectedsex+s(age)+s(weight),family=binomial) plot.gam(model7) Thesenon-parametricsmoothersareexcellentatshowingthehumpedrelationshipbetween infection and age, and at highlighting the possibility of a threshold at weight \u00198 in the relationshipbetweenweightandinfection.",
    "target": "Q: What is A good start? A: A good start is often a generalized additive model when we have continuous covariates: BINARY RESPONSEVARIABLE 283 library(mgcv) model7<-gam(infectedsex+s(age)+s(weight),family=binomial) plot.gam(model7) Thesenon-parametricsmoothersareexcellentatshowingthehumpedrelationshipbetween infection and age, and at highlighting the possibility of a threshold at weight \u00198 in the relationshipbetweenweightandinfection."
  },
  {
    "input": "WecannowreturntoaGLMtoincorporatetheseideas.Weshallfitageandage^2as before,buttryapiecewiselinearfitforweight,estimatingthethresholdweightatarange ofvalues(say,814)andselectingthethresholdthatgivesthelowestresidualdeviance;this turnsouttobeathresholdof12(abithigherthanitappearsfromthegamplot,above).The piecewise regression is specified by the term: I((weight12)*(weight>12)) The I (as is) is necessary to stop the * being evaluated as an interaction term in the modelformula.Whatthisexpressionsaysis:regressinfectiononthevalueofweight12, butonlydothiswhenweight>12istrue;otherwise,assumethatinfectionisindependent of weight.",
    "target": "Q: What is WecannowreturntoaGLMtoincorporatetheseideas.Weshallfitageandage^2as before,buttryapiecewiselinearfitforweight,estimatingthethresholdweightatarange ofvalues(say,814)andselectingthethresholdthatgivesthelowestresidualdeviance;this turnsouttobeathresholdof12(abithigherthanitappearsfromthegamplot,above).The piecewise regression? A: WecannowreturntoaGLMtoincorporatetheseideas.Weshallfitageandage^2as before,buttryapiecewiselinearfitforweight,estimatingthethresholdweightatarange ofvalues(say,814)andselectingthethresholdthatgivesthelowestresidualdeviance;this turnsouttobeathresholdof12(abithigherthanitappearsfromthegamplot,above).The piecewise regression is specified by the term: I((weight12)*(weight>12)) The I (as is) is necessary to stop the * being evaluated as an interaction term in the modelformula.Whatthisexpressionsaysis:regressinfectiononthevalueofweight12, butonlydothiswhenweight>12istrue;otherwise,assumethatinfectionisindependent of weight."
  },
  {
    "input": "The minimal adequate model is therefore model9: summary(model9) Coefficients: Estimate Std.Error zvalue Pr(>|z|) (Intercept) -3.1207552 1.2665593 -2.464 0.0137* age 0.0765784 0.0323376 2.368 0.0179* I(age^2) -0.0003843 0.0001846 -2.081 0.0374* I((weight-12)*(weight>12)) -1.3511706 0.5134681 -2.631 0.0085** (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:83.234 on80 degreesoffreedom Residualdeviance:51.953 on77 degreesoffreedom AIC:59.953 NumberofFisherScoringiterations:7 Weconcludethereisahumpedrelationshipbetweeninfectionandage,andathreshold effect of weight on infection.",
    "target": "Q: What is The minimal adequate model? A: The minimal adequate model is therefore model9: summary(model9) Coefficients: Estimate Std.Error zvalue Pr(>|z|) (Intercept) -3.1207552 1.2665593 -2.464 0.0137* age 0.0765784 0.0323376 2.368 0.0179* I(age^2) -0.0003843 0.0001846 -2.081 0.0374* I((weight-12)*(weight>12)) -1.3511706 0.5134681 -2.631 0.0085** (Dispersionparameterforbinomialfamilytakentobe1) Nulldeviance:83.234 on80 degreesoffreedom Residualdeviance:51.953 on77 degreesoffreedom AIC:59.953 NumberofFisherScoringiterations:7 Weconcludethereisahumpedrelationshipbetweeninfectionandage,andathreshold effect of weight on infection."
  },
  {
    "input": "The effect of sex is marginal, but might repay further investigation (p=0.071).",
    "target": "Q: What is The effect of sex? A: The effect of sex is marginal, but might repay further investigation (p=0.071)."
  },
  {
    "input": "The variance in age at death, however, is not constant: tapply(death,treatment,var) control high low 0.4167347 2.4751020 0.8265306 The variance is much greater for the longer-lived individuals, so we should not use standardstatisticalmodelswhichassumeconstantvarianceandnormalerrors.Butwecan use a GLM with gamma errors: model<-glm(deathtreatment,Gamma) summary(model) Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.",
    "target": "Q: What is The variance in age at death, however,? A: The variance in age at death, however, is not constant: tapply(death,treatment,var) control high low 0.4167347 2.4751020 0.8265306 The variance is much greater for the longer-lived individuals, so we should not use standardstatisticalmodelswhichassumeconstantvarianceandnormalerrors.Butwecan use a GLM with gamma errors: model<-glm(deathtreatment,Gamma) summary(model) Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley."
  },
  {
    "input": "Checking the model using plot(model) shows that it is reasonably well behaved (you might like to compare the behaviour of lm(deathtreatment)).",
    "target": "Q: What is Checking the model using plot(model) shows that it? A: Checking the model using plot(model) shows that it is reasonably well behaved (you might like to compare the behaviour of lm(deathtreatment))."
  },
  {
    "input": "A common difficulty with data on time at death is that some (or even many) of the individualsdonotdieduringthetrial,sotheirageatdeathremainsunknown(theymight DEATH ANDFAILURE DATA 287 recover, they might leave the trial, or the experiment might end before they die).",
    "target": "Q: What is A common difficulty with data on time at death? A: A common difficulty with data on time at death is that some (or even many) of the individualsdonotdieduringthetrial,sotheirageatdeathremainsunknown(theymight DEATH ANDFAILURE DATA 287 recover, they might leave the trial, or the experiment might end before they die)."
  },
  {
    "input": "These individualsaresaidtobecensored.Censoringmakestheanalysismuchmorecomplicated, becausethecensoredindividualsprovidesomeinformation(weknowtheageatwhichthey werelastseenalive)butthedataareofadifferenttypefromtheinformationonageatdeath which is the response variable in the main analysis.",
    "target": "Q: What is These individualsaresaidtobecensored.Censoringmakestheanalysismuchmorecomplicated, becausethecensoredindividualsprovidesomeinformation(weknowtheageatwhichthey werelastseenalive)butthedataareofadifferenttypefromtheinformationonageatdeath which? A: These individualsaresaidtobecensored.Censoringmakestheanalysismuchmorecomplicated, becausethecensoredindividualsprovidesomeinformation(weknowtheageatwhichthey werelastseenalive)butthedataareofadifferenttypefromtheinformationonageatdeath which is the response variable in the main analysis."
  },
  {
    "input": "There is a whole field of statistical modelling for such data: it is called survival analysis.",
    "target": "Q: What is There? A: There is a whole field of statistical modelling for such data: it is called survival analysis."
  },
  {
    "input": "We fit the full analysis of covariance model to begin with: model<-survreg(Surv(death,status)weight*group,dist=\"exponential\") summary(model) Call: survreg(formula=Surv(death,status)weight*group,dist= \"exponential\") Value Std.Error z p (Intercept) 3.8702 0.3854 10.041 1.00e-23 weight -0.0803 0.0659 -1.219 2.23e-01 groupB -0.8853 0.4508 -1.964 4.95e-02 groupC -1.7804 0.4386 -4.059 4.92e-05 weight:groupB 0.0643 0.0674 0.954 3.40e-01 weight:groupC 0.0796 0.0674 1.180 2.38e-01 Scalefixedat1 Exponentialdistribution Loglik(model)=-480.6 Loglik(interceptonly)=-502.1 Chisq=43.11on5degreesoffreedom,p=3.5e-08 NumberofNewton-RaphsonIterations:5 n=150 Modelsimplificationproceedsinthenormalway.Youcoulduseupdate,buthere(for varietyonly)werefitprogressivelysimplermodelsandtestthemusinganova.Firstwetake out the different slopes for each group: model2<-survreg(Surv(death,status)weight+group,dist=\"exponential\") anova(model,model2,test=\"Chi\") TermsResid.Df -2*LL TestDf Deviance P(>|Chi|) 1 weight*group 144961.1800 NA NA NA 2 weight+group 146962.9411 -weight:group-2 -1.761142 0.4145462 The interaction is not significant so we leave it out and try deleting weight: model3<-survreg(Surv(death,status)group,dist=\"exponential\") anova(model2,model3,test=\"Chi\") TermsResid.Df -2*LL TestDf Deviance P(>|Chi|) 1 weight+group 146962.9411 NA NA NA 2 group 147963.9393 -weight-1 -0.9981333 0.3177626 DEATH ANDFAILURE DATA 289 This is not significant, so we leave it out and try deleting group: model4<-survreg(Surv(death,status)1,dist=\"exponential\") anova(model3,model4,test=\"Chi\") TermsResid.Df -2*LL TestDf Deviance P(>|Chi|) 1 group 147 963.9393 NA NA NA 2 1 149 1004.2865 -2 -40.34721 1.732661e-09 Thisishighlysignificant,soweadditback.Theminimaladequatemodelismodel3with the three-level factor group, but there is no evidence that initial body weight had any influence on survival.",
    "target": "Q: What is We fit the full analysis of covariance model to begin with: model<-survreg(Surv(death,status)weight*group,dist=\"exponential\") summary(model) Call: survreg(formula=Surv(death,status)weight*group,dist= \"exponential\") Value Std.Error z p (Intercept) 3.8702 0.3854 10.041 1.00e-23 weight -0.0803 0.0659 -1.219 2.23e-01 groupB -0.8853 0.4508 -1.964 4.95e-02 groupC -1.7804 0.4386 -4.059 4.92e-05 weight:groupB 0.0643 0.0674 0.954 3.40e-01 weight:groupC 0.0796 0.0674 1.180 2.38e-01 Scalefixedat1 Exponentialdistribution Loglik(model)=-480.6 Loglik(interceptonly)=-502.1 Chisq=43.11on5degreesoffreedom,p=3.5e-08 NumberofNewton-RaphsonIterations:5 n=150 Modelsimplificationproceedsinthenormalway.Youcoulduseupdate,buthere(for varietyonly)werefitprogressivelysimplermodelsandtestthemusinganova.Firstwetake out the different slopes for each group: model2<-survreg(Surv(death,status)weight+group,dist=\"exponential\") anova(model,model2,test=\"Chi\") TermsResid.Df -2*LL TestDf Deviance P(>|Chi|) 1 weight*group 144961.1800 NA NA NA 2 weight+group 146962.9411 -weight:group-2 -1.761142 0.4145462 The interaction? A: We fit the full analysis of covariance model to begin with: model<-survreg(Surv(death,status)weight*group,dist=\"exponential\") summary(model) Call: survreg(formula=Surv(death,status)weight*group,dist= \"exponential\") Value Std.Error z p (Intercept) 3.8702 0.3854 10.041 1.00e-23 weight -0.0803 0.0659 -1.219 2.23e-01 groupB -0.8853 0.4508 -1.964 4.95e-02 groupC -1.7804 0.4386 -4.059 4.92e-05 weight:groupB 0.0643 0.0674 0.954 3.40e-01 weight:groupC 0.0796 0.0674 1.180 2.38e-01 Scalefixedat1 Exponentialdistribution Loglik(model)=-480.6 Loglik(interceptonly)=-502.1 Chisq=43.11on5degreesoffreedom,p=3.5e-08 NumberofNewton-RaphsonIterations:5 n=150 Modelsimplificationproceedsinthenormalway.Youcoulduseupdate,buthere(for varietyonly)werefitprogressivelysimplermodelsandtestthemusinganova.Firstwetake out the different slopes for each group: model2<-survreg(Surv(death,status)weight+group,dist=\"exponential\") anova(model,model2,test=\"Chi\") TermsResid.Df -2*LL TestDf Deviance P(>|Chi|) 1 weight*group 144961.1800 NA NA NA 2 weight+group 146962.9411 -weight:group-2 -1.761142 0.4145462 The interaction is not significant so we leave it out and try deleting weight: model3<-survreg(Surv(death,status)group,dist=\"exponential\") anova(model2,model3,test=\"Chi\") TermsResid.Df -2*LL TestDf Deviance P(>|Chi|) 1 weight+group 146962.9411 NA NA NA 2 group 147963.9393 -weight-1 -0.9981333 0.3177626 DEATH ANDFAILURE DATA 289 This is not significant, so we leave it out and try deleting group: model4<-survreg(Surv(death,status)1,dist=\"exponential\") anova(model3,model4,test=\"Chi\") TermsResid.Df -2*LL TestDf Deviance P(>|Chi|) 1 group 147 963.9393 NA NA NA 2 1 149 1004.2865 -2 -40.34721 1.732661e-09 Thisishighlysignificant,soweadditback.Theminimaladequatemodelismodel3with the three-level factor group, but there is no evidence that initial body weight had any influence on survival."
  },
  {
    "input": "It is straightforward to compare error distributions for the same model structure: model3<-survreg(Surv(death,status)group,dist=\"exponential\") model4<-survreg(Surv(death,status)group,dist=\"extreme\") model5<-survreg(Surv(death,status)group,dist=\"gaussian\") model6<-survreg(Surv(death,status)group,dist=\"logistic\") anova(model3,model4,model5,model6) TermsResid.Df -2*LL TestDf Deviance Pr(>Chi) 1 group 147 963.9393 NA NA NA 2 group 146 1225.3512 = 1-261.411949 NA 3 group 146 1178.6582 = 0 46.692975 NA 4 group 146 1173.9478 = 0 4.710457 NA Our initial choice of exponential was clearly the best, giving much the lowest residual deviance (963.94).",
    "target": "Q: What is It? A: It is straightforward to compare error distributions for the same model structure: model3<-survreg(Surv(death,status)group,dist=\"exponential\") model4<-survreg(Surv(death,status)group,dist=\"extreme\") model5<-survreg(Surv(death,status)group,dist=\"gaussian\") model6<-survreg(Surv(death,status)group,dist=\"logistic\") anova(model3,model4,model5,model6) TermsResid.Df -2*LL TestDf Deviance Pr(>Chi) 1 group 147 963.9393 NA NA NA 2 group 146 1225.3512 = 1-261.411949 NA 3 group 146 1178.6582 = 0 46.692975 NA 4 group 146 1173.9478 = 0 4.710457 NA Our initial choice of exponential was clearly the best, giving much the lowest residual deviance (963.94)."
  },
  {
    "input": "But when there is censoring, the arithmetic mean underestimates the age at death, and when the censoring is substantial (as in group A) this underestimate is very large (23.08 vs. 32.06 months).",
    "target": "Q: What is But when there? A: But when there is censoring, the arithmetic mean underestimates the age at death, and when the censoring is substantial (as in group A) this underestimate is very large (23.08 vs. 32.06 months)."
  },
  {
    "input": "Appendix Essentials of the R Language RasaCalculator The command line after the screen prompt > is an excellent calculator: >log(42/7.3) [1]1.749795 By default, logs in R are base e (natural or Napierian, not base 10 logs), but you canspecifyanybaseyouwant,asasecondargumenttothelogfunction.Hereislogbase 2 of 16: >log(16,2) [1]4 Each line can have many characters, but if you want to evaluate a complicated expression, you might like to continue it on one or more further lines for clarity.",
    "target": "Q: What is Appendix Essentials of the R Language RasaCalculator The command line after the screen prompt >? A: Appendix Essentials of the R Language RasaCalculator The command line after the screen prompt > is an excellent calculator: >log(42/7.3) [1]1.749795 By default, logs in R are base e (natural or Napierian, not base 10 logs), but you canspecifyanybaseyouwant,asasecondargumenttothelogfunction.Hereislogbase 2 of 16: >log(16,2) [1]4 Each line can have many characters, but if you want to evaluate a complicated expression, you might like to continue it on one or more further lines for clarity."
  },
  {
    "input": "From here onwards (and throughout the book), the prompt character > is omitted.",
    "target": "Q: What is From here onwards (and throughout the book), the prompt character >? A: From here onwards (and throughout the book), the prompt character > is omitted."
  },
  {
    "input": "The output from R is shown in dark blue Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.",
    "target": "Q: What is The output from R? A: The output from R is shown in dark blue Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley."
  },
  {
    "input": "Twoormoreexpressionscanbeplacedonasinglelinesolongastheyareseparatedby semicolons: 2+3; 5*7; 3-7 [1]5 [1]35 [1]-4 Alloftheusualcalculationscanbedonedirectly,andthestandardorderofprecedence applies:powersandrootsareevaluatedfirst,thenmultiplicationanddivision,thenfinally additionandsubtraction.Althoughthereisanamedfunctionforsquareroots,sqrt,roots aregenerallycalculatedasfractionalpowers.Exponentiation(powers)usesthecaret(hat) ^operator(not**asinsomecomputinglanguageslikeFortranorGLIM).Sothecuberoot of 8 is 8^(1/3) [1]2 The exponentiation operator ^ groups right to left, but all other operators group left to right.Thus,2^2^3is2^8,not4^3,whereas1-1-1is-1,not1.Usebracketsas jy \u0000y j necessarytooverridetheprecedence.Forattest,wherethevaluerequiredis A B and SE diff the numbers are 5.7, 6.8 and 0.38, you type: abs(5.7-6.8)/0.38 [1]2.894737 ThedefaultnumberofdigitsprintedoutbyRis7(asabove).Youcancontrolthenumber of digits printed using the digitsoption like this: options(digits=3) abs(5.7-6.8)/0.38 [1]2.89 Built-inFunctions Allthemathematicalfunctionsyoucouldeverwantarehere(TableA.1).Wehavealready met the log function; the antilog function to the base e is exp: exp(1) [1]2.718282 APPENDIX: ESSENTIALS OF THE RLANGUAGE 293 ThetrigonometricfunctionsinRmeasureanglesinradians.Acircleis2 radians,and this is 360, so a right-angle (90) is =2 radians.",
    "target": "Q: What is Twoormoreexpressionscanbeplacedonasinglelinesolongastheyareseparatedby semicolons: 2+3; 5*7; 3-7 [1]5 [1]35 [1]-4 Alloftheusualcalculationscanbedonedirectly,andthestandardorderofprecedence applies:powersandrootsareevaluatedfirst,thenmultiplicationanddivision,thenfinally additionandsubtraction.Althoughthereisanamedfunctionforsquareroots,sqrt,roots aregenerallycalculatedasfractionalpowers.Exponentiation(powers)usesthecaret(hat) ^operator(not**asinsomecomputinglanguageslikeFortranorGLIM).Sothecuberoot of 8? A: Twoormoreexpressionscanbeplacedonasinglelinesolongastheyareseparatedby semicolons: 2+3; 5*7; 3-7 [1]5 [1]35 [1]-4 Alloftheusualcalculationscanbedonedirectly,andthestandardorderofprecedence applies:powersandrootsareevaluatedfirst,thenmultiplicationanddivision,thenfinally additionandsubtraction.Althoughthereisanamedfunctionforsquareroots,sqrt,roots aregenerallycalculatedasfractionalpowers.Exponentiation(powers)usesthecaret(hat) ^operator(not**asinsomecomputinglanguageslikeFortranorGLIM).Sothecuberoot of 8 is 8^(1/3) [1]2 The exponentiation operator ^ groups right to left, but all other operators group left to right.Thus,2^2^3is2^8,not4^3,whereas1-1-1is-1,not1.Usebracketsas jy \u0000y j necessarytooverridetheprecedence.Forattest,wherethevaluerequiredis A B and SE diff the numbers are 5.7, 6.8 and 0.38, you type: abs(5.7-6.8)/0.38 [1]2.894737 ThedefaultnumberofdigitsprintedoutbyRis7(asabove).Youcancontrolthenumber of digits printed using the digitsoption like this: options(digits=3) abs(5.7-6.8)/0.38 [1]2.89 Built-inFunctions Allthemathematicalfunctionsyoucouldeverwantarehere(TableA.1).Wehavealready met the log function; the antilog function to the base e is exp: exp(1) [1]2.718282 APPENDIX: ESSENTIALS OF THE RLANGUAGE 293 ThetrigonometricfunctionsinRmeasureanglesinradians.Acircleis2 radians,and this is 360, so a right-angle (90) is =2 radians."
  },
  {
    "input": "While this is a very small number it is clearly not exactly zero (so you need to be careful when testing for exact equality of real numbers; see p. 313).",
    "target": "Q: What is While this? A: While this is a very small number it is clearly not exactly zero (so you need to be careful when testing for exact equality of real numbers; see p. 313)."
  },
  {
    "input": "acosh(x),asinh(x),atanh(x) inverse hyperbolic trigonometric transformations onrealorcomplexnumbers abs(x) the absolute value of x, ignoring the minus sign if thereisone NumberswithExponents For very big numbers or very small numbers R uses the following scheme: 1.2e3 means1200becausethee3meansmovethedecimalpoint3placestothe right 1.2e-2 means0.012becausethee-2meansmovethedecimalpoint2placestothe left 3.9+4.5i isacomplexnumberwithreal(3.9)andimaginary(4.5)parts,andiisthe squarerootof\u00001 ModuloandIntegerQuotients Integer quotients and remainders are obtained using the notation %/% (percent, divide, percent)and%%(percent,percent)respectively.Supposewewanttoknowtheintegerpartof a division  say, how many 13s are there in 119: 119%/%13 [1]9 Nowsupposewewantedtoknowtheremainder(whatisleftoverwhen119isdividedby 13), known in maths as modulo: 119%%13 [1]2 Moduloisveryusefulfortestingwhethernumbersareoddoreven:oddnumbersare1 modulo 2 and even numbers are 0 modulo 2: 9%%2 [1]1 8%%2 [1]0 APPENDIX: ESSENTIALS OF THE RLANGUAGE 295 Likewise, you use modulo to test if one number is an exact multiple of some other number.",
    "target": "Q: What is acosh(x),asinh(x),atanh(x) inverse hyperbolic trigonometric transformations onrealorcomplexnumbers abs(x) the absolute value of x, ignoring the minus sign if thereisone NumberswithExponents For very big numbers or very small numbers R uses the following scheme: 1.2e3 means1200becausethee3meansmovethedecimalpoint3placestothe right 1.2e-2 means0.012becausethee-2meansmovethedecimalpoint2placestothe left 3.9+4.5i isacomplexnumberwithreal(3.9)andimaginary(4.5)parts,andiisthe squarerootof\u00001 ModuloandIntegerQuotients Integer quotients and remainders are obtained using the notation %/% (percent, divide, percent)and%%(percent,percent)respectively.Supposewewanttoknowtheintegerpartof a division  say, how many 13s are there in 119: 119%/%13 [1]9 Nowsupposewewantedtoknowtheremainder(whatisleftoverwhen119isdividedby 13), known in maths as modulo: 119%%13 [1]2 Moduloisveryusefulfortestingwhethernumbersareoddoreven:oddnumbersare1 modulo 2 and even numbers are 0 modulo 2: 9%%2 [1]1 8%%2 [1]0 APPENDIX: ESSENTIALS OF THE RLANGUAGE 295 Likewise, you use modulo to test if one number? A: acosh(x),asinh(x),atanh(x) inverse hyperbolic trigonometric transformations onrealorcomplexnumbers abs(x) the absolute value of x, ignoring the minus sign if thereisone NumberswithExponents For very big numbers or very small numbers R uses the following scheme: 1.2e3 means1200becausethee3meansmovethedecimalpoint3placestothe right 1.2e-2 means0.012becausethee-2meansmovethedecimalpoint2placestothe left 3.9+4.5i isacomplexnumberwithreal(3.9)andimaginary(4.5)parts,andiisthe squarerootof\u00001 ModuloandIntegerQuotients Integer quotients and remainders are obtained using the notation %/% (percent, divide, percent)and%%(percent,percent)respectively.Supposewewanttoknowtheintegerpartof a division  say, how many 13s are there in 119: 119%/%13 [1]9 Nowsupposewewantedtoknowtheremainder(whatisleftoverwhen119isdividedby 13), known in maths as modulo: 119%%13 [1]2 Moduloisveryusefulfortestingwhethernumbersareoddoreven:oddnumbersare1 modulo 2 and even numbers are 0 modulo 2: 9%%2 [1]1 8%%2 [1]0 APPENDIX: ESSENTIALS OF THE RLANGUAGE 295 Likewise, you use modulo to test if one number is an exact multiple of some other number."
  },
  {
    "input": "For instance, to find out whether 15 421 is a multiple of 7, type: 15421%%7==0 [1]TRUE Assignment Objects obtain values in R by assignment (x gets a value).",
    "target": "Q: What is For instance, to find out whether 15 421? A: For instance, to find out whether 15 421 is a multiple of 7, type: 15421%%7==0 [1]TRUE Assignment Objects obtain values in R by assignment (x gets a value)."
  },
  {
    "input": "This is achieved by the getsarrowwhichisacompositesymbolmadeupfromlessthanandminus<-withouta space between them.",
    "target": "Q: What is This? A: This is achieved by the getsarrowwhichisacompositesymbolmadeupfromlessthanandminus<-withouta space between them."
  },
  {
    "input": "Notice that there is a potential ambiguity if you get the spacing wrong.",
    "target": "Q: What is Notice that there? A: Notice that there is a potential ambiguity if you get the spacing wrong."
  },
  {
    "input": "The greatest integer less than function is floor: floor(5.7) [1]5 The next integer function is ceiling: ceiling(5.7) [1]6 Youcanroundtothenearestintegerbyadding0.5tothenumberthenusingfloor.There isabuilt-infunctionforthis,butwecaneasilywriteoneofourowntointroducethenotion of writing functions.",
    "target": "Q: What is The greatest integer less than function? A: The greatest integer less than function is floor: floor(5.7) [1]5 The next integer function is ceiling: ceiling(5.7) [1]6 Youcanroundtothenearestintegerbyadding0.5tothenumberthenusingfloor.There isabuilt-infunctionforthis,butwecaneasilywriteoneofourowntointroducethenotion of writing functions."
  },
  {
    "input": "Call it rounded, then define it as a function like this: rounded<-function(x) floor(x+0.5) Now we can use the new function: rounded(5.7) [1]6 rounded(5.4) [1]5 296 STATISTICS:ANINTRODUCTIONUSINGR InfinityandThingsthatAreNotaNumber(NaN) Calculations can lead to answers that are plus infinity, represented in R by Inf: 3/0 [1]Inf or minus infinity, which is Inf in R: -12/0 [1]-Inf Calculations involving infinity can be evaluated: exp(-Inf) [1]0 0/Inf [1]0 (0:3)^Inf [1] 0 1InfInf Thesyntax0:3isveryuseful.Itgeneratersaseries(0,1,2,3inthiscase)andthefunction isevaluatedforeachofthevaluestoproduceavectorofanswers.Inthiscase,thevectoris of length = 4.",
    "target": "Q: What is Call it rounded, then define it as a function like this: rounded<-function(x) floor(x+0.5) Now we can use the new function: rounded(5.7) [1]6 rounded(5.4) [1]5 296 STATISTICS:ANINTRODUCTIONUSINGR InfinityandThingsthatAreNotaNumber(NaN) Calculations can lead to answers that are plus infinity, represented in R by Inf: 3/0 [1]Inf or minus infinity, which? A: Call it rounded, then define it as a function like this: rounded<-function(x) floor(x+0.5) Now we can use the new function: rounded(5.7) [1]6 rounded(5.4) [1]5 296 STATISTICS:ANINTRODUCTIONUSINGR InfinityandThingsthatAreNotaNumber(NaN) Calculations can lead to answers that are plus infinity, represented in R by Inf: 3/0 [1]Inf or minus infinity, which is Inf in R: -12/0 [1]-Inf Calculations involving infinity can be evaluated: exp(-Inf) [1]0 0/Inf [1]0 (0:3)^Inf [1] 0 1InfInf Thesyntax0:3isveryuseful.Itgeneratersaseries(0,1,2,3inthiscase)andthefunction isevaluatedforeachofthevaluestoproduceavectorofanswers.Inthiscase,thevectoris of length = 4."
  },
  {
    "input": "Here are some of the classic cases: 0/0 [1]NaN Inf-Inf [1]NaN Inf/Inf [1]NaN YouneedtounderstandclearlythedistinctionbetweenNaNandNA(thisstandsfornot availableandisthemissingvaluesymbolinR;seebelow).Therearebuilt-inteststocheck whether a number is finite or infinite: is.finite(10) [1]TRUE APPENDIX: ESSENTIALS OF THE RLANGUAGE 297 is.infinite(10) [1]FALSE is.infinite(Inf) [1]TRUE MissingValues(NA) Missingvaluesindataframesarearealsourceofirritationbecausetheyaffectthewaythat model-fittingfunctionsoperate,andtheycangreatlyreducethepowerofthemodellingthat we would like to do.",
    "target": "Q: What is Here are some of the classic cases: 0/0 [1]NaN Inf-Inf [1]NaN Inf/Inf [1]NaN YouneedtounderstandclearlythedistinctionbetweenNaNandNA(thisstandsfornot availableandisthemissingvaluesymbolinR;seebelow).Therearebuilt-inteststocheck whether a number? A: Here are some of the classic cases: 0/0 [1]NaN Inf-Inf [1]NaN Inf/Inf [1]NaN YouneedtounderstandclearlythedistinctionbetweenNaNandNA(thisstandsfornot availableandisthemissingvaluesymbolinR;seebelow).Therearebuilt-inteststocheck whether a number is finite or infinite: is.finite(10) [1]TRUE APPENDIX: ESSENTIALS OF THE RLANGUAGE 297 is.infinite(10) [1]FALSE is.infinite(Inf) [1]TRUE MissingValues(NA) Missingvaluesindataframesarearealsourceofirritationbecausetheyaffectthewaythat model-fittingfunctionsoperate,andtheycangreatlyreducethepowerofthemodellingthat we would like to do."
  },
  {
    "input": "Somefunctionsdonotworkwiththeirdefaultsettingswhentherearemissingvaluesin the data, and mean is a classic example of this: x<-c(1:8,NA) mean(x) [1]NA Inordertocalculatethemeanofthenon-missingvalues,youneedtospecifythattheNA are to be removed, using the na.rm=TRUE argument: mean(x,na.rm=T) [1]4.5 Tocheckforthelocationofmissingvalueswithinavector,usethefunctionis.na(x).",
    "target": "Q: What is Somefunctionsdonotworkwiththeirdefaultsettingswhentherearemissingvaluesin the data, and mean? A: Somefunctionsdonotworkwiththeirdefaultsettingswhentherearemissingvaluesin the data, and mean is a classic example of this: x<-c(1:8,NA) mean(x) [1]NA Inordertocalculatethemeanofthenon-missingvalues,youneedtospecifythattheNA are to be removed, using the na.rm=TRUE argument: mean(x,na.rm=T) [1]4.5 Tocheckforthelocationofmissingvalueswithinavector,usethefunctionis.na(x)."
  },
  {
    "input": "Hereisanexamplewherewewanttofindthelocations(7and8)ofmissingvalueswithina vector called vmv: (vmv<-c(1:6,NA,NA,9:12)) [1] 1 2 3 4 5 6 NA NA 9 10 11 12 Notetheuseofroundbracketstogettheanswerprintedaswellasallocated.Makingan index of the missing values in an array could use the seq function: seq(along=vmv)[is.na(vmv)] [1]78 However, the result is achieved more simply using which like this: which(is.na(vmv)) [1]78 Ifthemissingvaluesaregenuinecountsofzero,youmightwanttoedittheNAto0.Use the is.na function to generate subscripts for this: (vmv[is.na(vmv)]<-0) [1] 1 2 3 4 5 6 0 0 9 10 11 12 298 STATISTICS:ANINTRODUCTIONUSINGR Alternatively, use the ifelse function like this: vmv<-c(1:6,NA,NA,9:12) ifelse(is.na(vmv),0,vmv) [1] 1 2 3 4 5 6 0 0 9 10 11 12 Operators R uses the following operator tokens + -* / %% ^ arithmetic > >= < <= == != relational !",
    "target": "Q: What is Hereisanexamplewherewewanttofindthelocations(7and8)ofmissingvalueswithina vector called vmv: (vmv<-c(1:6,NA,NA,9:12)) [1] 1 2 3 4 5 6 NA NA 9 10 11 12 Notetheuseofroundbracketstogettheanswerprintedaswellasallocated.Makingan index of the missing values in an array could use the seq function: seq(along=vmv)[is.na(vmv)] [1]78 However, the result? A: Hereisanexamplewherewewanttofindthelocations(7and8)ofmissingvalueswithina vector called vmv: (vmv<-c(1:6,NA,NA,9:12)) [1] 1 2 3 4 5 6 NA NA 9 10 11 12 Notetheuseofroundbracketstogettheanswerprintedaswellasallocated.Makingan index of the missing values in an array could use the seq function: seq(along=vmv)[is.na(vmv)] [1]78 However, the result is achieved more simply using which like this: which(is.na(vmv)) [1]78 Ifthemissingvaluesaregenuinecountsofzero,youmightwanttoedittheNAto0.Use the is.na function to generate subscripts for this: (vmv[is.na(vmv)]<-0) [1] 1 2 3 4 5 6 0 0 9 10 11 12 298 STATISTICS:ANINTRODUCTIONUSINGR Alternatively, use the ifelse function like this: vmv<-c(1:6,NA,NA,9:12) ifelse(is.na(vmv),0,vmv) [1] 1 2 3 4 5 6 0 0 9 10 11 12 Operators R uses the following operator tokens + -* / %% ^ arithmetic > >= < <= == != relational !"
  },
  {
    "input": "pmax(x,y,z) vector, of length equal to the longest of x, y, or z containing the maximumofx,yorzfortheithpositionineach pmin(x,y,z) vector, of length equal to the longest of x, y, or z containing the minimumofx,yorzfortheithpositionineach colMeans(x) columnmeansofdataframeormatrixx colSums(x) columntotalsofdataframeormatrixx rowMeans(x) rowmeansofdataframeormatrixx rowSums(x) rowtotalsofdataframeormatrixx SummaryInformationfromVectorsbyGroups Oneofthemostimportantandusefulvectorfunctionstomasteristapply.Thetstands for table and the idea is to apply a function to produce a table from the values in the vector,basedononeormoregroupingvariables(oftenthegroupingisbyfactorlevels).This sounds much more complicated than it really is: data<-read.csv(\"c:\\\\temp\\\\daphnia.csv\") attach(data) names(data) [1]\"Growth.rate\"\"Water\" \"Detergent\" \"Daphnia\" APPENDIX: ESSENTIALS OF THE RLANGUAGE 301 TheresponsevariableisGrowth.rateandtheotherthreevariablesarefactors.Suppose we want the mean growth rate for each Detergent: tapply(Growth.rate,Detergent,mean) BrandA BrandB BrandC BrandD 3.88 4.01 3.95 3.56 Thisproducesatablewithfourentries,oneforeachlevelofthefactorcalledDetergent.",
    "target": "Q: What is pmax(x,y,z) vector, of length equal to the longest of x, y, or z containing the maximumofx,yorzfortheithpositionineach pmin(x,y,z) vector, of length equal to the longest of x, y, or z containing the minimumofx,yorzfortheithpositionineach colMeans(x) columnmeansofdataframeormatrixx colSums(x) columntotalsofdataframeormatrixx rowMeans(x) rowmeansofdataframeormatrixx rowSums(x) rowtotalsofdataframeormatrixx SummaryInformationfromVectorsbyGroups Oneofthemostimportantandusefulvectorfunctionstomasteristapply.Thetstands for table and the idea? A: pmax(x,y,z) vector, of length equal to the longest of x, y, or z containing the maximumofx,yorzfortheithpositionineach pmin(x,y,z) vector, of length equal to the longest of x, y, or z containing the minimumofx,yorzfortheithpositionineach colMeans(x) columnmeansofdataframeormatrixx colSums(x) columntotalsofdataframeormatrixx rowMeans(x) rowmeansofdataframeormatrixx rowSums(x) rowtotalsofdataframeormatrixx SummaryInformationfromVectorsbyGroups Oneofthemostimportantandusefulvectorfunctionstomasteristapply.Thetstands for table and the idea is to apply a function to produce a table from the values in the vector,basedononeormoregroupingvariables(oftenthegroupingisbyfactorlevels).This sounds much more complicated than it really is: data<-read.csv(\"c:\\\\temp\\\\daphnia.csv\") attach(data) names(data) [1]\"Growth.rate\"\"Water\" \"Detergent\" \"Daphnia\" APPENDIX: ESSENTIALS OF THE RLANGUAGE 301 TheresponsevariableisGrowth.rateandtheotherthreevariablesarefactors.Suppose we want the mean growth rate for each Detergent: tapply(Growth.rate,Detergent,mean) BrandA BrandB BrandC BrandD 3.88 4.01 3.95 3.56 Thisproducesatablewithfourentries,oneforeachlevelofthefactorcalledDetergent."
  },
  {
    "input": "This selection is done using subscripts (also known asindices ).",
    "target": "Q: What is This selection? A: This selection is done using subscripts (also known asindices )."
  },
  {
    "input": "But sum(x) adds up the values of the xs and sum(x<5) counts up the number of cases that pass the logical condition x is less than 5.",
    "target": "Q: What is But sum(x) adds up the values of the xs and sum(x<5) counts up the number of cases that pass the logical condition x? A: But sum(x) adds up the values of the xs and sum(x<5) counts up the number of cases that pass the logical condition x is less than 5."
  },
  {
    "input": "The logical condition x<5 is either true or false: x<5 [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE [10]FALSE FALSE Youcanimaginefalseasbeingnumeric0andtrueasbeingnumeric1.Thenthevectorof subscripts [x<5] is five 1s followed by six 0s: 1*(x<5) [1]11111000000 Now imagine multiplying the values of x by the values of the logical vector: x*(x<5) [1]01234000000 APPENDIX: ESSENTIALS OF THE RLANGUAGE 303 Whenthefunctionsumisapplied,itgivesustheanswerwewant:thesumofthevaluesof the numbers 0+1+2+3+4=10.",
    "target": "Q: What is The logical condition x<5? A: The logical condition x<5 is either true or false: x<5 [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE [10]FALSE FALSE Youcanimaginefalseasbeingnumeric0andtrueasbeingnumeric1.Thenthevectorof subscripts [x<5] is five 1s followed by six 0s: 1*(x<5) [1]11111000000 Now imagine multiplying the values of x by the values of the logical vector: x*(x<5) [1]01234000000 APPENDIX: ESSENTIALS OF THE RLANGUAGE 303 Whenthefunctionsumisapplied,itgivesustheanswerwewant:thesumofthevaluesof the numbers 0+1+2+3+4=10."
  },
  {
    "input": "To compute this we just specify the subscript [2]: rev(sort(y))[2] [1]10 Arangeofsubscriptsissimplyaseriesgeneratedusingthecolonoperator.Wewantthe subscripts 1 to 3, so this is: rev(sort(y))[1:3] [1]1110 9 So the answer to the exercise is just: sum(rev(sort(y))[1:3]) [1]30 Note that we have not changed the vector y in any way, nor have we created any new space-consuming vectors during intermediate computational steps.",
    "target": "Q: What is To compute this we just specify the subscript [2]: rev(sort(y))[2] [1]10 Arangeofsubscriptsissimplyaseriesgeneratedusingthecolonoperator.Wewantthe subscripts 1 to 3, so this is: rev(sort(y))[1:3] [1]1110 9 So the answer to the exercise? A: To compute this we just specify the subscript [2]: rev(sort(y))[2] [1]10 Arangeofsubscriptsissimplyaseriesgeneratedusingthecolonoperator.Wewantthe subscripts 1 to 3, so this is: rev(sort(y))[1:3] [1]1110 9 So the answer to the exercise is just: sum(rev(sort(y))[1:3]) [1]30 Note that we have not changed the vector y in any way, nor have we created any new space-consuming vectors during intermediate computational steps."
  },
  {
    "input": "304 STATISTICS:ANINTRODUCTIONUSINGR AddresseswithinVectors Therearetwoimportantfunctionsforfindingaddresseswithinarrays.Thefunctionwhich is very easy to understand.",
    "target": "Q: What is 304 STATISTICS:ANINTRODUCTIONUSINGR AddresseswithinVectors Therearetwoimportantfunctionsforfindingaddresseswithinarrays.Thefunctionwhich? A: 304 STATISTICS:ANINTRODUCTIONUSINGR AddresseswithinVectors Therearetwoimportantfunctionsforfindingaddresseswithinarrays.Thefunctionwhich is very easy to understand."
  },
  {
    "input": "length(y) [1]15 length(y[y>5]) [1]9 TrimmingVectorsUsingNegativeSubscripts An extremely useful facility is to use negative subscripts to drop terms from a vector.",
    "target": "Q: What is length(y) [1]15 length(y[y>5]) [1]9 TrimmingVectorsUsingNegativeSubscripts An extremely useful facility? A: length(y) [1]15 length(y[y>5]) [1]9 TrimmingVectorsUsingNegativeSubscripts An extremely useful facility is to use negative subscripts to drop terms from a vector."
  },
  {
    "input": "Let us try it out: trim.mean(x) [1]5.2 LogicalArithmetic Arithmeticinvolvinglogicalexpressionsisveryusefulinprogrammingandinselectionof variables(seeTableA.3).Iflogicalarithmeticisunfamiliartoyou,thenperseverewithit, becauseitwillbecomeclearhowusefulitis,oncethepennyhasdropped.Thekeythingto understand is that logical expressions evaluate to either true or false (represented in R by TRUE or FALSE), and that R can coerce TRUE or FALSE into numerical values: 1 for TRUE and 0 for FALSE.",
    "target": "Q: What is Let us try it out: trim.mean(x) [1]5.2 LogicalArithmetic Arithmeticinvolvinglogicalexpressionsisveryusefulinprogrammingandinselectionof variables(seeTableA.3).Iflogicalarithmeticisunfamiliartoyou,thenperseverewithit, becauseitwillbecomeclearhowusefulitis,oncethepennyhasdropped.Thekeythingto understand? A: Let us try it out: trim.mean(x) [1]5.2 LogicalArithmetic Arithmeticinvolvinglogicalexpressionsisveryusefulinprogrammingandinselectionof variables(seeTableA.3).Iflogicalarithmeticisunfamiliartoyou,thenperseverewithit, becauseitwillbecomeclearhowusefulitis,oncethepennyhasdropped.Thekeythingto understand is that logical expressions evaluate to either true or false (represented in R by TRUE or FALSE), and that R can coerce TRUE or FALSE into numerical values: 1 for TRUE and 0 for FALSE."
  },
  {
    "input": "The syntax for the three arguments is this: gl(up to, with repeats of, to total length) Hereisthesimplestcasewherewewantfactorlevelsupto4withrepeatsof3repeated only once (i.e.",
    "target": "Q: What is The syntax for the three arguments? A: The syntax for the three arguments is this: gl(up to, with repeats of, to total length) Hereisthesimplestcasewherewewantfactorlevelsupto4withrepeatsof3repeated only once (i.e."
  },
  {
    "input": "to total length=12): gl(4,3) [1]111222333444 Levels:1234 Here is the function when we want that whole pattern repeated twice: gl(4,3,24) [1]111222333444111222333444 Levels:1234 If the total length is not a multiple of the length of the pattern, the vector is truncated: gl(4,3,20) [1]11122233344411122233 Levels:1234 If you want text for the factor levels, rather than numbers, use labelslike this: gl(3,2,24,labels=c(\"A\",\"B\",\"C\")) [1]AABBCCAABBCCAABBCCAABBCC Levels:ABC GeneratingRegularSequencesofNumbers For regular series of whole numbers use the colon operator (: as on p. 296).",
    "target": "Q: What is to total length=12): gl(4,3) [1]111222333444 Levels:1234 Here? A: to total length=12): gl(4,3) [1]111222333444 Levels:1234 Here is the function when we want that whole pattern repeated twice: gl(4,3,24) [1]111222333444111222333444 Levels:1234 If the total length is not a multiple of the length of the pattern, the vector is truncated: gl(4,3,20) [1]11122233344411122233 Levels:1234 If you want text for the factor levels, rather than numbers, use labelslike this: gl(3,2,24,labels=c(\"A\",\"B\",\"C\")) [1]AABBCCAABBCCAABBCCAABBCC Levels:ABC GeneratingRegularSequencesofNumbers For regular series of whole numbers use the colon operator (: as on p. 296)."
  },
  {
    "input": "The class and attributes of X indicate that it is a matrix of 3 rows and 3 columns (these are its dimattributes): 308 STATISTICS:ANINTRODUCTIONUSINGR class(X) [1]\"matrix\" attributes(X) $dim [1]33 In the next example, the data in the vector appear row-wise, so we indicate this with byrow=T: vector<-c(1,2,3,4,4,3,2,1) V<-matrix(vector,byrow=T,nrow=2) V [,1][,2][,3][,4] [1,] 1 2 3 4 [2,] 4 3 2 1 Anotherwaytoconvertavectorintoamatrixisbyprovidingthevectorobjectwithtwo dimensions (rows and columns) using the dim function like this: dim(vector)<-c(4,2) and we can check that vector has now become a matrix: is.matrix(vector) [1]TRUE Weneedtobecareful,however,becausewehavemadenoallowanceatthisstageforthe fact that the data were entered row-wise into vector: vector [,1][,2] [1,] 1 4 [2,] 2 3 [3,] 3 2 [4,] 4 1 The matrix we want is the transpose, t, of this matrix: (vector<-t(vector)) [,1][,2][,3][,4] [1,] 1 2 3 4 [2,] 4 3 2 1 APPENDIX: ESSENTIALS OF THE RLANGUAGE 309 CharacterStrings InR,characterstringsaredefinedbydoublequotationmarks.Webeingbydefiningaphrase: phrase<-\"thequickbrownfoxjumpsoverthelazydog\" The function called substr is used to extract substrings of a specified number of charactersfromacharacterstring.Hereisthecodetoextractthefirst,thefirstandsecond, the first, second and third, and so on (up to 20) characters from our phrase: q<-character(20) for(iin1:20) q[i]<- substr(phrase,1,i) q [1] \"t\" \"th\" \"the\" [4] \"the\" \"theq\" \"thequ\" [7] \"thequi\" \"thequic\" \"thequick\" [10]\"thequick\" \"thequickb\" \"thequickbr\" [13]\"thequickbro\" \"thequickbrow\" \"thequickbrown\" [16]\"thequickbrown\" \"thequickbrownf\" \"thequickbrownfo\" [19]\"thequickbrownfox\" \"thequickbrownfox\" Thesecondargumentinsubstristhenumberofthecharacteratwhichextractionisto begin(inthiscasealwaysthefirst),andthethirdargumentisthenumberofthecharacterat whichextractionistoend(inthiscase,theith).Tosplitupacharacterstringintoindividual characters, we use strsplit like this: strsplit(phrase,split=character(0)) [[1]] [1] \"t\"\"h\"\"e\"\"\"\"q\"\"u\"\"i\"\"c\"\"k\"\"\"\"b\"\"r\"\"o\"\"w\"\"n\"\"\" [17] \"f\"\"o\"\"x\"\"\"\"j\"\"u\"\"m\"\"p\"\"s\"\"\"\"o\"\"v\"\"e\"\"r\" [31] \"\"\"t\"\"h\"\"e\"\"\"\"l\"\"a\"\"z\"\"y\"\"\"\"d\"\"o\"\"g\" The table function is useful for counting the number of occurrences of characters of different kinds: table(strsplit(phrase,split=character(0))) abcdefghijklmnopqrstuvwxyz 811113112111111411212211111 Thisdemonstratesthatallofthelettersofthealphabetwereusedatleastoncewithinour phrase,andthattherewereeightblankswithinphrase.Thissuggestsawayofcountingthe numberofwordsinaphrase,given that thiswillalwaysbeonemore than thenumberof blanks: words<-1+table(strsplit(phrase,split=character(0)))[1] words 9 310 STATISTICS:ANINTRODUCTIONUSINGR It is easy to switch between upper and lower cases using the toupper and tolower functions: toupper(phrase) [1]\"THEQUICKBROWNFOXJUMPSOVERTHELAZYDOG\" tolower(toupper(phrase)) [1]\"thequickbrownfoxjumpsoverthelazydog\" WritingFunctionsinR FunctionsinRareobjectsthatcarryoutoperationsonargumentsthataresuppliedtothem and return one or more values.",
    "target": "Q: What is The class and attributes of X indicate that it? A: The class and attributes of X indicate that it is a matrix of 3 rows and 3 columns (these are its dimattributes): 308 STATISTICS:ANINTRODUCTIONUSINGR class(X) [1]\"matrix\" attributes(X) $dim [1]33 In the next example, the data in the vector appear row-wise, so we indicate this with byrow=T: vector<-c(1,2,3,4,4,3,2,1) V<-matrix(vector,byrow=T,nrow=2) V [,1][,2][,3][,4] [1,] 1 2 3 4 [2,] 4 3 2 1 Anotherwaytoconvertavectorintoamatrixisbyprovidingthevectorobjectwithtwo dimensions (rows and columns) using the dim function like this: dim(vector)<-c(4,2) and we can check that vector has now become a matrix: is.matrix(vector) [1]TRUE Weneedtobecareful,however,becausewehavemadenoallowanceatthisstageforthe fact that the data were entered row-wise into vector: vector [,1][,2] [1,] 1 4 [2,] 2 3 [3,] 3 2 [4,] 4 1 The matrix we want is the transpose, t, of this matrix: (vector<-t(vector)) [,1][,2][,3][,4] [1,] 1 2 3 4 [2,] 4 3 2 1 APPENDIX: ESSENTIALS OF THE RLANGUAGE 309 CharacterStrings InR,characterstringsaredefinedbydoublequotationmarks.Webeingbydefiningaphrase: phrase<-\"thequickbrownfoxjumpsoverthelazydog\" The function called substr is used to extract substrings of a specified number of charactersfromacharacterstring.Hereisthecodetoextractthefirst,thefirstandsecond, the first, second and third, and so on (up to 20) characters from our phrase: q<-character(20) for(iin1:20) q[i]<- substr(phrase,1,i) q [1] \"t\" \"th\" \"the\" [4] \"the\" \"theq\" \"thequ\" [7] \"thequi\" \"thequic\" \"thequick\" [10]\"thequick\" \"thequickb\" \"thequickbr\" [13]\"thequickbro\" \"thequickbrow\" \"thequickbrown\" [16]\"thequickbrown\" \"thequickbrownf\" \"thequickbrownfo\" [19]\"thequickbrownfox\" \"thequickbrownfox\" Thesecondargumentinsubstristhenumberofthecharacteratwhichextractionisto begin(inthiscasealwaysthefirst),andthethirdargumentisthenumberofthecharacterat whichextractionistoend(inthiscase,theith).Tosplitupacharacterstringintoindividual characters, we use strsplit like this: strsplit(phrase,split=character(0)) [[1]] [1] \"t\"\"h\"\"e\"\"\"\"q\"\"u\"\"i\"\"c\"\"k\"\"\"\"b\"\"r\"\"o\"\"w\"\"n\"\"\" [17] \"f\"\"o\"\"x\"\"\"\"j\"\"u\"\"m\"\"p\"\"s\"\"\"\"o\"\"v\"\"e\"\"r\" [31] \"\"\"t\"\"h\"\"e\"\"\"\"l\"\"a\"\"z\"\"y\"\"\"\"d\"\"o\"\"g\" The table function is useful for counting the number of occurrences of characters of different kinds: table(strsplit(phrase,split=character(0))) abcdefghijklmnopqrstuvwxyz 811113112111111411212211111 Thisdemonstratesthatallofthelettersofthealphabetwereusedatleastoncewithinour phrase,andthattherewereeightblankswithinphrase.Thissuggestsawayofcountingthe numberofwordsinaphrase,given that thiswillalwaysbeonemore than thenumberof blanks: words<-1+table(strsplit(phrase,split=character(0)))[1] words 9 310 STATISTICS:ANINTRODUCTIONUSINGR It is easy to switch between upper and lower cases using the toupper and tolower functions: toupper(phrase) [1]\"THEQUICKBROWNFOXJUMPSOVERTHELAZYDOG\" tolower(toupper(phrase)) [1]\"thequickbrownfoxjumpsoverthelazydog\" WritingFunctionsinR FunctionsinRareobjectsthatcarryoutoperationsonargumentsthataresuppliedtothem and return one or more values."
  },
  {
    "input": "The syntax for writing a function is function (argument list) body Thefirstcomponentofthefunctiondeclarationisthekeywordfunctionwhichindicates toRthatyouwanttocreateafunction.Anargumentlistisacomma-separatedlistofformal arguments.Aformalargumentcanbeasymbol(i.e.avariablenamelikexory),astatement oftheformsymbol=expression(e.g.pch=16)orthespecialformalargument...(dot dot dot).",
    "target": "Q: What is The syntax for writing a function? A: The syntax for writing a function is function (argument list) body Thefirstcomponentofthefunctiondeclarationisthekeywordfunctionwhichindicates toRthatyouwanttocreateafunction.Anargumentlistisacomma-separatedlistofformal arguments.Aformalargumentcanbeasymbol(i.e.avariablenamelikexory),astatement oftheformsymbol=expression(e.g.pch=16)orthespecialformalargument...(dot dot dot)."
  },
  {
    "input": "Generally, the body is a group of expressions contained in curly brackets { } with each expression on a separateline.Functionsaretypicallyassignedtosymbols,buttheydonotneedtobe.This will only begin to mean anything after you have seen several examples in operation.",
    "target": "Q: What is Generally, the body? A: Generally, the body is a group of expressions contained in curly brackets { } with each expression on a separateline.Functionsaretypicallyassignedtosymbols,buttheydonotneedtobe.This will only begin to mean anything after you have seen several examples in operation."
  },
  {
    "input": "ArithmeticMeanofaSingleSample P P The mean is the sum of the numbers y divided by the number of numbers n 1 (summing over the number of numbers in the vector called y).",
    "target": "Q: What is ArithmeticMeanofaSingleSample P P The mean? A: ArithmeticMeanofaSingleSample P P The mean is the sum of the numbers y divided by the number of numbers n 1 (summing over the number of numbers in the vector called y)."
  },
  {
    "input": "The R function for n is P length(y) and for y is sum(y), so a function to compute arithmetic means is: arithmetic.mean<-function(x) sum(x)/length(x) We should test the function with some data where we know the right answer: y<-c(3,3,4,5,5) arithmetic.mean(y) [1]4 Needless to say, there is a built- in function for arithmetic means called mean: mean(y) [1]4 MedianofaSingleSample The median (or 50th percentile) is the middle value of the sorted values of a vector of numbers: sort(y)[ceiling(length(y)/2) APPENDIX: ESSENTIALS OF THE RLANGUAGE 311 There is slight hitch here, of course, because if the vector contains an even number of numbers,thenthereisnomiddlevalue.Thelogicisthatweneedtoworkoutthearithmetic averageofthetwovaluesofyoneithersideofthemiddle.Thequestionnowarisesastohow weknow,ingeneral,whetherthevectorycontainsanoddoranevennumberofnumbers,so thatwecandecidewhichofthetwomethodstouse.Thetrickhereistousemodulo2(p.46).",
    "target": "Q: What is The R function for n? A: The R function for n is P length(y) and for y is sum(y), so a function to compute arithmetic means is: arithmetic.mean<-function(x) sum(x)/length(x) We should test the function with some data where we know the right answer: y<-c(3,3,4,5,5) arithmetic.mean(y) [1]4 Needless to say, there is a built- in function for arithmetic means called mean: mean(y) [1]4 MedianofaSingleSample The median (or 50th percentile) is the middle value of the sorted values of a vector of numbers: sort(y)[ceiling(length(y)/2) APPENDIX: ESSENTIALS OF THE RLANGUAGE 311 There is slight hitch here, of course, because if the vector contains an even number of numbers,thenthereisnomiddlevalue.Thelogicisthatweneedtoworkoutthearithmetic averageofthetwovaluesofyoneithersideofthemiddle.Thequestionnowarisesastohow weknow,ingeneral,whetherthevectorycontainsanoddoranevennumberofnumbers,so thatwecandecidewhichofthetwomethodstouse.Thetrickhereistousemodulo2(p.46)."
  },
  {
    "input": "Nowwehaveallthetoolsweneedtowriteageneralfunctiontocalculatemedians.Letus call the function med and define it like this: med<-function(x){ odd.even<-length(x)%%2 if(odd.even==0) (sort(x)[length(x)/2]+sort(x)[1+length(x)/2])/2 else sort(x)[ceiling(length(x)/2)] } Noticethatwhentheifstatementistrue(i.e.wehaveanevennumberofnumbers)then the expression immediately following the if function is evaluated (this is the code for calculatingthemedianwith anevennumberofnumbers).",
    "target": "Q: What is Nowwehaveallthetoolsweneedtowriteageneralfunctiontocalculatemedians.Letus call the function med and define it like this: med<-function(x){ odd.even<-length(x)%%2 if(odd.even==0) (sort(x)[length(x)/2]+sort(x)[1+length(x)/2])/2 else sort(x)[ceiling(length(x)/2)] } Noticethatwhentheifstatementistrue(i.e.wehaveanevennumberofnumbers)then the expression immediately following the if function? A: Nowwehaveallthetoolsweneedtowriteageneralfunctiontocalculatemedians.Letus call the function med and define it like this: med<-function(x){ odd.even<-length(x)%%2 if(odd.even==0) (sort(x)[length(x)/2]+sort(x)[1+length(x)/2])/2 else sort(x)[ceiling(length(x)/2)] } Noticethatwhentheifstatementistrue(i.e.wehaveanevennumberofnumbers)then the expression immediately following the if function is evaluated (this is the code for calculatingthemedianwith anevennumberofnumbers)."
  },
  {
    "input": "we have an odd number of numbers, and odd.even == 1) then the expression followingtheelsefunctionisevaluated(thisisthecodeforcalculatingthemedianwithan oddnumberofnumbers).Letustryitout,firstwiththeodd-numberedvectory,thenwith theeven-numberedvectory[-1],afterthefirstelementofy(whichisy[1]=3)hasbeen dropped (using the negative subscript): med(y) [1]4 med(y[-1]) [1]4.5 Again,youwontbesurprisedthatthereisabuilt-infunctionforcalculatingmedians,and helpfully it is called median.",
    "target": "Q: What is we have an odd number of numbers, and odd.even == 1) then the expression followingtheelsefunctionisevaluated(thisisthecodeforcalculatingthemedianwithan oddnumberofnumbers).Letustryitout,firstwiththeodd-numberedvectory,thenwith theeven-numberedvectory[-1],afterthefirstelementofy(whichisy[1]=3)hasbeen dropped (using the negative subscript): med(y) [1]4 med(y[-1]) [1]4.5 Again,youwontbesurprisedthatthereisabuilt-infunctionforcalculatingmedians,and helpfully it? A: we have an odd number of numbers, and odd.even == 1) then the expression followingtheelsefunctionisevaluated(thisisthecodeforcalculatingthemedianwithan oddnumberofnumbers).Letustryitout,firstwiththeodd-numberedvectory,thenwith theeven-numberedvectory[-1],afterthefirstelementofy(whichisy[1]=3)hasbeen dropped (using the negative subscript): med(y) [1]4 med(y[-1]) [1]4.5 Again,youwontbesurprisedthatthereisabuilt-infunctionforcalculatingmedians,and helpfully it is called median."
  },
  {
    "input": "LoopsandRepeats Theclassic,Fortran-likeloopisavailableinR.Thesyntaxisalittledifferent,buttheideais identical;yourequestthatanindex,i,takesonasequenceofvalues,andthatoneormore linesofcommandsareexecutedasmanytimesastherearedifferentvaluesofi.Hereisa loop executed five times with the values of i: we print the square of each value for(iin1:5)print(i^2) [1]1 [1]4 [1]9 [1]16 [1]25 312 STATISTICS:ANINTRODUCTIONUSINGR Formultiplelinesofcode,youusecurlybrackets{}toenclosematerialoverwhichthe loopistowork.Notethatthehardreturn(theEnterkey)attheendofeachcommandlineis anessentialpartofthestructure(youcanreplacethehardreturnsbysemicolonsifyoulike, but clarity is improved if you put each command on a separate line).",
    "target": "Q: What is LoopsandRepeats Theclassic,Fortran-likeloopisavailableinR.Thesyntaxisalittledifferent,buttheideais identical;yourequestthatanindex,i,takesonasequenceofvalues,andthatoneormore linesofcommandsareexecutedasmanytimesastherearedifferentvaluesofi.Hereisa loop executed five times with the values of i: we print the square of each value for(iin1:5)print(i^2) [1]1 [1]4 [1]9 [1]16 [1]25 312 STATISTICS:ANINTRODUCTIONUSINGR Formultiplelinesofcode,youusecurlybrackets{}toenclosematerialoverwhichthe loopistowork.Notethatthehardreturn(theEnterkey)attheendofeachcommandlineis anessentialpartofthestructure(youcanreplacethehardreturnsbysemicolonsifyoulike, but clarity? A: LoopsandRepeats Theclassic,Fortran-likeloopisavailableinR.Thesyntaxisalittledifferent,buttheideais identical;yourequestthatanindex,i,takesonasequenceofvalues,andthatoneormore linesofcommandsareexecutedasmanytimesastherearedifferentvaluesofi.Hereisa loop executed five times with the values of i: we print the square of each value for(iin1:5)print(i^2) [1]1 [1]4 [1]9 [1]16 [1]25 312 STATISTICS:ANINTRODUCTIONUSINGR Formultiplelinesofcode,youusecurlybrackets{}toenclosematerialoverwhichthe loopistowork.Notethatthehardreturn(theEnterkey)attheendofeachcommandlineis anessentialpartofthestructure(youcanreplacethehardreturnsbysemicolonsifyoulike, but clarity is improved if you put each command on a separate line)."
  },
  {
    "input": "Here is a function that uses the whilefunction in converting a specified number to its binaryrepresentation.Thetrickisthatthesmallestdigit(0forevenor1foroddnumbers)is always at the right-hand side of the answer (in location 32 in this case): binary<-function(x) { if(x==0)return(0) i<-0 string<-numeric(32) while(x>0) { string[32-i]<-x%%2 x<-x%/%2 i<-i+1 } first<-match(1,string) string[first:32] } Theleadingzeros(1tofirst1)withinthestringarenotprinted.Werunthefunctionto find the binary representation of the numbers 15 through 17: sapply(15:17,binary) [[1]] [1] 1 1 1 1 [[2]] [1] 10000 [[3]] [1] 10001 TheifelseFunction Sometimes you want to do one thing if a condition is true and a different thing if the condition is false (rather than do nothing, as in the last example).",
    "target": "Q: What is Here? A: Here is a function that uses the whilefunction in converting a specified number to its binaryrepresentation.Thetrickisthatthesmallestdigit(0forevenor1foroddnumbers)is always at the right-hand side of the answer (in location 32 in this case): binary<-function(x) { if(x==0)return(0) i<-0 string<-numeric(32) while(x>0) { string[32-i]<-x%%2 x<-x%/%2 i<-i+1 } first<-match(1,string) string[first:32] } Theleadingzeros(1tofirst1)withinthestringarenotprinted.Werunthefunctionto find the binary representation of the numbers 15 through 17: sapply(15:17,binary) [[1]] [1] 1 1 1 1 [[2]] [1] 10000 [[3]] [1] 10001 TheifelseFunction Sometimes you want to do one thing if a condition is true and a different thing if the condition is false (rather than do nothing, as in the last example)."
  },
  {
    "input": "The ifelse function allowsyoutodothisforentirevectorswithoutusingforloops.Wemightwanttoreplace any negative values of y by \u00001 and any positive values and zero by +1: z<-ifelse(y<0, -1,1) EvaluatingFunctionswithapply The apply function is used for applying functions to the rows (subscript 1) or columns (subscript 2) of matrices or dataframes: (X<-matrix(1:24,nrow=4)) [,1][,2][,3][,4][,5][,6] [1,] 1 5 9 13 17 21 [2,] 2 6 10 14 18 22 [3,] 3 7 11 15 19 23 [4,] 4 8 12 16 20 24 APPENDIX: ESSENTIALS OF THE RLANGUAGE 313 Here are the row totals (four of them) across margin 1 of the matrix: apply(X,1,sum) [1]66727884 Here are the column totals (six of them) across margin 2 of the matrix: apply(X,2,sum) [1]102642587490 Notethatinbothcases,theanswerproducedbyapplyisavectorratherthanamatrix.",
    "target": "Q: What is The ifelse function allowsyoutodothisforentirevectorswithoutusingforloops.Wemightwanttoreplace any negative values of y by \u00001 and any positive values and zero by +1: z<-ifelse(y<0, -1,1) EvaluatingFunctionswithapply The apply function? A: The ifelse function allowsyoutodothisforentirevectorswithoutusingforloops.Wemightwanttoreplace any negative values of y by \u00001 and any positive values and zero by +1: z<-ifelse(y<0, -1,1) EvaluatingFunctionswithapply The apply function is used for applying functions to the rows (subscript 1) or columns (subscript 2) of matrices or dataframes: (X<-matrix(1:24,nrow=4)) [,1][,2][,3][,4][,5][,6] [1,] 1 5 9 13 17 21 [2,] 2 6 10 14 18 22 [3,] 3 7 11 15 19 23 [4,] 4 8 12 16 20 24 APPENDIX: ESSENTIALS OF THE RLANGUAGE 313 Here are the row totals (four of them) across margin 1 of the matrix: apply(X,1,sum) [1]66727884 Here are the column totals (six of them) across margin 2 of the matrix: apply(X,2,sum) [1]102642587490 Notethatinbothcases,theanswerproducedbyapplyisavectorratherthanamatrix."
  },
  {
    "input": "numeric function geometric<-function(x){ if(!is.numeric(x)) stop(\"Inputmustbenumeric\") exp(mean(log(x))) } Here is what happens when you try to work out the geometric mean of character data: geometric(c(\"a\",\"b\",\"c\")) Erroringeometric(c(\"a\",\"b\",\"c\")):Inputmustbenumeric Youmightalsowanttocheckthattherearenozeros ornegativenumbers intheinput, because it would make no sense to try to calculate a geometric mean of such data: geometric<-function(x){ if(!is.numeric(x)) stop(\"Inputmustbenumeric\") if(min(x)<=0)stop(\"Inputmustbegreaterthanzero\") exp(mean(log(x))) } Testing this: geometric(c(2,3,0,4)) Erroringeometric(c(2,3,0,4)):Inputmustbegreaterthanzero But when the data are OK there will be no messages, just the numeric answer: geometric(c(10,1000,10,1,1)) [1]10 DatesandTimesinR Themeasurementoftimeishighlyidiosyncratic.Successiveyearsstartondifferentdaysof theweek.Therearemonthswithdifferentnumbersofdays.Leapyearshaveanextradayin February.AmericansandBritonsputthedayandthemonthindifferentplaces:3/4/2006is4 Marchinonecaseand3Aprilinanother.Occasionalyearshaveanadditionalleapsecond addedtothembecausefrictionfromthetidesisslowingdowntherotationoftheearthfrom when the standard time was set on the basis of the tropical year in 1900.",
    "target": "Q: What is numeric function geometric<-function(x){ if(!is.numeric(x)) stop(\"Inputmustbenumeric\") exp(mean(log(x))) } Here? A: numeric function geometric<-function(x){ if(!is.numeric(x)) stop(\"Inputmustbenumeric\") exp(mean(log(x))) } Here is what happens when you try to work out the geometric mean of character data: geometric(c(\"a\",\"b\",\"c\")) Erroringeometric(c(\"a\",\"b\",\"c\")):Inputmustbenumeric Youmightalsowanttocheckthattherearenozeros ornegativenumbers intheinput, because it would make no sense to try to calculate a geometric mean of such data: geometric<-function(x){ if(!is.numeric(x)) stop(\"Inputmustbenumeric\") if(min(x)<=0)stop(\"Inputmustbegreaterthanzero\") exp(mean(log(x))) } Testing this: geometric(c(2,3,0,4)) Erroringeometric(c(2,3,0,4)):Inputmustbegreaterthanzero But when the data are OK there will be no messages, just the numeric answer: geometric(c(10,1000,10,1,1)) [1]10 DatesandTimesinR Themeasurementoftimeishighlyidiosyncratic.Successiveyearsstartondifferentdaysof theweek.Therearemonthswithdifferentnumbersofdays.Leapyearshaveanextradayin February.AmericansandBritonsputthedayandthemonthindifferentplaces:3/4/2006is4 Marchinonecaseand3Aprilinanother.Occasionalyearshaveanadditionalleapsecond addedtothembecausefrictionfromthetidesisslowingdowntherotationoftheearthfrom when the standard time was set on the basis of the tropical year in 1900."
  },
  {
    "input": "All these things mean that working with dates and times is excruciatingly complicated.",
    "target": "Q: What is All these things mean that working with dates and times? A: All these things mean that working with dates and times is excruciatingly complicated."
  },
  {
    "input": "Now obviously, there is a lot going on here.",
    "target": "Q: What is Now obviously, there? A: Now obviously, there is a lot going on here."
  },
  {
    "input": "Itis hard to remember these acronyms, but it is well worth making the effort.",
    "target": "Q: What is Itis hard to remember these acronyms, but it? A: Itis hard to remember these acronyms, but it is well worth making the effort."
  },
  {
    "input": "Finally, there is a logical variable isdst which asks whether daylight saving time is in operation (0=FALSE in this case because we are on Greenwich Mean Time).",
    "target": "Q: What is Finally, there? A: Finally, there is a logical variable isdst which asks whether daylight saving time is in operation (0=FALSE in this case because we are on Greenwich Mean Time)."
  },
  {
    "input": "We start by reading the Excel dates into a dataframe: data<-read.table(\"c:\\\\temp\\\\date.txt\",header=T) head(data) x date 1 3 15/06/2014 2 1 16/06/2014 3 6 17/06/2014 4 7 18/06/2014 5 8 19/06/2014 6 9 20/06/2014 As things stand, the date is not recognized by R: class(date) [1]\"factor\" 318 STATISTICS:ANINTRODUCTIONUSINGR The strptime function takes the name of the factor (date) and a format statement showingexactlywhateachelementofthecharacterstringrepresentsandwhatsymbolsare usedasseparators(forwardslashesinthiscase).Inourexample,thedayofthemonthcomes first(%d)thenaslash,thenthemonth(%m)thenanotherslash,thentheyearinfull(%Y;ifthe yearwasjustthelasttwodigits15thentheappropriatecodewouldbe%yinlowercase).",
    "target": "Q: What is We start by reading the Excel dates into a dataframe: data<-read.table(\"c:\\\\temp\\\\date.txt\",header=T) head(data) x date 1 3 15/06/2014 2 1 16/06/2014 3 6 17/06/2014 4 7 18/06/2014 5 8 19/06/2014 6 9 20/06/2014 As things stand, the date? A: We start by reading the Excel dates into a dataframe: data<-read.table(\"c:\\\\temp\\\\date.txt\",header=T) head(data) x date 1 3 15/06/2014 2 1 16/06/2014 3 6 17/06/2014 4 7 18/06/2014 5 8 19/06/2014 6 9 20/06/2014 As things stand, the date is not recognized by R: class(date) [1]\"factor\" 318 STATISTICS:ANINTRODUCTIONUSINGR The strptime function takes the name of the factor (date) and a format statement showingexactlywhateachelementofthecharacterstringrepresentsandwhatsymbolsare usedasseparators(forwardslashesinthiscase).Inourexample,thedayofthemonthcomes first(%d)thenaslash,thenthemonth(%m)thenanotherslash,thentheyearinfull(%Y;ifthe yearwasjustthelasttwodigits15thentheappropriatecodewouldbe%yinlowercase)."
  },
  {
    "input": "For instance, what isthe mean valueofxforeachdayoftheweek?Youneedtoknowthatthenameofthedayoftheweek within the list called Rdate is wday, then tapply(x,Rdate$wday,mean) 0 1 2 3 4 5 6 5.660 2.892 5.092 7.692 8.692 9.692 8.892 The value was lowest on Mondays (day 1) and highest on Fridays (day 5).",
    "target": "Q: What is For instance, what isthe mean valueofxforeachdayoftheweek?Youneedtoknowthatthenameofthedayoftheweek within the list called Rdate? A: For instance, what isthe mean valueofxforeachdayoftheweek?Youneedtoknowthatthenameofthedayoftheweek within the list called Rdate is wday, then tapply(x,Rdate$wday,mean) 0 1 2 3 4 5 6 5.660 2.892 5.092 7.692 8.692 9.692 8.892 The value was lowest on Mondays (day 1) and highest on Fridays (day 5)."
  },
  {
    "input": "Here is the data file: time<-read.csv(\"c:\\\\temp\\\\times.csv\") attach(time) head(time) hrs min sec experiment 1 2 23 6 A 2 3 16 17 A 3 3 2 56 A 4 2 45 0 A 5 3 4 42 A 6 2 56 25 A Create a vector of times y using paste with a colon separator: y<-paste(hrs,min,sec,sep=\":\") y [1]\"2:23:6\" \"3:16:17\" \"3:2:56\" \"2:45:0\" \"3:4:42\" \"2:56:25\" [7]\"3:12:28\" \"1:57:12\" \"2:22:22\" \"1:42:7\" \"2:31:17\" \"3:15:16\" [13]\"2:28:4\" \"1:55:34\" \"2:17:7\" \"1:48:48\" YouthenneedtoconvertthesecharacterstringsintosomethingthatRwillrecognizeasa dateand/oratime.IfyouusestrptimethenRwillautomaticallyaddtodaysdatetothe APPENDIX: ESSENTIALS OF THE RLANGUAGE 319 POSIXctobject.Notethat\"%T\"isashortcutfor\"%H:%M:%S\".Formoredetailsandafull list of codes, see The R Book (Crawley, 2013).",
    "target": "Q: What is Here? A: Here is the data file: time<-read.csv(\"c:\\\\temp\\\\times.csv\") attach(time) head(time) hrs min sec experiment 1 2 23 6 A 2 3 16 17 A 3 3 2 56 A 4 2 45 0 A 5 3 4 42 A 6 2 56 25 A Create a vector of times y using paste with a colon separator: y<-paste(hrs,min,sec,sep=\":\") y [1]\"2:23:6\" \"3:16:17\" \"3:2:56\" \"2:45:0\" \"3:4:42\" \"2:56:25\" [7]\"3:12:28\" \"1:57:12\" \"2:22:22\" \"1:42:7\" \"2:31:17\" \"3:15:16\" [13]\"2:28:4\" \"1:55:34\" \"2:17:7\" \"1:48:48\" YouthenneedtoconvertthesecharacterstringsintosomethingthatRwillrecognizeasa dateand/oratime.IfyouusestrptimethenRwillautomaticallyaddtodaysdatetothe APPENDIX: ESSENTIALS OF THE RLANGUAGE 319 POSIXctobject.Notethat\"%T\"isashortcutfor\"%H:%M:%S\".Formoredetailsandafull list of codes, see The R Book (Crawley, 2013)."
  },
  {
    "input": "Once they are POSIXlt objects, it is straightforward to calculate means, differences and so on.",
    "target": "Q: What is Once they are POSIXlt objects, it? A: Once they are POSIXlt objects, it is straightforward to calculate means, differences and so on."
  },
  {
    "input": "Here we want to calculate the number of days between two dates, 22 October 2003 and 22 October 2005: y2<-as.POSIXlt(\"2018-10-22\") y1<-as.POSIXlt(\"2015-10-22\") 320 STATISTICS:ANINTRODUCTIONUSINGR Now you can do calculations with the two dates: y2-y1 Timedifferenceof1096days Notethatyoucannotaddtwodates.Itiseasytocalculatedifferencesbetweentimesusing thissystem.Notethatthedatesareseparatedbyhyphenswhereasthetimesareseparatedby colons: y3<-as.POSIXlt(\"2018-10-2209:30:59\") y4<-as.POSIXlt(\"2018-10-2212:45:06\") y4-y3 Timedifferenceof3.235278hours Alternatively, you can do these calculations using the difftime function: difftime(\"2018-10-2212:45:06\",\"2018-10-2209:30:59\") Timedifferenceof3.235278hours Hereisanexampleoflogicaloperationondatetimeobjects.Wewanttoknowwhether y4 was later than y3: y4>y3 [1]TRUE UnderstandingtheStructureofanRObjectUsingstr We finish with a simple but important function for understanding what kind of object is representedbyagivennameinthecurrentRsession.Welookatthreeobjectsofincreasing complexity: a vector of numbers, a list of various classes and a linear model.",
    "target": "Q: What is Here we want to calculate the number of days between two dates, 22 October 2003 and 22 October 2005: y2<-as.POSIXlt(\"2018-10-22\") y1<-as.POSIXlt(\"2015-10-22\") 320 STATISTICS:ANINTRODUCTIONUSINGR Now you can do calculations with the two dates: y2-y1 Timedifferenceof1096days Notethatyoucannotaddtwodates.Itiseasytocalculatedifferencesbetweentimesusing thissystem.Notethatthedatesareseparatedbyhyphenswhereasthetimesareseparatedby colons: y3<-as.POSIXlt(\"2018-10-2209:30:59\") y4<-as.POSIXlt(\"2018-10-2212:45:06\") y4-y3 Timedifferenceof3.235278hours Alternatively, you can do these calculations using the difftime function: difftime(\"2018-10-2212:45:06\",\"2018-10-2209:30:59\") Timedifferenceof3.235278hours Hereisanexampleoflogicaloperationondatetimeobjects.Wewanttoknowwhether y4 was later than y3: y4>y3 [1]TRUE UnderstandingtheStructureofanRObjectUsingstr We finish with a simple but important function for understanding what kind of object? A: Here we want to calculate the number of days between two dates, 22 October 2003 and 22 October 2005: y2<-as.POSIXlt(\"2018-10-22\") y1<-as.POSIXlt(\"2015-10-22\") 320 STATISTICS:ANINTRODUCTIONUSINGR Now you can do calculations with the two dates: y2-y1 Timedifferenceof1096days Notethatyoucannotaddtwodates.Itiseasytocalculatedifferencesbetweentimesusing thissystem.Notethatthedatesareseparatedbyhyphenswhereasthetimesareseparatedby colons: y3<-as.POSIXlt(\"2018-10-2209:30:59\") y4<-as.POSIXlt(\"2018-10-2212:45:06\") y4-y3 Timedifferenceof3.235278hours Alternatively, you can do these calculations using the difftime function: difftime(\"2018-10-2212:45:06\",\"2018-10-2209:30:59\") Timedifferenceof3.235278hours Hereisanexampleoflogicaloperationondatetimeobjects.Wewanttoknowwhether y4 was later than y3: y4>y3 [1]TRUE UnderstandingtheStructureofanRObjectUsingstr We finish with a simple but important function for understanding what kind of object is representedbyagivennameinthecurrentRsession.Welookatthreeobjectsofincreasing complexity: a vector of numbers, a list of various classes and a linear model."
  },
  {
    "input": "x<-runif(23) str(x) num[1:23]0.9710.230.6450.6970.537... We see that x is a number (num) in a vector of length 23 ([1:23]) with the first five values as shown (0.971, .",
    "target": "Q: What is x<-runif(23) str(x) num[1:23]0.9710.230.6450.6970.537... We see that x? A: x<-runif(23) str(x) num[1:23]0.9710.230.6450.6970.537... We see that x is a number (num) in a vector of length 23 ([1:23]) with the first five values as shown (0.971, ."
  },
  {
    "input": "In this example of intermediate complexity the variable called basket is a list: basket<-list(rep(\"a\",4),c(\"b0\",\"b1\",\"b2\"),9:4,gl(5,3)) basket [[1]] [1] \"a\"\"a\"\"a\"\"a\" [[2]] [1] \"b0\"\"b1\"\"b2\" [[3]] [1] 987654 [[4]] [1] 11 1 2 2 2 3 3 3 4 4 4 5 5 5 Levels:1 2 3 4 5 APPENDIX: ESSENTIALS OF THE RLANGUAGE 321 The first element isa character vector of length 4 (all \"a\"), while thesecond has three differentcharacterstrings.Thethirdisanumericvectorandthefourthisafactoroflength 12 with four levels.",
    "target": "Q: What is In this example of intermediate complexity the variable called basket? A: In this example of intermediate complexity the variable called basket is a list: basket<-list(rep(\"a\",4),c(\"b0\",\"b1\",\"b2\"),9:4,gl(5,3)) basket [[1]] [1] \"a\"\"a\"\"a\"\"a\" [[2]] [1] \"b0\"\"b1\"\"b2\" [[3]] [1] 987654 [[4]] [1] 11 1 2 2 2 3 3 3 4 4 4 5 5 5 Levels:1 2 3 4 5 APPENDIX: ESSENTIALS OF THE RLANGUAGE 321 The first element isa character vector of length 4 (all \"a\"), while thesecond has three differentcharacterstrings.Thethirdisanumericvectorandthefourthisafactoroflength 12 with four levels."
  },
  {
    "input": "Here is what you get from str: str(basket) Listof4 $:chr[1:4]\"a\"\"a\"\"a\"\"a\" $:chr[1:3]\"b0\"\"b1\"\"b2\" $:int[1:6]987654 $:Factorw/5levels\"1\",\"2\",\"3\",\"4\",..:1112223334... Firstyouseethelengthofthelist(4)thentheattributesofeachofthefourcomponents: thefirsttwoarecharacterstrings(chr),thethirdcontainsintegers(int)andthefourthisa factor with five levels.",
    "target": "Q: What is Here? A: Here is what you get from str: str(basket) Listof4 $:chr[1:4]\"a\"\"a\"\"a\"\"a\" $:chr[1:3]\"b0\"\"b1\"\"b2\" $:int[1:6]987654 $:Factorw/5levels\"1\",\"2\",\"3\",\"4\",..:1112223334... Firstyouseethelengthofthelist(4)thentheattributesofeachofthefourcomponents: thefirsttwoarecharacterstrings(chr),thethirdcontainsintegers(int)andthefourthisa factor with five levels."
  }
]