Statistics
Statistics
An Introduction Using R
Second Edition
Michael J. Crawley
ImperialCollegeLondon,UK
Thiseditionfirstpublished2015
2015JohnWiley&Sons,Ltd
Registeredoffice
JohnWiley&SonsLtd,TheAtrium,SouthernGate,Chichester,WestSussex,PO198SQ,UnitedKingdom
Fordetailsofourglobaleditorialoffices,forcustomerservicesandforinformationabouthowtoapplyfor
permissiontoreusethecopyrightmaterialinthisbookpleaseseeourwebsiteatwww.wiley.com.
Therightoftheauthortobeidentifiedastheauthorofthisworkhasbeenassertedinaccordancewiththe
Copyright,DesignsandPatentsAct1988.
Allrightsreserved.Nopartofthispublicationmaybereproduced,storedinaretrievalsystem,ortransmitted,in
anyformorbyanymeans,electronic,mechanical,photocopying,recordingorotherwise,exceptaspermittedby
theUKCopyright,DesignsandPatentsAct1988,withoutthepriorpermissionofthepublisher.
Wileyalsopublishesitsbooksinavarietyofelectronicformats.Somecontentthatappearsinprintmaynotbe
availableinelectronicbooks.
Designationsusedbycompaniestodistinguishtheirproductsareoftenclaimedastrademarks.Allbrandnames
andproductnamesusedinthisbookaretradenames,servicemarks,trademarksorregisteredtrademarksoftheir
respectiveowners.Thepublisherisnotassociatedwithanyproductorvendormentionedinthisbook.
LimitofLiability/DisclaimerofWarranty:Whilethepublisherandauthorhaveusedtheirbesteffortsinpreparing
thisbook,theymakenorepresentationsorwarrantieswithrespecttotheaccuracyorcompletenessofthecontents
ofthisbookandspecificallydisclaimanyimpliedwarrantiesofmerchantabilityorfitnessforaparticularpurpose.
Itissoldontheunderstandingthatthepublisherisnotengagedinrenderingprofessionalservicesandneitherthe
publishernortheauthorshallbeliablefordamagesarisingherefrom.Ifprofessionaladviceorotherexpert
assistanceisrequired,theservicesofacompetentprofessionalshouldbesought.
LibraryofCongressCataloging-in-PublicationData
Crawley,MichaelJ.
Statistics:anintroductionusingR/MichaelJ.Crawley.–Secondedition.
pagescm
Includesbibliographicalreferencesandindex.
ISBN978-1-118-94109-6(pbk.)
1. Mathematicalstatistics–Textbooks. 2. R(Computerprogramlanguage) I. Title.
QA276.12.C732015
519.50285'5133–dc23
2014024528
AcataloguerecordforthisbookisavailablefromtheBritishLibrary.
ISBN:9781118941096(pbk)
Setin10/12pt,TimesLTStd-RomanbyThomsonDigital,Noida,India.
1 2015
Contents
Preface xi
Chapter1 Fundamentals 1
EverythingVaries 2
Significance 3
GoodandBadHypotheses 3
NullHypotheses 3
pValues 3
Interpretation 4
ModelChoice 4
StatisticalModelling 5
MaximumLikelihood 6
ExperimentalDesign 7
ThePrincipleofParsimony(Occam’sRazor) 8
Observation,TheoryandExperiment 8
Controls 8
Replication:It’sthensthatJustifytheMeans 8
HowManyReplicates? 9
Power 9
Randomization 10
StrongInference 14
WeakInference 14
HowLongtoGoOn? 14
Pseudoreplication 15
InitialConditions 16
OrthogonalDesignsandNon-OrthogonalObservationalData 16
Aliasing 16
MultipleComparisons 17
SummaryofStatisticalModelsinR 18
OrganizingYourWork 19
HousekeepingwithinR 20
References 22
FurtherReading 22
vi CONTENTS
Chapter2 Dataframes 23
SelectingPartsofaDataframe:Subscripts 26
Sorting 27
SummarizingtheContentofDataframes 29
SummarizingbyExplanatoryVariables 30
FirstThingsFirst:GettoKnowYourData 31
Relationships 34
LookingforInteractionsbetweenContinuousVariables 36
GraphicstoHelpwithMultipleRegression 39
InteractionsInvolvingCategoricalVariables 39
FurtherReading 41
Chapter3 CentralTendency 42
FurtherReading 49
Chapter4 Variance 50
DegreesofFreedom 53
Variance 53
Variance:AWorkedExample 55
VarianceandSampleSize 58
UsingVariance 59
AMeasureofUnreliability 60
ConfidenceIntervals 61
Bootstrap 62
Non-constantVariance:Heteroscedasticity 65
FurtherReading 65
Chapter5 SingleSamples 66
DataSummaryintheOne-SampleCase 66
TheNormalDistribution 70
CalculationsUsingzoftheNormalDistribution 76
PlotsforTestingNormalityofSingleSamples 79
InferenceintheOne-SampleCase 81
BootstrapinHypothesisTestingwithSingleSamples 81
Student’stDistribution 82
Higher-OrderMomentsofaDistribution 83
Skew 84
Kurtosis 86
Reference 87
FurtherReading 87
CONTENTS vii
Chapter6 TwoSamples 88
ComparingTwoVariances 88
ComparingTwoMeans 90
Student’stTest 91
WilcoxonRank-SumTest 95
TestsonPairedSamples 97
TheBinomialTest 98
BinomialTeststoCompareTwoProportions 100
Chi-SquaredContingencyTables 100
Fisher’sExactTest 105
CorrelationandCovariance 108
CorrelationandtheVarianceofDifferencesbetweenVariables 110
Scale-DependentCorrelations 112
Reference 113
FurtherReading 113
Chapter7 Regression 114
LinearRegression 116
LinearRegressioninR 117
CalculationsInvolvedinLinearRegression 122
PartitioningSumsofSquaresinRegression:SSY=SSR+SSE 125
MeasuringtheDegreeofFit,r2 133
ModelChecking 134
Transformation 135
PolynomialRegression 140
Non-LinearRegression 142
GeneralizedAdditiveModels 146
Influence 148
FurtherReading 149
Chapter8 AnalysisofVariance 150
One-WayANOVA 150
ShortcutFormulas 157
EffectSizes 159
PlotsforInterpretingOne-WayANOVA 162
FactorialExperiments 168
Pseudoreplication:NestedDesignsandSplitPlots 173
Split-PlotExperiments 174
RandomEffectsandNestedDesigns 176
FixedorRandomEffects? 177
RemovingthePseudoreplication 178
AnalysisofLongitudinalData 178
DerivedVariableAnalysis 179
viii CONTENTS
DealingwithPseudoreplication 179
VarianceComponentsAnalysis(VCA) 183
References 184
FurtherReading 184
Chapter9 AnalysisofCovariance 185
FurtherReading 192
Chapter10 MultipleRegression 193
TheStepsInvolvedinModelSimplification 195
Caveats 196
OrderofDeletion 196
CarryingOutaMultipleRegression 197
ATrickierExample 203
FurtherReading 211
Chapter11 Contrasts 212
ContrastCoefficients 213
AnExampleofContrastsinR 214
APrioriContrasts 215
TreatmentContrasts 216
ModelSimplificationbyStepwiseDeletion 218
ContrastSumsofSquaresbyHand 222
TheThreeKindsofContrastsCompared 224
Reference 225
FurtherReading 225
Chapter12 OtherResponseVariables 226
IntroductiontoGeneralizedLinearModels 228
TheErrorStructure 229
TheLinearPredictor 229
FittedValues 230
AGeneralMeasureofVariability 230
TheLinkFunction 231
CanonicalLinkFunctions 232
Akaike’sInformationCriterion(AIC)asaMeasureoftheFitofaModel 233
FurtherReading 233
Chapter13 CountData 234
ARegressionwithPoissonErrors 234
AnalysisofDeviancewithCountData 237
CONTENTS ix
TheDangerofContingencyTables 244
AnalysisofCovariancewithCountData 247
FrequencyDistributions 250
FurtherReading 255
Chapter14 ProportionData 256
AnalysesofDataonOneandTwoProportions 257
AveragesofProportions 257
CountDataonProportions 257
Odds 259
OverdispersionandHypothesisTesting 260
Applications 261
LogisticRegressionwithBinomialErrors 261
ProportionDatawithCategoricalExplanatoryVariables 264
AnalysisofCovariancewithBinomialData 269
FurtherReading 272
Chapter15 BinaryResponseVariable 273
IncidenceFunctions 275
ANCOVAwithaBinaryResponseVariable 279
FurtherReading 284
Chapter16 DeathandFailureData 285
SurvivalAnalysiswithCensoring 287
FurtherReading 290
Appendix EssentialsoftheRLanguage 291
RasaCalculator 291
Built-inFunctions 292
NumberswithExponents 294
ModuloandIntegerQuotients 294
Assignment 295
Rounding 295
InfinityandThingsthatAreNotaNumber(NaN) 296
MissingValues(NA) 297
Operators 298
CreatingaVector 298
NamedElementswithinVectors 299
VectorFunctions 299
SummaryInformationfromVectorsbyGroups 300
SubscriptsandIndices 301
x CONTENTS
WorkingwithVectorsandLogicalSubscripts 301
AddresseswithinVectors 304
TrimmingVectorsUsingNegativeSubscripts 304
LogicalArithmetic 305
Repeats 305
GenerateFactorLevels 306
GeneratingRegularSequencesofNumbers 306
Matrices 307
CharacterStrings 309
WritingFunctionsinR 310
ArithmeticMeanofaSingleSample 310
MedianofaSingleSample 310
LoopsandRepeats 311
TheifelseFunction 312
EvaluatingFunctionswithapply 312
TestingforEquality 313
TestingandCoercinginR 314
DatesandTimesinR 315
CalculationswithDatesandTimes 319
UnderstandingtheStructureofanRObjectUsingstr 320
Reference 322
FurtherReading 322
Index 323
Preface
This book is an introduction to the essentials of statistical analysis for students who have
littleornobackgroundinmathematicsorstatistics.Theaudienceincludesfirst-andsecond-
yearundergraduatestudentsinscience,engineering,medicine andeconomics, alongwith
post-experienceandothermaturestudentswhowanttorelearntheirstatistics,ortoswitchto
the powerful new language of R.
Formanystudents,statisticsistheleastfavouritecourseoftheirentiretimeatuniversity.
Part of this isbecausesome students have convinced themselves that they areno good at
sums,andconsequentlyhavetriedtoavoidcontactwithanythingremotelyquantitativein
theirchoiceofsubjects.Theyaredismayed,therefore,whentheydiscoverthatthestatistics
courseiscompulsory.Anotherpartoftheproblemisthatstatisticsisoftentaughtbypeople
whohaveabsolutelynoideahowdifficultsomeofthematerialisfornon-statisticians.As
oftenasnot,thisleadstoarecipe-followingapproachtoanalysis,ratherthantoanyattempt
to understand the issues involved and how to deal with them.
Theapproachadoptedhereinvolvesvirtuallynostatisticaltheory.Instead,theassump-
tionsofthevariousstatisticalmodelsarediscussedatlength,andthepracticeofexposing
statisticalmodelstorigorouscriticismisencouraged.Aphilosophyofmodelsimplification
is developed in which the emphasis is placed on estimating effect sizes from data, and
establishing confidence intervals for these estimates. The role of hypothesis testing at an
arbitrary threshold of significance like α0:05 is played down. The text starts from
absolute basics and assumes absolutely no background in statistics or mathematics.
Astopresentation,theideaisthatbackgroundmaterialwouldbecoveredinaseriesof
1-hour lectures, then this book could be used as a guide to the practical sessions and for
homework,withthestudentsworkingontheirownatthecomputer.Myexperienceisthat
thematerialcanbecoveredin10–30lectures,dependingonthebackgroundofthestudents
and the depth of coverage it is hoped to achieve. The practical work is designed to be
covered in 10–15 sessions of about 1½ hours each, again depending on the ambition and
depthofthecoverage,andontheamountofone-to-onehelpavailabletothestudentsasthey
work at their computers.
TheRlanguageofstatisticalcomputinghasaninterestinghistory.ItevolvedfromtheS
language,whichwasfirstdevelopedattheAT&TBellLaboratoriesbyRickBecker,John
Chambers and Allan Wilks. Their idea was to provide a software tool for professional
statisticians who wanted to combine state-of-the-art graphics with powerful model-fitting
capability.Sismadeupofthreecomponents.Firstandforemost,itisapowerful toolfor
statisticalmodelling.Itenablesyoutospecifyandfitstatisticalmodelstoyourdata,assess
thegoodnessoffitanddisplaytheestimates,standarderrorsandpredictedvaluesderived
xii PREFACE
fromthemodel.Itprovidesyouwiththemeanstodefineandmanipulateyourdata,butthe
way you go about the job of modelling is not predetermined, and the user is left with
maximum controloverthemodel-fittingprocess.Second,Scanbeusedfordataexplora-
tion,intabulatingandsortingdata,indrawingscatterplotstolookfortrendsinyourdata,or
to check visually for the presence of outliers. Third, it can be used as a sophisticated
calculator to evaluate complex arithmetic expressions, and a very flexible and general
object-orientatedprogramminglanguagetoperformmoreextensivedatamanipulation.One
ofitsgreatstrengthsisinthewayinwhichitdealswithvectors(listsofnumbers).These
may be combined in general expressions, involving arithmetic, relational and transforma-
tional operators such as sums, greater-than tests, logarithms or probability integrals. The
ability to combine frequently-used sequences of commands into functions makes S a
powerful programming language, ideally suited for tailoring one’s specific statistical
requirements. S is especially useful in handling difficult or unusual data sets, because
itsflexibilityenablesittocopewithsuchproblemsasunequalreplication,missingvalues,
non-orthogonaldesigns,andsoon.Furthermore,theopen-endedstyleofSisparticularly
appropriateforfollowingthroughoriginalideasanddevelopingnewconcepts.Oneofthe
greatadvantagesoflearningSisthatthesimpleconceptsthatunderlieitprovideaunified
frameworkforlearningaboutstatisticalideasingeneral.Byviewingparticularmodelsina
generalcontext,Shighlightsthefundamentalsimilaritiesbetweenstatisticaltechniquesand
helps play down their superficial differences. As a commercial product S evolved into
S-PLUS,buttheproblemwasthatS-PLUSwasveryexpensive.Inparticular,itwasmuch
tooexpensivetobelicensedforuseinuniversitiesforteachinglargenumbersofstudents.In
response to this, two New Zealand-based statisticians, Ross Ihaka and Robert Gentleman
fromtheUniversityofAuckland,decidedtowriteastripped-downversionofSforteaching
purposes.TheletterR‘comesbeforeS’,sowhatwouldbemorenaturalthanfortwoauthors
whosefirstinitialwas‘R’tochristentheircreationR.ThecodeforRwasreleasedin1995
under a General Public License, and the core team was rapidly expanded to 15 members
(they are listed on the website, below). Version 1.0.0 was released on 29 February 2000.
This book is written using version 3.0.1, but all the code will run under earlier releases.
ThereisnowavastnetworkofRusersworld-wide,exchangingfunctionswithoneanother,
andavastresourceofpackagescontainingdataandprograms.Thereisausefulpublication
calledTheRJournal(formerlyRNews)thatyoucanreadatCRAN.Makesurethatyoucite
theRCoreTeamwhenyouuseRinpublishedwork;youshouldcitethemlikethis:
R Core Team (2014). R: A Language and Environment for Statistical Computing,
RFoundationforStatisticalComputing,Vienna.Availablefromhttp://www.r-project
.org/.
RisanOpenSourceimplementationandassuchcanbefreelydownloaded.Ifyoutype
CRAN into your Google window you will find the site nearest to you from which to
download it. Or you can go directly to
http://cran.r-project.org
The present book has its own website at
http://www.imperial.ac.uk/bio/research/crawley/statistics
PREFACE xiii
Hereyouwillfindallthedatafilesusedinthetext;youcandownloadthesetoyourhard
disk andthen runalloftheexamplesdescribedinthetext.Theexecutablestatementsare
showninthetextinredCourierNewfont.Therearefilescontainingallthecommandsfor
eachchapter,soyoucanpastethecodedirectlyintoRinsteadoftypingitfromthebook.
Thereisaseriesof12fully-workedstand-alonepracticalsessionscoveringawiderangeof
statistical analyses. Learning R is not easy, but you will not regret investing the effort to
master the basics.
M.J.Crawley
Ascot
April2014
1
Fundamentals
Thehardestpartofanystatisticalworkisgettingstarted.Andoneofthehardestthingsabout
gettingstartedischoosingtherightkindofstatisticalanalysis.Thechoicedependsonthe
natureofyourdataandontheparticularquestionyouaretryingtoanswer.Thetruthisthat
thereisnosubstituteforexperience:thewaytoknowwhattodoistohavedoneitproperly
lots of times before.
Thekeyistounderstandwhatkindofresponsevariableyouhavegot,andtoknowthe
natureofyourexplanatoryvariables.Theresponsevariableisthethingyouareworkingon:it
isthevariablewhosevariationyouareattemptingtounderstand.Thisisthevariablethatgoes
ontheyaxisofthegraph(theordinate).Theexplanatoryvariablegoesonthexaxisofthe
graph(theabscissa);youareinterestedintheextenttowhichvariationintheresponsevariable
is associated with variation in the explanatory variable. A continuous measurement is a
variablelikeheightorweightthatcantakeanyrealnumberedvalue.Acategoricalvariableisa
factorwithtwoormorelevels:sexisafactorwithtwolevels(maleandfemale),andrainbow
mightbeafactorwithsevenlevels(red,orange,yellow,green,blue,indigo,violet).
It is essential, therefore, that you know:
(cid:129) which of your variables is the response variable?
(cid:129) which are the explanatory variables?
(cid:129) are the explanatory variables continuous or categorical, or a mixture of both?
(cid:129) whatkindofresponsevariablehaveyougot–isitacontinuousmeasurement,acount,a
proportion, a time-at-death, or a category?
These simple keys will then lead you to the appropriate statistical method:
1. The explanatory variables (pick one of the rows):
(a)Allexplanatoryvariablescontinuous Regression
(b)Allexplanatoryvariablescategorical Analysisofvariance(ANOVA)
(c)Someexplanatoryvariablescontinuous Analysisofcovariance(ANCOVA)
somecategorical
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
2 STATISTICS:ANINTRODUCTIONUSINGR
2. The response variable (pick one of the rows):
(a)Continuous Regression,ANOVAorANCOVA
(b)Proportion Logisticregression
(c)Count Loglinearmodels
(d)Binary Binarylogisticanalysis
(e)Timeatdeath Survivalanalysis
Thereisasmallcoreofkeyideasthatneedtobeunderstoodfromtheoutset.Wecover
these here before getting into any detail about different kinds of statistical model.
EverythingVaries
If you measure the same thing twice you will get two differentanswers. If you measure
thesamethingondifferentoccasionsyouwillgetdifferentanswersbecausethethingwill
have aged. If you measure different individuals, they will differ for both genetic and
environmental reasons (nature and nurture). Heterogeneity is universal: spatial hetero-
geneity means that places always differ, and temporal heterogeneity means that times
always differ.
Becauseeverythingvaries,findingthatthingsvaryissimplynotinteresting.Weneeda
wayofdiscriminatingbetweenvariationthatisscientificallyinteresting,andvariationthat
justreflectsbackgroundheterogeneity.Thatiswhyyouneedstatistics.Itiswhatthiswhole
book is about.
The key concept is the amount of variation that we would expect to occur by chance
alone, when nothing scientifically interesting was going on. If we measure bigger differ-
encesthanwewouldexpectbychance,wesaythattheresultisstatisticallysignificant.Ifwe
measurenomorevariationthanwemightreasonablyexpecttooccurbychancealone,then
wesaythatourresultisnotstatisticallysignificant.Itisimportanttounderstandthatthisis
not to say that the result is not important. Non-significant differences in human life span
betweentwodrugtreatmentsmaybemassivelyimportant(especiallyifyouarethepatient
involved).Non-significantisnotthesameas‘notdifferent’.Thelackofsignificancemaybe
due simply to the fact that our replication is too low.
Ontheotherhand,whennothingreallyisgoingon,thenwewanttoknowthis.Itmakes
lifemuchsimplerifwecanbereasonablysurethatthereisnorelationshipbetweenyandx.
Some students think that ‘the only good result is a significant result’. They feel that their
study has somehow failed if it shows that ‘A has no significant effect on B’. This is an
understandablefailingofhumannature,butitisnotgoodscience.Thepointisthatwewant
toknowthetruth,onewayortheother.Weshouldtrynottocaretoomuchabouttheway
thingsturnout.Thisisnotanamoralstance,itjusthappenstobethewaythatscienceworks
best.Ofcourse,itishopelesslyidealistictopretendthatthisisthewaythatscientistsreally
behave.Scientistsoftenwantpassionatelythataparticularexperimentalresultwillturnout
tobestatisticallysignificant,sothattheycangetaNaturepaperandgetpromoted.Butthat
does not make it right.
FUNDAMENTALS 3
Significance
Whatdowemeanwhenwesaythataresultissignificant?Thenormaldictionarydefinitions
ofsignificantare‘havingorconveyingameaning’or‘expressive;suggestingorimplying
deeper or unstated meaning’. But in statistics we mean something very specific indeed.
We mean that ‘a result was unlikely to have occurred by chance’. In particular, we mean
‘unlikely to have occurred by chance if the null hypothesis was true’. So there are two
elementstoit:weneedtobeclearaboutwhatwemeanby‘unlikely’,andalsowhatexactly
we mean by the ‘null hypothesis’. Statisticians have an agreed convention about what
constitutes‘unlikely’.Theysaythataneventisunlikelyifitoccurslessthan5%ofthetime.
Ingeneral,thenullhypothesissaysthat‘nothingishappening’andthealternativesaysthat
‘something is happening’.
GoodandBadHypotheses
Karl Popper was the first to point out that a good hypothesis was one that was capable
of rejection. He argued that a good hypothesis is a falsifiable hypothesis. Consider the
following two assertions:
A. there are vultures in the local park
B. there are no vultures in the local park
Both involve the same essential idea, but one is refutable and the other is not. Ask
yourselfhowyouwouldrefuteoptionA.Yougooutintotheparkandyoulookforvultures.
Butyoudonotseeany.Ofcourse,thisdoesnotmeanthattherearenone.Theycouldhave
seenyoucoming,andhiddenbehindyou.Nomatterhowlongorhowhardyoulook,you
cannotrefutethehypothesis.Allyoucansayis‘IwentoutandIdidn’tseeanyvultures’.
Oneofthemostimportantscientificnotionsisthatabsenceofevidenceisnotevidenceof
absence.
Option B is fundamentally different. You reject hypothesis B the first time you see a
vultureinthepark.Untilthetimethatyoudoseeyourfirstvultureinthepark,youworkon
theassumptionthatthehypothesisistrue.Butifyouseeavulture,thehypothesisisclearly
false, so you reject it.
NullHypotheses
Thenullhypothesissays‘nothingishappening’.Forinstance,whenwearecomparingtwo
sample means, the null hypothesis is that the means of the two populations are the same.
Ofcourse,thetwosamplemeansarenotidentical,becauseeverythingvaries. Again,when
workingwithagraphofyagainstxinaregressionstudy,thenullhypothesisisthattheslope
oftherelationshipiszero(i.e.yisnotafunctionofx,oryisindependentofx).Theessential
pointisthatthenullhypothesisisfalsifiable.Werejectthenullhypothesiswhenourdata
show that the null hypothesis is sufficiently unlikely.
pValues
Hereweencounteramuch-misunderstoodtopic.Thepvalueisnottheprobabilitythatthe
nullhypothesisistrue,althoughyouwilloftenhearpeoplesayingthis.Infact,pvaluesare
4 STATISTICS:ANINTRODUCTIONUSINGR
calculatedontheassumptionthatthenullhypothesisistrue.Itiscorrecttosaythatpvalues
have to do with the plausibility of the null hypothesis, but in a rather subtle way.
As you will see later, we typically base our hypothesis testing on what are known as
test statistics: you may have heard of some of these already (Student’s t, Fisher’s F and
Pearson’s chi-squared, for instance): p values are about the size of the test statistic.
In particular, a p value is an estimate of the probability that a value of the test statistic,
oravaluemoreextremethanthis,couldhaveoccurredbychancewhenthenullhypothesis
istrue.Bigvaluesoftheteststatisticindicatethatthenullhypothesisisunlikelytobetrue.
Forsufficientlylargevaluesoftheteststatistic,werejectthenullhypothesisandacceptthe
alternative hypothesis.
Note also that saying ‘we do not reject the null hypothesis’ and ‘the null hypothesis is
true’aretwoquitedifferent things.Forinstance,wemay havefailed torejectafalsenull
hypothesisbecauseoursamplesizewastoolow,orbecauseourmeasurementerrorwastoo
large. Thus, p values are interesting, but they do not tell the whole story: effect sizes and
samplesizesareequallyimportantindrawingconclusions.Themodernpracticeistostate
thepvalueratherthanjusttosay‘werejectthenullhypothesis’.Thatway,thereadercan
form their own judgement about the effect size and its associated uncertainty.
Interpretation
Itshouldbeclearbythispointthatwecanmaketwokindsofmistakesintheinterpretation
of our statistical models:
(cid:129) we can reject the null hypothesis when it is true
(cid:129) we can accept the null hypothesis when it is false
ThesearereferredtoasTypeIandTypeIIerrors,respectively.Supposingweknewthe
true state of affairs (which, of course, we seldom do). Then in tabular form:
Nullhypothesis Actualsituation
True False
Accept Correctdecision TypeII
Reject TypeI Correctdecision
ModelChoice
Thereareagreatmanymodelsthatwecouldfittoourdata,andselectingwhichmodeltouse
involves considerable skill and experience. All models are wrong, but some models are
better than others. Model choice is one of the most frequently ignored of the big issues
involved in learning statistics.
In the past, elementary statistics was taught as a series of recipes that you followed
withouttheneedforanythought.Thiscausedtwobigproblems.Peoplewhoweretaught
thiswayneverrealizedthatmodelchoiceisareallybigdeal(‘I’monlytryingtodoattest’).
Andtheyneverunderstoodthatassumptionsneedtobechecked(‘allIneedisthepvalue’).
FUNDAMENTALS 5
Throughout this book you are encouraged to learn the key assumptions. In order of
importance, these are
(cid:129) random sampling
(cid:129) constant variance
(cid:129) normal errors
(cid:129) independent errors
(cid:129) additive effects
Crucially, because these assumptions are often not met with the kinds of data that we
encounterinpractice,weneedtoknowwhattodoaboutit.Therearesomethingsthatitis
muchmoredifficulttodoanythingabout(e.g.non-randomsampling)thanothers(e.g.non-
additive effects).
The book also encourages users to understand that in most cases there are literally
hundreds of possible models, and that choosing the best model is an essential part of the
processofstatisticalanalysis.Whichexplanatoryvariablestoincludeinyourmodel,what
transformationtoapplytoeachvariable,whethertoincludeinteractionterms:allofthese
are key issues that you need to resolve.
The issuesareattheirsimplestwith designed manipulative experimentsinwhich there
was thorough randomization and good levels of replication. The issues are most difficult
with observational studies where there are large numbers of (possibly correlated) explan-
atoryvariables,littleornorandomizationandsmallnumbersofdatapoints.Muchofyour
data is likely to come from the second category.
StatisticalModelling
Theobjectistodeterminethevaluesoftheparametersinaspecificmodelthatleadtothe
best fit of the model to the data. The data are sacrosanct, and they tell us what actually
happenedunderagivensetofcircumstances.Itisacommonmistaketosay‘thedatawere
fittedtothemodel’asifthedataweresomethingflexible,andwehadaclearpictureofthe
structure of the model. On the contrary, what we are looking for is the minimal adequate
modeltodescribethedata.Themodelisfittedtodata,nottheotherwayaround.Thebest
model is the model that produces the least unexplained variation (the minimal residual
deviance),subjecttotheconstraintthattheparametersinthemodelshouldallbestatistically
significant.
You have to specify the model. It embodies your mechanistic understanding of the
factorsinvolved,andofthewaythattheyarerelatedtotheresponsevariable.Wewantthe
modeltobeminimalbecauseoftheprincipleofparsimony,andadequatebecausethereis
nopointinretaininganinadequatemodelthatdoesnotdescribeasignificantfractionof
thevariationinthedata.Itisveryimportanttounderstandthatthereisnotonemodel;this
is one of the common implicit errors involved in traditional regression and ANOVA,
where the same models are used, often uncritically, over and over again. In most
circumstances, there will be a large number of different, more or less plausible models
thatmightbefittedtoanygivensetofdata.Partofthejobofdataanalysisistodetermine
6 STATISTICS:ANINTRODUCTIONUSINGR
which, if any, of the possible models are adequate, and then, out of the set of adequate
models,whichistheminimaladequatemodel.Insomecasestheremaybenosinglebest
model and a set of different models may all describe the data equally well (or equally
poorly if the variability is great).
MaximumLikelihood
What,exactly,dowemeanwhenwesaythattheparametervaluesshouldaffordthe‘bestfit
of the model to the data’? The convention we adopt is that our techniques should lead to
unbiased, variance minimizing estimators. We define ‘best’ in terms of maximum likeli-
hood.Thisnotionislikelytobeunfamiliar,soitisworthinvestingsometimetogetafeelfor
it. This is how it works:
(cid:129) given the data
(cid:129) and given our choice of model
(cid:129) what values of the parameters of that model
(cid:129) make the observed data most likely?
Letustakeasimpleexamplefromlinearregressionwherethemodelwewanttofitisy=
a+ bxandwewantthebestpossibleestimatesofthetwoparameters(theinterceptaandthe
slope b) from the data in our scatterplot.
Iftheinterceptwere0(left-handgraph,above),wouldthedatabelikely?Theanswer
of course, is no. If the intercept were 8 (centre graph) would the data be likely? Again,
the answer is obviouslyno. The maximumlikelihood estimate of the interceptisshown
in the right-hand graph (its value turns out to be 4.827). Note that the point at which
the graph cuts the y axis is not the intercept when (as here) you let R decide where to
put the axes.
We could have a similar debate about the slope. Suppose we knew that the intercept
was4.827,thenwouldthedatabelikelyifthegraphhadaslopeof1.5(left-handgraph,
below)?
FUNDAMENTALS 7
Theanswer,ofcourse,isno.Whataboutaslopeof0.2(centregraph)?Again,thedataare
notatall likely ifthe graph hassucha gentle slope. The maximum likelihoodofthe data
given the model is obtained with a slope of 0.679 (right-hand graph).
This is not how the procedure is carried out in practice, but it makes the point that we
judgethemodelonthebasishowlikelythedatawouldbeifthemodelwerecorrect.When
we do the analysis in earnest, both parameters are estimated simultaneously.
ExperimentalDesign
There are only two key concepts:
(cid:129) replication
(cid:129) randomization
You replicate to increase reliability. You randomize to reduce bias. If you replicate
thoroughly and randomize properly, you will not go far wrong.
Thereareanumberofotherissueswhosemasterywillincrease thelikelihoodthatyou
analyse your data the right way rather than the wrong way:
(cid:129) the principle of parsimony
(cid:129) the power of a statistical test
(cid:129) controls
(cid:129) spotting pseudoreplication and knowing what to do about it
(cid:129) the difference between experimental and observational data (non-orthogonality)
Itdoesnotmatterverymuchifyoucannotdoyourownadvancedstatisticalanalysis.If
yourexperimentisproperlydesigned,youwilloftenbeabletofindsomebodytohelpyou
with the stats. But if your experiment is not properly designed, or not thoroughly
randomized, or lacking adequate controls, then no matter how good you are at stats,
some(orpossiblyevenall)ofyourexperimentaleffortwillhavebeenwasted.Noamountof
high-poweredstatisticalanalysiscanturnabadexperimentintoagoodone.Risgood,but
not that good.
8 STATISTICS:ANINTRODUCTIONUSINGR
ThePrincipleofParsimony(Occam’sRazor)
Oneofthemostimportantthemesrunningthroughthisbookconcernsmodelsimplification.
The principle of parsimony is attributed to the fourteenth-century English nominalist
philosopherWilliamofOccamwhoinsistedthat,givenasetofequallygoodexplanations
for a given phenomenon, then the correct explanation is the simplest explanation. It is
called Occam’s razor because he ‘shaved’ his explanations down to the bare minimum.
In statistical modelling, the principle of parsimony means that:
(cid:129) models should have as few parameters as possible
(cid:129) linear models should be preferred to non-linear models
(cid:129) experimentsrelyingonfewassumptionsshouldbepreferredtothoserelyingonmany
(cid:129) models should be pared down until they are minimal adequate
(cid:129) simple explanations should be preferred to complex explanations
The process of model simplification is an integral part of statistical analysis in R. In
general,avariableisretainedinthemodelonlyifitcausesasignificantincreaseindeviance
when it is removed from the current model. Seek simplicity, then distrust it.
Inourzealformodelsimplification,wemustbecarefulnottothrowthebabyoutwiththe
bathwater.EinsteinmadeacharacteristicallysubtlemodificationtoOccam’srazor.Hesaid:
‘A model should be as simple as possible. But no simpler.’
Observation,TheoryandExperiment
Thereisnodoubtthatthebestwaytosolvescientificproblemsisthroughathoughtfulblend
of observation, theory and experiment. In most real situations, however, there are con-
straintsonwhatcanbedone,andonthewaythingscanbedone,whichmeanthatoneor
more of the trilogy has to be sacrificed. There are lots of cases, for example, where it is
ethicallyorlogisticallyimpossibletocarryoutmanipulativeexperiments.Inthesecasesitis
doublyimportanttoensurethatthestatisticalanalysisleadstoconclusionsthatareascritical
and as unambiguous as possible.
Controls
No controls, no conclusions.
Replication:It’sthensthatJustifytheMeans
Therequirementforreplicationarisesbecauseifwedothesamethingtodifferentindividuals
wearelikelytogetdifferentresponses.Thecausesofthisheterogeneityinresponsearemany
andvaried(genotype,age,sex,condition,history,substrate,microclimate,andsoon).The
objectofreplicationistoincreasethereliabilityofparameterestimates,andtoallowusto
quantifythevariabilitythatisfoundwithinthesametreatment.Toqualifyasreplicates,the
repeatedmeasurements:
FUNDAMENTALS 9
(cid:129) must be independent
(cid:129) must not form part of a time series (data collected from the same place on successive
occasions are not independent)
(cid:129) mustnotbegroupedtogetherinoneplace(aggregatingthereplicatesmeansthattheyare
not spatially independent)
(cid:129) must be measured at an appropriate spatial scale
(cid:129) ideally,onereplicatefromeachtreatmentoughttobegroupedtogetherintoablock,and
all treatments repeated in many different blocks.
(cid:129) repeated measures (e.g.from thesameindividualorthesame spatial location) arenot
replicates (this is probably the commonest cause of pseudoreplication in statistical
work)
HowManyReplicates?
Theusualansweris‘asmanyasyoucanafford’.Analternativeansweris30.Averyuseful
ruleofthumbisthis:asampleof30ormoreisabigsample,butasampleoflessthan30isa
smallsample.Theruledoesn’talwayswork,ofcourse:30wouldbederisivelysmallasa
sample in an opinion poll, for instance. In other circumstances, it might be impossibly
expensive to repeat an experiment as many as 30 times. Nevertheless, it is a rule of great
practical utility, if only for giving you pause as you design your experiment with 300
replicatesthatperhapsthismightreallybeabitoverthetop.Orwhenyouthinkyoucould
get away with just five replicates this time.
There are ways of working out the replication necessary for testing a given hypothesis
(theseareexplainedbelow).Sometimesweknowlittleornothingaboutthevarianceofthe
responsevariablewhenweareplanninganexperiment.Experienceisimportant.Soarepilot
studies. These should give an indication of the variance between initial units before the
experimentaltreatmentsareapplied,andalsooftheapproximatemagnitudeoftheresponses
toexperimentaltreatmentthatarelikelytooccur.Sometimesitmaybenecessarytoreduce
the scope and complexity of the experiment, and to concentrate the inevitably limited
resources of manpower and money on obtaining an unambiguous answer to a simpler
question.Itisimmenselyirritatingtospendthreeyearsonagrandexperiment,onlytofind
attheendofitthattheresponseisonlysignificantatp=0.08.Areductioninthenumberof
treatmentsmightwellhaveallowedanincreaseinreplicationtothepointwherethesame
result would have been unambiguously significant.
Power
Thepowerofatestistheprobabilityofrejectingthenullhypothesiswhenitisfalse.Ithasto
dowithTypeIIerrors:βistheprobabilityofacceptingthenullhypothesiswhenitisfalse.
In an ideal world, we would obviously make β as small as possible. But there is a snag.
ThesmallerwemaketheprobabilityofcommittingaTypeIIerror,thegreaterwemakethe
probabilityofcommittingaTypeIerror,andrejectingthenullhypothesiswhen,infact,itis
correct. A compromise is called for. Most statisticians work with α0:05 and β0:2.
Nowthepowerofatestisdefinedas1 β0:8underthestandardassumptions.Thisis
10 STATISTICS:ANINTRODUCTIONUSINGR
usedtocalculatethesamplesizesnecessarytodetectaspecifieddifferencewhentheerror
variance is known (or can be guessed at).
Let’s think about the issues involved with power analysis in the context of a Student’s
t-testtocomparetwosamplemeans.Asexplainedonp.91,theteststatisticist=difference/
(the standard error of the difference) and we can rearrange the formula to obtain n, the
sample size necessary in order that that a given difference, d, is statistically significant:
2s2t2
n
d2
Youcanseethatthelargerthevariances2,andthesmallerthesizeofthedifference,the
biggerthesampleweshallneed.Thevalueoftheteststatistictdependsonourdecisions
aboutTypeIandTypeIIerrorrates(conventionally0.05and0.2).Forsamplesizesoforder
30,thetvaluesassociatedwiththeseprobabilitiesare1.96and0.84respectively:theseadd
to2.80,andthesquare of2.80is7.84.To thenearestwholenumber,theconstants inthe
numeratorevaluateto2×8=16.Soasagoodruleofthumb,thesamplesizeyouneedin
each treatment is given by
16s2
n
d2
Wesimplyneedtoworkout16timesthesamplevariance(obtainedfromtheliteratureor
fromasmallpilotexperiment)anddividebythesquareofthedifferencethatwewanttobe
abletodetect.Sosupposethatourcurrentcerealyieldis10t/hawithastandarddeviationof
sd=2.8t/ha(givings2=7.84)andwewanttobeabletosaythatayieldincrease(delta)of
2t/ha is significant at 95% with power=80%, then we shall need to have 16×7.84/
4=31.36 replicates in each treatment. The built in R function
power.t.test(delta=2,sd=2.8,power=0.8)
also gives n=32 replicates per treatment on rounding-up.
Randomization
Randomization is something that everybody says they do, but hardly anybody does
properly. Take a simple example. How do I select one tree from a forest of trees, on
whichtomeasurephotosyntheticrates?Iwanttoselectthetreeatrandominordertoavoid
bias.Forinstance,Imightbetemptedtoworkonatreethathadaccessiblefoliageneartothe
ground,oratreethatwasclosetothelab.Oratreethatlookedhealthy.Oratreethathad
niceinsect-freeleaves.Andsoon.Ileaveittoyoutolistthebiasesthatwouldbeinvolvedin
estimating photosynthesis on any of those trees.
Onecommonwayofselectinga‘random’treeistotakeamapoftheforestandselecta
randompairofcoordinates(say157meastofthereferencepoint,and228mnorth).Then
paceoutthesecoordinatesand,havingarrivedatthatparticularspotintheforest,selectthe
nearest tree to those coordinates. But is this really a randomly selected tree?
Ifitwererandomlyselected,thenitwouldhaveexactlythesamechanceofbeingselected
as every other tree in the forest. Let us think about this. Look at the figure below, which
showsamapofthedistributionoftreesontheground.Eveniftheywereoriginallyplanted
outinregularrows,accidents,tree-falls,andheterogeneityinthesubstratewouldsoonlead
FUNDAMENTALS 11
toanaggregatedspatialdistributionoftrees.Nowaskyourselfhowmanydifferentrandom
pointswouldleadtotheselectionofagiventree.Startwithtree(a).Thiswillbeselectedby
any points falling in the large shaded area.
Nowconsidertree(b).Itwillonlybeselectediftherandompointfallswithinthetinyarea
surrounding that tree. Tree (a) has a much greater chance of being selected than tree (b),
and so the nearest tree to a random point is not a randomly selected tree. In a spatially
heterogeneouswoodland,isolatedtreesandtreesontheedgesofclumpswillalwayshavea
higher probability of being picked than trees in the centre of clumps.
The answer is that to select a tree at random, every single tree in the forest must be
numbered (all 24 683 of them, or whatever), and then a random number between 1 and
24 683must be drawn outofa hat.There isno alternative.Anythingless than that is not
randomization.
Nowaskyourselfhowoftenthisisdoneinpractice,andyouwillseewhatImeanwhen
I say that randomization is a classic example of ‘Do as I say, and not do as I do’. As an
exampleofhowimportantproperrandomizationcanbe,considerthefollowingexperiment
thatwasdesignedtotestthetoxicityoffivecontactinsecticidesbyexposingbatchesofflour
beetlestothechemicalonfilterpapersinPetridishes.Theanimalswalkaboutandpickup
thepoisonontheirfeet.TheTriboliumculture jarwasinverted,flourandall,intoalarge
tray,andbeetleswerecollectedastheyemergedfromtheflour.Theanimalswereallocated
to the five chemicals in sequence; three replicate Petri dishes were treated with the first
chemical,and10beetleswereplacedineachPetridish.Doyouseethesourceofbiasinthis
procedure?
12 STATISTICS:ANINTRODUCTIONUSINGR
It is entirely plausible that flour beetles differ in their activity levels (sex differences,
differencesinbodyweight,age,etc.).Themostactivebeetlesmightemergefirstfromthe
pileofflour.Thesebeetlesallendupinthetreatmentwiththefirstinsecticide.Bythetime
wecometofindingbeetlesforthelastreplicateofthefifthpesticide,wemaybegrubbing
round in the centre of the pile, looking for the last remaining Tribolium. This matters,
because the amount of pesticide picked by the beetles up will depend upon their activity
levels. The more active the beetles, the more chemical they pick up on their feet, and the
morelikelytheyaretodie.Thus,thefailuretorandomizewillbiastheresultinfavourofthe
first insecticide because this treatment received the most active beetles.
Whatweshouldhavedoneisthis.Ifwethinkthatinsectactivitylevelisimportantinour
experiment,thenweshouldtakethisintoaccountatthedesignstage.Wemightdecideto
havethreelevelsofactivity:active,averageandsluggish.WefillthefirstfivePetridishes
with10eachoftheactiveinsectsthatemergefirstfromthepile.Thenext50insectswefind
go10-at-a-timeintofivePetridishesthatarelabelledaverage.Finally,weputlast50insects
toemergeintoasetoffivePetridisheslabelledsluggish.Thisprocedurehascreatedthree
blocksbasedonactivitylevels:wedonotknowpreciselywhytheinsectsdifferedintheir
activitylevels,butwethinkitmightbeimportant.Activityleveliscalledarandomeffect:it
isafactorwiththreelevels. Nextcomestherandomization.Weputthenamesofthefive
insecticidesintoahat,shufflethemup,anddrawthemoutone-at-a-timeatrandom.Thefirst
Petridishcontainingactivebeetlesreceivestheinsecticidethatisfirstoutofthehat,andso
onuntilallfiveactivePetridisheshavebeenallocatedtheirowndifferentpesticide.Then
thefivelabelsgobackinthehatandarereshuffled.Theprocedureisrepeatedtoallocate
insecticidetreatmentatrandomtothefiveaverageactivityPetridishes.Finally,weputthe
labelsbackinthehatanddrawtheinsecticidetreatmentforthefivePetridishescontaining
sluggish insects.
Butwhygotoallthistrouble?Theanswerisveryimportant,andyoushouldreaditagain
andagainuntilyouunderstandit.Theinsectsdifferandtheinsecticidesdiffer.ButthePetri
dishesmaydiffer,too,especiallyifwestoretheminslightlydifferentcircumstances(e.g.
neartothedoorofthecontrolledtemperaturecabinetorawayatthebackofthecabinet).The
pointisthattherewillbeatotalamountofvariationintimetodeathacrossalltheinsectsin
thewholeexperiment(all3×5×10=150ofthem).Wewanttopartitionthisvariationinto
thatwhichcanbeexplainedbydifferencesbetweentheinsecticidesandthatwhichcannot.
explained variation
total variation
unexplained variation
FUNDAMENTALS 13
Iftheamountofvariationexplainedbydifferencesbetweentheinsecticidetreatmentsis
large,thenweconcludethattheinsecticidesaresignificantlydifferentfromoneanotherin
theireffects onmean ageatdeath. Wemake thisjudgementonthebasisofacomparison
betweentheexplainedvariationSSAandtheunexplainedvariationSSE.Iftheunexplained
variationislarge,itisgoingtobeverydifficulttoconcludeanythingaboutourfixedeffect
(insecticide in this case).
Thegreatadvantageofblockingisthatitreducesthesizeoftheunexplainedvariation.In
our example, if activity level had a big effect on age at death (block variation), then the
unexplainedvariationSSEwouldbemuchsmallerthanwouldhavebeenthecaseifwehad
ignored activity and the significance of our fixed effect will be correspondingly higher:
explained variation
total variation
block variation
unexplained variation
SSE
TheideaofgoodexperimentaldesignistomakeSSEassmallaspossible,andblockingis
the most effective way to bring this about.
Risveryusefulduringtherandomizationstagebecauseithasafunctioncalledsample
which can shuffle the factor levels into a random sequence. Put the names of the five
insecticides into a vector like this:
treatments<-c("aloprin","vitex","formixin","panto","allclear")
Then use sample to shuffle them for the active insects in dishes 1 to 5:
sample(treatments)
[1]"formixin""panto" "vitex" "aloprin" "allclear"
then for the insects with average activity levels in dishes 6 to 10:
sample(treatments)
[1]"formixin""allclear""aloprin" "panto" "vitex"
then finally for the sluggish ones in dishes 11 to 15:
sample(treatments)
[1]"panto" "aloprin" "allclear""vitex" "formixin"
Therecenttrendtowards‘haphazard’samplingisacop-out.Whatitmeansisthat‘Iadmit
thatIdidn’trandomize,butyouhavetotakemywordforitthatthisdidnotintroduceany
important biases’. You can draw your own conclusions.
14 STATISTICS:ANINTRODUCTIONUSINGR
StrongInference
One of the most powerful means available to demonstrate the accuracy of an idea is an
experimentalconfirmationofapredictionmadebyacarefullyformulatedhypothesis.There
are two essential steps to the protocol of strong inference (Platt, 1964):
(cid:129) formulate a clear hypothesis
(cid:129) devise an acceptable test
Neitheroneismuchgoodwithouttheother.Forexample,thehypothesisshouldnotlead
to predictions that are likely to occur by other extrinsic means. Similarly, the test should
demonstrate unequivocally whether the hypothesis is true or false.
A great many scientific experiments appear to be carried out with no particular
hypothesis in mind at all, but simply to see what happens. While this approach may
be commendable in the early stages of a study, such experiments tend to be weak as an
end in themselves, because there will be such a large number of equally plausible
explanationsfortheresults.Withoutcontemplationtherewillbenotestablepredictions;
without testable predictions there will be no experimental ingenuity; without experi-
mental ingenuity there is likely to be inadequate control; in short, equivocal interpreta-
tion. The results could be due to myriad plausible causes. Nature has no stake in being
understood byscientists. We need to work at it.Without replication, randomizationand
good controls we shall make little progress.
WeakInference
Thephrase‘weakinference’isused(oftendisparagingly)todescribetheinterpretationof
observational studies and the analysis of so-called ‘natural experiments’. It is silly to be
disparagingaboutthesedata,becausetheyareoftentheonlydatathatwehave.Theaimof
good statistical analysis is to obtain the maximum information from a given set of data,
bearing the limitations of the data firmly in mind.
Natural experiments arise when an event (often assumed to be an unusual event, but
frequentlywithoutmuchjustificationofwhatconstitutesunusualness)occursthatislikean
experimentaltreatment(ahurricaneblowsdownhalfofaforestblock;alandslidecreates
a bare substrate; a stock market crash produces lots of suddenly poor people, etc.). ‘The
requirementofadequateknowledgeofinitialconditionshasimportantimplicationsforthe
validityofmanynaturalexperiments.Inasmuchasthe“experiments”arerecognizedonly
whentheyarecompleted,orinprogressattheearliest,itisimpossibletobecertainofthe
conditions that existed before such an “experiment” began. It then becomes necessary to
make assumptions about these conditions, and any conclusions reached on the basis of
naturalexperimentsaretherebyweakenedtothepointofbeinghypotheses,andtheyshould
be stated as such’ (Hairston, 1989).
HowLongtoGoOn?
Ideally,thedurationofanexperimentshouldbedeterminedinadvance,lestonefallspreyto
one of the twin temptations:
FUNDAMENTALS 15
(cid:129) to stop the experiment as soon as a pleasing result is obtained
(cid:129) to keep going with the experiment until the ‘right’ result is achieved (the ‘Gregor
Mendel effect’)
In practice, most experiments probably run for too short a period, because of the
idiosyncrasies of scientific funding. This short-term work is particularly dangerous in
medicine and the environmental sciences, because the kind of short-term dynamics
exhibited after pulse experiments may be entirely different from the long-term dynamics
of the same system. Only by long-term experiments of both the pulse and the press kind
will the full range of dynamics be understood. The other great advantage of long-term
experiments is that a wide range of patterns (e.g. ‘kinds of years’) is experienced.
Pseudoreplication
Pseudoreplicationoccurswhenyouanalysethedataasifyouhadmoredegreesoffreedom
than you really have. There are two kinds of pseudoreplication:
(cid:129) temporalpseudoreplication,involvingrepeatedmeasurementsfromthesameindividual
(cid:129) spatialpseudoreplication,involvingseveralmeasurementstakenfromthesamevicinity
Pseudoreplication is a problem because one of the most important assumptions of
standard statistical analysis is independence of errors. Repeated measures through time
on the same individual will have non-independent errors because peculiarities of the
individual will be reflected in all of the measurements made on it (the repeated measures
willbetemporallycorrelatedwithoneanother).Samplestakenfromthesamevicinitywill
havenon-independenterrorsbecausepeculiaritiesofthelocationwillbecommontoallthe
samples (e.g. yields will all be high in a good patch and all be low in a bad patch).
Pseudoreplicationisgenerallyquiteeasytospot.Thequestiontoaskisthis.Howmany
degreesoffreedomforerrordoestheexperimentreallyhave?Ifafieldexperimentappears
tohavelotsofdegreesoffreedom,itisprobablypseudoreplicated.Takeanexamplefrom
pestcontrolofinsectsonplants.Thereare20plots,10sprayedand10unsprayed.Within
eachplotthereare50plants.Eachplantismeasuredfivetimesduringthegrowingseason.
Now this experiment generates 20×50×5=5000 numbers. There are two spraying
treatments,sotheremustbe1degreeoffreedomforsprayingand4998degreesoffreedom
forerror.Ormustthere?Countupthereplicatesinthisexperiment.Repeatedmeasurements
on the same plants (the five sampling occasions) are certainly not replicates. The
50 individual plants within each quadrat are not replicates either. The reason for this is
thatconditionswithineachquadratarequitelikelytobeunique,andsoall 50plantswill
experience more or less the same unique set of conditions, irrespective of the spraying
treatment they receive. In fact, there are 10 replicates in this experiment. There are
10 sprayed plots and 10 unsprayed plots, and each plot will yield only one independent
datum to the response variable (the proportion of leaf area consumed by insects, for
example). Thus, there are 9 degrees of freedom within each treatment, and 2×9=18
degreesoffreedomforerrorintheexperimentasawhole.Itisnotdifficulttofindexamples
ofpseudoreplicationonthisscale intheliterature (Hurlbert, 1984). Theproblemisthat it
16 STATISTICS:ANINTRODUCTIONUSINGR
leads to the reporting of masses of spuriously significant results (with 4998 degrees of
freedomforerror,itisalmostimpossiblenottohavesignificantdifferences).Thefirstskill
to be acquired by the budding experimenter is the ability to plan an experiment that is
properly replicated.
There are various things that you can do when your data are pseudoreplicated:
(cid:129) averageawaythepseudoreplicationandcarryoutyourstatisticalanalysisonthemeans
(cid:129) carry out separate analyses for each time period
(cid:129) use more advanced statistical techniques such as time series analysisor mixed effects
models
InitialConditions
Manyotherwiseexcellentscientificexperimentsarespoiledbyalackofinformationabout
initialconditions.Howcanweknowifsomethinghaschangedifwedonotknowwhatit
was like to begin with? It is often implicitly assumed that all the experimental units were
alikeatthebeginningoftheexperiment,butthisneedstobedemonstratedratherthantaken
onfaith.One ofthemost important uses of data on initial conditions isasa check onthe
efficiencyofrandomization.Forexample,youshouldbeabletorunyourstatisticalanalysis
todemonstratethattheindividualorganismswerenotsignificantlydifferentinmeansizeat
the beginning of a growth experiment. Without measurements of initial size, it is always
possible to attribute the end result to differences in initial conditions. Another reason for
measuring initial conditions is that the information can often be used to improve the
resolution of the final analysis through analysis of covariance (see Chapter 9).
OrthogonalDesignsandNon-OrthogonalObservationalData
Thedatainthisbookfallintotwodistinctcategories.Inthecaseofplannedexperiments,all
ofthetreatmentcombinationsareequallyrepresentedand,barringaccidents,therewillbe
nomissingvalues.Suchexperimentsaresaidtobeorthogonal.Inthecaseofobservational
studies, however, we have no control over the number of individuals for which we have
data,oroverthecombinationsofcircumstancesthatareobserved.Manyoftheexplanatory
variablesarelikelytobecorrelatedwithoneanother,aswellaswiththeresponsevariable.
Missing treatment combinations will be commonplace, and such data are said to be non-
orthogonal. This makes an important difference to our statistical modelling because, in
orthogonaldesigns,thevariabilitythatisattributedtoagivenfactorisconstant,anddoesnot
dependupontheorderinwhichthatfactorisremovedfromthemodel.Incontrast,withnon-
orthogonaldata,wefindthatthevariabilityattributabletoagivenfactordoesdependupon
theorderinwhichthefactorisremovedfromthemodel.Wemustbecareful,therefore,to
judgethesignificanceoffactorsinnon-orthogonalstudies,whentheyareremovedfromthe
maximal model (i.e. from the model including all the other factors and interactions with
which they might be confounded). Remember, for non-orthogonal data, order matters.
Aliasing
ThistopiccausesconcernbecauseitmanifestsitselfasoneormorerowsofNAappearing
unexpectedlyintheoutputofyourmodel.Aliasingoccurswhenthereisnoinformationon
FUNDAMENTALS 17
whichtobaseanestimateofaparametervalue.Intrinsicaliasingoccurswhenitisduetothe
structure of the model. Extrinsic aliasing occurs when it is due to the nature of the data.
Parameters can be aliased for one of two reasons:
(cid:129) there are no data in the dataframe from which to estimate the parameter (e.g. missing
values, partial designs or correlation amongst the explanatory variables)
(cid:129) themodelisstructuredinsuchawaythattheparametervaluecannotbeestimated(e.g.
over-specified models with more parameters than necessary)
Ifwehadafactorwithfourlevels(saynone,light,mediumandheavyuse)thenwecould
estimatefourmeansfromthedata,oneforeachfactorlevel.Butthemodellookslikethis:
yμβ x β x β x β x
1 1 2 2 3 3 4 4
wherethex aredummyvariableshavingthevalue0or1foreachfactorlevel(seep.158),the
i
β are the effect sizes and μ is the overall mean. Clearly there is no point in having five
i
parametersinthemodelifwecanestimateonlyfourindependenttermsfromthedata.Oneof
theparametersmustbeintrinsicallyaliased.ThistopicisexplainedindetailinChapter11.
Inamultipleregressionanalysis,ifoneofthecontinuousexplanatoryvariablesisperfectly
correlatedwithanothervariablethathasalreadybeenfittedtothedata(perhapsbecauseitisa
constantmultipleofthefirstvariable),thenthesecondtermisaliasedandaddsnothingtothe
descriptivepowerofthemodel.Supposethatx =0.5x ;thenfittingamodelwithx +x will
2 1 1 2
leadtox beingintrinsicallyaliasedandgivenaparameterestimateofNA.
2
Ifallofthevaluesofaparticularexplanatoryvariablearesettozeroforagivenlevelofa
particular factor, then that level is said to have been intentionally aliased. This sort of
aliasingisausefulprogrammingtrickduringmodelsimplificationinANCOVAwhenwe
wish a covariate to be fitted to some levels of a factor but not to others.
Finally,supposethatinafactorialexperiment,alloftheanimalsreceivinglevel2ofdiet
(factorA)andlevel3oftemperature(factorB)havediedaccidentallyasaresultofattackby
a fungal pathogen. This particular combination of diet and temperature contributes no
data to the response variable, so the interaction term A(2):B(3) cannot be estimated. It is
extrinsically aliased, and its parameter estimate is set to NA.
MultipleComparisons
Thethornyissueofmultiplecomparisonsarisesbecausewhenwedomorethanonetestwe
arelikelytofind‘falsepositives’ataninflatedrate(i.e.byrejectingatruenullhypothesis
more often than indicated by the value of α). The old fashioned approach was to use
Bonferroni’scorrection;inlookingupavalueforStudent’st,youdivideyourαvaluebythe
numberofcomparisonsyouhavedone.Iftheresultisstillsignificantthenalliswell,butit
often will not be. Bonferroni’s correction is very harsh and will often throw out the baby
with the bathwater. An old-fashioned alternative was to use Duncan’s multiple range
tests(youmayhaveseentheseinoldstatsbooks,wherelower-caseletterswerewrittenat
the head of each bar in a barplot: bars with different letters were significantly different,
while bars with the same letter were not significantly different. The modern approach is
18 STATISTICS:ANINTRODUCTIONUSINGR
to use contrasts wherever possible, and where it is essential to do multiple compari-
sons, then to use the wonderfully named Tukey’s honestly significant differences (see
?TukeyHSD).
SummaryofStatisticalModelsinR
Modelsarefittedtodata(nottheotherwayround),usingoneofthefollowingmodel-fitting
functions:
(cid:129) lm:fitsalinearmodelassumingnormalerrorsandconstantvariance;generallythisis
usedforregressionanalysisusingcontinuousexplanatoryvariables.Thedefaultoutput
is summary.lm
(cid:129) aov:analternativetolmwithsummary.aovasthedefaultoutput.Typicallyusedonly
when there are complex error terms to be estimated (e.g. in split-plot designs where
different treatments are applied to plots of different sizes)
(cid:129) glm:fitsgeneralizedlinearmodelstodatausingcategoricalorcontinuousexplanatory
variables,byspecifyingoneofafamilyoferrorstructures(e.g.Poissonforcountdata
or binomial for proportion data) and a particular link function
(cid:129) gam:fitsgeneralizedadditivemodelstodatawithoneofafamilyoferrorstructures(e.g.
Poisson for count data or binomial for proportion data) in which the continuous
explanatory variables can (optionally) be fitted as arbitrary smoothed functions using
non-parametric smoothers rather than specific parametric functions.
(cid:129) lmer: fits linear mixed effects models with specified mixtures of fixed effects and
random effects and allows for the specification of correlation structure amongst the
explanatory variables and autocorrelation of the response variable (e.g. time series
effects with repeated measures). The older lmeis an alternative
(cid:129) nls:fitsanon-linearregressionmodelvialeastsquares,estimatingtheparametersofa
specified non-linear function
(cid:129) nlme: fits a specified non-linear function in a mixed effects model where the
parameters of the non-linear function are assumed to be random effects; allows
for the specification of correlation structure amongst the explanatory variables and
autocorrelation of the response variable (e.g. time series effects with repeated
measures).
(cid:129) loess:fitsalocalregressionmodelwithoneormorecontinuousexplanatoryvariables
using non-parametric techniques to produce a smoothed model surface
(cid:129) rpart:fitsaregressiontreemodelusingbinaryrecursivepartitioningwherebythedata
aresuccessivelysplitalongcoordinateaxesoftheexplanatoryvariablessothatatany
node,thesplitischosen that maximally distinguishes theresponse variable in theleft
and the right branches. With a categorical response variable, the tree is called a
classification tree, and the model used for classification assumes that the response
variable follows a multinomial distribution
FUNDAMENTALS 19
Formostofthesemodels,arangeofgenericfunctionscanbeusedtoobtaininformation
about the model. The most important and most frequently used are
summary produces parameter estimates and standard errors from
lm, and ANOVA tables from aov; this will often
determine your choice between lm and aov. For either
lm or aov you can choose summary.aov or summary.
lm to get the alternative form of output (an ANOVA
table or a table of parameter estimates and standard
errors; see p. 158)
plot produces diagnostic plots for model checking, including
residuals against fitted values, influence tests, etc.
anova a useful function for comparing two or more different
models and producing ANOVA tables (and alternative to
AIC)
update used to modify the last model fit; it saves both typing
effort and computing time
Other useful generics include:
coef thecoefficients(estimatedparameters)fromthemodel
fitted thefittedvalues,predictedbythemodelforthevaluesofthe
explanatoryvariablesthatappearinthedataframe
resid theresiduals(thedifferencesbetweenmeasuredand
predictedvaluesofy)
predict usesinformationfromthefittedmodeltoproducesmooth
functionsforplottingacurvethroughthescatterplotofyour
data.Thetrickistorealizethatyouneedtoprovidevalues
foralloftheexplanatoryvariablesthatareinthemodel
(bothcontinuousandcategorical)asalist,andthatthe
vectorsofexplanatoryvariablesmustallbeexactlythesame
length(seep.248foraworkedexample).Youcanback-
transformautomaticallyusingtheoption
type="response".
OrganizingYourWork
There are three things that you really must keep for each separate R session:
(cid:129) the dataframe, stored in a comma-delimited (.csv) or a tab-delimited (.txt) file
(cid:129) the script, stored in a text file (.txt)
(cid:129) theresultsobtainedduringthissession(tables,graphs,modelobjects,etc.)storedina
PDF so that you can retain the graphics along with model outputs
20 STATISTICS:ANINTRODUCTIONUSINGR
Tomakesureyourememberwhichdatafilesandresultsgowithwhichscripts,itisgood
practice to save the script, results and data files in the same, sensibly named folder.
Oncethedataarecheckedandedited,youarenotlikelyevertowanttoalterthedatafile.
Ontheotherhand,youarelikelytowanttokeepaseparatescriptforeachworkingsession
ofR. One ofthe great advantagesof using scripts isthat you can copy(potentially large)
sections of code from previous successful sessions and save yourself a huge amount of
typing (and wheel reinvention).
Therearetwosensiblewaysofworkingandyoushouldchoosetheonethatsuitsyoubest.
Thefirstistowriteallofyourcodeinascripteditor,saveitregularly,andpepperitliberally
with comments (use the hatch symbol to start each comment):
# thisisacomment
Whenyoumakemistakes,cutthemoutofthescript,takingcarenottodeleteimportant
bits of code accidentally.
The alternative is to save the script of the whole session just before you finish. This is
stored in what R calls the history file. At the end of the session, type
history(Inf)
and R will open a script window called ‘R History’ containing a full transcript of all the
commands(bothrightandwrong)thatyouenteredduringthatsession.Copyallthematerial
toatextfile,editoutthemistakes(again,makingsurethatyoudonotremoveanyessential
lines of code), add the necessary comments, then save the text file as your script for the
session in its customized directory along with the data file and the results (the output
of tables, models and graphics).
Whichevermethodyouuse,thesavedscriptisapermanentrecordofwhatyoudid(with
commentspointingoutexactlywhyyoudidit).Youarelikelytocopyandpastethecode
into R on future occasions when you want to do similar analyses, and you want the code
to work seamlessly (which it will not do if you have unintentionally removed key lines
of code).
Itisabadideatocreateyourscriptsinawordprocessorbecauseseveralofthesymbols
you will use may not be readable within R. Double quotes is a classic example of this;
yourwordprocessorwillhave“(openquotes)and”(closequotes)butRwillreadonly
"(simplequotes).However,youmightwanttosavetheresultsfromyourRsessionsina
word processor because this can include graphs as well as input and output in the same
document.
HousekeepingwithinR
The simplest way to work is to start a new R session for each separate activity. The
advantage of working this way is that things from one session do not get mixed up with
things from another session.
Theclassicthingtogowrongisthatyougettwodifferentobjectswiththesamename,
andyoudonotknowwhichiswhich.Forinstance,avariablecalledxfromoneanalysismay
contain 30 numbers and a different variable called x from another analysis might have
FUNDAMENTALS 21
50numbersinit.Atleast,inthatcase,youcantestthelengthoftheobjecttoseewhichoneit
is (if it is of length 50 then it must be the x variable from the second analysis). Worse
problemsarisewhenthetwodifferentvariablescalledxareboththesamelength.Thenyou
really do not know where you are.
IfyouinsistondoingseveralthingsduringthesameRsession,thenitpaystobereally
wellorganized.Inthisbookweattachdataframes,sothatyoucanrefertoyourvariables
by name without reference to the name of the dataframe from which they come (experts
generallydonotuseattach).The disadvantageofusing attachisthat you might have
severaldataframesattachedthatcontainexactlythesamevariablename.Rwillwarnyouof
this by writing
Thefollowingobjectismaskedfromfirst.frame:
temp,wind
whenyouattachadataframecontainingvariablesthatarealreadyattachedfromadifferent
dataframe.Themessagemeansthatwhenyouattachedanewdataframe,itcontainedtwo
variables, called temp and wind respectively, that were already attached from a previous
dataframe calledfirst.frame. Thisstateof affairs isconfusing andunsatisfactory. The
waytoavoiditistomakesurethatyoudetachallunnecessarydataframesbeforeattaching
a new dataframe. Here is the problem situation in full:
first.frame<-read.csv("c:\\temp\\test.pollute.csv")
second.frame<-read.csv("c:\\temp\\ozone.data.csv")
attach(first.frame)
attach(second.frame)
Thefollowingobjectismaskedfromfirst.frame:
temp,wind
Here is how to avoid the problem
first.frame<-read.csv("c:\\temp\\test.pollute.csv")
second.frame<-read.csv("c:\\temp\\ozone.data.csv")
attach(first.frame)
. . . this is where you work on the information from first.frame. Then when you are
finished . . .
detach(first.frame)
attach(second.frame)
Nowarningmessageisprintedbecausetempandrainarenolongerduplicatevariable
names.
The other big problem arises when you create variables during a session by allocation
(this typically involves calculation or the use of data-generating functions within R to
22 STATISTICS:ANINTRODUCTIONUSINGR
producpeffirffiffiandomnumbers,forinstance,orsequences).Soifinthefirstsessionyouwantedx
to be 2 then you would put:
x<-sqrt(2)
Now,inalatersessionyouusexfortheaxisofagraph,andgiveitthesequenceofvalues
0 to 10:
x<-0:10
Ifthefactthatyouhaddonepthffiffiffiisslippedyourmind,thenyoumightlaterusexthinking
thatiswasthesinglenumber 2.ButRknowsittobethevectorof11numbers0to10,
and this could have seriously bad consequences. The way to avoid problems like this
is to remove all the variables you have calculated before you start on another project
during the same session of R. The function for this is rm (or remove)
rm(x)
Ifyouaskforavariabletoberemovedthatdoesnotexist,thenRwillwarnyouofthisfact:
rm(y,z)
Warningmessages:
1:Inrm(y,z):object’y’notfound
2:Inrm(y,z):object’z’notfound
We are now in a position to start using R in earnest. The first thing to learn is how to
structureadataframeandhowtoreadadataframeintoR.Itisimmenselyirritatingthatthis
firststepoftenturnsouttobesodifficultforbeginnerstogetright.OncethedataareintoR,
the rest is plain sailing.
References
Hairston,N.G.(1989)EcologicalExperiments:Purpose,DesignandExecution,CambridgeUniver-
sityPress, Cambridge.
Hurlbert,S.H.(1984)Pseudoreplicationandthedesignofecologicalfieldexperiments.Ecological
Monographs,54,187–211.
Platt, J.R.(1964)Stronginference.Science,146, 347–353.
FurtherReading
Atkinson,A.C. (1985)Plots, Transformations, andRegression, ClarendonPress, Oxford.
Box,G.E.P.,Hunter,W.G.andHunter,J.S.(1978)StatisticsforExperimenters:AnIntroductionto
Design, DataAnalysis andModel Building,JohnWiley & Sons,New York.
Chambers,J.M.,Cleveland,W.S.,Kleiner,B.andTukey,P.A.(1983)GraphicalMethodsforData
Analysis,Wadsworth, Belmont,CA.
Winer,B.J.,Brown,D.R.andMichels,K.M.(1991)StatisticalPrinciplesinExperimentalDesign,
McGraw-Hill, NewYork.
2
Dataframes
Learninghowtohandleyourdata,howtoenteritintothecomputer,andhowtoreadthe
dataintoRareamongstthemostimportanttopicsyouwillneedtomaster.Rhandlesdata
in objects known as dataframes. A dataframe is an object with rows and columns (a bit
likeatwo-dimensionalmatrix).Therowscontaindifferentobservationsfromyourstudy,
or measurements from your experiment. The columns contain the values of different
variables. Thevalues in the body ofthe dataframe canbe numbers (as theywouldbe in
as matrix), but they could also be text (e.g. the names of factor levels for categorical
variables, like ‘male’ or ‘female’ in a variable called ‘gender’), they could be calendar
dates(like23/5/04),ortheycouldbelogicalvariables(like‘TRUE’or‘FALSE’).Hereis
a spreadsheet in the form of a dataframe with seven variables, the leftmost of which
comprises the row names, and other variables are numeric (Area, Slope, Soil pH and
Worm density), categorical (Field Name and Vegetation) or logical (Damp is either
true=T or false=F).
FieldName Area Slope Vegetation SoilpH Damp Wormdensity
Nash’sField 3.6 11 Grassland 4.1 F 4
SilwoodBottom 5.1 2 Arable 5.2 F 7
NurseryField 2.8 3 Grassland 4.3 F 2
RushMeadow 2.4 5 Meadow 4.9 T 5
Gunness’Thicket 3.8 0 Scrub 4.2 F 6
OakMead 3.1 2 Grassland 3.9 F 2
ChurchField 3.5 3 Grassland 4.2 F 3
Ashurst 2.1 0 Arable 4.8 F 4
TheOrchard 1.9 0 Orchard 5.7 F 9
RookerySlope 1.5 4 Grassland 5 T 7
GardenWood 2.9 10 Scrub 5.2 F 8
NorthGravel 3.3 1 Grassland 4.1 F 1
SouthGravel 3.7 2 Grassland 4 F 2
ObservatoryRidge 1.8 6 Grassland 3.8 F 0
PondField 4.1 0 Meadow 5 T 6
WaterMeadow 3.9 0 Meadow 4.9 T 8
(continued)
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
24 STATISTICS:ANINTRODUCTIONUSINGR
(Continued)
FieldName Area Slope Vegetation SoilpH Damp Wormdensity
Cheapside 2.2 8 Scrub 4.7 T 4
PoundHill 4.4 2 Arable 4.5 F 5
GravelPit 2.9 1 Grassland 3.5 F 1
FarmWood 0.8 10 Scrub 5.1 T 3
Perhaps the most important thing about analysing your own data properly is getting
your dataframe absolutely right. The expectation is that you will have used a spreadsheet
likeExceltoenterandeditthedata,andthatyouwillhaveusedplotstocheckforerrors.
The thing that takes some practice is learning exactly how to put your numbers into the
spreadsheet.Therearecountlesswaysofdoingitwrong,butonlyonewayofdoingitright.
And this way is not the way that most people find intuitively to be the most obvious.
Thekeythingisthis:allthevaluesofthesamevariablemustgointhesamecolumn.It
does not sound like much, but this is what people tend to get wrong. If you had an
experiment with three treatments (control, pre-heated and pre-chilled), and four measure-
ments per treatment, it might seem like a good idea to create the spreadsheet like this:
Control Preheated Prechilled
6.1 6.3 7.1
5.9 6.2 8.2
5.8 5.8 7.3
5.4 6.3 6.9
Butthisisnotadataframe,becausevaluesoftheresponsevariableappearinthreedifferent
columns,ratherthanallinthesamecolumn.Thecorrectwaytoenterthesedataistohavetwo
columns: one for the response variable and one for the levels of the experimental factor
(control,pre-heatedandpre-chilled).Herearethesamedata,enteredcorrectlyasadataframe:
Response Treatment
6.1 control
5.9 control
5.8 control
5.4 control
6.3 preheated
6.2 preheated
5.8 preheated
6.3 preheated
7.1 prechilled
8.2 prechilled
7.3 prechilled
6.9 prechilled
DATAFRAMES 25
AgoodwaytopracticethislayoutistousetheExcelfunctioncalledPivotTable(found
undertheInserttabonthemainmenubar)onyourowndata:itrequiresyourspreadsheetto
be in the form of a dataframe, with each of the explanatory variables in its own column.
OnceyouhavemadeyourdataframeinExcelandcorrectedalltheinevitabledataentry
andspellingerrors,thenyouneedtosavethedataframeinafileformatthatcanbereadby
R.MuchthesimplestwayistosaveallyourdataframesfromExcelascomma-delimited
files: File/Save As/ . . . then from the ‘Save as type’ options choose ‘CSV (Comma
delimited)’.Thereisnoneedtoaddasuffix,becauseExcelwillautomaticallyadd‘.csv’
to your file name. This file can then be read into R directly as a dataframe, using the
read.csv function.
Thinkofanameforthedataframe(say,‘worms’inthiscase).Nowusethegetsarrow
<  which is a composite symbol made up of the two characters < (less than) and  
(minus) like this
worms<-read.csv("c:\\temp\\worms.csv")
To see which variables are included in this dataframe, we use the names function:
names(worms)
[1] "Field.Name" "Area" "Slope" "Vegetation"
[5] "Soil.pH" "Damp" "Worm.density"
Inorderthatwecanrefertothevariablenamesdirectly(withoutprefixingthembythe
dataframe name) we attach the dataframe:
attach(worms)
To see the contents of the dataframe, just type its name:
worms
Field.Name Area Slope Vegetation Soil.pH Damp Worm.density
1 Nashs.Field 3.6 11 Grassland 4.1 FALSE 4
2 Silwood.Bottom 5.1 2 Arable 5.2 FALSE 7
3 Nursery.Field 2.8 3 Grassland 4.3 FALSE 2
4 Rush.Meadow 2.4 5 Meadow 4.9 TRUE 5
5 Gunness.Thicket 3.8 0 Scrub 4.2 FALSE 6
6 Oak.Mead 3.1 2 Grassland 3.9 FALSE 2
7 Church.Field 3.5 3 Grassland 4.2 FALSE 3
8 Ashurst 2.1 0 Arable 4.8 FALSE 4
9 The.Orchard 1.9 0 Orchard 5.7 FALSE 9
10 Rookery.Slope 1.5 4 Grassland 5.0 TRUE 7
11 Garden.Wood 2.9 10 Scrub 5.2 FALSE 8
12 North.Gravel 3.3 1 Grassland 4.1 FALSE 1
13 South.Gravel 3.7 2 Grassland 4.0 FALSE 2
14 Observatory.Ridge 1.8 6 Grassland 3.8 FALSE 0
15 Pond.Field 4.1 0 Meadow 5.0 TRUE 6
16 Water.Meadow 3.9 0 Meadow 4.9 TRUE 8
26 STATISTICS:ANINTRODUCTIONUSINGR
17 Cheapside 2.2 8 Scrub 4.7 TRUE 4
18 Pound.Hill 4.4 2 Arable 4.5 FALSE 5
19 Gravel.Pit 2.9 1 Grassland 3.5 FALSE 1
20 Farm.Wood 0.8 10 Scrub 5.1 TRUE 3
Thevariablenamesappearinrownumber1.NoticethatRhasexpandedourabbreviated
T and F into TRUE and FALSE.
SelectingPartsofaDataframe:Subscripts
We often want to extract part of a dataframe. This is a very general procedure in R,
accomplished using what are called subscripts. You can think of subscripts as addresses
withinavector,amatrixoradataframe.SubscriptsinRappearwithinsquarebrackets,thus
y[7]isthe7thelementofthevectorcalledyandz[2,6]isthe2ndrowofthe6thcolumn
of a two-dimensional matrix called z. This is in contrast to arguments to functions in R,
which appear in round brackets (4,7).
Wemightwanttoselectalltherowsofadataframeforcertainspecifiedcolumns.Orwe
might want to select all the columns for certain specified rows of the dataframe. The
conventioninRisthatwhenwedonotspecifyanysubscript,thenalltherows,orallthe
columns is assumed. This syntax is difficult to understand on first acquaintance, but [,
‘blankthencomma’means‘alltherows’and,]‘commathenblank’meansallthecolumns.
For instance, to select the first column of the dataframe, use subscript [,1]. To select
groupsofcolumnsweprovideseveralcolumnnumbers.Thus,toselectalltherowsofthe
first three columns of worms, we write:
worms[,1:3]
Field.Name Area Slope
1 Nashs.Field 3.6 11
2 Silwood.Bottom 5.1 2
3 Nursery.Field 2.8 3
4 Rush.Meadow 2.4 5
5 Gunness.Thicket 3.8 0
6 Oak.Mead 3.1 2
7 Church.Field 3.5 3
8 Ashurst 2.1 0
9 The.Orchard 1.9 0
10 Rookery.Slope 1.5 4
11 Garden.Wood 2.9 10
12 North.Gravel 3.3 1
13 South.Gravel 3.7 2
14 Observatory.Ridge 1.8 6
15 Pond.Field 4.1 0
16 Water.Meadow 3.9 0
17 Cheapside 2.2 8
18 Pound.Hill 4.4 2
19 Gravel.Pit 2.9 1
20 Farm.Wood 0.8 10
DATAFRAMES 27
To select just the middle 11 rows for all the columns of the dataframe, use subscript
[5:15,] like this:
worms[5:15,]
Field.Name Area Slope Vegetation Soil.pH Damp Worm.density
5 Gunness.Thicket 3.8 0 Scrub 4.2FALSE 6
6 Oak.Mead 3.1 2 Grassland 3.9FALSE 2
7 Church.Field 3.5 3 Grassland 4.2FALSE 3
8 Ashurst 2.1 0 Arable 4.8FALSE 4
9 The.Orchard 1.9 0 Orchard 5.7FALSE 9
10 Rookery.Slope 1.5 4 Grassland 5.0 TRUE 7
11 Garden.Wood 2.9 10 Scrub 5.2FALSE 8
12 North.Gravel 3.3 1 Grassland 4.1FALSE 1
13 South.Gravel 3.7 2 Grassland 4.0FALSE 2
14 Observatory.Ridge 1.8 6 Grassland 3.8FALSE 0
15 Pond.Field 4.1 0 Meadow 5.0 TRUE 6
Itisoftenusefultoselectcertainrows,basedonlogicaltestsonthevaluesofoneormore
variables.HereisthecodetoselectonlythoserowswhichhaveArea>3andSlope<3
using ‘comma then blank’ to specify all the columns like this:
worms[Area>3&Slope<3,]
Field.Name Area Slope Vegetation Soil.pH Damp Worm.density
2 Silwood.Bottom 5.1 2 Arable 5.2FALSE 7
5 Gunness.Thicket 3.8 0 Scrub 4.2FALSE 6
6 Oak.Mead 3.1 2 Grassland 3.9FALSE 2
12 North.Gravel 3.3 1 Grassland 4.1FALSE 1
13 South.Gravel 3.7 2 Grassland 4.0 FALSE 2
15 Pond.Field 4.1 0 Meadow 5.0 TRUE 6
16 Water.Meadow 3.9 0 Meadow 4.9 TRUE 8
18 Pound.Hill 4.4 2 Arable 4.5FALSE 5
Sorting
Youcansorttherowsorthecolumnsofthedataframeinanywayyouchoose,buttypically
youwillwanttoseeallofthecolumnsandtosortonthebasisofthevaluesinoneormoreof
thecolumns.Bydefault,thingsinRaresortedintoascendingorder(i.e.intoalphabetical
order for character data, and increasing numeric order for numbers). The simplest way to
sortistousethenameofthevariable.SupposewewantthewholedataframesortedbyArea:
worms[order(Area),]
Field.Name Area Slope Vegetation Soil.pH Damp Worm.density
20 Farm.Wood 0.8 10 Scrub 5.1 TRUE 3
10 Rookery.Slope 1.5 4 Grassland 5.0 TRUE 7
14 Observatory.Ridge 1.8 6 Grassland 3.8 FALSE 0
9 The.Orchard 1.9 0 Orchard 5.7 FALSE 9
8 Ashurst 2.1 0 Arable 4.8 FALSE 4
28 STATISTICS:ANINTRODUCTIONUSINGR
17 Cheapside 2.2 8 Scrub 4.7 TRUE 4
4 Rush.Meadow 2.4 5 Meadow 4.9 TRUE 5
3 Nursery.Field 2.8 3 Grassland 4.3 FALSE 2
11 Garden.Wood 2.9 10 Scrub 5.2 FALSE 8
19 Gravel.Pit 2.9 1 Grassland 3.5 FALSE 1
6 Oak.Mead 3.1 2 Grassland 3.9 FALSE 2
12 North.Gravel 3.3 1 Grassland 4.1 FALSE 1
7 Church.Field 3.5 3 Grassland 4.2 FALSE 3
1 Nashs.Field 3.6 11 Grassland 4.1 FALSE 4
13 South.Gravel 3.7 2 Grassland 4.0 FALSE 2
5 Gunness.Thicket 3.8 0 Scrub 4.2 FALSE 6
16 Water.Meadow 3.9 0 Meadow 4.9 TRUE 8
15 Pond.Field 4.1 0 Meadow 5.0 TRUE 6
18 Pound.Hill 4.4 2 Arable 4.5 FALSE 5
2 Silwood.Bottom 5.1 2 Arable 5.2 FALSE 7
Thekeypointtonoteisthatorder(Area)comesbeforethecomma,andthereisablank
afterthecomma(whichmeansthatwewantallofthecolumns).Theoriginalrownumbers
appear on the left of the sorted dataframe.
Nowsupposewejustwantthecolumnscontainingnumericinformationtoappearinthe
output; these are column numbers 2, 3, 5 and 7:
worms[order(Area),c(2,3,5,7)]
Area Slope Soil.pH Worm.density
20 0.8 10 5.1 3
10 1.5 4 5.0 7
14 1.8 6 3.8 0
9 1.9 0 5.7 9
8 2.1 0 4.8 4
17 2.2 8 4.7 4
4 2.4 5 4.9 5
3 2.8 3 4.3 2
11 2.9 10 5.2 8
19 2.9 1 3.5 1
6 3.1 2 3.9 2
12 3.3 1 4.1 1
7 3.5 3 4.2 3
1 3.6 11 4.1 4
13 3.7 2 4.0 2
5 3.8 0 4.2 6
16 3.9 0 4.9 8
15 4.1 0 5.0 6
18 4.4 2 4.5 5
2 5.1 2 5.2 7
To sort things into descending order we employ the reverse function rev like this:
worms[rev(order(worms[,5])),c(5,7)]
DATAFRAMES 29
Soil.pH Worm.density
9 5.7 9
11 5.2 8
2 5.2 7
20 5.1 3
15 5.0 6
10 5.0 7
16 4.9 8
4 4.9 5
8 4.8 4
17 4.7 4
18 4.5 5
3 4.3 2
7 4.2 3
5 4.2 6
12 4.1 1
1 4.1 4
13 4.0 2
6 3.9 2
14 3.8 0
19 3.5 1
whichsortsintodescendingorderbySoilpH,withonlySoilpHandWormdensityasoutput
(because of the c(5,7)). This makes the point that you can specify the sorting variable
eitherbyname(aswedidwithAreaabove)orbycolumnnumber(aswedidwithSoil.pH
here by specifying column number 5).
SummarizingtheContentofDataframes
The object called worms now has all the attributes of a dataframe. For example, you can
summarize it, using summary:
summary(worms)
Field.Name Area Slope Vegetation
Ashurst :1 Min. :0.800 Min. :0.00 Arable :3
Cheapside :1 1stQu.:2.175 1stQu.:0.75 Grassland:9
Church.Field:1 Median:3.000 Median:2.00 Meadow :3
Farm.Wood :1 Mean :2.990 Mean :3.50 Orchard :1
Garden.Wood :1 3rdQu.:3.725 3rdQu.:5.25 Scrub :4
Gravel.Pit :1 Max. :5.100 Max. :11.00
(Other) :14
Soil.pH Damp Worm.density
Min. :3.500 Mode :logical Min. :0.00
1stQu.:4.100 FALSE:14 1stQu.:2.00
Median:4.600 TRUE :6 Median:4.00
Mean :4.555 NA’s :0 Mean :4.35
3rdQu.:5.000 3rdQu.:6.25
Max. :5.700 Max. :9.00
30 STATISTICS:ANINTRODUCTIONUSINGR
Valuesof continuous variables aresummarized undersix headings: oneparametric (the
arithmeticmean)andfivenon-parametric(maximum,minimum,median,25thpercentileor
firstquartile,and75thpercentileorthirdquartile).Levelsofcategoricalvariablesarecounted.
SummarizingbyExplanatoryVariables
Thefunctionyouneedtomasterforsummarizingquantitativeinformationindataframesis
calledaggregate.Youwilloftenwanttoknowtheaveragevaluesofcontinuousvariables
within the dataframe summarized by factor levels from one or more of the categorical
variables.Forinstance,wemaywanttoknowthemeannumberofwormsunderdifferent
plantcommunities.ForasingleresponsevariablelikeWorm.density,youcanusetapply
and with like this:
with(worms,tapply(Worm.density,Vegetation,mean))
Arable Grassland Meadow Orchard Scrub
5.33 2.44 6.33 9.00 5.25
Intheearlystagesofanalysis,however,weoftenwanttoseethemeanvaluesofallthe
continuousvariablessummarizedatthesametime,andthisiswhereaggregatecomesinto
itsown.Weneedtodoalittlebitofworkinadvance,bynotingdownthecolumnnumbers
that contain the variables for which it would be useful and sensible to calculate the mean
values(i.e.thecolumnscontainingrealnumbers).TheseareAreaandSlopeincolumns2
and 3 respectively, Soil.pH in column 5 and Worm.density in column 7. To get their
mean values classified by plant communities we need only type:
aggregate(worms[,c(2,3,5,7)],list(Vegetation),mean)
Group.1 Area Slope Soil.pH Worm.density
1 Arable 3.87 1.33 4.83 5.33
2 Grassland 2.91 3.67 4.10 2.44
3 Meadow 3.47 1.67 4.93 6.33
4 Orchard 1.90 0.00 5.70 9.00
5 Scrub 2.42 7.00 4.80 5.25
whichcausesallofthemeanvaluestobeprinted.Doyouknowwhythereisacommaafter
the left hand square bracket on worms? Note that the column containing the levels of
VegetationisheadedGroup.1.Thisisthedefaultusedbyaggregate.Withinthecolumn,
the levels of Vegetation appear in alphabetical order. To get the column headed by
‘Community’ instead of Group.1 we annotate the list like this:
aggregate(worms[,c(2,3,5,7)],list(Community=Vegetation),mean)
Community Area Slope Soil.pH Worm.density
1 Arable 3.87 1.33 4.83 5.33
2 Grassland 2.91 3.67 4.10 2.44
3 Meadow 3.47 1.67 4.93 6.33
4 Orchard 1.90 0.00 5.70 9.00
5 Scrub 2.42 7.00 4.80 5.25
DATAFRAMES 31
Youcandomultipleclassificationsusingtwoormorecategoricalexplanatoryvariables:
hereisasummarythatasksforthemeanvaluesseparatelyforeachlevelofsoilmoisture
within each vegetation type
aggregate(worms[,c(2,3,5,7)],
list(Moisture=Damp,Community=Vegetation),mean)
Moisture Community Area Slope Soil.pH Worm.density
1 FALSE Arable 3.87 1.33 4.83 5.33
2 FALSE Grassland 3.09 3.62 3.99 1.88
3 TRUE Grassland 1.50 4.00 5.00 7.00
4 TRUE Meadow 3.47 1.67 4.93 6.33
5 FALSE Orchard 1.90 0.00 5.70 9.00
6 FALSE Scrub 3.35 5.00 4.70 7.00
7 TRUE Scrub 1.50 9.00 4.90 3.50
Youwillnoticethataggregateproducesonlythoserowsforwhichthereisoutput(there
arenodrymeadows,andnowetarableorwetorchards,forinstance).Thisisincontrastto
tapply, which produces NA for missing combinations:
with(worms,tapply(Slope,list(Damp,Vegetation),mean))
Arable Grassland Meadow Orchard Scrub
FALSE 1.33 3.62 NA 0 5
TRUE NA 4.00 1.67 NA 9
Youchoosebetweenaggregateandtapplyonthebasisofwhichofthemservesyour
needs best in a particular case, bearing in mind that tapply can summarize only one
variable at a time.
Theanswertothequiz(above)isthatthecommaafterthesquarebracketmeans‘selectall
oftherowsinthedataframe’.Ifwewantedonlysomeoftherows,thenwewouldneedto
specifywhichrows,justlikewespecifiedwhichcolumnswewantedusingc(2,3,5,7).
FirstThingsFirst:GettoKnowYourData
Once the data are in the computer, the temptation is to rush straight into statistical
analysis.Thisisexactlythewrongthingtodo.Youneedtogettoknowyourdatafirst.
Thisisparticularlyimportantearlyoninaproject,becausethereisaveryhighchancethat
thedatacontainmistakes.Obviously,theseneedtobeputrightbeforeanythingsensible
can be done.
Justasimportant,ifyoudonotknowwhatyourdatalooklike,thenyouwillnotknow
whatmodeltoselecttofittothedata(e.g.astraightlineoracurvedline),orwhetherthe
assumptions of your intended model are met by the data (e.g. constancy of variance and
normality of errors).
The recommended procedure is this. First, just plot the response variable on its
own.Thisiscalledanindexplot.Itwillshowifthereareanyglaringerrorsinthedata
and whether there are trends or cycles in the values of the response as they appear in
the dataframe.
32 STATISTICS:ANINTRODUCTIONUSINGR
data<-read.csv("c:\\temp\\das.csv")
attach(data)
head(data)
y
1 2.514542
2 2.559668
3 2.460061
4 2.702720
5 2.571997
6 2.412833
Data inspection could not be simpler. Just plot the values of y using plot(y). This
producesascatterplotwiththevaluesofyappearingfromlefttorightintheorderinwhich
they appear in the dataframe:
plot(y)
Onedatapointsticksoutlikeasorethumb.Weneedtogobacktothelabnotebookand
check what has happened. It is useful to know what line of the dataframe contains the
unusuallylargevalueofy.Thisiseasytoworkoutusingthewhichfunction.Inspectionof
theplotshowsthatourquestionabledatapointistheonlyonelargerthan10,sowecanuse
the which function like this:
which(y>10)
[1]50
sotheoutlyingdatapointisonline50ofthespreadsheet.Nowweneedtogobacktothelab
book and try to work out what happened. What is the exact value of the outlier? We use
subscripts [square brackets] for this. We want the 50th value of y:
DATAFRAMES 33
y[50]
[1]21.79386
Thelabbookshowsthatthevalueshouldbe2.179386.Whathashappenedisthatatyping
errorhasputthedecimalpointinthewrongplace.Clearlyweshouldchangetheentryinthe
spreadsheetandstartagain.Nowwhenweuseplottocheckthedata,itlookslikethis
plot(y)
There is no reason to suspect any of the data points. The data are not trended with the
orderofentry,andthedegreeofscatterlookstobeconsistentfromlefttoright.Thatisgood
news.
Itisimportanttoappreciatethatoutliersarenotnecessarilymistakes.Mostoutlierswill
begenuinevaluesoftheresponse.Thekeypointaboutoutliersisthatyouneedtobeaware
oftheirexistence(theplotswillshowthis)andtounderstandhowinfluentialtheoutliersare
indeterminingeffectsizesandstandarderrorsinyourchosenmodel.Thiscrucialpointis
discussedonp.135.Nowdothesamethingforeachofthecontinuousexplanatoryvariables
in turn.
Tocheckformistakesinthefactorlevelsofcategoricalexplanatoryvariables,youusethe
table function. This shows how many times each factor level appears in a particular
column of the data frame. Mistakes will be highlighted because there will be more factor
levels than there should be. Here is a variable called treatment from an experiment on
fertilizerapplicationandplantgrowth.Therearefourlevelsofthefactor:control,nitrogen,
phosphorus and both N and P. This is how we check the data:
yields<-read.csv("c:\\temp\\fertyield.csv")
attach(yields)
head(yields)
34 STATISTICS:ANINTRODUCTIONUSINGR
treatment yield
1 control 0.8274156
2 control 3.6126275
3 control 2.6192581
4 control 1.7412190
5 control 0.6590589
6 control 0.4891107
Asyoucansee,thevariablecalledtreatmentisafactorwithtext(‘control’onthesesix
lines) entries to describe which of the four treatments was applied to obtain the yield in
column 2. This is how we check the factor levels:
table(treatment)
variable
bothNandP control nitogen nitrogen phosphorus
10 10 1 9 10
There are five factor levels printed rather than the four we expected, so it is clear that
something is wrong. What has happened is that one of the nitrogen values has been
misspelledasnitogenwiththeresultthatthereareonly9nitrogenvalues(notthe10we
expect)andanextracolumninthetablewithoneentryforthespellingmistake.Thenext
step is to find out which row the mistake occurs in, then armed with this information, go
backtotheoriginalspreadsheetandcorrecttheerror.Thisiseasyusingthewhichfunction.
Note the use of ‘double equals’ to check for equality):
which(treatment=="nitogen")
[1]11
Themistakeisinlinenumber11.Weneedtocorrectthatlineinthespreadsheet.Make
thechangethenstartagain,byreadingthenewcorrecteddataframeintoR.Checkallofyour
variables,continuousandcategorical,oneatatimeandcorrectallofthemistakesyoufind.
Now it is time to look at relationships between variables.
detach(yields)
Relationships
The place to start is with pairwise relationships. When both variables are continuous, the
appropriate graph is a scatterplot:
data<-read.csv("c:\\temp\\scatter.csv")
attach(data)
head(data)
x y
1 0.000000 0.00000
2 5.112000 61.04000
3 1.320000 11.11130
DATAFRAMES 35
4 35.240000 140.65000
5 1.632931 26.15218
6 2.297635 10.00100
Theresponsevariableisyandtheexplanatoryvariableisx,sowewriteplot(x,y)or
plot(y∼x) to see the relationship (the two forms of plot produce the same graph; the
choiceofwhichtouseisentirelyuptoyou).Weintroducetwonewfeaturestocustomizethe
plot: changing the plotting symbol (pch stands for ‘plotting character’) from the default
opencircleswehavebeenusingsofartocoloureddiscswithablackedge(pch=21)and
select a bright colour for the fill of the disc (the ‘background’ as it is called in R, using
bg="red"):
plot(x,y,pch=21,bg="red")
Thisplotimmediatelyalertsustotwoimportantissues:(1)therelationshipbetweenthe
response and the explanatory variable is curved, not a straight line; and (2) the degree of
scatter of the response variable increases from left to right (this is what non-constant
variance(heteroscedasticity)lookslike).Thesetwofeaturesofthedataarenotmistakes,but
they are very important elements in model selection (despite the positive correlation
between x and y, we would not do a linear regression on these data, for instance).
When the explanatory variable is categorical the plot function produces a box-and-
whisker plot. This is very useful for error-checking, as the following example illustrates:
data<-read.csv("c:\\temp\\weather.data.csv")
attach(data)
head(data)
upper lower rain month yr
1 10.8 6.5 12.2 1 1987
2 10.5 4.5 1.3 1 1987
3 7.5 -1.0 0.1 1 1987
36 STATISTICS:ANINTRODUCTIONUSINGR
4 6.5 -3.3 1.1 1 1987
5 10.0 5.0 3.5 1 1987
6 8.0 3.0 0.1 1 1987
Therearethreepotentialcontinuousresponsevariablesinthisdataframe(dailymaximum
temperature ‘upper’ in degrees Celsius, daily minimum temperature ‘lower’ in degrees
Celsius,anddailytotalprecipitation‘rain’inmillimetres)andtwoquantitativevariablesthat
we might choose to define as categorical (month and year). Here, we carry out an initial
inspectionofthemaximumtemperaturedata.Notethatwhentheplotfunctionisinvoked
withacategoricalexplanatoryvariable–factor(month)inthiscase–thenRproducesa
box-and-whisker plot rather than a scatterplot (see p. 67 for details).
plot(factor(month),upper)
Thebox-and-whiskerplotshowstheveryclearseasonalpatternofmediantemperature,
peaking in July and August and reaching a minimum in January. The details of what the
boxesandwhiskersmeanareexplainedonp.161.Forourpresentpurposesweconcentrate
on error-checking.
Theplotshowsafreezingday(0maximum)inJune(month=6),whichisunheardofatthis
location.Itturnsoutthatthethermometerbrokeonthisdayandwasreplacedbyanewone.
Themissingvalueforthedayofthebreakdownwasfoolishlyenteredasazero(itshouldhave
beenNA).Again,wegobacktothespreadsheetandreplacetheerroneous0bythecorrectNA
(thisstandsfornotavailable;noticethatNAisnotenclosedinquotationmarks).
LookingforInteractionsbetweenContinuousVariables
Oncetheobviouserrorshavebeencorrected,thenextquestionsconcernmodelchoice.For
instance,doestheresponsetoonevariabledependuponthelevelofanothervariable?(inthe
jargon, this is known as a statistical interaction). With continuous explanatory variables,
we can look for interaction effects using conditioning plots (typically known as coplots).
DATAFRAMES 37
With categoricalexplanatory variables, we can lookforinteraction effects using barplots.
Herewehaveoneresponsevariable(y)andtwocontinuousexplanatoryvariables(xandz):
data<-read.csv("c:\\temp\\coplot.csv")
attach(data)
head(data)
x y z
1 95.73429 107.8087 14.324408
2 36.20660 223.9257 10.190577
3 28.71378 245.2523 12.566815
4 78.36956 132.7344 13.084384
5 38.37717 222.2966 9.960033
6 57.18078 184.8372 10.035677
Twoscatterplotssidebysidelookbetterifwechangetheshapeoftheplottingwindow
from the default square (7 × 7 inches) to a rectangle (7 × 4 inches) like this:
windows(7,4)
thenalterthegraphicsparametertospecifytwosetsofaxisonthesamerow(seep.134for
details):
par(mfrow=c(1,2))
There is no clear relationship between the response and either of the two continuous
explanatory variables when they are fitted on their own:
plot(x,y)
plot(z,y)
To look for interactions between continuous explanatory variables (like x and z in this
example) we employ a superb graphical function called coplot. The function plots y
againstxconditionalonthevalueofz.Thefunctionisverysimpletouse.Thesymbolthat
might benewtoyouistheverticalbar|whichisreadas‘given’.Thesyntaxsays‘ploty
againstxgiventhevalueofz’andiswrittenasplot(y∼x|z).Thedefaultistosplitthedata
intosixgraphs(butyoucanchangethisifyouwantto).Thesixthofthedatawiththelowest
38 STATISTICS:ANINTRODUCTIONUSINGR
values of z appear in the bottom left-hand panel as a plot of y against x. To improve the
appearanceoftheplotwecanusesolidblackplottingsymbols(pch=16)andfitatrendline
throughthescatterplotinred(thisiscalledanon-parametricsmoother,andisinvokedby
thefunction panel.smooth).Firstwereturntothedefault window shape (7×7inches):
windows(7,7)
then we draw the conditioning plot:
coplot(y∼x|z,pch=16,panel=panel.smooth)
Thisshowsareallyclearrelationshipbetweentheresponsevariableandx,buttheformof
the relationship depends on the value of z (the second of our continuous explanatory
variables).Forlowvaluesofz(inthebottomleft-handpanel)therelationshipbetweenyand
xisstronglynegative.Forhighvaluesofz(topright)therelationshipisstronglypositive.As
zincreases(frombottomlefttobottomright,thenfromtoplefttotopright),theslopeofthe
relationshipbetween yandxincreases rathersmoothlyfromlargenegativevaluestozero
then to increasingly positive values. Only coplot can show this kind of interaction so
simply and clearly.
DATAFRAMES 39
Thetoppartofthefigurecanbeconfusingforbeginners.Theshadedhorizontalbarsare
calledshingles(aftertheAmericanwordforrooftiles)andtheyshowtherangeofvaluesof
thevariable(zinthisexample)usedtoproduceeachofthesixpanelplots.Thebottom(left-
hand)shingleshowsthatthebottomleft-handpanel(withthestrongnegativerelationship
betweenyandx)wasbasedondataselectedtohavezvaluesbetween10and20(lookatthe
scaleonthetopaxisbeneath‘Given:z’).Thenextpaneluseddatawithzvaluesbetween13
and 30, the next between 20 and 40, and so on. The shingles overlap because that is the
defaultsetting(see?coplotfordetailsofoverlap=0.5):foranon-edgeplot,halfofthe
datapointsaresharedwiththepaneltoitsleft,andhalfaresharedwiththepanelonitsright.
You can specify non-overlapping panels if that is what you want (overlap =0).
GraphicstoHelpwithMultipleRegression
The problems become most acute when we have many continuous explanatory variables
andthedatacomefromobservationalstudieswherewehavenocontroloverreplicationor
randomization.Indatasetslikethis,theexplanatoryvariablesareoftencorrelatedwitheach
other (most simple models assume that the explanatory variables are independent –
orthogonal in the jargon). We discuss these issues in detail in Chapter 10, but at this
stage we simply observe that there are no easy cures for this. Two very useful tools for
preliminary investigation of multiple regression data are tree models and generalized
additive models, as illustrated on p. 197.
InteractionsInvolvingCategoricalVariables
The following data are from a factorial experiment involving nitrogen and phosphorus
fertilizers applied separately and in combination:
data<-read.csv("c:\\temp\\np.csv")
attach(data)
head(data)
yield nitrogen phosphorus
1 0.8274156 no no
2 3.6126275 no no
3 2.6192581 no no
4 1.7412190 no no
5 0.6590589 no no
6 0.4891107 no no
There is one continuous response variable (yield) and two categorical explanatory
variables(nitrogenandphosphorus)eachwithtwolevels(yesandno,meaningthefertilizer
was or was not applied to the plot in question). First we look at the effects of the two
nutrients separately:
windows(7,4)
par(mfrow=c(1,2))
plot(nitrogen,yield,main="N")
plot(phosphorus,yield,main="P")
40 STATISTICS:ANINTRODUCTIONUSINGR
Theseplotsshowwhatarecalledthe‘maineffects’ofnitrogenandphosphorus:itlooks
as if nitrogen increases yield slightly more than does phosphorus. The median for plus
nitrogen(‘yes’intheleft-handplot)isabovethetopoftheboxfornonitrogen,whereasthe
medianforplusphosphorusisbelowthetopoftheboxfornophosphorus(right-handplot).
Whatthesemaineffectsfailtoshowusiswhethertheresponsetophosphorusdependson
thelevelofnitrogen.Whatweneedisaninteractionplotshowingeffectsizesforfourlevels
of response: neither nutrient, just nitrogen, just phosphorus, or both N and P. We use the
function tapply to do this:
tapply(yield,list(nitrogen,phosphorus),mean)
no yes
no 1.47384 1.875928
yes 2.28999 3.480184
Therowsrefertonitrogenandthecolumnstophosphorus.WithnoP(leftcolumn),the
effectsizeforNis2.290/1.474=1.55,butwithPtheeffectsizeofNis3.480/1.876=1.86.
The effect size for nitrogen depends on the level of phosphorus (55% increase in yield
without P, but 86% increase with P). That is an example of a statistical interaction: the
response to one factor depends upon the level of another factor.
Weneedawayofshowinginteractionsgraphically.Therearemanywayofdoingthis,
but perhaps the most visually effective is to use the barplot function. We can use the
output from tapply directly for this, but it is a good idea to add a legend to show the
nitrogen treatments associated using two colours of shading:
barplot(tapply(yield,list(nitrogen,phosphorus),mean),
beside=TRUE,xlab="phosphorus")
Thelocatorfunctionallowsyoutoputthelegendinaplacewhereitdoesnotinterfere
withanyofthebars.Putthecursorwhereyouwantthetopleftcornerofthelegendboxto
appear, then left click:
legend(locator(1),legend=c("no","yes"),title="nitrogen",
fill=c("black","lightgrey"))
DATAFRAMES 41
Foryourfinalpresentation,youwouldwanttoadderrorbarstotheplot,butweshalldeal
withthislater,oncewehavediscussedhowtomeasuretheunreliabilityofeffects(seep.162).
Itisessentialtospendtimeonunderstandingthepatternsinyourdatabeforeyouembark
onthestatistics.Thesepreliminaryplotsarenotoptionalextras.Theyareabsolutelyvitalfor
deciding what kind of statistical modelling will be appropriate, and what kinds of
assumptions about the data (linearity of response, constancy of variance, normality of
errors)arelikelytobejustified.Asyouwillsee,wereturntochecktheassumptionsagain,
once the statistical modelling has been carried out (see p. 134).
That concludes the initial data inspection. Now we can begin to think about statistical
analysisofthedata.Weshallconcentrateonmeasuringeffectsizesandtheirunreliabilities
(the modern approach) and pay relatively little attention to hypothesis testing (the old-
fashioned approach).
FurtherReading
Chambers,J.M.andHastie,T.J.(1992)StatisticalModelsinS,Wadsworth&Brooks/Cole,Pacific
Grove,CA.
Crawley, M.J.(2013) The RBook,2ndedn,JohnWiley & Sons,Chichester.
3
Central Tendency
Despitethefactthateverythingvaries,measurementsoftenclusteraroundcertaininterme-
diate values; this attribute is called central tendency. Even if the data themselves do not
show much tendency to cluster round some central value, the parameters derived from
repeatedexperiments(e.g.replicatedsamplemeans)almostinevitablydo(thisiscalledthe
centrallimittheorem;seep.70).Weneedsomedatatoworkwith.Thedataareinavector
called y stored in a text file called yvalues
yvals<-read.csv("c:\\temp\\yvalues.csv")
attach(yvals)
So how should we quantifycentraltendency? Perhaps the mostobvious way isjust by
lookingatthedata,withoutdoinganycalculationsatall.Thedatavaluesthatoccurmost
frequentlyarecalledthemode,andwediscoverthevalueofthemodesimplybydrawinga
histogram of the data like this:
hist(y)
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
CENTRALTENDENCY 43
Sowewouldsaythatthemodalclassofywasbetween1.10and1.12(wewillseehowto
control the location of the break points in a histogram later).
ThemoststraightforwardquantitativemeasurPeofcentraltendencyisthearithmeticmean
ofthedata.ThisisthesumPofallthedatavalues ydividedbythenumberofdatavalues,n.
ThecapitalGreeksigma justmeans‘addupallthevalues’ofwhatfollows;inthiscase,
a set of y values. So if we call the arithmetic mean ‘y bar’, y, we can write
P
y
y
n
Theformulashowshowwewouldwriteageneralfunctiontocalculatearithmeticmeans
for any vector of y values. First, we need to add them up. We could do it like this:
y[1]+y[2]+y[3]+. ..+ y[n]
but that is very long-winded, and it supposes that we know the value of n in advance.
Fortunately,thereisabuilt-infunctioncalledsumthatworksoutthetotalforanylengthof
vector, so
total<-sum(y)
gives us the value for the numerator. But what about the number of data values? This is
likely to vary from application to application. We could print out the y values and count
them,butthatistediousanderror-prone.Thereisaveryimportant,generalfunctioninRto
workthisoutforus.Thefunctioniscalledlength(y)anditreturnsthenumberofnumbers
in the vector called y:
n<-length(y)
So our recipe for calculating the arithmetic mean would be ybar <-total/n.
ybar<-total/n
[1]1.103464
There is no need to calculate the intermediate values, total and n, so it would be more
efficienttowriteybar<-sum(y)/length(y).Toputthislogicintoageneralfunctionwe
need to pick a name for the function, say arithmetic.mean, then define it as follows:
arithmetic.mean<-function(x)sum(x)/length(x)
Noticetwothings:wedonotassigntheanswersum(x)/length(x)toavariablename
likeybar;andthenameofthevectorxusedinsidethefunction(x)maybedifferentfrom
thenamesonwhichwemightwanttousethefunctioninfuture(y,forinstance).Ifyoutype
the name of a function on its own, you get a listing of the contents:
arithmetic.mean
function(x)sum(x)/length(x)
44 STATISTICS:ANINTRODUCTIONUSINGR
Nowwecantestthefunctiononsomedata.Firstweuseasimpledatasetwhereweknow
the answer already, so that we can check that the function works properly: say
data<-c(3,4,6,7)
where we can see immediately that the arithmetic mean is 5.
arithmetic.mean(data)
[1]5
So that’s OK. Now we can try it on a realistically big data set:
arithmetic.mean(y)
[1]1.103464
YouwillnotbesurprisedtolearnthatRhasabuilt-infunctionforcalculatingarithmetic
meansdirectly,andagain,notsurprisingly,itiscalledmean.Itworksinthesamewayasdid
our home-made function:
mean(y)
[1]1.103464
Arithmeticmeanisnottheonlyquantitativemeasureofcentraltendency,andinfactithas
someratherunfortunateproperties.Perhapsthemostseriousfailingofthearithmeticmeanis
thatitishighlysensitivetooutliers.Justasingleextremelylargeorextremelysmallvalueinthe
datasetwillhaveabigeffectonthevalueofthearithmeticmean.Weshallreturntothisissue
later,butournextmeasureofcentraltendencydoesnotsufferfrombeingsensitivetooutliers.
Itiscalledthemedian,andisthe‘middlevalue’inthedataset.Towriteafunctiontoworkout
themedian,thefirstthingweneedtodoissortthedataintoascendingorder:
sorted<-sort(y)
Nowwejustneedtofindthemiddlevalue.Thereisslighthitchhere,becauseifthevector
containsanevennumberofnumbers,thenthereisnomiddlevalue.Letusstartwiththeeasy
casewherethevectorcontainsanoddnumberofnumbers.Thenumberofnumbersinthe
vector is given by length(y)and the middle value is half of this:
length(y)/2
[1]19.5
Sothemedianvalueisthe20thvalueinthesorteddataset.Toextractthemedianvalueofy
weneedtouse20asasubscript,not19.5,soweshouldconvertthevalueoflength(y)/2
intoaninteger.Weuseceiling(‘thesmallestintegergreaterthan’)forthis:
ceiling(length(y)/2)
[1]20
CENTRALTENDENCY 45
So now we can extract the median value of y:
sorted[20]
[1]1.108847
or, more generally:
sorted[ceiling(length(y)/2)]
[1]1.108847
or even more generally, omitting the intermediate variable called sorted:
sort(y)[ceiling(length(y)/2)]
[1]1.108847
Butwhataboutthecasewherethevectorcontainsanevennumberofnumbers?Letus
manufacture such a vector, by dropping the first element from our vector called y using
negative subscripts like this:
y.even<-y[-1]
length(y.even)
[1]38
Thelogicisthatweshallworkoutthearithmeticaverageofthetwovaluesofyoneither
side of the middle; in this case, the average of the 19th and 20th sorted values:
sort(y.even)[19]
[1]1.108847
sort(y.even)[20]
[1]1.108853
So in this case, the median would be:
(sort(y.even)[19]+sort(y.even)[20])/2
[1]1.10885
But to make it general we need to replace the 19 and 20 by
ceiling(length(y.even)/2)
and
ceiling(1+length(y.even)/2)
respectively.
46 STATISTICS:ANINTRODUCTIONUSINGR
Thequestionnowarisesastohowweknow,ingeneral,whetherthevectorycontainsan
oddoranevennumberofnumbers,sothatwecandecidewhichofthetwomethodstouse.
Thetrickhereistouse‘modulo’.Thisistheremainder(theamount‘leftover’)whenone
integerisdividedbyanother.Anevennumberhasmodulo0whendividedby2,andanodd
numberhasmodulo1.ThemodulofunctioninRis%%(twosuccessivepercentsymbols)and
itisusedwhereyouwoulduseslash/tocarryoutaregulardivision.Youcanseethisin
action with an even number, 38, and odd number, 39:
38%%2
[1]0
39%%2
[1]1
Nowwehaveallthetoolsweneedtowriteageneralfunctiontocalculatemedians.Letus
call the function med and define it like this:
med<-function(x){
modulo<-length(x)%%2
if(modulo==0) (sort(x)[ceiling(length(x)/2)]+sort(x)[ceiling
(1+length(x)/2)])/2
else sort(x)[ceiling(length(x)/2)]
}
Noticethatwhentheifstatementistrue(i.e.wehaveanevennumberofnumbers)then
the expression immediately following the if statement is evaluated (this is the code for
calculatingthemedianwithanevennumberofnumbers). Whenthe ifstatementisfalse
(i.e.wehaveanoddnumberofnumbers,andmodulo==1)thentheexpressionfollowing
the else statement is evaluated (this is the code for calculating the median with an odd
numberofnumbers).Letustryitout,firstwiththeodd-numberedvectory,thenwiththe
even-numbered vector y.even, to check against the values we obtained earlier.
med(y)
[1]1.108847
med(y.even)
[1]1.10885
Bothofthesecheckout.Again,youwillnotbesurprisedthatthereisabuilt-infunction
for calculating medians, and helpfully it is called median:
median(y)
[1]1.108847
median(y.even)
[1]1.10885
CENTRALTENDENCY 47
For processes that change multiplicatively rather than additively, then neither the
arithmetic mean nor the median is an ideal measure of central tendency. Under these
conditions,theappropriatemeasureisthegeometricmean.Theformaldefinitionofthisis
somewhatabstract:thegeometricmeanisthenthrootoftheproductofthedata.Ifweuse
capitalGreekpi(∏)torepresentmultiplication,andy‘hat’(^y)torepresentthegeometric
mean, then
pffiffiffiffiffiffiffi
^y n ∏y
Letustakeasimpleexamplewecanworkoutbyhand:thenumbersofinsectsonfive
differentplantswereasfollows:10,1,1000,1,10.Multiplyingthenumberstogethergives
100000.Therearefivenumbers,sowewantthefifthrootofthis.Rootsarehardtodoin
yourhead,sowewilluseRasacalculator.Rememberthatrootsarefractionalpowers,so
thefifthrootisanumberraisedtothepower1/5=0.2.InR,powersaredenotedbythe^
symbol (the caret), which is found above number 6 on the keyboard:
100000^0.2
[1]10
Sothegeometricmeanoftheseinsectnumbersis10insectsperstem.Notethattwoofthe
fivecountswereexactlythisnumber,soitseemsareasonableestimateofcentraltendency.
Thearithmeticmean,ontheotherhand,ishopelessforthesedata,becausethelargevalue
(1000)issoinfluential:10+1+1000+1+10=1022and1022/5=204.4.Notethatnone
of the counts were close to 204.4, so the arithmetic mean is a poor estimate of central
tendency in this case.
insects<-c(1,10,1000,10,1)
mean(insects)
[1]204.4
Anotherwaytocalculategeometricmeaninvolvestheuseoflogarithms.Recallthatto
multiply numbers together we add up their logarithms. And to take roots, we divide the
logarithmbytheroot. Soweshouldbeabletocalculate ageometricmeanbyfindingthe
antilog (exp) of the average of the logarithms (log) of the data:
exp(mean(log(insects)))
[1]10
Writing a general function to compute geometric means is left to you as an exercise.
The use of geometric means draws attention to a general scientific issue. Look at the
figure below, which shows numbers varying through time in two populations. Now
ask yourself ‘Which population is the more variable?’ Chances are, you will pick the
upper line:
48 STATISTICS:ANINTRODUCTIONUSINGR
Butnowlookatthescaleontheyaxis.Theupperpopulationisfluctuating100,200,100,
200andsoon.Inotherwords,itisdoublingandhalving,doublingandhalving.Thelower
curve is fluctuating 10, 20, 10, 20, 10, 20 and so on. It, too, is doubling and halving,
doublingandhalving.Sotheanswertothequestionis:‘Theyareequallyvariable’.Itisjust
thatonepopulationhasahighermeanvaluethantheother(150vs.15inthiscase).Inorder
nottofallintothetrapofsayingthattheuppercurveismorevariablethanthelowercurve,it
isgoodpracticetographthelogarithmsratherthantherawvaluesofthingslikepopulation
sizes that change multiplicatively.
CENTRALTENDENCY 49
Now it is clear that both populations are equally variable.
Finally,weshoulddealwitharatherdifferentmeasureofcentraltendency.Considerthe
following problem. An elephant has a territory which is a square of side 2km. Each
morning,theelephantwalkstheboundaryofthisterritory.Itbeginsthedayatasedatepace,
walkingthefirstsideoftheterritoryataspeedof1km/hr.Onthesecondside,hehasspedup
to2km/hr.Bythethirdsidehehasacceleratedtoanimpressive4km/hr,butthissowears
himout,thathehastoreturnonthefinalsideatasluggish1km/hr.Sowhatishisaverage
speed over the ground? You might say he travelled at 1, 2, 4 and 1km/hr so the average
speedis(1+2+4+1)/4=8/4=2km/hr.Butthatiswrong.Canyouseehowtoworkout
therightanswer?Recallthatvelocityisdefinedasdistancetravelleddividedbytimetaken.
Thedistancetravellediseasy:itisjust4×2=8km.Thetimetakenisabitharder.Thefirst
edgewas2kmlongand,travellingat1km/hr,thismusthavetaken2hr.Thesecondedge
was2kmlongand,travellingat2km/hr,thismusthavetaken1hr.Thethirdedgewas2km
longand,travellingat4km/hr,thismusthavetaken0.5hr.Thefinaledgewas2kmlong
and, travelling at 1km/hr, this must have taken 2hr. So the total time taken was 2+1+
0.5+2=5.5 hr. So the average speed is not 2km/hr but 8/5.5=1.4545km/hr.
ThewaytosolvethisproblemistousetheHARMONICMEAN.Thisisthereciprocalofthe
averageofthe(cid:3)reciprocals.(cid:4)Rememberthatareciprocalis‘oneover’.Sotheaverageofour
reciprocals is 1111 =42:7540:6875. The reciprocal of this average is the
1 2 4 1
harmonicmean1=0:68751:4545.Insymbols,therefore,theharmonicmean,~y,(y‘curl’)
is given by
1 n
~y
P1

P1
y y
n
In R, we would write either
v<-c(1,2,4,1)
length(v)/sum(1/v)
[1]1.454545
or
1/mean(1/v)
[1]1.454545
FurtherReading
Zar,J.H.(2009)Biostatistical Analysis,5th edn, Pearson, NewYork.
4
Variance
Ameasureofvariabilityisperhapsthemostimportantquantityinstatisticalanalysis.The
greater the variability in the data, the greater will be our uncertainty in the values of
parametersestimatedfromthedata,andthelowerwillbeourabilitytodistinguishbetween
competing hypotheses about the data.
Considerthefollowingdata,y,whichareplottedsimplyintheorderinwhichtheywere
measured:
y<-c(13,7,5,12,9,15,6,11,9,7,12)
plot(y,ylim=c(0,20))
How can we quantify the variation (the scatter) in y that we can see here? Perhaps the
simplest measure is the range of y values:
range(y)
[1] 515
plot(1:11,y,ylim=c(0,20),pch=16,col="blue")
lines(c(4.5,4.5),c(5,15),col="brown")
lines(c(4.5,3.5),c(5,5),col="brown",lty=2)
lines(c(4.5,5.5),c(15,15),col="brown",lty=2)
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
VARIANCE 51
Thisisareasonablemeasureofvariability,butitistoodependentonoutlyingvaluesfor
mostpurposes.Also,wewantallofourdatatocontributetothemeasureofvariability,not
just the maximum and minimum values.
How about estimating the mean value, and looking at the departures from the mean
(known as ‘residuals’ or ‘deviations’)?
plot(1:11,y,ylim=c(0,20),pch=16,col="blue")
abline(h=mean(y),col="green")
for(iin1:11)lines(c(i,i),c(mean(y),y[i]),col="red")
Thelongertheredlines,themorevPariablethedata.Sothislookspromising.Howabout
addingupthelengthsoftheredlines: y y?Amoment’sthoughtwillshowthatthisis
nogood,becausethenegativeresiduals(fromthepointsbelowthemean)willcanceloutthe
positive rPesiduals (from the points above the line). In fact, it is easy to prove that this
quantity y y is zero, no matter what the variation in the data, so that’s no good
(see Box 4.1).
But the only problem was the minus signsP. How about ignoring the minus signs and
addinguptheabsolutevaluesoftheresiduals: |y y|.Thisisaverygoodmeasureof
variability,andisusedinsomemodern,computationallyintensivemethods.Theproblemis
thatitmakesthesumshard,andwedonotwantthat.Amuchmorestraightforwardwayof
gettiPngridoftheproblemoftheminussignsistosquaretheresidualsbeforeweaddthem
up: y y2. This is perhaps the most important single quantity in all of statistics. It is
called, somewhat unimaginatively, the sum of squares. So, in our figure (above) imagine
squaring the lengths of each of the vertical red lines:
y-mean(y)
[1] 3.3636364-2.6363636-4.6363636 2.3636364-0.6363636 5.3636364
[7]-3.6363636 1.3636364-0.6363636-2.6363636 2.3636364
52 STATISTICS:ANINTRODUCTIONUSINGR
P
Box4.1. Thesumofthedifferences y yiszero
Start by writing down the differences explicitly:
X X
d  y y
P
Take through the brackets:
X X X
d  y  y
P
The important point is that y is the same as n:y so
X X
d  y ny
P
and we know that y y=n, so
P
X X n y
d  y 
n
The ns cancel, leaving
X X X
d  y  y0
(y-mean(y))^2
[1] 11.3140496 6.9504132 21.4958678 5.5867769 0.4049587 28.7685950
[7] 13.2231405 1.8595041 0.4049587 6.9504132 5.5867769
then adding up all these squared differences:
sum((y-mean(y))^2)
[1]102.5455
Sothesumofsquaresforourdatais102.5455.Butwhatareitsunits?Wellthatdepends
ontheunitsinwhichyismeasured.Supposetheyvalueswerelengthsinmm.Sotheunitsof
the sum of squares are mm2 (like an area).
Nowwhatwouldhappentothesumofsquaresifweaddeda12thdatapoint?Itwouldget
bigger,ofcourse.Anditwouldgetbiggerforeveryextradatapointweadded(exceptinthe
unlikelyeventthatournewdatapointwasexactlyequaltothemeanvalue,inwhichcase
wewouldaddzerosquared=0).Wedonotwantourmeasureofvariabilitytodependon
samplesizeinthisway,sotheobvioussolutionistodividebythenumberofsamples,toget
the mean squared deviation.
Atthispointweneedtomakeabrief,butimportant,diversion.Beforewecanprogress,
we need to understand the concept of degrees of freedom.
VARIANCE 53
DegreesofFreedom
Supposewehadasampleoffivenumbersandtheiraveragewas4.Whatwasthesumofthe
fivenumbers?Itmusthavebeen20,otherwisethemeanwouldnothavebeen4.Sonowlet
us think about each of the five numbers in turn.
Wewillputanumberineachofthefiveboxes. Ifwe allow thatthenumberscouldbe
positive or negative real numbers, we ask how many values could the first number take?
OnceyouseewhatI’mdoing,youwillrealizeitcouldtakeanyvalue.Supposeitwasa2.
2
How many values could the next number take? It could be anything. Say, it was a 7.
2 7
And the third number? Anything. Suppose it was a 4.
2 7 4
The fourth number could be anything at all. Say it was 0.
2 7 4 0
Nowthen.Howmanyvaluescouldthelastnumbertake?Justone.Ithastobeanother7
because the numbers have to add up to 20 because the mean of the five numbers is 4.
2 7 4 0 7
To recap. We have total freedom in selecting the first number. And the second, third
and fourth numbers. But we have no choice at all in selecting the fifth number. We have
4 degrees of freedom when we have five numbers. In general we have n 1 degrees of
freedom if we estimated the mean from a sample of size n. More generally still, we can
proposeaformaldefinitionofdegreesoffreedom:degreesoffreedomisthesamplesize,n,
minus the number of parameters, p, estimated from the data.
Thisissoimportant,youshouldmemorizeit.Intheexampleweplottedearlierwehad
n=11andweestimatedjustoneparameterfromthedata:thesamplemean,y.Sowehave
n 1=10 d.f.
Variance
We return to developing our quaPntitative measure of variability. We have come to the
conclusionthatthesumofsquares y y2isagoodbasisforassessingvariability,butwe
54 STATISTICS:ANINTRODUCTIONUSINGR
havetheproblemthatthesumofsquaresincreaseswitheverynewdatapointweaddtothe
sample. The intuitive thing todo would be to divide by thenumber of numberPs, n, toget
the mean squared deviation. But look at the formula for the sum of squares: y y2.
Wecannotbegintocalculateituntilweknowthevalueofthesamplemean,y.Andwheredo
wegetthevalueofyfrom?Doweknowitinadvance?Canwelookitupintables?No,we
needtocalculateitfromthedata.Themeanvalueyisaparameterestimatedfromthedata,
soweloseonedegreeoffreedomasaresult.Thus,incalculatingthemeansquareddeviation
wedividebythedegreesoffreedom,n 1,ratherthanbythesamplesize,n.Inthejargon,
thisprovidesuswithanunbiasedestimateofthevariance,becausewehavetakenaccountof
the fact that one parameter was estimated from the data prior to computation.
Nowwecanformalizeourdefinitionofthemeasurethatweshallusethroughoutthebook
forquantifyingvariability.Itiscalledvarianceanditisrepresentedconventionallybys2:
sum of squares
variance
degrees of freedom
This is one of the most important definitions in the book, and you should commit it to
memory. Wecanputitintoamore mathematicalform,byspellingoutwhatwe meanby
each of the phrases in the numerator and the denominator:
P
y y2
variances2 
n 1
Let’swriteanRfunctiontodothis.Wehavemostofthenecessarycomponentsalready
(seeabove);thesumofsquaresisobtainedassum((y-mean(y))^2).Forthedegreesof
freedom,weneedtoknowthenumberofnumbersinthevector,y.Thisisobtainedbythe
function length(y). Let us call the function variance and write it like this:
variance<-function(x) sum((x-mean(x))^2)/(length(x)-1)
Now we can try out the function on our data, like this:
variance(y)
[1]10.25455
So there we have it. Our quantification of the variation we saw in the first plot is the
samplevariance,s2=10.25455.YouwillnotbesurprisedthatRprovidesitsown,built-in
functionforcalculatingvariance,andithasanevensimplernamethanthefunctionwejust
wrote: var
var(y)
[1]10.25455
Varianceisusedincountlesswaysinstatisticalanalysis,sothissectionisprobablythe
mostimportantinthewholebook,andyoushouldrereadituntilyouaresurethatyouknow
exactly what variance is, and precisely what it measures (Box 4.2).
VARIANCE 55
P
Box4.2. Shortcutformulaforthesumofsquares y y2
The main problem with the formula defining variance is that it involves all those
subtractions,y y.Itwouldbegoodtofindasimplerwayofcalculatingthesumof
squares.Letusexpandthebracketedterm y y2toseeifwecanmakeanyprogress
towards a subtraction-free solution:
y y2  y y y yy2 2yyy2
So far, so good. Now we apply the summation
P (cid:2)P (cid:3)
X X X yX y 2
y2 2y yny2  y2 2 yn
n n
P
NotethatonlytheystakPethesummationsign.Thisisbecausewecanreplace yby
ny.Nowreplaceywith y=nontheright-handside,thencancelthensandcollectthe
terms:
(cid:4)P (cid:5) (cid:4)P (cid:5) (cid:4)P (cid:5)
X y 2 y 2 X y 2
y2 2 n  y2 
n n2 n
This is the shortcut formula for computing the sum of squares. It requiPres only two
quantitiestobeestimatedfromthed(cid:4)aPta.T(cid:5)hesumofthesquaredyvalues y2andthe
square of the sum of the y values y 2 .
Variance:AWorkedExample
Thedatainthefollowingtablecomefromthreemarketgardens.Thedatashowtheozone
concentrations in parts per hundred million (pphm) on 10 summer days.
ozone<-read.csv("c:\\temp\\gardens.csv")
attach(ozone)
ozone
gardenA gardenB gardenC
1 3 5 3
2 4 5 3
3 4 6 2
4 3 7 1
5 2 4 10
6 3 4 4
7 1 3 3
8 3 5 11
9 5 6 3
10 2 5 10
56 STATISTICS:ANINTRODUCTIONUSINGR
The first step in calculating variance is to work out the mean:
mean(gardenA)
[1]3
Now we subtract the mean value (3) from each of the data points:
gardenA-mean(gardenA)
[1] 0 1 1 0 -1 0 -2 0 2 -1
This produces a vector of differences (of length = 10). We need to square these
differences:
(gardenA-mean(gardenA))^2
[1] 0 1 1 0 1 0 4 0 4 1
then add up the squared differences:
sum((gardenA-mean(gardenA))^2)
[1]12
This important quantity is called ‘the sum of squares’. Variance is the sum of squares
divided by degrees of freedom. We have 10 numbers, and have estimated one parameter
from the data (the mean) in calculating the sum of squares, so we have 10 – 1=9 d.f.
sum((gardenA-mean(gardenA))^2)/9
[1]1.333333
So the mean ozone concentration in garden A is 3.0 and the variance in ozone
concentration is 1.33. We now do the same for garden B:
mean(gardenB)
[1]5
It has a much higher mean ozone concentration than garden A. But what about its
variance?
gardenB-mean(gardenB)
[1] 0 0 1 2 -1 -1 -2 0 10
(gardenB-mean(gardenB))^2
[1] 0 0 1 4 1 1 4 0 10
sum((gardenB-mean(gardenB))^2)
[1]12
VARIANCE 57
sum((gardenB-mean(gardenB))^2)/9
[1]1.333333
This is interesting: so although the mean values are quite different, the variances are
exactly the same (both have s2=1.33333). What about garden C?
mean(gardenC)
[1]5
Its mean ozone concentration is exactly the same as in garden B.
gardenC-mean(gardenC)
[1] -2 -2 -3 -4 5 -1 -2 6 -2 5
(gardenC-mean(gardenC))^2
[1] 4 4 9 16 25 1 4 36 4 25
sum((gardenC-mean(gardenC))^2)
[1]128
sum((gardenC-mean(gardenC))^2)/9
[1]14.22222
So,althoughthemeansingardensBandCareidentical,thevariancesarequitedifferent
(1.33and14.22,respectively).Arethevariancessignificantlydifferent?WedoanFtestfor
this, dividing the larger variance by the smaller variance:
var(gardenC)/var(gardenB)
[1]10.66667
ThenlookuptheprobabilityofgettinganFratioasbigasthisbychancealoneifthetwo
variances were really the same. We need thecumulative probability of the Fdistribution,
whichisafunctioncalledpfthatweneedtosupplywiththreearguments:thesizeofthe
variance ratio (10.667), the number of degrees of freedom in the numerator (9) and the
number of degrees of freedom in the denominator (also 9). We did not know in advance
whichgardenwasgoingtohavethehighervariance,sowedowhatiscalledatwo-tailedtest
(we simply multiply the probability by 2):
2*(1-pf(10.667,9,9))
[1]0.001624002
Thisprobabilityismuchlessthan5%,soweconcludethatthereisahighlysignificant
difference between these twovariances.Wecould dothisevenmore simply by using the
built-in F test:
var.test(gardenB,gardenC)
58 STATISTICS:ANINTRODUCTIONUSINGR
Ftesttocomparetwovariances
data: gardenBandgardenC
F=0.0938,numdf=9,denomdf=9,p-value=0.001624
alternativehypothesis:trueratioofvariancesisnotequalto1
95percentconfidenceinterval:
0.023286170.37743695
sampleestimates:
ratioofvariances
0.09375
So the two variances are significantly different. But why does this matter?
Whatfollowsisoneofthemostimportantlessonssofar,sokeeprereadingituntilyouare
surethatyouunderstandit.ComparinggardensAandB,wecanseethattwosamplescan
havedifferentmeans,butthesamevariance.Thisisassumedtobethecasewhenwecarry
outstandardtests(likeStudent’sttest)tocomparetwomeans,orananalysisofvarianceto
comparethreeormoremeans.Constancyofvarianceisthemostimportantassumptionwe
make in most statistical analyses.
ComparinggardensBandC,wecanseethattwosamplescanhavethesamemeanbut
different variances.Sampleswith the same mean areidentical,right?Wrong! Let uslook
intothescienceinabitmoredetail.Thedamagethresholdforlettucesis8pphmozone,so
lookingatthemeansshowsthatbothgardensarefreeofozonedamageontheirlettuces(the
meanof5forbothBandCiswellbelowthethresholdof8).Letuslookattherawdatafor
gardenB.Howmanyofthedayshadozone>8?Lookatthedataframeandyouwillseethat
none of the days exceeded the threshold. What about garden C?
gardenC
[1] 3 3 2 1 10 4 3 11 3 10
IngardenCozonereacheddamagingconcentrationsonthreedaysoutof10,so30%of
thetimethelettuceplantswouldbesufferingozonedamage.Thisisthekeypoint:whenthe
variancesaredifferent,weshouldnotmakeinferencesbycomparingthemeans.Whenwe
comparethemeans,weconcludethatgardenCislikegardenB,andthattherewillbeno
ozone damage to the lettuces. When we look at the data, we see that this is completely
wrong:thereisozonedamage30%ofthetimeingardenCandnoneofthetimeingardenB.
The moral of this story is: when the variances are different, don’t compare the means.
If you do, you run the risk of coming to entirely the wrong conclusion.
VarianceandSampleSize
Itisimportanttounderstandtherelationshipbetweenthesizeofasample(thereplication,n)
and the value of variance that is estimated. We can do a simple simulation experiment to
investigate this:
plot(c(0,32),c(0,15),type="n",xlab="Samplesize",ylab="Variance")
The plan is to select random numbers from a normal distribution using the function
rnorm.Ourdistributionisdefinedashavingameanof10andastandarddeviationof2(this
VARIANCE 59
isthesquarerootofthevariance,sos2=4).Weshallworkoutthevarianceforsamplesizes
between n=3 and n=31, and plot 30 independent instances of variance at each of the
selected sample sizes:
for(ninseq(3,31,2)){
for(iin1:30){
x<-rnorm(n,mean=10,sd=2)
points(n,var(x))}}
You see that as sample size declines, the range of the estimates of sample variance
increases dramatically (remember that the population variance is constant at s2=4
throughout).Theproblembecomesseverebelowsamplesof13orso,andisveryserious
forsamplesof7orfewer.Evenforreasonablylargesamples(liken=31)thevariancevaries
morethanthreefoldinjust30trials(youcanseethattherightmostgroupofpointsvaryfrom
about 2 to about 6). This means that for small samples, the estimated variance is badly
behaved, and this has serious consequences for estimation and hypothesis testing.
When people ask‘How many samples do I need?’a statistician will often answerwith
another question, ‘How many can you afford?’ Other things being equal, what we have
learnedinthischapteristhat30isareasonablygoodsample.Anythinglessthanthisisa
smallsample,andanythinglessthan10isaverysmallsample.Anythingmorethan30may
beanunnecessaryluxury(i.e.awasteofresources).Weshallseelaterwhenwestudypower
analysishowthequestionofsamplesizecanbeaddressedmoreobjectively.Butforthetime
being, take n=30 samples if you can afford it, and you will not go far wrong.
UsingVariance
Variance is used in two main ways:
(cid:129) for establishing measures of unreliability (e.g. confidence intervals)
(cid:129) for testing hypotheses (e.g. Student’s t test)
60 STATISTICS:ANINTRODUCTIONUSINGR
AMeasureofUnreliability
Consider the properties that you would like a measure of unreliability to possess. As the
variance of the data increases, what would happen to the unreliability of estimated
parameters? Would it go up or down? Unreliability would go up as variance increased,
sowewouldwanttohavethevariance onthetop(thenumerator) ofanydivisionsinour
formula for unreliability:
unreliability
∝s2
Whataboutsamplesize?Wouldyouwantyourestimateofunreliabilitytogoupordown
assamplesize,n,increased?Youwouldwantunreliabilitytogodownassamplesizewent
up,soyouwouldputsamplesizeonthebottomoftheformulaforunreliability(i.e.inthe
denominator):
s2
∝
unreliability
n
Finally,considertheunitsinwhichyouwouldwantunreliabilitytobeexpressed.What
aretheunitsinwhichourcurrentmeasureisexpressed?Samplesizeisdimensionless,but
varianceisbasedonthesumofsquareddifferences,soithasdimensionsofsquaredvalues
ofy.Soifthemean was alength incm,thevariancewouldbeanarea incm2.Thisisan
unfortunate state of affairs. It would make good sense to have the dimensions of the
unreliability measure the same as the dimensions of the parameter whose unreliability is
beingmeasured.Thatiswhyallunreliabilitymeasuresareenclosedinsideabigsquare-root
term.Unreliabilitymeasuresarecalledstandarderrors.Whatwehavejustworkedoutisthe
standard error of the mean:
rffiffiffiffi
s2
SE 
y n
Thisisaveryimportantequationandshouldbememorized.Letuscalculatethestandard
errors of each of our market garden means:
sqrt(var(gardenA)/10)
[1]0.3651484
sqrt(var(gardenB)/10)
[1]0.3651484
sqrt(var(gardenC)/10)
[1]1.19257
In written work one shows the unreliability of any estimated parameter in a formal,
structured way like this:
‘The mean ozone concentration in garden A was 3.0±0.365 pphm (1 s.e., n=10)’
You write plus or minus, then the unreliability measure, the units (parts per hundred
millioninthiscase)then,inbrackets,tellthereaderwhattheunreliabilitymeasureis(inthis
caseonestandarderror)andthesizeofthesampleonwhichtheparameter estimatewas
VARIANCE 61
based(inthiscase10).Thismayseemratherstilted,unnecessaryeven.Buttheproblemis
thatunlessyoudothis,thereaderwillnotknowwhatkindofunreliabilitymeasureyouhave
used. For example, you might have used a 95% confidence interval or a 99% confidence
interval instead of one standard error.
ConfidenceIntervals
Aconfidenceintervalshowsthelikelyrangeinwhichthemeanwouldfallifthesampling
exerciseweretoberepeated.Itisaveryimportantconceptthatpeoplealwaysfinddifficult
tograspatfirst.Itisprettyclearthattheconfidenceintervalwillgetwiderastheunreliability
goes up, so:
rffiffiffiffi
s2
∝ ∝
confidence interval unreliability measure
n
Butwhatdowemeanby‘confidence’?Thisisthehardthingtograsp.Askyourselfthis
question.Wouldtheintervalbewiderornarrowerifwewantedtobemoreconfidentthatout
repeatsamplemeanwillfallinsidetheinterval?Itmaytakesomethought,butyoushouldbe
abletoconvinceyourselfthatthemoreconfidentyouwanttobe,thewidertheintervalwill
need to be. You can see this clearly by considering the limiting case of complete and
absolutecertainty.Nothingiscertaininstatisticalscience,sotheintervalwouldhavetobe
infinitely wide.
Wecanproduceconfidenceintervalsofdifferentwidthsbyspecifyingdifferentlevels
of confidence. The higher the confidence, the wider the interval. How exactly does this
∝
work?Howdoweturntheproportionality( )intheequationaboveintoequality?The
answer is by resorting to an appropriate theoretical distribution (as explained below).
Supposeoursamplesizeistoosmalltousethenormaldistribution(n<30,ashere);then
we traditionally use Student’s t distribution. The values of Student’s t associated with
differentlevelsofconfidenceareavailableinthefunctionqt,whichgivesthequantilesof
thetdistribution.Confidenceintervalsarealwaystwo-tailedbecausetheparametermay
belargerorsmallerthanourestimateofit.Thus,ifwewanttoestablisha95%confidence
intervalweneedtocalculatethevalueofStudent’stassociatedwithα0:025(i.e.with
0.01(100%–95%)/2).Thevalueisfoundlikethisfortheleft-hand(0.025)andright-hand
(0.975) tails:
qt(.025,9)
[1]-2.262157
qt(.975,9)
[1]2.262157
Thefirstargumentinqtistheprobabilityandthesecondisthedegreesoffreedom.This
says that values as small as  2.262 standard errors below the mean are to be expected in
2.5% of cases (p=0.025), and values as large as +2.262 standard errors above the mean
withsimilarprobability(p=0.975).ValuesofStudent’starenumbersofstandarderrors
to be expected with specified probability and for a given number of degrees of freedom.
The values of t for 99% are bigger than these (0.005 in each tail):
62 STATISTICS:ANINTRODUCTIONUSINGR
qt(.995,9)
[1]3.249836
and the value for 99.5% confidence are bigger still (0.0025 in each tail):
qt(.9975,9)
[1]3.689662
Values of Student’s t like these appear in the formula for calculating the width of the
confidence interval, and their inclusion is the reason why the width of the confidence
interval goes up as our degree of confidence is increased. The other component of the
formula, the standard error, is not affected by our choice of confidence level. So, finally,
we can write down the formula for the confidence interval of a mean based on a small
sample (n<30):
confidence intervalt-valuestandard error
rffiffiffiffi
s2
CI 95% t α0:025;d:f:9 n
For garden B, therefore, we calculate
qt(.975,9)*sqrt(1.33333/10)
[1]0.826022
and we would present the result in written work like this:
‘The mean ozone concentration in garden B was 5.0±0.826 (95% CI, n=10).’
Bootstrap
Acompletelydifferentwayofcalculatingconfidenceintervalsiscalledbootstrapping.You
have probably heard the old phrase about ‘pulling yourself up by your own bootlaces’.
Thatiswherethetermcomesfrom.Itisusedinthesenseofgetting‘somethingfornothing’.
Theideaisverysimple.Youhaveasinglesampleofnmeasurements,butyoucansample
fromthisinverymanyways,solongasyouallowsomevaluesappearmorethanonce,and
other samples to be left out (i.e. sampling with replacement). All you do is calculate the
sample mean lots of times, once for each sampling from your data, then obtain the
confidence interval by looking at the extreme highs and lows of the estimated means
using a function called quantile to extract the interval you want (e.g. a 95% interval is
specified using c(0.0275, 0.975) to locate the lower and upper bounds). Here are
the data:
data<-read.csv("c:\\temp\\skewdata.csv")
attach(data)
names(data)
[1]"values"
VARIANCE 63
Weshallsimulatesamplesizes(k)between5and30,andforeachsamplesizeweshall
take 10000 independent samples from our data (the vector called values), using the
function called sample with replacement (replace=T):
plot(c(0,30),c(0,60),type="n",xlab="Samplesize",
ylab="Confidenceinterval")
for(kinseq(5,30,3)){
a<-numeric(10000)
for(iin1:10000){
a[i]<-mean(sample(values,k,replace=T))
}
points(c(k,k),quantile(a,c(.025,.975)),type="b",pch=21,bg="red")
}
Theconfidenceintervalnarrowsrapidlyovertherangeofsamplesizesuptoabout20,but
more slowly thereafter. At n=30, the bootstrapped CI based on 1000 simulations was
quantile(a,c(0.025,0.975))
2.5% 97.5%
24.86843 37.6895
(youwillgetslightlydifferentvaluesbecauseoftherandomization).Itisinterestingtosee
qffiffiffi qffiffiffiffiffiffiffiffiffiffiffi
howthiscompareswiththenormaltheoryconfidenceinterval:1:96 s2 1:96 337:065
n 30
6:5698 implying that a repeat of the sample is likely to have a mean value in the range
24.39885to37.53846.Asyousee,theestimatesfromthebootstrapandnormaltheoryare
reassuringly close. But they are not identical.
64 STATISTICS:ANINTRODUCTIONUSINGR
Here are the bootstrapped intervals compared with the intervals calculated from the
normal (blue solid line):
xv<-seq(5,30,0.1)
yv<-mean(values)+1.96*sqrt(var(values)/xv)
lines(xv,yv,col="blue")
yv<-mean(values)-1.96*sqrt(var(values)/xv)
lines(xv,yv,col="blue")
and Student’s t distribution (green dotted line):
yv<-mean(values)-qt(.975,xv 1)*sqrt(var(values)/xv)
lines(xv,yv,lty=2,col="green")
yv<-mean(values)+qt(.975,xv 1)*sqrt(var(values)/xv)
lines(xv,yv,lty=2,col="green")
For the upper interval, you see that the bootstrapped intervals (vertical lines and red
symbols)fallbetweenthenormal(thelower,blueline)andtheStudent’stdistribution(the
greater,dottedgreenline).Forthelowerinterval,however,thebootstrappedintervalsarequite
different.Thisisbecauseoftheskewnessexhibitedbythesedata.Verysmallvaluesofthe
responsearesubstantiallylesslikelythanpredictedbythesymmetricalnormal(blueline)or
Student’st(dottedgreenline)distributions.Recallthatforsmall-sampleconfidenceintervals
using Students t distribution, the sample size, n, enters the equation twice: once as the
denominatorintheformulaforthestandarderrorofthemean,thenagainasadeterminantof
the quantileof the t distribution qt(0.975,n-1). That iswhy thedifference between the
normalandtheStudent’stconfidenceintervalsgetsbiggerassamplesizegetssmaller.
So which kind of confidence interval should you choose? I prefer the bootstrapped
estimatebecauseitmakesfewerassumptions.If,asinourexample,thedataareskew,then
this is reflected in the asymmetry of the confidence intervals above and below the mean
(6.7abovethemean,and6.1belowit,atn=30).BothnormalandStudent’stassumethat
there is no skew, and so their confidence intervals are symmetrical, whatever the data
actually show.
Non-constantVariance:Heteroscedasticity
One of themost importantassumptionsin classical statistical analysis isthat the variance
doesnotchangewiththemeanvalueoftheresponsevariable.Agoodmodelmustaccount
adequately for the observed variance-mean relationship. A plot of standardised residuals
againstfittedvaluesshouldlookliketheskyatnight(pointsscatteredatrandomoverthe
wholeplottingregionasinthelefthandpanel),withnotrendinthesizeordegreeofscatter
oftheresiduals.Acommonproblemisthatthevarianceincreaseswiththemean,sothatwe
obtain an expanding, fan-shaped pattern of residuals (right hand panel).
fitted(model)
)ledom(diser
30 35 40
01
5
0
5–
01–
fitted(model)
)ledom(diser
25 30 35 40 45
2
1
0
1–
2–
VARIANCE 65
Theplotontheleftiswhatwewanttosee:notrendintheresidualswiththefittedvalues.
Theplotontherightisaproblem.Thereisaclearpatternofincreasingresidualsasthefitted
values get larger. This is a picture of what heteroscedasticity looks like.
FurtherReading
Rowntree, D. (1981) Statistics without Tears: An Introduction for Non-Mathematicians, Penguin,
London.
5
Single Samples
Suppose we have a single sample. The questions we might want to answer are these:
(cid:129) what is the mean value?
(cid:129) is the mean value significantly different from current expectation or theory?
(cid:129) what is the level of uncertainty associated with our estimate of the mean value?
Inordertobereasonablyconfidentthatourinferencesarecorrect,weneedtoestablish
some facts about the distribution of the data:
(cid:129) are the values normally distributed or not?
(cid:129) are there outliers in the data?
(cid:129) if data were collected over a period of time, is there evidence for serial correlation?
Non-normality, outliers and serial correlation can all invalidate inferences made by
standard parametric such as like Student’s t test. It is much better in cases with non-
normalityand/oroutlierstouseanon-parametrictechniquesuchasWilcoxon’ssigned-rank
test. If there is serial correlation in the data, then you need to use time series analysis or
mixed effects models.
DataSummaryintheOne-SampleCase
To see what is involved, read the data called y from the file called example.csv:
data<-read.csv("c:\\temp\\example.csv")
attach(data)
names(data)
[1]"y"
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
SINGLESAMPLES 67
Summarizingthedatacouldnotbesimpler.Weusethebuilt-infunctioncalledsummary
like this:
summary(y)
Min. 1stQu. Median Mean 3rdQu. Max.
1.904 2.241 2.414 2.419 2.568 2.984
Thisgivesussixpiecesofinformationaboutthevectorcalledy.Thesmallestvalueis
1.904 (labelled Min. for minimum) and the largest value is 2.984 (labelled Max. for
maximum). There are two measures of central tendency: the median is 2.414 and the
arithmeticmeanin2.419.Whatyoumaybeunfamiliarwitharethefigureslabelled‘1st
Qu.’ and‘3rdQu.’The‘Qu.’ isanabbreviationofquartile,whichmeansonequarterof
thedata.Thefirstquartileisthevalueofthedatabelowwhichliethesmallest25%ofthe
data. The median is the second quartile by definition (half the data are smaller than
themedian).Thethirdquartileisthevalueofthedataabovewhichliethelargest25%of
the data (it is sometimes called the 75th percentile, because 75% of the values of y are
smaller than this value). The graphical equivalent of this summary table is known as a
box-and-whisker plot:
boxplot(y)
Thereisalotofinformationhere.Theboldhorizontalbarinthemiddleoftheboxshows
themedianvalueofy.Thetopoftheboxshowsthe75thpercentile,andthebottomofthe
boxshowsthe25thpercentile.Theboxasawholeshowswherethemiddle50%ofthedata
lie (this is called the ‘interquartile range’; we can see that this is between about 2.25 and
2.55).Iftheboxesaboveandbelowthemedianaredifferentsizes,thenthisisindicativeof
skewinthedata.Thewhiskersshowthemaximumandminimumvaluesofy(lateronwe
shall see what happens when the data contain ‘outliers’).
68 STATISTICS:ANINTRODUCTIONUSINGR
Another sort of plot that we might want to use for a single sample is the histogram:
hist(y)
Histogramsarefundamentallydifferentfromthegraphsthatwehaveencounteredsofar,
becauseinallcasestodatetheresponsevariablehasbeenontheyaxis(theordinate).Witha
histogram,theresponsevariableisonthexaxis(theabscissa).Theordinateofahistogram
showsthefrequencywithwhichdifferentvaluesoftheresponsewereobserved.Wecansee
that rather few values of y were less that 2.0 or greater than 2.8. The most frequently
observedvaluesofywerebetween2.4and2.6.Histogramsarerelatedtoprobabilitydensity
functions. We shall come across several very important statistical distributions in various
partsofthebook;wehavemetthenormalandStudent’stdistributionsalready,andlaterwe
shallmeetthePoisson,binomialandnegativebinomialdistributions.Whattheyallhavein
commonisthatyisontheabscissaandtheordinateshowstheprobabilitydensityassociated
witheachvalueofy.Youneedtobecarefulnottofallintothetrapofconfusinggraphsand
probability distributions (see p. 1).
Ourhistogram(above)isclearlynotsymmetricalaboutitsmode(2.5to2.6).Therearesix
barsbelowthemodebutonlyfourabovethemode.Datalikethisaresaidtobe‘skewtothe
left’ because the longer tail is on the left of the distribution.
Simple as they seem at first sight, there are actually lots of issues about histograms.
Perhapsthemostimportantissueiswhereexactlytodrawthelinesbetween thebars(the
‘binwidths’inthejargon).Forwhole-number(integer)datathisisoftenaneasydecision
(wecoulddrawabarofthehistogramforeachoftheintegervaluesofy).Butforcontinuous
(real number) data like we have here, that approach is a non-starter. How many different
valuesofydowehaveinourvectorof100numbers?Theappropriatefunctiontoanswer
questionslikethisistable:wedonotwanttoseeallthevaluesofy,wejustwanttoknow
howmanydifferentvaluesofythereare.Thatistosay,wewanttoknowthelengthofthe
table of different y values:
length(table(y))
[1]100
Thisshowsusthattherearenorepeatsofanyoftheyvalues,andahistogramofunique
values would be completely uninformative (a plot like this is called a ‘rug plot’, and has
short vertical bars placed at each value of y).
plot(range(y),c(0,10),type="n",xlab="yvalues",ylab="")
for(iin1:100)lines(c(y[i],y[i]),c(0,1),col="blue")
01
8
6
4
2
0
SINGLESAMPLES 69
2.0 2.2 2.4 2.6 2.8 3.0
y values
Let us look more closely to see what R has chosen on our behalf in designing the
histogram.Thexaxisislabelledevery0.2units,ineachofwhichtherearetwobars.Sothe
chosen bin width is 0.1. R uses simple rules to select what it thinks will make a ‘pretty’
histogram.Itwantstohaveareasonablenumberofbars(toofewbarslooksdumpy,whiletoo
manymakestheshapetoorough);thereare11barsinthiscase.Thenextcriterionistohave
‘sensible’widthsforthebins.Itmakesmoresense,forinstancetohavethebinsexactly0.1
unitswide(ashere)thantouseonetenthoftherangeofyvalues,oroneeleventhoftherange
(notetheuseofthediffandrangefunctions)whicharecloseto0.1butnotequalto0.1:
(max(y)-min(y))/10
[1]0.1080075
diff(range(y))/11
[1]0.09818864
70 STATISTICS:ANINTRODUCTIONUSINGR
Soawidthof0.1isa‘pretty’compromise.Asweshallseelater,youcanspecifythewidth
ofthebinsifyoudonotlikethechoicethatRhasmadeforyou,orifyouwanttodrawtwo
histograms that are exactly comparable.
ThereallyimportantthingtounderstandishowRdecideswheretoputvaluesthatfall
exactlyononeoftheedgesofabin.Itcoulddecidetoputtheminthelowerbin(theoneto
theleft)orthehigherbin(totheright)oritcouldtossacoin(headsinthelefthandbin,tails
in the right). This is hard to understand at first. Suppose that a is the value of the lower
breakandbisthevalueofthehigherbreakforagivenbarofthehistogram.Theconvention
aboutwhattodoisindicatedbytheuseofroundbracketsandsquarebrackets:(a,b]or
[a,b).Thenumbernexttothesquarebracketisincludedinthebar,whilethenumbernext
totheroundbracketisexcludedfromthisbar.Thefirstconvention(a,b]isthedefaultin
R,andmeansincludetheright-handendpointb,butnottheleft-handoneainthisbar(in
the function definition, this is written as right = TRUE). In our histogram (above) the
modalbinisbetween2.5and2.6.Thiswouldbewrittenas(2.5,2.6]anditmeansthata
valueofexactly2.60wouldbeincludedinthisbin,butavalueofexactly2.50wouldnot(it
wouldbeincludedinthebintotheleft).Youwillmeetthisconventionagainlater,whenwe
learn about the cut function for converting a continuous variable into a categorical
variable (p. 276).
Themainproblemwithhistogramsisthearbitrarinessaboutwherethebreaksareputand
how wide the bins are defined to be. For instance, distributions that look bimodal with
narrowbinscanlookunimodalwithwiderbins.Themoralabouthistogramsis‘takecare;all
may not be as it seems’.
TheNormalDistribution
This famous distribution has a central place is statistical analysis. If you take repeated
samples from a population and calculate their averages, then these averages will be
normally distributed. This is called the central limit theorem. Let us demonstrate it for
ourselves.
Youmaybefamiliarwiththeancientgameof‘craps’.Itinvolvestheuseoftwo6-sided
dice.Initssimplestformthetwodicearethrown andthetwoscoresaddedtogether.The
lowestscoreyoucangetis1+1=2andthehighestis6+6=12.Thereisonlyonewayof
gettingeachofthesescoressotheybothhavethesamelowprobability(1/6×1/6=1/36).
You can score 3 by throwing 1 and 2 or 2 and 1 (so the probability of getting 3 is 2×1/
36=1/18;thesameasscoring11bygetting5and6or6and5).Themostlikelyscoreis7
becausetherearesomanywaysofgettingthis:1and6,2and5,3and4,4and3,5and2or6
and1).Letussimulate10000playsofthegameandproduceahistogramoftheresults.The
possible scores are the 11 numbers from 2 to 12:
score<-2:12
The number of ways of getting each score are:
ways<-c(1,2,3,4,5,6,5,4,3,2,1)
SINGLESAMPLES 71
We can use the rep function to produce a vector of all the 36 possible outcomes:
game<-rep(score,ways)
game
[1] 2 3 3 4 4 4 5 5 5 5 6 6 6 6 6 7 7 7 7 7 7
[22] 8 8 8 8 8 9 9 9 9 10 10 10 11 11 12
Nowwedrawasinglerandomsamplefromthisvectortorepresenttheoutcomeofone
throw (this game produced a score of 5):
sample(game,1)
[1]5
and we record this score ina vector called outcome. The game is repeated 10000 times:
outcome<-numeric(10000)
for(iin1:10000)outcome[i]<-sample(game,1)
This is what the distribution of outcomes looks like:
hist(outcome,breaks=(1.5:12.5))
Notethetrickofspecifyingthebreakpointstobeoffsetby0.5inordertogetthecorrect
labels in the centre of the relevant bars.
Thedistributionisverywellbehaved,butitisclearlytriangular,notbell-shapedlikethe
normal.Whatifweworkoutthemeanscoreover,say,threegames?Presumably,themean
72 STATISTICS:ANINTRODUCTIONUSINGR
scoreisstillgoingtobe7(asabove)butwhatwillthedistributionofmeanscoreslooklike?
We shall try it and see:
mean.score<-numeric(10000)
for(iin1:10000)mean.score[i]<-mean(sample(game,3))
hist(mean.score,breaks=(1.5:12.5))
That was a demonstration of the central limit theorem in action. The triangular
distribution of scores has become a normal distribution of mean scores, even though we
wereaveragingacrossonlythreegames.Todemonstratethegoodnessoffittothenormal
distribution, we can overlay the histogram with a smooth probability density function
generatedfromanormaldistribution(dnorm)withthesamemeanandstandarddeviationof
our actual sample of games:
mean(mean.score)
[1]6.9821
sd(mean.score)
[1]1.366118
To accommodate the top of the smooth density function, we need to make the y axis a
littlelonger:ylim=c(0,3000).Togenerateasmoothcurve,weneedaseriesofvaluesfor
thexaxisrangingbetween2and12(asaruleofthumb,youneed100orsovaluestomakea
smooth-looking curve in R):
xv<-seq(2,12,0.1)
SINGLESAMPLES 73
Nowcalculatetheheightofthecurve.Thestandardnormalhasanintegralof1.0butour
histogram has an integral of 10000 so we calculate the height of the curve like this
yv<-10000*dnorm(xv,mean(mean.score),sd(mean.score))
We shall make a few minor embellishments by removing the heading from the plot
(main=""), colouring the bars in yellow (col="yellow"):
hist(mean.score,breaks=(1.5:12.5),ylim=c(0,3000),
col="yellow",main="")
and overlaying the normal probability density in red:
lines(xv,yv,col="red")
As you can see, the fit to the normal distribution is excellent, even though we were
averaging across just three throws of the dice. The central limit theorem really works.
Almostanydistribution,evena‘badlybehaved’onelikethenegativebinomial(p.251),will
produce a normal distribution of sample means taken from it.
Thegreatthingaboutthenormaldistributionisthatweknowsomuchaboutitsshape.
Obviously,allvaluesmustliebetweenminusinfinityandplusinfinity,sotheareaunderthe
wholecurveis1.0.Thedistributionissymmetrical,sohalfofoursampleswillfallbelowthe
mean,andhalfwillbeaboveit(i.e.theareabeneaththecurvetotheleftofthemeanis0.5).
Theimportantthingisthatwecanpredictthedistributionofsamplesinvariouspartsofthe
curve. For example, c.16% of samples will be more than 1 standard deviation above the
mean,andc.2.5%ofsampleswillbemorethan2standarddeviationsbelowthemean.But
how do I know this?
Thereisaninfinityofdifferentpossiblenormaldistributions:themeancanbeanythingat
all,andsocanthestandarddeviation.Forconvenience,itisusefultohaveastandardnormal
74 STATISTICS:ANINTRODUCTIONUSINGR
distribution,whosepropertieswecantabulate.Butwhatwouldbeasensiblechoiceforthe
mean of such a standard normal distribution? 12.7? Obviously not. 1? Not bad, but the
distributionissymmetrical,soitwouldbegoodtohavetheleftandrighthalveswithsimilar
scales(not1to4ontheright,but 2to1ontheleft).Theonlyreallysensiblechoiceisto
have the mean equal to 0. What about the standard deviation? Should that be 0 as well?
Hardly,sincethatwouldbeadistributionwithnospreadatall.Notveryuseful.Itcouldbe
anypositivenumber,butinpracticethemostsensiblechoiceis1.Sothereyouhaveit.The
standardnormaldistributionisonespecificcaseofthenormalwithmean=0andstandard
deviation=1. So how does this help?
Ithelpsalot,becausenowwecanworkouttheareabelowthecurveuptoanynumberof
standard deviations (these are the values on the x axis):
standard.deviations<-seq(-3,3,0.01)
pd<-dnorm(standard.deviations)
plot(standard.deviations,pd,type="l",col="blue")
You can see that almost all values fall within 3 standard deviations of the mean, one
wayortheother.Itiseasytofindtheareabeneaththecurveforanyvalueonthexaxis
(i.e. for any specified value of the standard deviation). Let us start with standard
deviation= 2. What is the area beneath the curve to the left of  2? It is obviously a
small number, but the curvature makes it hard to estimate the area accurately from the
plot. R provides the answer with a function called pnorm (‘probability for a normal
distribution’; strictly ‘cumulative probability’, as we shall see). Because we are dealing
with a standard normal (mean=0, sd=1) we need only specify the value of the normal
deviate, which is  2 in our case:
pnorm(-2)
[1]0.02275013
SINGLESAMPLES 75
Thistellsusthatjustabitlessthan2.5%ofvalueswillbelowerthan 2.Whatabout1
standard deviation below the mean?
pnorm(-1)
[1]0.1586553
In this case, about 16% of random samples will be smaller than 1 standard deviation
belowthemean.Whataboutbigvaluesofthenormaldeviate?Thedensityfunctionshowsa
maximumof+3.Whatistheprobabilityofgettingasamplefromanormaldistributionthat
is more than 3 standard deviations above the mean? The only point to note here is that
pnormgivestheprobabilityofgettingavaluelessthanthevaluespecified(notmore,aswe
wanthere).Thetrickissimplytosubtractthevaluegivenbypnormfrom1togettheanswer
we want:
1-pnorm(3)
[1]0.001349898
This tells us that a value as large as 3 or more is very unlikely indeed: less than 0.2%,
in fact.
Probablythemostfrequentuseofthestandardnormaldistributionisinworkingoutthe
valuesofthenormaldeviatethatcanbeexpectedbychancealone.This,ifyoulike,isthe
oppositekindofproblemtotheoneswehavejustbeendealingwith.There,weprovideda
value of the normal deviate (such as  1, or  2 or +3) and asked what probability was
associatedwithsuchavalue.Now,wewanttoprovideaprobabilityandfindoutwhatvalue
ofthenormaldeviateisassociatedwiththatprobability.Letustakeanimportantexample.
Supposewewanttoknowtheupperandlowervaluesofthenormaldeviatebetweenwhich
95% of samples are expected to lie. This means that 5% of samples will lie outside this
range,andbecausethenormalisasymmetricaldistribution,thismeansthat2.5%ofvalues
willbeexpectedtobesmallerthanthelowerbound(i.e.lietotheleftofthelowerbound)
and2.5%ofvalueswillbeexpectedtobegreaterthantheupperbound(i.e.lietotherightof
the upper bound). The function we need is called qnorm (‘quantiles of the normal
distribution’) and it is used by specifying our two probabilities 0.025 and 0.975 in a
vector like this c(0.025,0.975):
qnorm(c(0.025,0.975))
[1]-1.959964 1.959964
These are two very important numbers in statistics. They tell us that with a normal
distribution,95%ofrandomlyselectedvalueswillfallbetween 1.96and+1.96standard
deviations of the mean. Let us shade in these areas under the normal probability density
function to see what is involved:
76 STATISTICS:ANINTRODUCTIONUSINGR
Inthegreenareabetweenthetwoverticallines,wecanexpect95%ofallrandomsamples
tofall;weexpect2.5%ofsamplestobemorethan1.96standarddeviationsbelowthemean
(the left-hand red area), and we expect 2.5% of samples to be greater than 1.96 standard
deviationsabovethemean(theright-handredarea).Ifwediscoverthatthisisnotthecase,
then our sample is not normally distributed. It might, for instance, follow a Student’s t
distribution (see p. 82).
Tosumup:ifwewanttoprovidevaluesofthenormaldeviateandworkoutprobabilities,
we use pnorm; if we want to provide probabilities and work out values of the normal
deviate, we use qnorm. You should try and remember this important distinction.
CalculationsUsingzoftheNormalDistribution
Supposewehavemeasuredtheheightsof100people.Themeanheightwas170cmandthe
standard deviation was 8cm. The normal distribution looks like this:
ht<-seq(150,190,0.01)
plot(ht,dnorm(ht,170,8),type="l",col="brown",
ylab="Probabilitydensity",xlab="Height")
(thetopleft-handpanelintheplotsbelow).Wecanaskthreesortsofquestionsaboutdata
like these. What is the probability that a randomly selected individual will be:
(cid:129) shorter than a particular height?
(cid:129) taller than a particular height?
(cid:129) between one specified height and another?
The area under the whole curve is exactly 1; everybody has a height between minus
infinityandplusinfinity.True,butnotparticularlyhelpful.Supposewewanttoknowthe
probability that one of our people, selected at random from the group, will be less than
SINGLESAMPLES 77
160cmtall.Weneedtoconvertthisheightintoavalueofz;thatistosay,weneedtoconvert
160cmintoanumberofstandarddeviationsfromthemean.Whatdoweknowaboutthe
standardnormaldistribution?Ithasameanof0andastandarddeviationof1.Sowecan
convertanyvaluey,fromadistributionwithmeanyandstandarddeviationstoastandard
normal very simply by calculating:
y y
z
s
Soweconvert160cmintoanumberofstandarddeviations.Itislessthanthemeanheight
(170cm) so its value will be negative:
160 170
z  1:25
8
Nowweneedtofindtheprobabilityofavalueofthestandardnormaltakingavalueof
 1.25orsmaller.Thisistheareaundertheleft-handtailofthedistribution.Thefunctionwe
needforthisispnorm:weprovideitwithavalueofz(or,moregenerally,withaquantile)
and it provides us with the probability we want:
pnorm(-1.25)
[1]0.1056498
So the answer to our first question is just over 10% (the orange shaded area, below).
Thesecondquestionis:Whatistheprobabilityofselectingoneofourpeopleandfinding
thattheyaretallerthan185cm?Thefirsttwopartsoftheexerciseareexactlythesameas
before. First we convert our value of 185cm into a number of standard deviations:
185 170
z 1:875
8
Then we ask what probability is associated with this, using pnorm:
pnorm(1.875)
[1]0.9696036
Butthisistheanswertoadifferentquestion.Thisistheprobabilitythatsomeonewillbe
lessthan185cmtall(thatiswhatthefunctionpnormhasbeenwrittentoprovide).Allwe
need to do is to work out the complement of this:
1-pnorm(1.875)
[1]0.03039636
So the answer to the second question is about 3% (the blue shaded area, below).
78 STATISTICS:ANINTRODUCTIONUSINGR
Finally,wemightwanttoknowtheprobabilityofselectingapersonbetween165cmand
180cm. We have a bit more work to do here, because we need to calculate two z values:
165 170 180 170
z   0:625 and z  1:25
1 8 2 8
Theimportantpointtograspisthis:wewanttheprobabilityofselectingapersonbetween
these two z values, so we subtract the smaller probability from the larger probability. It
might help to sketch the normal curve and shade in the area you are interested in:
pnorm(1.25)-pnorm(-0.625)
[1]0.6283647
Thuswehavea63%chanceofselectingamedium-sizedperson(tallerthan165cmand
shorter than 180cm) from this sample with a mean height of 170cm and a standard
deviation of 8cm (the green shaded area, below).
Thefunctioncalledpolygonisusedforcolouringindifferentshapedareasunderthecurve:
toseehowitisused,type?polygon
par(mfrow=c(2,2))
ht<-seq(150,190,0.01)
pd<-dnorm(ht,170,8)
plot(ht,dnorm(ht,170,8),type="l",col="brown",
ylab="Probabilitydensity",xlab="Height")
plot(ht,dnorm(ht,170,8),type="l",col="brown",
ylab="Probabilitydensity",xlab="Height")
yv<-pd[ht<=160]
SINGLESAMPLES 79
xv<-ht[ht<=160]
xv<-c(xv,160,150)
yv<-c(yv,yv[1],yv[1])
polygon(xv,yv,col="orange")
plot(ht,dnorm(ht,170,8),type="l",col="brown",
ylab="Probabilitydensity",xlab="Height")
xv<-ht[ht>=185]
yv<-pd[ht>=185]
xv<-c(xv,190,185)
yv<-c(yv,yv[501],yv[501])
polygon(xv,yv,col="blue")
plot(ht,dnorm(ht,170,8),type="l",col="brown",
ylab="Probabilitydensity",xlab="Height")
xv<-ht[ht>=160&ht<=180]
yv<-pd[ht>=160&ht<=180]
xv<-c(xv,180,160)
yv<-c(yv,pd[1],pd[1])
polygon(xv,yv,col="green")
PlotsforTestingNormalityofSingleSamples
Thesimplesttestofnormality(andinmanywaysthebest)isthe‘quantile–quantileplot’;it
plotstherankedsamplesfromourdistributionagainstasimilarnumberofrankedquantiles
takenfromanormaldistribution.Ifthesampleisnormallydistributedthenthelinewillbe
straight.Departuresfromnormalityshowupasvarioussortsofnon-linearity(e.g.S-shapes
orbananashapes).Thefunctionsyouneedareqqnormandqqline(quantile–quantileplot
against a normal distribution):
data<-read.csv("c:\\temp\\skewdata.csv")
attach(data)
qqnorm(values)
qqline(values,lty=2)
80 STATISTICS:ANINTRODUCTIONUSINGR
This shows a marked S-shape, indicative of non-normality (as we already know, our
distribution is non-normal because it is skew to the left; see p. 68).
We can investigate the issues involved with Michelson’s (1880) famous data on
estimating the speed of light. The actual speed is 299000kms
 1
plus the values in our
dataframe called light:
light<-read.csv("c:\\temp\\light.csv")
attach(light)
names(light)
[1]"speed"
hist(speed)
We get a summary of the non-parametric descriptors of the sample like this:
summary(speed)
Min. 1stQu. Median Mean3rdQu. Max.
650 850 940 909 980 1070
From this, you see at once that the median (940) is substantially bigger than the mean
(909),asaconsequenceofthestrongnegativeskewinthedataseeninthehistogram.The
interquartilerange,thedifferencebetweenthefirstandthirdquartiles,is980 850=130.
This is useful in the detection of outliers: a good rule of thumb is this
an outlier is a value more than 1.5 times the interquartile range above the third
quartile, or below the first quartile.
(130×1.5=195).Inthiscase,therefore,outlierswouldbemeasurementsofspeedthatwere
lessthan850 195=655orgreaterthan980+195=1175.Youwillseethatthereareno
large outliers in this data set, but one or more small outliers (the minimum is 650).
SINGLESAMPLES 81
InferenceintheOne-SampleCase
WewanttotestthehypothesisthatMichelson’sestimateofthespeedoflightissignificantly
different from the value of 299990 thought to prevail at the time. The data have all had
299000subtractedfromthem,sothetestvalueis990.Becauseofthenon-normality,theuse
ofStudent’sttestinthiscaseisilladvised.ThecorrecttestisWilcoxon’ssigned-ranktest:
wilcox.test(speed,mu=990)
Wilcoxonsignedranktestwithcontinuitycorrection
data: speed
V=22.5,p-value=0.00213
alternativehypothesis:truelocationisnotequalto990
Warningmessage:
Inwilcox.test.default(speed,mu=990):
cannotcomputeexactp-valuewithties
Werejectthenullhypothesisandacceptthealternativehypothesisbecausep=0.00213
(i.e. much less than 0.05). The speed of light is significantly less than 990.
BootstrapinHypothesisTestingwithSingleSamples
Weshallmeetparametricmethodsforhypothesistestinglater.Hereweusebootstrappingto
illustrateanothernon-parametricmethodofhypothesistesting.Oursamplemeanvalueofyis
909.Thequestionwehavebeenaskedtoaddressisthis:‘Howlikelyisitthatthepopulation
meanthatwearetryingtoestimatewithourrandomsampleof100valuesisasbigas990?’
Wetake10000randomsampleswithreplacementusingn=100fromthe100valuesof
lightandcalculate10000valuesofthemean.Thenweask:whatistheprobabilityofobtaining
a mean as large as 990 by inspecting the right-hand tail of the cumulative probability
distributionofour10000bootstrappedmeanvalues?Thisisnotashardasitsounds:
a<-numeric(10000)
for(iin1:10000) a[i]<-mean(sample(speed,replace=T))
hist(a)
82 STATISTICS:ANINTRODUCTIONUSINGR
Thetestvalueof990isoffthescaletotheright.Ameanof990isclearlymostunlikely,
given the data:
max(a)
[1]983
Inour10000samplesofthedata,weneverobtainedameanvaluegreaterthan983,sothe
probability that the mean is 990 is clearly p<0.0001.
Student’stDistribution
Student’s t distribution is used instead of the normal distribution when sample sizes are
small (n<30). Recall that the 95% intervals of the standard normalwere  1.96 to +1.96
standarddeviations.Student’stdistributionproducesbiggerintervalsthanthis.Thesmaller
thesample,thebiggertheinterval.Letusseethisinaction.Theequivalentsofpnormand
qnorm are pt and qt. We are going to plot a graph to show how the upper interval
(equivalenttothenormal’s1.96)varieswithsamplesizeinatdistribution.Thisisadeviate,
so the appropriate function is qt. We need to supply it with the probability (in this case
p=0.975)andthedegreesoffreedom(weshallvarythisfrom1to30toproducethegraph)
plot(c(0,30),c(0,10),type="n",
xlab="Degreesoffreedom",ylab="Studentstvalue")
lines(1:30,qt(0.975,df=1:30),col="red")
abline(h=1.96,lty=2,col="green")
The importance of using Student’s t rather than the normal is relatively slight until the
degreesoffreedomfallbelowabout10(above whichthecritical value isroughly2),and
then it increases dramatically below about 5 degrees of freedom. For samples with more
SINGLESAMPLES 83
than30degreesoffreedom,Student’stproducesanasymptoticvalueof1.96,justlikethe
normal(thisisthehorizontalgreendottedline).ThegraphdemonstratesthatStudent’st=2
is a reasonable rule of thumb; memorizing this will save you lots of time in looking up
critical values in later life.
So what does the t distribution look like, compared to a normal? Let us redraw the
standard normal in black:
xvs<-seq(-4,4,0.01)
plot(xvs,dnorm(xvs),type="l",
ylab="Probabilitydensity",xlab="Deviates")
Now we can overlay Student’s t with d.f.=5 as a red line to see the difference:
lines(xvs,dt(xvs,df=5),col="red")
Thedifferencebetweenthenormal(blackline)andStudent’stdistributions(redline)is
thatthetdistributionhas‘fattertails’.Thismeansthatextremevaluesaremorelikelywitha
tdistributionthanwithanormal,andtheconfidenceintervalsarecorrespondinglybroader.
So instead of a 95% interval of ±1.96 with a normal distribution we should have a 95%
interval of ±2.57 for a Student’s t distribution with just 5 degrees of freedom:
qt(0.975,5)
[1]2.570582
Higher-OrderMomentsofaDistribution
So far, and without saying so explPicitly, we have encountered the first two moments of a
sample distribution. The quantity y was used in the context of defining the arithmetic
P P
meanofasinglesample:thisisthefirstmoment,y y=n.Thequantity y y2,the
84 STATISTICS:ANINTRODUCTIONUSINGR
sumofsquares,wasusedincalculatingsamplevariance,andthisisthesecondmomentof
P
the distribution, s2  y y2= n 1. Higher-order moments involve powers of the
P P
difference greater than 2, such as y y3 and y y4.
Skew
Skew (or skewness) is the dimensionless version of the third moment about the mean
P
y y3
m 
3 n
which is rendered dimensionless by dividing by the cube of the standard deviation of y
(because this is also measured in units of y3):
(cid:3)pffiffiffiffi(cid:4)
3
s sd y3  s2
3
The skew is then given by
m
skewγ  3
1 s
3
Itmeasurestheextenttowhichadistributionhaslong,drawn-outtailsononesideorthe
other.Anormaldistributionissymmetricalandhasskew=0.Negativevaluesofγ mean
1
skewtotheleft(negativeskew)andpositivevaluesmeanskewtotheright.Totestwhether
aparticularvalueofskewissignificantlydifferentfrom0(andhencethedistributionfrom
whichitwascalculatedissignificantlynon-normal)wedividetheestimateofskewbyits
approximate standard error:
rffiffiffi
6
SE 
γ 1 n
ItisstraightforwardtowriteanRfunctiontocalculatethedegreeofskewforanyvector
of numbers, x, like this:
skew<-function(x){
m3<-sum((x-mean(x))^3)/length(x)
s3<-sqrt(var(x))^3
m3/s3 }
Notetheuseofthelength(x)functiontoworkoutthesamplesize,n,whateverthesize
ofthevectorx.Thelastexpressioninsideafunctionisnotassignedtoavariablename,and
is returned as the value of skew(x) when this is executed from the command line.
Weusethedatafromthefilecalledskewdata.txtthatwereadonp.79.Toillustrate
the skew, we plot a histogram of values, taking the opportunity to introduce two useful
options: main="" to suppress the production of a title and col="green" to fill in the
histogram bars in a chosen colour:
hist(values,main="",col="green")
SINGLESAMPLES 85
Thedataappeartobepositivelyskew(i.e.tohavealongertailontherightthanonthe
left). We use the new function skew to quantify the degree of skewness:
skew(values)
[1]1.318905
Nowweneedtoknowwhetheraskewof1.319issignificantlydiffpereffiffinffiffitffiffiffifffiromzero.Wedo
a t test, dividing the observed value of skew by its standard error 6=n:
skew(values)/sqrt(6/length(values))
[1]2.949161
Finally,weask:whatistheprobabilityofgettingatvalueof2.949bychancealone,given
that we have 28 degrees of freedom, when the skew value really is zero?
1-pt(2.949,28)
[1]0.003185136
Weconcludethatthesedatashowsignificantnon-normality(p<0.0032).Notethatwe
haven 2=28degreesoffreedom,becauseinordertocalculateskewweneededtoknow
thevaluesoftwoparametersthatwereestimatedfromthedata:themeanandthevariance.
Thenextstepmightbetolookforatransformationthatnormalizesthedatabyreducing
theskewness.Onewayofdrawinginthelargervaluesistotakesquareroots,soletustry
this to begin with:
skew(sqrt(values))/sqrt(6/length(values))
[1]1.474851
86 STATISTICS:ANINTRODUCTIONUSINGR
This is not significantly skew. Alternatively, we might take the logs of the values:
skew(log(values))/sqrt(6/length(values))
[1]-0.6600605
Thisisnowslightlyskewtotheleft(negativeskew),butthevalueofStudent’stissmaller
thanwithasquare-roottransformation,sowemightpreferalogtransformationinthiscase.
Kurtosis
Thisisameasureofnon-normalitythathastodowiththepeakyness,orflat-toppedness,ofa
distribution.Thenormaldistributionisbell-shaped,whereasakurtoticdistributionisother
thanbell-shaped.Inparticular,amoreflat-toppeddistributionissaidtobeplatykurtic,anda
morepointydistributionissaidtobeleptokurtic.Kurtosisisthedimensionlessversionof
the fourth moment about the mean
P
y y4
m 
4 n
whichisrendereddimensionlessbydividingbythesquareofthevarianceofy(becausethis
is also measured in units of y4):
s var y2  s22
4
Kurtosis is then given by
m
kurtosisγ  4 3
2 s
4
The minus 3 is included because a normal distribution has m /s =3. This formulation
4 4
thereforehasthedesirablepropertyofgivingzerokurtosisforanormaldistribution,whilea
flat-topped (platykurtic) distribution has a negative value of kurtosis, and a pointy
(leptokurtic) distribution has a positive value of kurtosis. The approximate standard error
of kurtosis is
rffiffiffiffiffi
24
SE 
γ 2 n
An R function to calculate kurtosis might look like this:
kurtosis<-function(x){
m4<-sum((x-mean(x))^4)/length(x)
s4<-var(x)^2
m4/s4-3 }
For our present data, we find that kurtosis is not significantly different from normal:
kurtosis(values)
SINGLESAMPLES 87
[1]1.297751
kurtosis(values)/sqrt(24/length(values))
[1]1.450930
because the t value (1.45) is substantially less than the rule of thumb (2.0).
Reference
Michelson,A.A.(1880)ExperimentaldeterminationofthevelocityoflightmadeattheU.S.Naval
Academy,Annapolis. Astronomical Papers,1,109–145.
FurtherReading
Field,A., Miles, J.andField, Z.(2012) Discovering Statistics UsingR,Sage,London.
Williams,D.(2001)WeighingtheOdds.ACourseinProbabilityandStatistics,CambridgeUniversity
Press, Cambridge.
6
Two Samples
There is absolutely no point in carrying out an analysis that is more complicated than it
needs tobe.Occam’srazor applies tothechoice ofstatistical model justasstrongly asto
anything else: simplest is best. The so-called classical tests deal with some of the most
frequently-used kinds of analysis, and they are the models of choice for:
(cid:129) comparing two variances (Fisher’s F test, var.test)
(cid:129) comparing two sample means with normal errors (Student’s t test, t.test)
(cid:129) comparing two means with non-normal errors (Wilcoxon’s test, wilcox.test)
(cid:129) comparing two proportions (the binomial test, prop.test)
(cid:129) correlating two variables (Pearson’s or Spearman’s rank correlation, cor.test)
(cid:129) testing for independence in contingency tables using chi-squared (chisq.test)
(cid:129) testing small samples for correlation with Fisher’s exact test (fisher.test)
ComparingTwoVariances
Beforewecancarryoutatesttocomparetwosamplemeans,weneedtotestwhetherthe
samplevariancesaresignificantlydifferent(seep.55).Thetestcouldnotbesimpler.Itis
calledFisher’sFtestafterthefamousstatisticianandgeneticistR.A.Fisher,whoworkedat
Rothamstedinsouth-eastEngland.Tocomparetwovariances,allyoudoisdividethelarger
variance by the smaller variance.
Obviously,ifthevariancesarethesame,theratiowillbe1.Inordertobesignificantly
different, theratio will need to besignificantly bigger than 1 (because thelargervariance
goesontop,inthenumerator).Howwillweknowasignificantvalueofthevarianceratio
from a non-significant one? The answer, as always, is to look up the critical value of the
varianceratio.Inthiscase,wewantcriticalvaluesofFisher’sF.TheRfunctionforthisisqf
whichstandsfor‘quantilesoftheFdistribution’.Forourexampleofozonelevelsinmarket
gardens (see Chapter 4) there were 10 replicates in each garden, so there were 10 1=9
degreesoffreedomforeachgarden.Incomparingtwogardens,therefore,wehave9d.f.in
the numerator and 9 d.f. in the denominator. Although F tests in analysis of variance are
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
TWOSAMPLES 89
typicallyone-tailed(thetreatmentvarianceisexpectedtobelargerthantheerrorvarianceif
themeansaresignificantlydifferent;seep.153),inthiscase,wehadnoexpectationasto
which garden was likely to have the higher variance, so we carry out a two-tailed test
(p1 α=2).Supposeweworkatthetraditionalα0:05,thenwefindthecriticalvalue
of F like this:
qf(0.975,9,9)
4.025994
Thismeansthatacalculatedvarianceratiowillneedtobegreaterthanorequalto4.026in
orderforustoconcludethatthetwovariancesaresignificantlydifferentatα0:05.Tosee
thetestinaction,wecancomparethevariancesinozoneconcentrationformarketgardensB
and C.
f.test.data<-read.csv("c:\\temp\\f.test.data.csv")
attach(f.test.data)
names(f.test.data)
[1]"gardenB""gardenC"
First, we compute the two variances:
var(gardenB)
[1]1.333333
var(gardenC)
[1]14.22222
The larger variance is clearly in garden C, so we compute the F ratio like this:
F.ratio<-var(gardenC)/var(gardenB)
F.ratio
[1]10.66667
TheteststatisticshowsusthatthevarianceingardenCismorethan10timesasbigasthe
varianceingardenB.ThecriticalvalueofFforthistest(with9d.f.inboththenumerator
and the denominator) is 4.026 (see qf, above), so we conclude: since the test statistic is
larger than the critical value, we reject the null hypothesis.
The null hypothesis was that the two variances were not significantly different, so we
acceptthealternativehypothesisthatthetwovariancesaresignificantlydifferent.Infact,it
isbetterpracticetopresentthepvalueassociatedwiththecalculatedFratioratherthanjust
torejectthenullhypothesis.Todothisweusepfratherthanqf.Wedoubletheresulting
probability to allow for the two-tailed nature of the test:
2*(1-pf(F.ratio,9,9))
[1]0.001624199
90 STATISTICS:ANINTRODUCTIONUSINGR
sotheprobabilityofobtaininganFratioaslargeasthisorlarger,ifthevarianceswerethe
same(asassumedbythenullhypothesis),islessthan0.002.Itisimportanttonotethatthep
value is not the probability that the null hypothesis is true (this is a common mistake
amongstbeginners).Thenullhypothesisisassumedtobetrueincarryingoutthetest.Keep
rereadingthisparagraphuntilyouaresurethatyouunderstandthisdistinctionbetweenwhat
p values are and what they are not.
Becausethevariancesaresignificantlydifferent,itwouldbewrongtocomparethetwo
samplemeansusingStudent’sttest.Inthiscasethereasonisveryobvious;themeansare
exactlythesame(5.0pphm)butthetwogardensareclearlydifferentintheirdailylevelsof
ozone pollution (see p. 55).
There is a built-in function called var.test for speeding up the procedure. All we
providearethenamesofthetwovariablescontainingtherawdatawhosevariancesaretobe
compared (we do not need to work out the variances first):
var.test(gardenB,gardenC)
Ftesttocomparetwovariances
data: gardenBandgardenC
F=0.0938,numdf=9,denomdf=9,p-value=0.001624
alternativehypothesis:trueratioofvariancesisnotequalto1
95percentconfidenceinterval:
0.023286170.37743695
sampleestimates:
ratioofvariances
0.09375
Notethatthevarianceratio,F,isgivenasroughly1/10ratherthanroughly10because
var.testputthevariablenamethatcamefirstinthealphabet(gardenB)ontop(i.e.inthe
numerator)insteadofthebiggerofthetwovariances.Butthepvalueof0.0016isexactly
the same, and we reject the null hypothesis. These two variances are highly significantly
different.
detach(f.test.data)
ComparingTwoMeans
Givenwhatweknowaboutthevariationfromreplicatetoreplicatewithineachsample(the
within-sample variance), how likely is it that our two sample means were drawn from
populationswiththesameaverage?Ifthisishighlylikely,then weshallsaythatourtwo
samplemeansarenotsignificantlydifferent.Ifitisratherunlikely,thenweshallsaythatour
sample means are significantly different. As with all of the classical tests, we do this by
calculatinga test statistic and then asking thequestion: howlikely are we toobtaina test
statistic this big (or bigger) if the null hypothesis is true? We judge the probability by
comparing the test statistic with a critical value. The critical value is calculated on the
assumption that the null hypothesis is true. It is a useful feature of R that it has built-in
statisticaltablesforalloftheimportantprobabilitydistributions,sothatifweprovideRwith
the relevant degrees of freedom, it can tell us the critical value for any particular case.
TWOSAMPLES 91
There are two simple tests for comparing two sample means:
(cid:129) Student’sttestwhenthesamplesareindependent,thevariancesconstant,andtheerrors
are normally distributed
(cid:129) Wilcoxon rank-sum test when the samples are independent but the errors are not
normally distributed (e.g. they are ranks or scores or some sort)
Whattodowhentheseassumptionsareviolated(e.g.whenthevariancesaredifferent)is
discussed later on.
Student’stTest
Student was the pseudonym of W.S. Gosset who published his influential paper in
Biometrika in 1908. He was prevented from publishing under his own name by dint of
thearchaicemploymentlawsinplaceatthetime,whichallowedhisemployer,theGuinness
BrewingCompany,topreventhim publishingindependentwork.Student’stdistribution,
later perfected by R. A. Fisher, revolutionised the study of small-sample statistics where
inferences need to be made on the basis of the sample variance s2 with the population
variance σ2 unknown (indeed, usually unknowable).
The test statistic is the number of standard errors by which the two sample means
are separated:
difference between the two means y  y
t   A B
SE of the difference SE
diff
Wealreadyknowthestandarderrorofthemean(seep.60)butwehavenotyetmetthe
standard error of the difference between two means. For two independent (i.e. non-
correlated) variables, the variance of a difference is the sum of the separate variances
(see Box 6.1).
Box6.1. Thevarianceofadifferencebetweentwoindependentsamples
We want to work out the sum of squares of a difference between samples A and B.
First we express each y variable as a departure from its own mean, μ:
X(cid:2) (cid:3)
y  μ   y  μ  2
A A B B
If we were to divide by the degrees of freedom, we would get the variance of the
difference, σ2 . Start by calculating the square of the difference:
y  y
A B
y  μ 2 y  μ 2 2 y  μ  y  μ 
A A B B A A B B
92 STATISTICS:ANINTRODUCTIONUSINGR
Then apply summation:
X X X
y  μ 2 y  μ 2 2 y  μ  y  μ 
A A B B A A B B
P
WealreadyknoPwthattheaverageof y
A
 μ
A
2isthevarianceofpopulationAand
theaverageof y  μ 2isthevarianceofpopulationB(Box4.2).Sothevariance
B B
ofthedifferencebetweenPthetwosamplemeansisthesumofthevariancesofthetwo
samples, minus a term 2 y  μ  y  μ , i.e. minus 2 times the covariance of
A A B B
samplesAandB(seeBox6.2).Butbecause(byassumption)thesamplesfromAand
BPare independently drawn they are uncorrelated, the covariance is zero, so
2 y  μ  y  μ 0. This important result needs to be stated separately:
A A B B
σ2 σ2 σ2
y  y A B
A B
If two samples are independent, the variance of the difference is the sum of the two
samplevariances.Thisisnottrue,ofcourse,ifthesamplesarepositivelyornegatively
correlated (see p. 108).
This important result allows us to write down the formula for the standard error of the
difference between two sample means:
sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
s2 s2
SE  A  B
diff n n
A B
AtthisstagewehaveeverythingweneedtocarryoutStudent’sttest.Ournullhypothesis
is that the two sample means are the same, and we shall accept this unless the value of
Student’s t is sufficiently large that it is unlikely that such a difference arose by chance
alone. We shall estimate this probability by comparing our test statistic with the critical
value from Student’s t distribution with the appropriate number of degrees of freedom.
Forourozoneexample,eachsamplehas9degreesoffreedom,sowehave18d.f.intotal.
Anotherwayofthinkingofthisistoreasonthatthecompletesamplesizeas20,andwehave
estimatedtwoparametersfromthedata,y andy ,sowehave20 2=18d.f.Wetypically
A B
use5%asthechanceofrejectingthenullhypothesiswhenitistrue(thisistheTypeIerror
rate).Becausewedidnotknowinadvancewhichofthetwogardenswasgoingtohavethe
higher mean ozone concentration (and we usually do not), this is a two-tailed test, so the
critical value of Student’s t is:
qt(0.975,18)
[1]2.100922
TWOSAMPLES 93
This means that our test statistic needs to be bigger than 2.1 in order to reject the null
hypothesis,andhencetoconcludethatthetwomeansaresignificantlydifferentatα0:05.
The data frame is attached like this:
t.test.data<-read.csv("c:\\temp\\t.test.data.csv")
attach(t.test.data)
names(t.test.data)
[1]"gardenA""gardenB"
A useful graphical test for two samples employs the ‘notches’ option of boxplot:
ozone<-c(gardenA,gardenB)
label<-factor(c(rep("A",10),rep("B",10)))
boxplot(ozone∼label,notch=T,xlab="Garden",
ylab="Ozonepphm",col="lightblue")
Because the notches of two plots do not overlap, we conclude that the medians are
significantly different at the 5% level. Note that the variability is similar in both gardens
(bothintermsoftherange–thelengthofthewhiskers–andtheinterquartilerange–thesize
of the boxes).
Tocarryoutattestlong-hand,webeginbycalculatingthevariancesofthetwosamples,
s2A and s2B:
s2A<-var(gardenA)
s2B<-var(gardenB)
We need to check that the two variances are not significantly different:
s2A/s2B
[1]1
94 STATISTICS:ANINTRODUCTIONUSINGR
They are identical, which is excellent. Constancy of variance is the most important
assumption of the t test. In general, the value of the test statistic for Student’s t is the
difference divided by the standard error of the difference.
Inourcase,thenumeratoristhedifferencebetweenthetwomeans(3 5= 2),andthe
denominator is the square root of the sum of the variances (1.333333) divided by their
sample sizes (10):
(mean(gardenA)-mean(gardenB))/sqrt(s2A/10+s2B/10)
Note that in calculating the standard errors we divide by the sample size (10) not
thedegreesoffreedom(9);degreesoffreedomwereusedincalculatingthevariances(see
p. 53).
This gives the value of Student’s t as
[1]-3.872983
Withttestsyoucanignoretheminussign;itisonlytheabsolutevalueofthedifference
betweenthetwosamplemeansthatconcernsus.Sothecalculatedvalueoftheteststatisticis
3.87andthecriticalvalueis2.10(qt(0.975,18),above).Sincetheteststatisticislarger
than the critical value, we reject the null hypothesis.
NoticethatthewordingofthatprevioussentenceisexactlythesameasitwasfortheF
test(above).Indeed,thewordingisalwaysthesameforallkindsoftests,andyoushouldtry
tomemorizeit.Theabbreviatedformiseasiertoremember:largerreject,smalleraccept.
Thenullhypothesiswasthatthetwomeansarenotsignificantlydifferent,sowerejectthis
andacceptthealternativehypothesisthatthetwomeansaresignificantlydifferent.Again,
ratherthanmerelyrejectingthenullhypothesis,itisbettertostatetheprobabilitythatatest
statisticasextremeasthis(ormoreextreme)wouldbeobservedifthenullhypothesiswas
true(i.e.themeanvalueswerenotsignificantlydifferent).Forthisweuseptratherthanqt,
and 2 × pt because we are doing a two-tailed test:
2*pt(-3.872983,18)
[1]0.001114540
sop<0.0015.Youwillnotbesurprisedtolearnthatthereisabuilt-infunctiontodoallthe
workforus.Itiscalled,helpfully,t.testandisusedsimplybyprovidingthenamesofthe
two vectors containing the samples on which the test is to be carried out (gardenA and
gardenB in our case).
t.test(gardenA,gardenB)
Thereisratheralotofoutput.Youoftenfindthis:thesimplerthestatisticaltest,themore
voluminous the output.
WelchTwoSamplet-test
data: gardenAandgardenB
TWOSAMPLES 95
t=-3.873,df=18,p-value=0.001115
alternativehypothesis:truedifferenceinmeansisnotequalto0
95percentconfidenceinterval:
-3.0849115-0.9150885
sampleestimates:
meanofxmeanofy
3 5
The result is exactly the same as we obtained long-hand. The value of t is  3.873 and
sincethesignisirrelevantinattestwerejectthenullhypothesisbecausetheteststatisticis
largerthanthecriticalvalueof2.1.Themeanozoneconcentrationissignificantlyhigherin
gardenBthaningardenA.Theoutputalsogivesapvalueandaconfidenceinterval.Note
that,becausethemeansaresignificantlydifferent,theconfidenceintervalonthedifference
does not include zero (in fact, it goesfrom  3.085 up to  0.915). You might present the
result like this:
OzoneconcentrationwassignificantlyhigheringardenB(mean=5.0pphm)thanin
garden A (mean=3.0pphm; t=3.873, p=0.0011 (two-tailed), d.f.=18).
whichgivesthereaderalloftheinformationtheyneedtocometoaconclusionaboutthe
size of the effect and the unreliability of the estimate of the size of the effect.
WilcoxonRank-SumTest
This is a non-parametric alternative to Student’s t test, which we could use if the errors
werenon-normal.TheWilcoxonrank-sumteststatistic,W,iscalculatedasfollows.Both
samplesareputintoasinglearraywiththeirsamplenamesclearlyattached(AandBin
this case, as explained below). Then the aggregate list is sorted, taking care to keep the
samplelabelswiththeirrespectivevalues.Arankisthenassignedtoeachvalue,withties
gettingtheappropriateaveragerank(two-waytiesget(ranki+(ranki+1))/2,three-way
ties get (rank i+(rank i+1)+(rank i+3))/3, and so on). Finally, the ranks are added
up for each of the two samples, and significance is assessed on size of the smaller sum
of ranks.
First we make a combined vector of the samples:
ozone<-c(gardenA,gardenB)
ozone
[1]34432313525567443565
Then we make a list of the sample names, A and B:
label<-c(rep("A",10),rep("B",10))
label
[1]"A""A""A""A""A""A""A""A""A""A""B""B""B""B""B""B""B""B""B""B"
96 STATISTICS:ANINTRODUCTIONUSINGR
Nowweusethebuilt-infunctionranktogetavectorcontainingtheranks,smallestto
largest, within the combined vector:
combined.ranks<-rank(ozone)
combined.ranks
[1] 6.010.5 10.5 6.0 2.5 6.0 1.0 6.0 15.0 2.5 15.0 15.0 18.5 20.0 10.5
[16]10.5 6.0 15.0 18.5 15.0
Noticethatthetieshavebeendealtwithbyaveragingtheappropriateranks.Nowallwe
needtodoiscalculatethesumoftheranksforeachgarden.Weusetapplywithsumasthe
required operation:
tapply(combined.ranks,label,sum)
A B
66144
Finally,wecomparethesmallerofthetwovalues(66)withvaluesintablesofWilcoxon
ranksums(e.g.SnedecorandCochran,1980,p.555),andrejectthenullhypothesisifour
valueof66issmallerthanthevalueintables.Forsamplesofsize10and10likeours,the
5%valueintablesis78.Ourvalueissmallerthanthis,sowerejectthenullhypothesis.The
twosamplemeansaresignificantlydifferent(inagreementwithourearlierttestonthesame
data).
Wecancarryoutthewholeprocedureautomatically,andavoidtheneedtousetablesof
critical values of Wilcoxon rank sums, by using the built-in function wilcox.test:
wilcox.test(gardenA,gardenB)
This produces the following output:
Wilcoxonranksumtestwithcontinuitycorrection
data: gardenAandgardenB
W=11,p-value=0.002988
alternativehypothesis:truelocationshiftisnotequalto0
Warningmessage:
Inwilcox.test.default(gardenA,gardenB):
cannotcomputeexactp-valuewithties
Thefunctionusesanormalapproximationalgorithmtoworkoutazvalue,andfromthisa
pvaluetoassessthehypothesisthatthetwomeansarethesame.Thispvalueof0.002988is
much less than 0.05, so we reject the null hypothesis, and conclude that the mean ozone
concentrationsingardensAandBaresignificantlydifferent.Thewarningmessageatthe
end draws attention to the fact that there are ties in the data (repeats of the same ozone
measurement),andthismeansthatthepvaluecannotbecalculatedexactly(thisisseldoma
real worry).
ItisinterestingtocomparethepvaluesofthettestandtheWilcoxontestwiththesame
data: p=0.001115 and 0.002988, respectively. The non-parametric test is much more
TWOSAMPLES 97
appropriate than the t-test when the errors are not normal, and the non-parametric test is
about95%aspowerfulwithnormalerrors,andcanbemorepowerfulthanthettestifthe
distributionisstronglyskewedbythepresenceofoutliers.Typically,ashere,thettestwill
give the lower p value, so the Wilcoxon test is said to be conservative: if a difference is
significant under a Wilcoxon test it would be even more significant under a t test.
TestsonPairedSamples
Sometimes,two-sampledatacomefrompairedobservations.Inthiscase,wemightexpecta
correlation between the two measurements, either because they were made on the same
individual,orbecausetheyweretakenfromthesamelocation.Youmightrecallthatearlier
(Box 6.1) we found that the variance of a difference was the average of
y  μ 2 y  μ 2 2 y  μ  y  μ 
A A B B A A B B
which is the variance of sample A, plus the variance of sample B, minus 2 times the
covariance of A and B. When the covariance of A and B is positive, this is a great help
becauseitreducesthevarianceofthedifference,whichmakesiteasiertodetectsignificant
differences between the means. Pairing is not always effective, because the correlation
between y and y may be weak.
A B
Thefollowingdataareacompositebiodiversityscorebasedonakicksampleofaquatic
invertebrates from 16 rivers:
streams<-read.csv("c:\\temp\\streams.csv")
attach(streams)
names(streams)
[1]"down""up"
The elements are paired because the two samples were taken on the same river, one
upstreamandonedownstreamfromthesamesewageoutfall.Ifweignorethefactthatthe
samples are paired, it appears that the sewage outfall has no impact on biodiversity score
(p=0.6856):
t.test(down,up)
WelchTwoSamplet-test
data: downandup
t=-0.4088,df=29.755,p-value=0.6856
alternativehypothesis:truedifferenceinmeansisnotequalto0
95percentconfidenceinterval:
-5.248256 3.498256
sampleestimates:
meanofxmeanofy
12.500 13.375
However, if we allow that the samples are paired (simply by specifying the option
paired=T), the picture is completely different:
98 STATISTICS:ANINTRODUCTIONUSINGR
t.test(down,up,paired=T)
Pairedt-test
data: downandup
t=-3.0502,df=15,p-value=0.0081
alternativehypothesis:truedifferenceinmeansisnotequalto0
95percentconfidenceinterval:
-1.4864388-0.2635612
sampleestimates:
meanofthedifferences
-0.875
Now the difference between the means is highly significant (p=0.0081). The moral is
clear.Ifyoucandoapairedttest,thenyoushouldalwaysdothepairedtest.Itcanneverdo
anyharm,andsometimes(ashere)itcandoahugeamountofgood.Ingeneral,ifyouhave
information on blocking or spatial correlation (in this case, the fact that the two samples
came from the same river), then you should always use it in the analysis.
Hereisthesamepairedtestcarriedoutasaone-samplet-testonthedifferencesbetween
the pairs:
d<-up-down
t.test(d)
OneSamplet-test
data: d
t=3.0502,df=15,p-value=0.0081
alternativehypothesis:truemeanisnotequalto0
95percentconfidenceinterval:
0.26356121.4864388
sampleestimates:
meanofx
0.875
Asyousee,theresultisidenticaltothetwo-samplettestwithpaired=T(p=0.0081).
Theupstreamvaluesofthebiodiversityscoreweregreaterby0.875onaverage,andthis
difference is highly significant. Working with the differences has halved the number of
degreesoffreedom(from30to15),butithasmorethancompensatedforthisbyreducing
theerrorvariance,becausethereissuchastrongpositivecorrelationbetweeny andy .The
A B
moral is simple: blocking always helps. The individual rivers are the blocks in this case.
TheBinomialTest
This is one of the simplest of all statistical tests. Suppose that you cannot measure a
difference, but you can see it (e.g. in judging a diving contest). For example, nine
springboard divers were scored as better or worse, having trained under a new regime
andundertheconventionalregime(theregimeswereallocatedinarandomizedsequenceto
eachathlete:newthenconventional,orconventionalthennew).Diverswerejudgedtwice:
onediverwasworseonthenewregime,andeightwerebetter.Whatistheevidencethatthe
newregimeproducessignificantlybetterscoresincompetition?Theanswercomesfroma
TWOSAMPLES 99
two-tailedbinomialtest.Howlikelyisaresponseof1/9(or8/9ormoreextremethanthis,
i.e.0/9or9/9)ifthepopulationsareactuallythesame(i.e.therewasnodifferencebetween
thetwotrainingregimes)?Weusebinom.testforthis,specifyingthenumberof‘failures’
(1) and the total sample size (9):
binom.test(1,9)
This produces the output:
Exactbinomialtest
data: 1and9
numberofsuccesses=1,numberoftrials=9,p-value=0.03906
alternativehypothesis:trueprobabilityofsuccessisnotequalto0.5
95percentconfidenceinterval:
0.0028091370.482496515
sampleestimates:
probabilityofsuccess
0.1111111
From this we would conclude that the new training regime is significantly better than the
traditional method, because p<0.05. The p value of 0.03906 is the exact probability of
obtainingtheobservedresult(1of9)oramoreextremeresult(0of9)ifthetwotraining
regimeswerethesame(sotheprobabilityofanyoneofthejudgingoutcomeswas0.5).If
thetworegimeshadidenticaleffectsonthejudges,thenascoreofeither‘better’or‘worse’
hasaprobabilityof0.5.Soeightsuccesseshasaprobabilityof0.58=0.0039andonefailure
has a probability of 0.5. The whole outcome has a probability of
0:003906250:50:001953125
Note,however,thatthereare9waysofgettingthisresult,sotheprobabilityof1outof
9 is
90:0019531250:01757812
Thisisnottheanswerwewant,becausethereisamoreextremecasewhenall9ofthejudges
thoughtthatthenewregimeproducedimprovedscores.Thereisonlyonewayofobtaining
thisoutcome(9successesandnofailures)soithasaprobabilityof0.59=0.001953125.This
meansthattheprobabilitytheobservedresultoramoreextremeoutcomeis
0:0019531250:017578120:01953124
Eventhisisnottheanswerwewant,becausethewholeprocessmighthaveworkedinthe
oppositedirection(thenewregimemighthaveproducedworsescores(8and1or9and0)so
whatweneedisatwo-tailedtest).Theresultwewantisobtainedsimplybydoublingthelast
result:
20:019531240:03906248
whichisthefigureproducedbythebuiltinfunctionbinom.test(above)
100 STATISTICS:ANINTRODUCTIONUSINGR
BinomialTeststoCompareTwoProportions
Supposethatinyourinstitutiononlyfourfemaleswerepromoted,comparedwith196men.
Isthisanexampleofblatantsexism,asitmightappearatfirstglance?Beforewecanjudge,
ofcourse,weneedtoknowthenumberofmaleandfemalecandidates.Itturnsoutthat196
men were promoted out of 3270 candidates, compared with 4 promotions out of only 40
candidatesforthewomen.Now,ifanything,itlookslikethefemalesdidbetterthanmalesin
the promotion round (10% success for women versus 6% success for men).
Thequestionthenarisesastowhethertheapparentpositivediscriminationinfavourof
women is statistically significant, or whether this sort of difference could arise through
chancealone.ThisiseasyinRusingthebuilt-inbinomialproportionstestprop.testin
which we specify two vectors, the first containing the number of successes for females
andmalesc(4,196)andsecondcontainingthetotalnumberoffemaleandmalecandidates
c(40,3270):
prop.test(c(4,196),c(40,3270))
2-sampletestforequalityofproportionswithcontinuitycorrection
data: c(4,196)outofc(40,3270)
X-squared=0.5229,df=1,p-value=0.4696
alternativehypothesis:two.sided
95percentconfidenceinterval:
-0.06591631 0.14603864
sampleestimates:
prop1 prop2
0.100000000.05993884
Warningmessage:
Inprop.test(c(4,196),c(40,3270)):
Chi-squaredapproximationmaybeincorrect
Thereisnoevidenceinfavourofpositivediscrimination(p=0.4696).Aresultlikethis
willoccurmorethan45%ofthetimebychancealone.Justthinkwhatwouldhavehappened
ifoneofthesuccessfulfemalecandidateshadnotapplied.Thenthesamepromotionsystem
wouldhaveproducedafemalesuccessrateof3/39insteadof4/40(7.7%insteadof10%).
The moral is very important: in small samples, small changes have big effects.
Chi-SquaredContingencyTables
A great deal of statistical information comes in the form of counts (whole numbers or
integers);thenumberofanimalsthatdied,thenumberofbranchesonatree,thenumberof
daysoffrost,thenumberofcompaniesthatfailed,thenumberofpatientsthatdied. With
count data, thenumber0 isoften thevalue ofa response variable (consider, forexample,
what a 0 would mean in the context of the examples just listed).
The dictionary definition of contingency is ‘a thing dependent on an uncertain event’
(OxfordEnglishDictionary).Instatistics,however,thecontingenciesarealltheeventsthat
couldpossiblyhappen.Acontingencytableshowsthecountsofhowmanytimeseachof
thecontingenciesactuallyhappenedinaparticularsample.Considerthefollowingexample
that has to do with the relationship between hair colour and eye colour in white people.
TWOSAMPLES 101
Forsimplicity,wejustchosetwocontingenciesforhaircolour:‘fair’and‘dark’.Likewise
we just chose two contingencies for eye colour: ‘blue’ and ‘brown’. Each of these two
categorical variables, eye colour and hair colour, has two levels (‘blue’ and ‘brown’ and
‘fair’ and ‘dark’, respectively). Between them, they define four possible outcomes (the
contingencies):fairhairandblueeyes,fairhairandbrowneyes,darkhairandblueeyes,and
darkhairandbrowneyes.Wetakeasampleofpeopleandcounthowmanyofthemfallinto
each of these four categories. Then we fill in the 2×2 contingency table like this:
Blueeyes Browneyes
Fairhair 38 11
Darkhair 14 51
Theseareourobservedfrequencies(orcounts).Thenextstepisveryimportant.Inorder
to make any progress in the analysis of these data we need a model which predicts the
expectedfrequencies.Whatwouldbeasensiblemodelinacaselikethis?Thereareallsorts
ofcomplicatedmodelsthatyoumightselect,butthesimplestmodel(Occam’srazor,orthe
principle of parsimony) is that hair colour and eye colour are independent. We may not
believethatthisisactuallytrue,butthehypothesishasthegreatvirtueofbeingfalsifiable.It
is also a very sensible model to choose because it makes it easy to predict the expected
frequencies based on the assumption that the model is true. We need to do some simple
probabilitywork.Whatistheprobabilityofgettingarandomindividualfromthissample
whosehairwasfair?Atotalof49people(38+11)hadfairhairoutofatotalsampleof114
people.Sotheprobabilityoffairhairis49/114andtheprobabilityofdarkhairis65/114.
Noticethatbecausewehaveonlytwolevelsofhaircolour,thesetwoprobabilitiesaddupto
1((49+65)/114).Whatabouteyecolour?Whatistheprobabilityofselectingsomeoneat
randomfromthissamplewithblueeyes?Atotalof52peoplehadblueeyes(38+14)outof
the sample of 114, so the probability of blue eyes is52/114 and the probability of brown
eyesis62/114.Asbefore,thesesumto1((52+62)/114).Ithelpstoappendthesubtotalsto
the margins of the contingency table like this:
Blueeyes Browneyes Rowtotals
Fairhair 38 11 49
Darkhair 14 51 65
Columntotals 52 62 114
Nowcomestheimportantbit.Wewanttoknowtheexpectedfrequencyofpeoplewithfair
hairandblueeyes,tocomparewithourobservedfrequencyof38.Ourmodelsaysthatthetwo
areindependent.Thisisessentialinformation,becauseitallowsustocalculatetheexpected
probabilityoffairhairandblueeyes.If,andonlyif,thetwotraitsareindependent,thenthe
probability of having fair hair and blue eyes is the product of the two probabilities. So,
102 STATISTICS:ANINTRODUCTIONUSINGR
followingourearliercalculations,theprobabilityoffairhairandblueeyesis49/114×52/114.
Wecandoexactlyequivalentthingsfortheotherthreecellsofthecontingencytable:
Blueeyes Browneyes Rowtotals
Fairhair 49  52 49  62 49
114 114 114 114
Darkhair 65  52 65  62 65
114 114 114 114
Columntotals 52 62 114
Nowweneedtoknowhowtocalculatetheexpectedfrequency.Itcouldnotbesimpler.It
isjusttheprobabilitymultipliedbythetotalsample(n=114).Sotheexpectedfrequencyof
blue eyes and fair hair is 49  52 11422:35 which is much less than our observed
114 114
frequencyof38.Itisbeginningtolookasifourhypothesisofindependenceofhairandeye
colour is false.
Youmighthavenoticedsomethingusefulinthelastcalculation:twoofthesamplesizes
cancelout.Therefore,theexpectedfrequencyineachcellisjusttherowtotal(R)timesthe
column total (C) divided by the grand total (G):
RC
E 
G
We can now work out the four expected frequencies.
Blueeyes Browneyes Rowtotals
Fairhair 22.35 26.65 49
Darkhair 29.65 35.35 65
Columntotals 52 62 114
Noticethattherowandcolumntotals(theso-called‘marginaltotals’)areretainedunder
the model. It is clear that the observed frequencies and the expected frequencies are
different.But insampling, everythingalways varies, sothis isno surprise. The important
question iswhethertheexpected frequencies aresignificantlydifferentfrom theobserved
frequencies.
We assess the significance of the differences between using a chi-squared test. We
calculate a test statistic χ2 (Pearson’s chi-squared) as follows:
X O E2
χ2 
E
PwhereOistheobservedfrequencyandEistheexpectedfrequency.CapitalGreeksigma
just means ‘add up all the values of’. It makes the calculations easier if we write the
TWOSAMPLES 103
observed and expected frequencies in parallel columns, so that we can work out the
corrected squared differences more easily.
O E (O E)2 O E2
E
Fairhairandblueeyes 38 22.35 244.92 10.96
Fairhairandbrowneyes 11 26.65 244.92 9.19
Darkhairandblueeyes 14 29.65 244.92 8.26
Darkhairandbrowneyes 51 35.35 244.92 6.93
All we need to do now is to add up the four components of chi-squared to get the test
statisticχ2 35:33.Thequestionnowarises:isthisabigvalueofchi-squaredornot?This
isimportant,becauseifitisabiggervalueofchi-squaredthanwewouldexpectbychance,
then we should reject the null hypothesis. If, on the other hand, it is within the range of
values that we would expect by chance alone, then we should accept the null hypothesis.
Wealwaysproceedinthesamewayatthisstage.Wehaveacalculatedvalueofthetest
statistic: χ2 35:33. We compare this value of the test statistic with the relevant critical
value. To work out the critical value of chi-squared we need two things:
(cid:129) the number of degrees of freedom
(cid:129) the degree of certainty with which to work
Ingeneral,acontingencytablehasanumberofrows(r)andanumberofcolumns(c),and
the degrees of freedom is given by
d:f: r 1 c 1
Sowehave(2 1)×(2 1)=1degreeoffreedomfora2×2contingencytable.Youcan
seewhythereisonlyonedegreeoffreedombyworkingthroughourexample.Takethe‘fair
hair and brown eyes’box (the topright in thetable)and ask how many values this could
possiblytake.Thefirstthingtonoteisthatthecountcouldnotbemorethan49,otherwise
therowtotalwouldbewrong.Butinprinciple,thenumberinthisboxisfreetobeanyvalue
between0and49.Wehaveonedegreeoffreedomforthisbox.Butwhenwehavefixedthis
boxtobe11,youwillseethatwehavenofreedomatallforanyoftheotherthreeboxes.
Blueeyes Browneyes Rowtotals
Fairhair 11 49
Darkhair 65
Columntotals 52 62 114
104 STATISTICS:ANINTRODUCTIONUSINGR
Thetopleftboxhastobe49 11=38becausetherowtotalisfixedat49.Oncethetop
leftboxisdefinedas38thenthebottomleftboxhastobe52 38=14becausethecolumn
totalisfixed(thetotalnumberofpeoplewithblueeyeswas52).Thismeansthatthebottom
rightboxhastobe65 14=51.Thus,becausethemarginaltotalsareconstrained,a2×2
contingency table has just 1 degree of freedom.
Thenextthingweneedtodoissayhowcertainwewanttobeaboutthefalsenessofthe
null hypothesis. The more certain we want to be, the larger the value of chi-squared we
wouldneedtorejectthenullhypothesis.Itisconventionaltoworkatthe95%level.That
isourcertaintylevel,soouruncertaintylevelis100 95=5%.Expressedasafraction,
this is called alpha (α0:05). Technically, alpha is the probability of rejecting the null
hypothesiswhenitistrue.ThisiscalledaTypeIerror.ATypeIIerrorisacceptingthe
null hypothesis when it is false (see p. 4).
Critical values in R are obtained by use of quantiles (q) of the appropriate statistical
distribution.Forthechi-squareddistribution,thisfunctioniscalledqchisq.Thefunction
has two arguments: the certainty level (1 α=0.95), and the degrees of freedom
(d.f.=1):
qchisq(0.95,1)
[1]3.841459
The critical value of chi-squared is 3.841.The logic goes like this: since the calculated
valueoftheteststatisticisgreaterthanthecriticalvalue,werejectthenullhypothesis.You
should memorize this sentence: put the emphasis on ‘greater’ and ‘reject’.
Whathavewelearnedsofar?Wehaverejectedthenullhypothesisthateyecolourand
hair colour are independent. But that is not the end of the story, because we have not
establishedthewayinwhichtheyarerelated(e.g.isthecorrelationbetweenthempositiveor
negative?).Todothisweneedtolookcarefullyatthedata,andcomparetheobservedand
expected frequencies. If fair hair and blue eyes were positively correlated, would the
observed frequency of getting this combination be greater or less than the expected
frequency? A moment’s thought should convince you that the observed frequency will
be greater than the expected frequency when the traits are positively correlated (and less
whentheyarenegativelycorrelated).Inourcaseweexpectedonly22.35butweobserved
38people(nearlytwiceasmany)tohavebothfairhairandblueeyes.Soitisclearthatfair
hair and blue eyes are positively associated.
In R the procedure is very straightforward. We start by defining the counts as a 2×2
matrix like this:
count<-matrix(c(38,14,11,51),nrow=2)
count
[,1][,2]
[1,] 38 11
[2,] 14 51
Noticethatyouenterthedatacolumn-wise(notrow-wise)intothematrix.Thenthetest
uses the chisq.test function, with the matrix of counts as its only argument:
TWOSAMPLES 105
chisq.test(count)
Pearson’sChi-squaredtestwithYates’continuitycorrection
data: count
X-squared=33.112,df=1,p-value=8.7e-09
The calculated value of chi-squared is slightly different from ours, because Yates’s
correctionhasbeenappliedasthedefault(see?chisq.test).Ifyouswitchthecorrection
off (correct=F), you get the exact value we calculated by hand:
chisq.test(count,correct=F)
Pearson’sChi-squaredtest
data: count
X-squared=35.3338,df=1,p-value=2.778e-09
It makes no difference at all to the interpretation: there is a highly significant positive
association between fair hair and blue eyes for this group of people.
Fisher’sExactTest
Thistestisusedfortheanalysisofcontingencytablesbasedonsmallsamplesinwhichone
ormoreoftheexpectedfrequenciesarelessthan5.Theindividualcountsarea,b,candd
like this:
2×2table Column1 Column2 Rowtotals
Row1 a b a+b
Row2 c d c+d
Columntotals a+c b+d n
The probability of any one particular outcome is given by
ab! cd! ac! bd!
p
a!b!c!d!n!
where n is the grand total, and ! means ‘factorial’ (the product of all the numbers from n
down to 1; 0! is defined as being 1).
Ourdataconcernthedistributionof8antsnestsover10treesofeachoftwospecies(A
and B). There are two categorical explanatory variables (ants and trees), and four
contingencies, ants (present or absent) and trees (A or B). The response variable is the
vector of four counts c(6,4,2,8).
106 STATISTICS:ANINTRODUCTIONUSINGR
TreeA TreeB Rowtotals
Withants 6 2 8
Withoutants 4 8 12
Columntotals 10 10 20
Now we can calculate the probability for this particular outcome:
factorial(8)*factorial(12)*factorial(10)*factorial(10)/
(factorial(6)*factorial(2)*factorial(4)*factorial(8)*factorial(20))
[1]0.07501786
Butthisisonlypartofthestory.Weneedtocomputetheprobabilityofoutcomesthatare
moreextremethanthis.Therearetwoofthem.Supposeonlyoneantcolonywasfoundon
tree B. Then thetablevalues wouldbe 7,1, 3, 9but therow andcolumn totals would be
exactly the same (the marginal totals are constrained). The numerator always stays the
same, so this case has probability
factorial(8)*factorial(12)*factorial(10)*factorial(10)/
(factorial(7)*factorial(3)*factorial(1)*factorial(9)*factorial(20))
[1]0.009526078
ThereisanevenmoreextremecaseifnoantcoloniesatallwerefoundontreeB.Nowthe
table elements become 8, 0, 2, 10 with probability
factorial(8)*factorial(12)*factorial(10)*factorial(10)/
(factorial(8)*factorial(2)*factorial(0)*factorial(10)*factorial(20))
[1]0.0003572279
We need to add these three probabilities together:
0.07501786+0.009526078+0.000352279
[1]0.08489622
Buttherewasnoapriorireasonforexpectingtheresulttobeinthisdirection.Itmight
havebeentreeAthathadrelativelyfewantcolonies.Weneedtoallowforextremecountsin
theoppositedirectionbydoublingthisprobability(allFisher’sexacttestsaretwo-tailed):
2*(0.07501786+0.009526078+0.000352279)
[1]0.1697924
Thisshowsthatthereisnostrongevidenceofacorrelationbetweentreeandantcolonies.
The observed pattern, or a more extreme one, could have arisen by chance alone with
probability p=0.17.
TWOSAMPLES 107
There is a built in function called fisher.test, which saves us all this tedious
computation. It takes as its argument a 2×2 matrix containing the counts of the four
contingencies.Wemakethematrixlikethis(comparewiththealternativemethodofmaking
a matrix, above):
x<-as.matrix(c(6,4,2,8))
dim(x)<-c(2,2)
x
[,1] [,2]
[1,] 6 2
[2,] 4 8
Then we run the test like this:
fisher.test(x)
Fisher’sExactTestforCountData
data: x
p-value=0.1698
alternativehypothesis:trueoddsratioisnotequalto1
95percentconfidenceinterval:
0.602680579.8309210
sampleestimates:
oddsratio
5.430473
Thefisher.testcanbeusedwithmatricesmuchbiggerthan2×2.Alternatively,the
functionmaybeprovidedwithtwovectorscontainingfactorlevels,insteadofamatrixof
counts,ashere;thissavesyouthetroubleofcountinguphowmanycombinationsofeach
factor level there are:
table<-read.csv("c:\\temp\\fisher.csv")
attach(table)
head(table)
treenests
1 A ants
2 B ants
3 A none
4 A ants
5 B none
6 A none
The function is invoked by providing the two vector names as arguments:
fisher.test(tree,nests)
108 STATISTICS:ANINTRODUCTIONUSINGR
CorrelationandCovariance
With two continuous variables, x and y, the question naturally arises as to whether their
values are correlated with each other (remembering, of course, that correlation does not
implycausation).Correlationisdefinedintermsofthevarianceofx,thevarianceofy,and
the covariance of x and y (the way the two vary together; the way they co-vary) on the
assumption that both variables are normally distributed. We have symbols already for
thetwovariances,s2 ands2.Wedenotethecovarianceofxandybycov(x,y),afterwhich
x y
the correlation coefficient r is defined as
cov x;y
r  qffiffiffiffiffiffiffiffi
s2s2
x y
Weknowhowtocalculatevariances(p.53),soitremainsonlytoworkoutthevalueof
the covariance of x and y. Covariance is defined as the expectation of the vector product
x×y, which sounds difficult, but is not (Box 6.2). The covariance of x and y is the
expectation of the product minus the product of the two expectations.
Box6.2. Correlationandcovariance
The correlation coefficient is defined in terms of the covariance of x and y, and the
geometric mean of the variances of x and y:
cov x;y
ρ x;ypffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
var xvar y
The covariance of x and y is defined as the expectation of the vector product:
x x y y:
cov x;yE x x y y
We start by multiplying through the brackets:
x x y yxy xy xyxy
Now applying expectations, and remembering that the expectation of x is x and the
expectation of y is y, we get
cov x;yE xy xE y E xyxyE xy xy xyxy
TWOSAMPLES 109
Then  xyxy cancels out, leaving  xy which is  E xE y so
cov x;yE xy E xE y
Noticethatwhenxandyareuncorrelated,E xyE xE y,sothecovarianceis0in
this case. The corrected sum of products SSXY (see p. 123) is given by
P P
X
x y
SSXY  xy 
n
so covariance is computed as
sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
1
cov x;ySSXY
n 12
which provides a shortcut formula for the correlation coefficient
SSXY
r pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
SSX:SSY
becausethedegreesoffreedom(n 1)cancelout.ThesignofrtakesthesignofSSXY:
positive for positive correlations and negative for negative correlations.
Let us work through a numerical example:
data<-read.csv("c:\\temp\\twosample.csv")
attach(data)
plot(x,y,pch=21,col="blue",bg="orange")
110 STATISTICS:ANINTRODUCTIONUSINGR
First we need the variance of x and the variance of y:
var(x)
[1]199.9837
var(y)
[1]977.0153
Thecovarianceofxandy,cov(x,y),isgivenbythevarfunctionwhenwesupplyitwith
two vectors like this:
var(x,y)
[1]414.9603
pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
Thus, the correlation coefficient should be 414:96= 199:98977:02:
var(x,y)/sqrt(var(x)*var(y))
[1]0.9387684
Let us see if this checks out:
cor(x,y)
[1]0.9387684
Great relief! So now you know the definition of the correlation coefficient: it is the
covariance divided by the geometric mean of the two variances.
CorrelationandtheVarianceofDifferencesbetweenVariables
Samplesoftenexhibitpositivecorrelationsthatresultfromthepairing,asintheupstream
anddownstreaminvertebratebiodiversitydatathatweinvestigatedearlier(p.97).Thereis
animportantgeneralquestionabouttheeffectofcorrelationonthevarianceofdifferences
betweenvariables.Intheextreme,whentwovariablesaresoperfectlycorrelatedthatthey
areidentical,thedifferencebetweenonevariableandtheotheriszero.Soitisclearthatthe
variance of a difference will decline as the strength of positive correlation increases.
Thesedatashowthedepthofthewatertable(mbelowthesurface)inwinterandsummer
at nine locations:
paired<-read.csv("c:\\temp\\water.table.csv")
attach(paired)
names(paired)
[1]"Location""Summer" "Winter"
Webeginbyaskingwhetherthereisacorrelationbetweensummerandwinterwatertable
depths across locations:
TWOSAMPLES 111
cor(Summer,Winter)
[1]0.8820102
There is a strong positive correlation. Not surprisingly, places where the water table is
highinsummertendtohaveahighwatertableinwinteraswell.Ifyouwanttodetermine
thesignificanceofacorrelation(i.e.thepvalueassociatedwiththecalculatedvalueofr)
thenusecor.testratherthancor.Thistesthasnon-parametricoptionsforKendall’stau
orSpearman’srankdependingonthemethodyouspecify(method="k"ormethod="s"),
but the default method is Pearson’s product-moment correlation (method="p"):
cor.test(Summer,Winter)
Pearson’sproduct-momentcorrelation
data: SummerandWinter
t=4.9521,df=7,p-value=0.001652
alternativehypothesis:truecorrelationisnotequalto0
95percentconfidenceinterval:
0.52599840.9750087
sampleestimates:
cor
0.8820102
Thecorrelationishighlysignificant(p=0.00165).Now,letusinvestigatetherelation-
shipbetweenthecorrelationcoefficientandthethreevariances:thesummervariance,the
winter variance, and the variance of the differences (summer–winter):
varS<-var(Summer)
varW<-var(Winter)
varD<-var(Summer-Winter)
The correlation coefficient ρ is related to these three variances by:
σ2σ2 σ2
ρ y z y z
2σ σ
y z
So, using the values we have just calculated, we get a correlation coefficient of
(varS+varW-varD)/(2*sqrt(varS)*sqrt(varW))
[1]0.8820102
whichchecksout.Wecanalsoseewhetherthevarianceofthedifferenceisequaltothesum
of the component variances (as we saw for independent variables on p. 97):
varD
[1]0.01015
112 STATISTICS:ANINTRODUCTIONUSINGR
varS+varW
[1]0.07821389
No,itisnot.Theywouldbeequalonlyifthetwosampleswereindependent.Infact,we
knowthatthetwovariablesarepositivelycorrelated,sothevarianceofthedifferenceshould
be less than the sum of the variances by an amount equal to 2rs s :
1 2
varS+varW-2*0.8820102*sqrt(varS)*sqrt(varW)
[1]0.01015
That’s more like it.
Scale-DependentCorrelations
Anothermajordifficultywithcorrelationsisthatscatterplotscangiveahighlymisleading
impressionofwhatisgoingon.Themoralofthisexerciseisveryimportant:thingsarenot
alwaysastheyseem.Thefollowingdatashowthenumberofspeciesofmammalsinforests
of differing productivity:
data<-read.csv("c:\\temp\\productivity.csv")
attach(data)
names(data)
[1]"productivity""mammals" "region"
plot(productivity,mammals,pch=16,col="blue")
There is a very clear positive correlation: increasing productivity is associated with
increasing species richness. The correlation is highly significant:
TWOSAMPLES 113
cor.test(productivity,mammals,method="spearman")
Spearman’srankcorrelationrho
data: productivityandmammals
S=6515.754,p-value=5.775e-11
alternativehypothesis:truerhoisnotequalto0
sampleestimates:
rho
0.7516389
Warningmessage:
Incor.test.default(productivity,mammals,method="spearman"):
Cannotcomputeexactp-valuewithties
Butwhatifwelookattherelationshipforeachregionseparately,usingadifferentcolour
for each region?
plot(productivity,mammals,pch=16,col=as.numeric(region))
The pattern is obvious. In every single case, increasing productivity is associated with
reduced mammal speciesrichnesswithineachregion.Thelessonisclear: youneedtobe
extremely careful when looking at correlations across different scales. Things that are
positivelycorrelatedovershorttimescalesmayturnouttobenegativelycorrelatedinthe
longterm.Thingsthatappeartobepositivelycorrelatedatlargespatialscalesmayturnout
(as in this example) to be negatively correlated at small scales.
Reference
Snedecor,G.W.andCochran,W.G.(1980)StatisticalMethods,IowaStateUniversityPress,Ames.
FurtherReading
Dalgaard,P. (2002)Introductory Statistics with R, Springer-Verlag, NewYork.
7
Regression
Regressionanalysisisthestatisticalmethodyouusewhenboththeresponsevariableand
theexplanatoryvariablearecontinuousvariables(i.e.realnumberswithdecimalplaces –
thingslikeheights,weights,volumes,ortemperatures).Perhapstheeasiestwayofknowing
when regression is the appropriate analysis is to see that a scatterplot is the appropriate
graphic(incontrasttoanalysisofvariance,say,whentheappropriateplotwouldhavebeen
a box-and-whisker or a bar chart).
Theessenceofregressionanalysisisusingsampledatatoestimateparametervaluesand
theirstandarderrors.First,however,weneedtoselectamodelwhichdescribestherelationship
betweentheresponsevariableandtheexplanatoryvariable(s).Thereareliterallyhundredsof
models from which we might choose. Perhaps the most important thing to learn about
regressionisthatmodelchoiceisareallybigdeal.Thesimplestmodelofallisthelinearmodel:
yabx
The response variable is y, and x is a continuous explanatory variable. There are two
parameters, a and b: the intercept is a (the value of y when x=0); and the slope is b (the
slope,orgradient,isthechangeinydividedbythechangeinxwhichbroughtitabout).The
slope is so important that it is worth drawing a picture to make clear what is involved.
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
REGRESSION 115
Thetaskistoworkouttheslopeandinterceptofthisnegativelinearrelationshipbetween
theresponsevariableandtheexplanatoryvariable.Itiseasiesttostartwiththeinterceptin
thiscase,becausethevalueofx=0appearsonthegraph(itdoesnotalways).Theintercept
is simply the value of y when x=0. We can get this by inspection. Draw a vertical line
from x=0 until it intercephs the black regression line (the green line) then a horizontal
line(red)fromtheregressionlineuntilitcutstheyaxis.Readoffthevaluedirectly.Itis80
in this case.
Estimating the slope is slightly more involved because we need to calculate
change iny
change inxthat brought it about
Inpractice,itisagoodideaforprecisiontoselectalargechangeinx.Letustakeitfrom2to
8.Becausetheslopeofthegraphisnegative,thevalueofyislowerwhenx=8thanitiswhen
x=2.Atx=2,wedrawabluelineverticallydownwardsfromtheregressionlinetothevalue
ofywhenx=8.Thelengthofthisbluelineisthechangeiny(oftendenotedas‘deltay’,orΔy
insymbols).Nowwedrawahorizontalbrownlineshowingthechangeinxfrom2to8.The
lengthofthisbrownlineisΔx.Whenx=2wecanreadoffthevalueofy(approximately)from
thegraph:itisroughly66.Similarly,whenx=8wecanreadoffthevalueofyas24.
Sothechangeinxis+6(from2upto8)andthechangeinyis 42(from66downto24).
Finally, we can calculate the slope of the straight line, b:
change iny 24 66  42
b    7:0
change inx 8 2 6
Wenowknowbothoftheparametervaluesoftheline:theintercepta=80andtheslope
b= 7.0. We can write the parameterised equation like this:
y80 7x
116 STATISTICS:ANINTRODUCTIONUSINGR
We can predict values of y at x values we have not measured (say, at x=10.5):
y80 7:010:56:5
Wecanalsoaskwhatxvaluesareassociatedwithparticularvaluesofy(say,y=40).Thisis
a bit more work, because we have to rearrange the equation
4080 7:0x
First, subtract 80 from both sides
40 80 7:0x
then divide both sides by  7 to find the value of x:
40 80  40
x  5:714286
 7  7
You can get a rough check on this value by inspection of the graph. You should work
through this example repeatedly until you understand it completely.
LinearRegression
Letusstartwithanexample.Thethingtounderstandisthatthereisnothingdifficult,or
mysteriousaboutestimatingtheregressionparameters.Wecandoareasonablejob,just
by eye.
reg.data<-read.csv("c:\\temp\\tannin.csv")
attach(reg.data)
names(reg.data)
[1]"growth""tannin"
plot(tannin,growth,pch=21,bg="blue")
REGRESSION 117
This is how we do regression ‘by eye’. We ask: what has happened to the y value? It
decreasedfromabout12toabout2,sothechangeinyis 10(theminussignisimportant).
Howdidthexvaluechange?Itincreasedfrom0to8,sothechangeinxis+8(whenworking
outregressionsbyeye,itisagoodideatotakeasbigarangeofxvaluesaspossible,sohere
wetookthecompleterangeofx).Whatisthevalueofywhenx=0?Itisabout12,sothe
interceptisroughlya≈12.Finally,whatisthevalueofb?Itisthechangeiny( 10)divided
bythechangeinxwhichbroughtitabout(8),sob≈ 10/8= 1.25.Soourroughguessat
the regression equation is
y12:0 1:25x
That’s all there is to it. Obviously, we want to make the procedure more objective
thanthis.Andwealsowanttoestimatetheunreliabilityofthetwoestimatedparameters
(i.e.thestandarderrorsoftheslopeandintercept).Butthebasicsarejustasstraightfor-
ward as that.
LinearRegressioninR
Howclosedidwegettothemaximumlikelihoodestimatesofaandbwithourguesstimates
of 12 and  1.25? It is easy to find out using the R function lm which stands for ‘linear
model’(notethatthefirstletterofthefunctionnamelmisalowercaseL,notanumberone).
AllweneeddoistellRwhichofthevariablesistheresponsevariable(growthinthiscase)
and which is the explanatory variable (tannin concentration in the diet). The response
variable goes on the left of the tilde ∼ and the explanatory variable goes on the right,
likethis:growth∼tannin.Thisisread‘growthismodelledasafunctionoftannin’.Now
we write:
lm(growth∼tannin)
Coefficients:
(Intercept) tannin
11.756 -1.217
The two parameters are called Coefficients in R: the intercept is11.756 (compared
with out guesstimate of 12), and the slope is  1.217 (compared with our guesstimate of
 1.25). Not bad at all.
SowheredoesRgetitscoefficientsfrom?Weneedtodosomecalculationstofindthis
out.Ifyouaremoremathematicallyinclined,youmightliketoworkthroughBox7.1,but
thisisnotessentialtounderstand whatisgoingon.Rememberthatwhatwe wantarethe
maximum likelihood estimates of the parameters. That is to say that, given the data, and
having selected a linear model, we want to find the values of the slope and intercept that
make the data most likely. Keep rereading this sentence until you understand what it is
saying.
Thebestwaytoseewhatisgoingonistodoitgraphically.Letuscheatabitbyfittingthe
best-fit straight line through our scatterplot, using abline like this:
abline(lm(growth∼tannin),col="green")
118 STATISTICS:ANINTRODUCTIONUSINGR
Box7.1. Theleast-squaresestimateoftheregressionslope,b
Thebest-fitslopeisfoundbyrotatingthelineunPtiltheerrorsumofsquares,SSE,is
minimized,sowewanttofindtheminimumof y a bx2.Westartbyfinding
the derivative of SSE with respect to b:
X
dSSE
 2 x y a bx
db
Now, multiplying through the bracketed term by x gives:
X
dSSE
 2 xy ax bx2
db
Applysummationtoeachtermseparately,setthederivativetozero,anddivideboth
sides by  2 to remove the unnecessary constant:
X X X
xy  ax  bx2 0
We cannot solve the equation as it stands because there areP2 unknowns, a and b.
HPowever, we know the value of a is y bx. Also, note that ax can be written as
a x, so, replacing a and taking both a and b outside their summations gives:
(cid:2)P P (cid:3)
X X X
y x
xy   b x b x2 0
n n
P
Now multiply out the central bracketed term by x to get
P P (cid:4)P (cid:5)
X 2 X
x y x
xy  b  b x2 0
n n
Finally,takethetwotermscontainingbtotheright-handside,andnotetheirchangeof
sign:
P P (cid:4)P (cid:5)
X X 2
x y x
xy  b x2 b
n n
P (cid:4)P (cid:5)
Then divide both sides by x2  x 2=n to obtain the required estimate b:
P P
P x y
xy 
b (cid:4)P n (cid:5)
P x 2
x2 
n
Thus, the value of b that minimizes the sum of squares of the departures is given
simply by (see Box 7.3 for more details):
SSXY
b
SSX
This is the maximum likelihood estimate of the slope of the linear regression.
REGRESSION 119
Thefitisreasonablygood,butitisnotperfect.Thedatapointsdonotlieonthefittedline.
The difference between eachdata point and thevalue predicted by the model atthe same
value ofxiscalledaresidual.Some residuals arepositive(above theline)andothersare
negative(belowtheline).Letusdrawverticallinestoindicatethesizeoftheresiduals.The
firstxpointisattannin=0.Theyvaluemeasuredatthispointwasgrowth=12.Butwhat
is the growth predicted by the model at tannin=0? There is a built-in function called
predict to work this out:
fitted<-predict(lm(growth∼tannin))
fitted
1 2 3 4 5 6 7
11.75555610.538889 9.322222 8.105556 6.888889 5.672222 4.455556
8 9
3.2388892.022222
Sothefirstpredictedvalueofgrowthis11.755556whentannin=0.Todrawthefirst
residual, both x coordinates will be 0. The first y coordinate will be 12 (the observed
value)andthesecondwillbe11.755556(thefitted(orpredicted)value).Weuselines,
like this:
lines(c(0,0),c(12,11.755556))
Wecouldgothrough,laboriously,anddraweachresiduallikethis.Butitismuchquicker
to automate the procedure, using a loop to deal with each residual in turn:
for(iin1:9)
lines(c(tannin[i],tannin[i]),c(growth[i],fitted[i]),col="red")
120 STATISTICS:ANINTRODUCTIONUSINGR
These residuals describe the goodness of fit of the regression line. Our maximum
likelihood model is defined as the model that minimizes the sum of the squares of these
residuals.Itisuseful,therefore,towritedownexactlywhatanyoneoftheresiduals,d,is:it
is the measured value, y, minus the fitted value called ^y (y ‘hat’):
d y ^y
We can improve on this, because we know that ^y is on the straight line abx, so
d y  abxy a bx
Theequationincludesa bxbecauseoftheminussignoutsidethebracket.Nowourbest-
fitline,bydefinition,isgivenbythevaluesofaaPndbthatminimizethesumsofthesquares
oftheds(PseeBoxP7.1).Note,also,thatjustas y y0(Box4.1),sothesumofthe
residuals d  y a bx0 (Box 7.2).
Box7.2. Thesumoftheresidualsinalinearregressioniszero
Eachpointonthegraph,y,islocatedatx.Themodelpredictsavalue^yabxwhere
a is the intercept and b is the slope. Each residual is the distance between y and the
fitted value ^y at location x and we are interested in the sum of the residuals:
X
y a bx
which we shall prove is equal to zero. First, we note that the best-fit regression line
passesthroughthepoint x;ysothatweknowthatyabx.Wecanrearrangethis
to find the value of the intercept:
ay bx
We can substitute this value in the equation for the sum of the residuals (above):
X
y ybx bx
P P
Now we apply the summation, remembering that yny and xnx
X X
y nybnx b x
P P
Recall that y y=n and that x x=n so we can replace the means:
P P
X X
y x
y n bn  b x
n n
Cancel the ns to get
X X X X
y  yb x b x  0
which completes the proof.
REGRESSION 121
Togetanoverviewofwhatisinvolved,itisusefultoplotthesumofthesquaresofthe
residualsagainstthevalueoftheparameterwearetryingtoestimate.Letustaketheslopeas
ourexample.Weknowonethingforcertainaboutourstraight-linemodel;itwillpassthrough
thepointinthecentreofthecloudofdatawhosecoordinatesare(x;y).Thebest-fitlinewill
bepivotedaboutthemeanvaluesofxandyandourjobistofindthebestvalueforthisslope–
theonethatminimizesthesumofsquaresoftheredlinesinthegraphabove.Itshouldbe
reasonablyclearthatifourestimateoftheslopeistoosteep,thenthefitwillbepoorandthe
sumofsquareswillbelarge.Likewise,ifourestimateoftheslopeistooshallow,thenthefit
willbepoorandthesumofsquareswillbelarge.Somewherebetweenthesetwoextremes,
therewillbeavaluefortheslopethatminimizesthesumofsquares.Thisisthebest-fitvalue
thatwewanttodiscover.Firstweneedtoloopthoroughanappropriaterangeofvalueswhich
will include the best-fit value (say  1.4<b< 1.0) and work out the sum of the squared
residuals:letuscallthisquantitysse(youwillseewhylater):
(cid:129) change the value of the slope b
(cid:129) work out the new intercept ay bx
(cid:129) predict the fitted values of growth for each level of tannin abx
(cid:129) work out the residuals y a bx
P
(cid:129) square them and add them up, y a bx2
(cid:129) associate this value of sse[i] with the current estimate of the slope b[i]
Once this process is complete, we can produce a U-shaped graph with the squared
residualsontheyaxisandtheestimateoftheslopeonthexaxis.Nowwefindtheminimum
valueofsse(itturnsouttobe20.072)anddrawahorizontaldashedgreenline.Atthepoint
wherethisminimumtouchesthegraph,wereaddowntothexaxistofindthebestvalueof
the slope (the red arrow). This is the value (b= 1.217) that R provided for us earlier.
122 STATISTICS:ANINTRODUCTIONUSINGR
Here is the R code that produces the figure and extracts the best estimate of b:
b<- seq(-1.43,-1,0.002)
sse<-numeric(length(b))
for(iin1:length(b)){
a<-mean(growth)-b[i]*mean(tannin)
residual<-growth-a-b[i]*tannin
sse[i]<-sum(residual^2)
}
plot(b,sse,type="l",ylim=c(19,24))
arrows(-1.216,20.07225,-1.216,19,col="red")
abline(h=20.07225,col="green",lty=2)
lines(b,sse)
b[which(sse==min(sse))]
CalculationsInvolvedinLinearRegression
P P
We want to find the minPimum ofP d2 P  y  Pa bx2. To work thisPout we need the
‘famousfive’:theseare y2and y, x2and xandanewquantity, xy,thesumof
products. The sum of products is worked out pointwise, so for our data, it is:
tannin
[1] 0 1 2 3 4 5 6 7 8
growth
[1] 12 10 8 11 6 7 2 3 3
tannin*growth
[1] 0 10 16 33 24 35 12 21 24
We have 0×12=0, plus 1×10=10, plus 2×8=16, and so on:
sum(tannin*growth)
[1]175
Thenextthingistousethefamousfivetoworkoutthreeessential‘correctedsums’:the
correctedsumofsquaresofx,thecorrectedsumofsquaresofyandthecorrectedsumof
products, xy. The corrected sums of squares of y and x should be familiar to you:
(cid:4)P (cid:5)
X 2
y
SSY  y2 
n
(cid:4)P (cid:5)
X 2
x
SSX  x2 
n
becauseifwewantedthevarianceiny,wewouldjustdivideSSYbyitsdegreesoffreedom
(andlikewiseforthevarianceinx;seep.55).Itisonlythecorrectedsumofproductsthatis
novel,butitsstructurePisdirectlyanalogous.Thinkabouttheformu(cid:4)lPafor(cid:5)SSY,above.Itis
‘thesumofytimesy’ y2,‘minusthesumofytimesthesumofy’ y 2‘dividedbythe
REGRESSION 123
P
samplesize’,n.Theformula(cid:4)fPorS(cid:5)SXissimilar.Itis‘thesumofxtimesx’ x2,‘minusthe
sumofxtimesthesumofx’ x 2‘dividedbythesamplesize’,n.Nowthecorrectedsum
of products is:
(cid:4)P (cid:5)(cid:4)P (cid:5)
X
x y
SSXY  xy 
n
IfyoulookcarPefullyyouwillseethatthishasexactlythesam(cid:4)ePkin(cid:5)d(cid:4)Pofst(cid:5)ructure.Itis‘the
sumofxtimesy’ xy,‘minusthesumofxtimesthesumofy’ x y ‘dividedbythe
sample size’, n.
These three corrected sums of squares are absolutely central to everything that follows
aboutregressionandanalysisofvariance,soitisagoodideatorereadthissectionasoftenas
necessary,untilyouareconfidentthatyouunderstandwhatSSX,SSYandSSXYrepresent
(Box 7.3).
Box7.3. Correctedsumsofsquaresandproductsinregression
ThetotalsumofsquaresisSSY,thesumofsquaresofxisSSX,andthecorrectedsum
of products is SSXY:
(cid:4)P (cid:5)
X 2
y
SSY  y2 
n
(cid:4)P (cid:5)
X 2
x
SSX  x2 
n
P P
X
x y
SSXY  xy 
n
The explained variation is the regression sum of squares, SSR:
SSXY2
SSR
SSX
The unexplained variation is the error sum of squares, SSE, can be obtained by
difference
SSE SSY  SSR
but SSE is defined as the sum of the squares of the residuals, which is
X
SSE  y a bx2
The correlation coefficient, r, is given by
SSXY
r pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
SSXSSY
124 STATISTICS:ANINTRODUCTIONUSINGR
The next question is how we use SSX, SSY and SSXY to find the maximum likelihood
estimatesoftheparametersandtheirassociatedstandarderrors.Itturnsoutthatthisstepis
muchsimplerthanwhathasgonebefore.Themaximumlikelihoodestimateoftheslope,b,
that we extracted from the graph (above) is just:
SSXY
b
SSX
(the detailed derivation of this is in Box 7.1).
Nowthatweknowthevalueoftheslope,wecanuseanypointonthefittedstraightlineto
workoutthemaximumlikelihoodestimateoftheintercept,a.Onepartofthedefinitionof
the best-fit straight line is that it passes through the point (x;y) determined by the mean
valuesofxandy.Sinceweknowthatyabx,itmustbethecasethatyabx,andso
P P
y x
ay bx  b
n n
We can work out the parameter values for our example. To keep things as simple as
possible,wecancallthevariablesSSX,SSYandSSXY(notethatRis‘casesensitive’sothe
variable SSX is different from ssx):
Box7.4. Theshortcutformulaforthesumofproducts,SSXY
SSXYisbasedontheexpectationoftheproduct x x y y.Startbymultiplyingout
the brackets:
x x y yxy xy yxxy
P P P P
Now apply the summation, remembering that xyy x and yxx y:
X X X
xy y x x ynxy
P P
We know that xnx and that yny, so
X X
xy nyx nxynxy xy nxy
P P
Now replace the product of the two means by x=n y=n
P P
X
x y
xy n
n n
which, on cancelling the ns, gives the corrected sum of products as
P P
X
x y
SSXY  xy 
n
REGRESSION 125
SSX<-sum(tannin^2)-sum(tannin)^2/length(tannin)
SSX
[1]60
SSY<-sum(growth^2)-sum(growth)^2/length(growth)
SSY
[1]108.8889
SSXY<-sum(tannin*growth)-sum(tannin)*sum(growth)/length(tannin)
SSXY
[1]-73
That is all we need. So the slope is
SSXY  73
b   1:2166667
SSX 60
and the intercept is given by
P P
y x 62 36
a  b:  1:2166667 6:88894:8666711:755556
n n 9 9
Now we can write the maximum likelihood regression equation in full:
y11:75556 1:216667x
This, however, is only half of the story. In addition to the parameter estimates,
a=11.756andb= 1.2167,weneedtomeasuretheunreliabilityassociatedwitheachof
the estimated parameters. In other words, we need to calculate the standard error of the
interceptandthestandarderroroftheslope.Wehavealreadymetthestandarderrorofa
mean, and we used it in calculating confidence intervals (p. 60) and in doing Student’s
t test (p. 91). Standard errors of regression parameters are similar in so far as they are
enclosedinsideabigsquare-rootterm(sothattheunitsofthestandarderrorarethesame
astheunitsoftheparameter),andtheyhavetheerrorvariance,s2,inthenumerator.There
are extra components, however, which are specific to the unreliability of a slope or an
intercept(seeBoxes7.6and7.7fordetails).Beforewecanworkoutthestandarderrors,
however,weneedtoknowthevalueoftheerrorvariances2,andforthisweneedtocarry
out an analysis of variance.
PartitioningSumsofSquaresinRegression:SSY=SSR+SSE
Theideaissimple:wetakethetotalvariationiny,SSY,andpartitionitintocomponentsthat
tell us about the explanatory power of our model. The variation that is explained by the
model is called the regression sum of squares (denoted by SSR), and the unexplained
variationiscalledtheerrorsumofsquares(denotedbySSE;thisisthesumofthesquaresof
thelengthsoftheredlinesthatwedrewonthescatterplotonp.119).ThenSSY=SSR+SSE
(the proof is given in Box 7.5).
126 STATISTICS:ANINTRODUCTIONUSINGR
Box7.5. ProofthatSSY=SSR+SSE
Let us start with what we know. The difference between y and y is the sum of the
differences y ^y and ^y y, as you can see from the figure:
Inspection of the figure shows clearly that y y y ^y ^y y. It is not
obvious, however, that the sums of the squares of these three quantities should be
equal. We need to prove that.
First, square the terms on both sides of the equation
y y2  y ^y2 ^y y2
to get
y2 2yyy2 y2 2y^y^y2^y2 2^yyy2
From both sides, subtract y2y2 to leave
 2yy 2y^y2^y2 2^yy
Apply summation then set to zero:
X X X X
0 2yy  2y^y 2^y2  2^yy
Now group the terms with and without y:
X X X X
0 2yy  2^yy 2^y2  2y^y
REGRESSION 127
Because y is a constant, we can take it outside the summation along with the 2:
X X
02y y ^y2 ^y ^y y
P
WPealreadyknowthat y ^y0fromBox7.2,soallthatremainsistoproveisthat
2 ^y ^y y0. This requires more algebra, because we have to replace ^y with
abxyb x x. First multiply through the bracket to get^y2 y^y, then replace
the ^y:
yb x x yb x x y yb x x
This is the gory bit. Multiply out the brackets:
y22by x xb2 x x2 yy by x x
P
Next, apply the summation, recalling that y2 ny2:
X X X X
ny22by x xb2 x x2 y y b y x x
P
P
Now replace y with y , cancel the n and note that ny2 y y to obtain
n
P
X X X
y
2b x xb2 x x2 b y x x
n
P
Noticethatthefirstofthethreetermsinvolves x xsothismustbezero,leaving
X X
b2 x x2 b y x x
P P
Take boutsidethebracketandnotethat x x2 SSXand y x xSSXY,
so
 b SSXY   bSSX
Finally, remember that the definition of the slope, b=SSXY/SSX so b.SSX=SSXY,
giving
 b SSXY  SSXY0
to complete the proof.
Now, in principle, we could compute SSE because we knowPthat it isPthe sum of the
squaresofthedeviationsofthedatapointsfromthefittedmodel, d2  y a bx2.
Sinceweknowthevaluesofaandb,weareinapositiontoworkthisout.Theformulais
fiddly,however,becauseofallthosesubtractions,squaringsandaddingsup.Fortunately,
thereisaverysimpleshortcutthatinvolvescomputingSSR,theexplainedvariation,rather
than SSE. This is because
SSRb:SSXY
128 STATISTICS:ANINTRODUCTIONUSINGR
so we can immediately work out SSR 1:21667 7388:81667. And since SSY=
SSR+SSE we can get SSE by subtraction:
SSE SSY  SSR108:8889 88:8166720:07222
These components are now drawn together in what is known as the ‘ANOVA table’.
Strictly,wehaveanalysedsumsofsquaresratherthanvariancesuptothispoint,butyouwill
seewhyitiscalledanalysisofvarianceshortly.TheleftmostcolumnoftheANOVAtable
lists the sources of variation: regression, error and total in our example. The next column
containsthesumsofsquares,SSR,SSEandSSY.Thethirdcolumnisinmanywaysthemost
importanttounderstand;itcontainsthedegreesoffreedom.Therearenpointsonthegraph
(n=9 in this example). So far, our table looks like this:
Source Sumofsquares Degreesoffreedom Meansquares Fratio
Regression 88.817
Error 20.072
Total 108.889
Weshallworkoutthedegreesoffreedomassociatedwitheachofthesumsofsquaresin
turn. The easiest to deal with is the total sum of squares, bPecause it always has the same
formulaforitsdegreesoffreedom.ThedefinitionisSSY  y y2,andyoucanseethat
there is just one parameter estimated from the data: the mean value, y. Because we have
estimated one parameter from the data, we have n 1degrees of freedom(where nisthe
total number of points on the graph; 9 in our example). The next easiest to work out is
theerrorsumofsquares.LetuslookatitsformulatosePehowmanyparametersneedtobe
estimatedfromthedatabeforewecanworkoutSSE  y a bx2.Weneedtoknow
thevaluesofbothaandbbeforewecancalculateSSE.Theseareestimatedfromthedata,so
thedegreesoffreedomforerroraren 2.Thisisimportant,sorereadthelastsentenceifyou
donotseeityet.Themostdifficultofthethreeistheregressiondegreesoffreedom,because
youneedtothinkabout thisoneinadifferentway. Thequestionisthis: how manyextra
parameters,overandabovethemeanvalueofy,didyouestimatewhenfittingtheregression
modeltothedata?Theansweris1.Theextraparameteryouestimatedwastheslope,b.So
theregressiondegreesoffreedominthissimplemodel,withjustoneexplanatoryvariable,is
1. This will only become clear with practice.
TocompletetheANOVAtable,weneedtounderstandthefourthcolumn,headed‘Mean
squares’.Thiscolumncontainsthevariances,onwhichanalysisofvarianceisbased.The
key point to recall is that
sum of squares
variance
degrees of freedom
Thisis very easy to calculate in thecontext of the ANOVA table, because the relevant
sums of squares and degrees of freedom are in adjacent columns. Thus the regression
variance isjust SSR/1=SSR, and the error variance iss2=SSE/(n 2). Traditionally, one
doesnotfillinthebottombox(itwouldbetheoverallvarianceiny,SSY/(n 1)).Finally,
REGRESSION 129
the ANOVA table is completed by working out the F ratio, which is a ratio between two
variances. In most simple ANOVA tables, you divide the treatment variance in the
numerator(theregressionvarianceinthiscase)bytheerrorvariances2inthedenominator.
Thenullhypothesisundertestinalinearregressionisthattheslopeoftheregressionlineis
zero(i.e.nodependenceofyonx).Thetwo-tailedalternativehypothesisisthattheslopeis
significantlydifferentfromzero(eitherpositiveornegative).Inmanyapplicationsitisnot
particularlyinterestingtorejectthenullhypothesis,becauseweareinterestedintheeffect
sizes(estimatesoftheslopeandintercept)andtheirstandarderrors.Weoftenknowfromthe
outsetthatthenullhypothesisisfalse.Nevertheless,totestwhethertheFratioissufficiently
largetorejectthenullhypothesis,wecompareourteststatistic(thecalculatedvalueofFin
the final column of the ANOVA table) with the critical value of F. Recall that the test
statisticisthevalueofFthatisexpectedbychancealonewhenthenullhypothesisistrue.
We find the critical value of F from quantiles of the F distribution qf, with 1 d.f. in the
numerator and n 2 d.f. in the denominator (as described below). Here is the completed
ANOVA table:
Source Sumofsquares Degreesoffreedom Meansquares Fratio
Regression 88.817 1 88.817 30.974
Error 20.072 7 s2=2.86746
Total 108.889 8
Noticethatthecomponentdegreesoffreedomadduptothetotaldegreesoffreedom(this
is always true, in any ANOVA table, and is a good check on your understanding of the
designoftheexperiment).ThelastquestionconcernsthemagnitudeoftheFratio=30.974:
isitbigenoughtojustifyrejectionofthenullhypothesis?ThecriticalvalueoftheFratiois
thevalueofFthatwouldariseduetochancealonewhenthenullhypothesiswastrue,given
thatwehave1d.f.inthenumeratorand7d.f.inthedenominator.Wehavetodecideonthe
levelofuncertaintythatwearewillingtoputupwith;thetraditionalvalueforworklikethis
is5%,soourcertaintyis0.95.NowwecanusequantilesoftheFdistribution,qf,tofindthe
critical value:
qf(0.95,1,7)
[1]5.591448
Because our calculated value of F (the test statistic=30.974) is much larger than the
criticalvalue(5.591),wecanbeconfidentinrejectingthenullhypothesis.Perhapsabetter
thing to do, rather than working rigidly at the 5% uncertainty level, is to ask what is the
probabilityofgettingavalueforFasbigas30.974orlargerifthenullhypothesisistrue.For
this we use 1-pf rather than qf:
1-pf(30.974,1,7)
[1]0.0008460725
It is very unlikely indeed (p<0.001).
130 STATISTICS:ANINTRODUCTIONUSINGR
Box7.6. Thestandarderroroftheregressionslope
This quantity is given by
rffiffiffiffiffiffiffiffi
s2
SE 
b SSX
The error variance s2 comes from the ANOVA table and is the quantity used in
calculatingstandarderrorsandconfidenceintervalsfortheparameters,andincarrying
outhypothesistesting.SSXmeasuresthespreadofthexvaluesalongthexaxis.Recall
that standard errors are unreliability estimates. Unreliability increases with the error
variance,soitmakessensetohaves2inthenumerator(ontopofthedivision).Itisless
obviouswhyunreliabilityshoulddependontherangeofxvalues.Lookatthesetwo
graphs that have exactly the same slopes (b=2) and intercepts (a=3).
The difference is that the left-hand graph has all of its x values close to the mean
valueofx,whilethegraphontherighthasabroadspanofxvalues.Whichofthese
doyouthinkwouldgivethemostreliableestimateoftheslope?Itisprettyclearthat
it is the graph on the right, with the wider range of x values. Increasing the spread
of the x values reduces unreliability of the estimated slope and hence appears in
the denominator (on the bottom of the equation). What is the purpose of the big
square root term? This is there to make sure that the units of the unreliability
estimate are the same as the units of the parameter whose unreliability is being
assessed.Theerrorvarianceisinunitsofysquared,buttheslopeisinunitsofyper
unit change in x.
Next, we can use the calculated error variance s2=2.867 to work out the standard
errors of the slope (Box 7.6) and the intercept (Box 7.7). First the standard error of
the slope:
rffiffiffiffiffiffiffiffi rffiffiffiffiffiffiffiffiffiffiffi
s2 2:867
SE   0:2186
b SSX 60
REGRESSION 131
Box7.7. Thestandarderroroftheintercept
This quantity is given by
rffiffiffiffiffiffiPffiffiffiffiffiffiffiffiffiffi
s2 x2
SE 
a nSSX
whichisliketheformulaforthestandarderroroftheslope,butwithtwoadditional
terms. Uncertainty declines withPincreasing sample size n. It is less clear why
uncertainty should increase with x2. The reason for this is that uncertainty in
the estimate of the intercept increases, the further away from the intercept that the
meanvalueofxlies.Youcanseethisfromthefollowinggraphsofy=3+2x(solid
black lines). On the left is a graph with a low value of x2 and on the right an
identicalgraph(sameslopeandintercept)butestimatedfromadatasetwithahigher
value of x9. In both cases there is a variation inthe slope from b=1.5 to b=2.5
(dotted blue lines). Compare the difference in the prediction of the intercept in the
two cases.
Confidence in predictions made with linear regression declines with the squareof
thedistancebetweenthemeanvalueofxandthevalueofxatwhichtheprediction
istobemade(i.e.with x x2).Thus,whentheoriginofthegraphisalongway
from the mean value of x, the standard error of the intercept will be large, and
vice versa.
In general, the standard error for a predicted value^y is given by:
sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
(cid:2) (cid:3)
1 x x2
SE^y  s2
n

SSX
Notethattheformulaforthestandarderroroftheinterceptisjustthespecialcaseof
this for x=0.
132 STATISTICS:ANINTRODUCTIONUSINGR
The formula for the standard error of the intercept is a little more involved (Box 7.7):
rffiffiffiffiffiffiPffiffiffiffiffiffiffiffiffiffi rffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
s2 x2 2:867204
SE   1:0408
a nSSX 960
Nowthatweknowwhereallthenumberscomefrom,wecanrepeattheanalysisinRand
see just how straightforward it is. It is good practice to give the statistical model a name:
model is as good as any.
model<-lm(growth∼tannin)
Then,youcandoavarietyofthingswiththemodel.Themostimportant,perhaps,isto
see the details of the estimated effects, which you get from the summary function:
summary(model)
Coefficients:
Estimate Std.Error tvalue Pr(>|t|)
(Intercept) 11.7556 1.0408 11.295 9.54e-06***
tannin -1.2167 0.2186 -5.565 0.000846***
Residualstandarderror:1.693on7degreesoffreedom
MultipleR-squared: 0.8157, AdjustedR-squared:0.7893
F-statistic:30.97on1and7DF, p-value:0.0008461
This shows everything you need to know about the parameters and their standard
errors(comparethevaluesforSE andSE withthoseyoucalculatedlong-hand,above).
a b
We shall meet the other terms shortly (residual standard error, multiple R-squared
and adjusted R-squared). The p value and the F statistic are familiar from the
ANOVA table.
If you want to see the ANOVA table rather than the parameter estimates, then the
appropriate function is summary.aov:
summary.aov(model)
Df SumSq MeanSq Fvalue Pr(>F)
tannin 1 88.82 88.82 30.97 0.000846***
Residuals 7 20.07 2.87
Thisshowstheerrorvariance(s2=2.87)alongwithSSR(88.82)andSSE(20.07),and
thepvaluewejustcomputedusing1-pf.Ofthetwosortsofsummarytable,summary.
lmisvastlythemoreinformative,becauseitshowstheeffectsizes(inthiscasetheslope
ofthegraphandtheintercept)andtheirunreliabilityestimates(thestandarderrorsofthe
slopeandintercept).Generally,youshouldresistthetemptationtoputANOVAtablesin
yourwrittenwork.Theimportantinformationlikethepvalueandtheerrorvariancecan
beputinthetext,orinfigurelegends,muchmoreefficiently.ANOVAtablesputfartoo
much emphasis on hypothesis testing, and show nothing directly about effect sizes or
their unreliabilities.
REGRESSION 133
MeasuringtheDegreeofFit,r 2
Thereisaveryimportantissuethatremainstobeconsidered.Tworegressionlinescanhave
exactly the same slopes and intercepts, and yet be derived from completely different
relationships:
Weneedtobeabletoquantifythedegreeoffit,whichislowinthegraphontheleft
and high in the one on the right. In the limit, all the data points might fall exactly on
the line. The degree of scatter in that case would be zero and the fit would be perfect
(we might define a perfect fit as 1). At the other extreme, x might explain none of
the variation in y at all; in this case, fit would be 0 and the degree of scatter would
be 100%.
CanwecombinewhatwehavelearnedaboutSSY,SSRandSSEintoameasureoffitthat
hastheseproperties?Ourproposedmetricisthefractionofthetotalvariationinythatis
explainedbytheregression.ThetotalvariationisSSYandtheexplainedvariationisSSR,
so our measure – let us call it r2 – is given by
SSR
r2 
SSY
Thisvariesfrom1,whentheregressionexplainsallofthevariationiny(SSR=SSY),to0
when the regression explains none of the variation in y (SSE=SSY).
134 STATISTICS:ANINTRODUCTIONUSINGR
Theformalnameofthisquantityisthecoefficientofdetermination,butthesedaysmost
peoplejustrefertoitas‘r-squared’.Wehavealreadymetthesquarerootofthisquantity
(r or ρ), as the correlation coefficient (p. 108).
ModelChecking
The final thing you will want to do is to expose the model to critical appraisal.
The assumptions we really want to be sure about are constancy of variance and
normality of errors. The simplest way to do this is with four built-in model-checking
plots:
par(mfrow=c(2,2))
plot(model)
The first graph (top left) shows residuals on the y axis against fitted values on the x
axis.Ittakesexperiencetointerprettheseplots,butwhatyoudonotwanttoseeislotsof
structure or pattern in the plot. Ideally, as here, the points should look like the sky at
night. It is a major problem if the scatter increases as the fitted values get bigger;
REGRESSION 135
thiswouldshowuplikeawedgeofcheeseonitsside(likethis◀orlesscommonlylike
this▶;seep.65).Butinourpresentcase,everythingisOKontheconstancyofvariance
front.
The next plot (top right) is the normal quantile–quantile plot (qqnorm, p. 79) which
shouldbeastraightlineiftheerrorsarenormallydistributed.Again,thepresentexample
looksfine.IfthepatternwereS-shapedorbanana-shaped,wewouldneedtofitadifferent
model to the data.
Thethirdplot(bottomleft)islikethefirst,butonadifferentscale;itshowsthesquareroot
ofthestandardizedresiduals(whereallthevaluesarepositive)againstthefittedvalues;if
there was a problem, the points would be distributed inside a triangular shape, with the
scatteroftheresidualsincreasingasthefittedvaluesincrease.Butthereisnosuchpattern
here, which is good.
The fourth and final plot (lower right) is all about highlighting influential points
(p.148);thesearethepointsonthegraphthathavethebiggesteffectsontheparameter
estimates. It shows Cook’s distance as red contours on a plane defined by leverage and
standardized residuals, with each point on the graph represented by an open circle, and
selectedpointsnumbered(basedontheorderinwhichtheyappearinthedataframe).You
can see that point number 9 has the highest leverage, point number 7 (all but hidden
behind the label) has the highest Cook’s distance, and point number 4 has the largest
residual. In my opinion, this information is more clearly displayed in tabular form; try
influence.measures(model).
It is the top two graphs of plot(model) that are most important, and you should
concentrate on these. The important point is that we always do model-checking; the
summary(model) table is not the end of the process of regression analysis.
Transformation
Youmustnotfallintothetrapofthinkingthaty=a+bxistheonlytwo-parametermodel
for describing the relationship between the response variable and a single continuous
explanatoryvariable.Modelchoiceisavitallyimportantpartofstatisticalanalysis.Hereare
some other useful two-parameter models:
logX y abln x 1.
logY yexp abx 2.
ax
asymptotic y 3.
1bx
reciprocal b 4.
ya
x
powerlaw yaxb 5.
exponential yaebx 6.
136 STATISTICS:ANINTRODUCTIONUSINGR
It is straightforward to estimate the parameters of such models if the equations can be
transformedsothatthey becomelinearintheirparameters.Anexample shouldmake this
clear. The following data show the relationship between radioactive emissions and time:
par(mfrow=c(1,1))
data<-read.csv("c:\\temp\\decay.csv")
attach(data)
names(data)
[1]"time" "amount"
plot(time,amount,pch=21,col="blue",bg="green")
Westartbyfittingastraightlinethroughthescatterplot,usingablinewithalinearmodel:
abline(lm(amount∼time),col="red")
REGRESSION 137
Thisdrawsattentiontothepronouncedcurvatureinthedata.Mostoftheresidualsatlow
values of time are positive, most of the residuals for intermediate values of time are
negative,andmostoftheresidualsathighvaluesoftimearepositive.Thisisclearlynota
good model for these data.
Thereisaveryimportantpointhere.If,insteadoflookingatthefitofthemodeltothe
datausingplot,wehadsimplydonethestatistics,thenwemighteasilyhavecometothe
opposite conclusion. Here is a summary of the linear model applied to these data:
summary(lm(amount∼time))
Coefficients:
Estimate Std.Error tvalue Pr(>|t|)
(Intercept) 84.5534 5.0277 16.82 <2e-16***
time -2.8272 0.2879 -9.82 9.94e-11***
Residualstandarderror:14.34on29degreesoffreedom
MultipleR-squared:0.7688, AdjustedR-squared:0.7608
F-statistic:96.44on1and29DF,p-value:9.939e-11
Themodelexplainsmorethan76%ofthevariationintheresponse(averyhighvalueof
r-squared)andthepvalueisvanishinglysmall.Themoralisthatpvaluesandr-squaredare
not good measures of model adequacy.
Becausethedatarelatetoadecayprocess,itmightbethatanexponentialfunctiony
ae bx describes thedata better. Ifwe canlinearizethisequation,then wecanestimatethe
parameter values using a linear model. Let us try taking logs of both sides
y ae  bx
log ylog a bx
If we replace log(y) by Y and log(a) by A, you can see that we have a linear model:
Y A bx
TheinterceptofthislinearmodelisAandtheslopeis b.Tofitthemodelwehavethe
untransformed values of time on the x axis and the log of amount on the y axis:
plot(time,log(amount),pch=21,col="blue",bg="red")
abline(lm(log(amount)∼time),col="blue")
138 STATISTICS:ANINTRODUCTIONUSINGR
The fit to the model is greatly improved. There is a new issue, however, in that the
variance appears to increase with time and, as you will recall, non-constant variance is a
potentiallyseriousproblem.Letusestimatetheparametervaluesofthisexponentialmodel
and then check its assumptions using plot(model).
model<-lm(log(amount)∼time)
summary(model)
Coefficients:
Estimate Std.Error tvalue Pr(>|t|)
(Intercept) 4.547386 0.100295 45.34 <2e-16***
time -0.068528 0.005743 -11.93 1.04e-12***
Residualstandarderror:0.286on29degreesoffreedom
MultipleR-squared:0.8308, AdjustedR-squared:0.825
F-statistic:142.4on1and29DF, p-value:1.038e-12
Theslopeofthestraightlineis 0.068528anditsstandarderroris0.005743.Thevalueof
r2 is even higher following transformation (83%) and the p value is even lower. The
interceptof4.547386withitsstandarderrorof0.100295isforA,notthevalueweneedfor
ourexponentialequation,butaistheantilogofA.Whenweback-transform,thestandard
errorsbecomeasymmetricupanddown.Itmaytakeamomentforyoutoseewhythisisthe
case.Letusaddonestandarderrortotheinterceptandsubtractonestandarderrorfromitto
get upper and lower intervals.
upper<-4.547386+ 0.100295
lower<-4.547386- 0.100295
Now we return to the original scale of measurement by taking antilogs using exp:
exp(upper)
[1]104.3427
exp(lower)
[1]85.37822
sotheinterceptontheoriginalaxisisbetween85.38and104.34,butthebestestimateforthe
intercept is
exp(4.547386)
[1]94.38536
which means that the interval above the intercept is 9.957 but the interval below it is
9.007. Beginners often find it disconcerting that the two unreliability measures are
different sizes.
Now we check the assumptions of the model using plot(model):
REGRESSION 139
Thegoodnewsisthatthenormalityoferrorsassumptionlooksgood(thetoprightplotis
reasonably straight). As we guessed by looking at the transformed data, however, the
variancedoesshowstrongsignsofnon-constancy(thetopleftandbottomleftplots).The
bottomrightplotshowsthatdatapoints30and31havehighleverageandpointnumber28
hasalargeresidual.Weshallseehowdealwiththeseissueslater,butforthemoment,we
want to plot the curved line through the scatterplot on the original scale of measurement.
par(mfrow=c(1,1))
plot(time,amount,pch=21,col="blue",bg="green")
ThekeythingtounderstandaboutdrawingcurvedlinesinRisthatcurvesaremadeupof
lotsofsmallstraight-linesections.Onceyougetmorethanabout100sectionsinthewidth
ofagraph,thecurvetypicallylooksquitesmooth.Lookingatthescatterplot,youcansee
thatwewantthevaluesoftimetogofrom0upto30.Togetmorethan100segmentstothe
curve we therefore wantmore than three steps per unit time; let ustake four steps, which
makes the sequence interval 0.25. We call the variable xv, to stand for ‘x values’:
xv<-seq(0,30,0.25)
Thisgivesus121values(length(xv)).Weknowtheequationfortheexponentialcurve
is94.38536exp( 0.068528x),sonowwecancalculatetheyvalues(amounts)associated
with each x value:
yv<-94.38536*exp(-0.068528*xv)
140 STATISTICS:ANINTRODUCTIONUSINGR
Now we use the lines function to add the curve to the scatterplot:
lines(xv,yv,col="red")
As you can see, our model is a good description of the data for intermediate values of
time, but the model is poor at predicting amount for time=0 and for time > 28. Clearly,
moreworkisrequiredtounderstandwhatisgoingonattheextremes,butexponentialdecay
describes the central part of the data reasonably well.
PolynomialRegression
Therelationshipbetweenyandxoftenturnsoutnottobeastraightline.ButOccam’srazor
requiresthatwefitalinearmodelunlessanon-linearrelationshipissignificantlybetterat
describing the data. So this begs the question: how do we assess the significance of
departures from linearity? One of the simplest ways is to use polynomial regression
yabxcx2dx3...
The idea of polynomial regression is straightforward. As before, we have just one
continuousexplanatoryvariable,x,butwecanfithigherpowersofx,suchasx2andx3,to
the model in addition to x to describe curvature in the relationship between y and x. It is
usefultoexperimentwiththekindsofgraphsthatcanbegeneratedwithverysimplemodels.
Evenifwerestrictourselvestotheinclusionofaquadraticterm,x2,therearemanycurves
we can describe, depending upon the signs of the linear and quadratic terms:
par(mfrow=c(2,2))
curve(4+2*x-0.1*x^2,0,10,col="red",ylab="y")
curve(4+2*x-0.2*x^2,0,10,col="red",ylab="y")
curve(12-4*x+0.3*x^2,0,10,col="red",ylab="y")
curve(4+0.5*x+0.1*x^2,0,10,col="red",ylab="y")
REGRESSION 141
In the top left panel, there is a curve with positive but declining slope, with no hint
of a hump (y42x 0:1x2). At top right we have a curve with a clear maximum
(y42x 0:2x2), and at bottom left a curve with a clear minimum (y12 4x
0:35x2). The bottom right curve shows a positive association between y and x with
the slope increasing as x increases (y40:5x0:1x2). So you can see that a simple
quadraticmodelwithjustthreeparameters(anintercept,aslopeforx,andaslopeforx2)is
capable of describing a wide range of functional relationships between y and x. It is very
importanttounderstandthatthequadraticmodeldescribestherelationshipbetweenyandx;
it does not pretend to explain the mechanistic (or causal) relationship between y and x.
Wecanusethedecaydataasanexampleofmodelcomparison.Howmuchbetterthana
linear model with two parameters (call it model2) is a quadratic with three parameters
(model3)?ThefunctionIstandsfor‘asis’andallowsyoutousearithmeticoperatorslike
caret(^forcalculatingpowers)inamodelformulawherethesamesymbolwouldotherwise
meansomethingdifferent(inamodelformula,caretmeanstheorderofinteractiontermsto
be fitted).
model2<-lm(amount∼time)
model3<-lm(amount∼time+I(time^2))
summary(model3)
Coefficients:
Estimate Std.Error tvalue Pr(>|t|)
(Intercept) 106.38880 4.65627 22.849 <2e-16***
time -7.34485 0.71844 -10.223 5.90e-11***
I(time^2) 0.15059 0.02314 6.507 4.73e-07***
Residualstandarderror:9.205on28degreesoffreedom
MultipleR-squared:0.908, AdjustedR-squared:0.9014
F-statistic:138.1on2and28DF,p-value:3.122e-15
142 STATISTICS:ANINTRODUCTIONUSINGR
Youcanseethattheslopeforthequadraticterm(0.15059)ishighlysignificant,which
indicates important curvature in the data. To see how much better the quadratic model is
when compared to the simpler linear model we can use AIC (see p. 232) or anova (see
p. 172):
AIC(model2,model3)
df AIC
model2 3 257.0016
model3 4 230.4445
The much lower AIC of the quadratic model3 means that it is preferred (see p. 232 for
details).Alternatively,ifyoulikepvalues,thencomparisonofthetwomodelsbyanova
shows that the curvature is highly significant (p<0.000001):
anova(model2,model3)
AnalysisofVarianceTable
Model1: amount∼time
Model2: amount∼time+I(time^2)
Res.Df RSSDfSumofSq F Pr(>F)
1 295960.6
2 282372.6 1 3588.1 42.344 4.727e-07***
Non-LinearRegression
Sometimeswehaveamechanisticmodelfortherelationshipbetweenyandx,andwewantto
estimate the parameters and standard errors of the parameters of a specific non-linear
equationfromdata.Thereareanumberoffrequently-usednon-linearmodelstochoosefrom.
Whatwemeaninthiscasebynon-linearisnotthattherelationshipiscurved(itwascurvedin
the caseofpolynomialregressions,butthesewerelinearmodels),butthattherelationship
cannotbelinearizedbytransformationoftheresponsevariableortheexplanatoryvariable
(orboth).Hereisanexample:itshowsjawbonelengthasafunctionofageindeer.Theory
indicates that the relationship is an ‘asymptotic exponential’ with three parameters:
ya be  cx
InR,themaindifferencebetweenlinearmodelsandnon-linearmodelsisthatwehaveto
tellRtheexactnatureoftheequationaspartofthemodelformulawhenweusenon-linear
modelling.Inplaceoflmwewritenls(thisstandsfor‘nonlinearleastsquares’).Thenwe
writey∼a-b*exp(-c*x)tospellouttheprecise nonlinear modelwewantRtofittothe
data.TheslightlytediousthingisthatRrequiresustospecifyinitialguessesatthevaluesof
theparametersa,bandc(note,however,thatsomecommonnon-linearmodelshave‘self-
starting’versionsinRwhichbypassthisstep).Letusplotthedataandworkoutsensible
startingvalues.Italwayshelpsincaseslikethistoevaluatetheequation’s‘behaviouratthe
limits’.Thatistosay,thevaluesofywhenx=0andwhenx=infinity.Forx=0,wehave
exp(-0)whichis1,and1×b=bsoy=a b.Forx=infinity,wehaveexp(-infinity)
which is 0, and 0 × b=0 so y=a. That is to say, the asymptotic value of y is a,
REGRESSION 143
and the intercept is a b. If you need to check your maths, you can do calculations with
infinity and zero in R like this:
exp(-Inf)
[1]0
exp(-0)
[1]1
Here are the data on bone length as a function of age:
deer<-read.csv("c:\\temp\\jaws.csv")
attach(deer)
names(deer)
[1]"age" "bone"
par(mfrow=c(1,1))
plot(age,bone,pch=21,bg="lightgrey")
Inspectionsuggeststhatareasonableestimateoftheasymptoteisa≈120andintercept
≈10,sob=120 10=110.Ourguessofthevalueofcisslightlyharder.Wherethecurve
isrisingmoststeeply,jawlengthisabout40whereageis5;rearrangingtheequationgives
log a y=b log 120 40=110
c    0:06369075
x 5
Nowthatwehavethethreeparameterestimates,wecanprovidethemtoRasthestarting
conditions as part of the nls call like this: list(a = 120,b= 110, c= 0.064)
144 STATISTICS:ANINTRODUCTIONUSINGR
model<-nls(bone∼a-b*exp(-c*age),start=list(a=120,b=110,c=0.064))
summary(model)
Formula:bone∼a-b*exp(-c*age)
Parameters:
Estimate Std.Error tvalue Pr(>|t|)
a 115.2528 2.9139 39.55 <2e-16***
b 118.6875 7.8925 15.04 <2e-16***
c 0.1235 0.0171 7.22 2.44e-09***
Residualstandarderror:13.21on51degreesoffreedom
Numberofiterationstoconvergence:5
Achievedconvergencetolerance:2.381e-06
Alltheparametersappeartobesignificantatp<0.001.Beware,however.Thisdoesnot
necessarily mean that all the parameters need to be retained in the model. In this case,
a=115.2528withSE=2.9139isclearlynotsignificantlydifferentfromb=118.6875with
SE=7.8925(theywouldneedtodifferbymorethan2standarderrorstobesignificant).So
we should try fitting the simpler two-parameter model
ya 1 e  cx
model2<-nls(bone∼a*(1-exp(-c*age)),start=list(a=120,c=0.064))
anova(model,model2)
AnalysisofVarianceTable
Model1:bone∼a-b*exp(-c*age)
Model2:bone∼a*(1-exp(-c*age))
Res.DfRes.SumSqDf SumSqFvaluePr(>F)
1 51 8897.3
2 52 8929.1-1-31.843 0.1825 0.671
Model simplification was clearly justified (p=0.671), so we accept the two-parameter
version,model2,asourminimaladequatemodel.Wefinishbyplottingthecurvethrough
the scatterplot. The age variable needs to go from 0 to 50:
av<-seq(0,50,0.1)
and we use predict with model2 to generate the predicted bone lengths:
bv<-predict(model2,list(age=av))
Notetheuseoflisttoassignourstepsalongthexaxis(calledav)tothevariableused
for the x axis in model2 (called age).
lines(av,bv,col="blue")
REGRESSION 145
The parameters of this curve are obtained from model2:
summary(model2)
Formula:bone∼a*(1-exp(-c*age))
Parameters:
Estimate Std.Error tvalue Pr(>|t|)
a 115.58056 2.84365 40.645 <2e-16***
c 0.11882 0.01233 9.635 3.69e-13***
Residualstandarderror:13.1on52degreesoffreedom
Numberofiterationstoconvergence:5
Achievedconvergencetolerance:1.356e-06
which we could write like this y115:58 1 e 0:1188x or like this y=115.58(1 
exp( 0.1188 x)) according to taste or journal style. If you want to present the standard
errors as well as the parameter estimates, you could write:
Themodely=a(1 exp( bx))hada=115.58±2.84(1s.e.,n=54)andb=0.1188
±0.0123 (1 s.e.) and explained 84.9% of the total variation in bone length.
Note that because there are only two parameters in the minimal adequate model, we
have called them a and b (rather than a and c as in the original formulation).
You may be puzzled as to how we know that model2 explained 84.9% of the
totalvariationinbonelength,becausethesummary doesnotgiveanr-squaredfigure.
We need to do a little more work to find SSY and SSR(see p. 123). The easiestway to
find SSY is to fit a null model, estimating only the intercept. In R, the intercept
isparameter1andisfittedlikethis:y∼1.Thesumofsquaresassociatedwiththismodel
is SSY:
146 STATISTICS:ANINTRODUCTIONUSINGR
null.model<-lm(bone∼1)
summary.aov(null.model)
DfSumSqMeanSqFvaluePr(>F)
Residuals 53 59008 1113
The key figure to extract from this is the total sum of squares SSY=59008. The non-
linear output (above) did not give us either SSE or SSR but it did print:
Residualstandarderror:13.1on52degreesoffreedom
Thisisusefulbecausewecangettheresidualvariancebysquaringtheresidualstandard
error(13.12=171.61)andconvertthisintotheresidualsumofsquaresSSEbymultiplying
it by the degrees of freedom (52×13.12=8923.72). Recall that r-squared is SSR/SST
expressedasapercentage,andthatSSR=SSY SSE.Thus,thefractionofthevariancein
bone length explained by our model is
100*(59008-8923.72)/59008
[1]84.8771
GeneralizedAdditiveModels
Sometimes we can see that the relationship between y and x is non-linear but we do not
have any theory or any mechanistic model to suggest a particular functional form
(mathematical equation) to describe the relationship. In such circumstances, generalized
additive models are particularly useful, because they fit non-parametric smoothers to the
datawithoutrequiringustospecifyanyparticularmathematicalmodeltodescribethenon-
linearity. This will become clear with an example.
library(mgcv)
hump<-read.csv("c:\\temp\\hump.csv")
attach(hump)
names(hump)
[1]"y""x"
We start by fitting the generalized additive model as a smoothed function of x, s(x):
model<-gam(y∼s(x))
Then we plot the model, and overlay the scatterplot of data points:
plot(model,col="blue")
points(x,y-mean(y),pch=21,bg="red")
REGRESSION 147
The y axis is labelled s(x,7.45) which is interpreted as saying that the smoothed
function of x shown by the solid blue curve (‘the non-parametric smoother’) involves the
equivalentof7.45degreesoffreedom(rememberthatastraightlinewoulduse2d.f.:the
intercept and the slope). The dotted lines show the confidence interval for the location of
thesmoothedfunctionofx.Onthexaxisyouseearugplot,showingwherethexpointson
the graph are located.
The model summary is obtained in the usual way:
summary(model)
Family:gaussian
Linkfunction:identity
Formula:
y∼s(x)
Parametriccoefficients:
Estimate Std.Error tvalue Pr(>|t|)
(Intercept) 1.95737 0.03446 56.8 <2e-16***
Approximatesignificanceofsmoothterms:
edf Ref.df F p-value
s(x) 7.452 8.403 116.9 <2e-16***
R-sq.(adj)=0.919 Devianceexplained=92.6%
GCVscore=0.1156 Scaleest.=0.1045 n=88
Thisshowsthatthehumpedrelationshipbetweenyandxishighlysignificant(thepvalue
ofthesmoothterms(x)islessthan0.0000001).Thefittedfunctionexplains91.9%ofthe
variance in y (r2=0.919). The intercept (1.95737) is just the mean value of y.
Note that because of the strong hump in the relationship, a linear model lm(y∼x)
indicatesnosignificantrelationshipbetweenthetwovariables(p=0.346).Thisisanobject
lesson in always plotting the data before you come to conclusions from the statistical
analysis;inthiscase,ifyouhadstartedwithalinearmodelyouwouldhavethrownoutthe
148 STATISTICS:ANINTRODUCTIONUSINGR
babywiththebathwaterbyconcludingthatnothingwashappening.Infact,somethingvery
significant is happening but it is producing a humped, rather than a trended relationship.
Influence
Oneofthecommonestreasonsforalackoffitisthroughtheexistenceofoutliersinthedata.
Itisimportanttounderstand,however,thatapointmayappeartobeanoutlierbecauseof
misspecification of the model, and not because there is anything wrong with the data.
Take this circle of data that shows absolutely no relationship between y and x:
x<-c(2,3,3,3,4)
y<-c(2,3,2,1,2)
Wewanttodrawtwographssidebyside,andwewantthemtohavethesameaxisscales:
windows(7,4)
par(mfrow=c(1,2))
plot(x,y,xlim=c(0,8),ylim=c(0,8))
Obviously,thereisnorelationshipbetweenyandxintheoriginaldata.Butlet’saddan
outlier at the point (7,6) using concatenation c and see what happens.
x1<-c(x,7)
y1<-c(y,6)
plot(x1,y1,xlim=c(0,8),ylim=c(0,8))
abline(lm(y1∼x1),col="blue")
Now,thereisasignificantregressionofyonx.Theoutlierissaidtobehighlyinfluential.
Testingforthepresenceofinfluentialpointsisanimportantpartofstatisticalmodelling.
Youcannotrelyonanalysisoftheresiduals,becausebytheirveryinfluence,thesepoints
force the regression line close to them.
Measuresofleverageforagivendatapointyareproportionalto x x2.Thecommonest
measure of leverage is
1 x  x2
h  Pi
i n x  x2
i
REGRESSION 149
wherethedenominatorisSSX.Agoodruleofthumbisthatapointishighlyinfluentialifits
2p
h > ;
i n
where p is the number of parameters in the model. There is a useful function called
influence.measures which highlights influential points in a given model
reg<-lm(y1∼x1)
influence.measures(reg)
Influencemeasuresof
lm(formula=y1∼x1):
dfb.1_ dfb.x1 dffit cov.r cook.d hat inf
1 0.687 -0.5287 0.7326 1.529 0.26791 0.348
2 0.382 -0.2036 0.5290 1.155 0.13485 0.196
3 -0.031 0.0165-0.0429 2.199 0.00122 0.196
4 -0.496 0.2645-0.6871 0.815 0.19111 0.196
5 -0.105 -0.1052-0.5156 1.066 0.12472 0.174
6 -3.023 4.1703 4.6251 4.679 7.62791 0.891 *
Youcanseepoint#6ishighlightedbyanasterisk,drawingattentiontoitshighinfluence.
FurtherReading
Cook, R.D. and Weisberg, S. (1982) Residuals and Influence in Regression, Chapman & Hall,
NewYork.
Hastie, T. andTibshirani,R. (1990) Generalized Additive Models,Chapman &Hall, London.
Wetherill, G.B., Duncombe, P., Kenward, M. et al. (1986) Regression Analysis with Applications,
Chapman &Hall, London.
8
Analysis of Variance
AnalysisofvarianceorANOVAisthetechniqueweusewhenalltheexplanatoryvariables
arecategorical.Theexplanatoryvariablesarecalledfactors,andeachfactorhastwoormore
levels.Whenthereisasinglefactorwiththreeormorelevelsweuseone-wayANOVA.If
wehadasinglefactorwithjusttwolevels,wewoulduseStudent’sttest(seep.97),andthis
wouldgiveusexactlythesameanswerthatwewouldhaveobtainedbyANOVA(remember
therulethatwithjusttwolevelsthenF=t2).Wheretherearetwoormorefactors,thenwe
use two-way or three-way ANOVA, depending on the number of explanatory variables.
Whenthereisreplicationateachlevelinamulti-wayANOVA,theexperimentiscalleda
factorialdesign,andthisallowsustostudyinteractionsbetweenvariables,inwhichwetest
whether the response to one factor depends on the level of another factor.
One-WayANOVA
Thereisarealparadoxaboutanalysisofvariance,whichoftenstandsinthewayofaclear
understandingofexactlywhatisgoingon.TheideaofANOVAistocomparetwoormore
means. But it does this by comparing variances. How can that work?
Thebestwaytoseewhatishappeningistoworkthroughagraphicalexample.Tokeep
thingsassimpleaspossible,weshalluseafactorwithjusttwolevelsatthisstage,butthe
argument extends to any number of levels. Suppose that we have atmospheric ozone
concentrations measured in parts per hundred million (pphm) in two commercial lettuce-
growing gardens (we shall call the gardens A and B for simplicity).
oneway<-read.csv("c:\\temp\\oneway.csv")
attach(oneway)
names(oneway)
[1]"ozone" "garden"
As usual, we begin by plotting the data, but here we plot the y values (ozone
concentrations) against the order in which they were measured:
plot(1:20,ozone,ylim=c(0,8),ylab="y",xlab="order",pch=21,bg="red")
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
ANALYSIS OFVARIANCE 151
Thereislotsofscatter,indicatingthatthevarianceinyislarge.Togetafeelfortheoverall
variance,wecanplotthemeanvalueofy,andindicateeachoftheresidualsbyaverticalline
from mean(y) to the value of y, like this:
abline(h=mean(ozone),col="blue")
for(iin1:20)lines(c(i,i),c(mean(ozone),ozone[i]),col="green")
152 STATISTICS:ANINTRODUCTIONUSINGR
Werefertothisoverallvariationasthetotalsumofsquares,SSY,whichisthesumofthe
squares of the lengths of the green lines. More formally, this is given by:
X
SSY  y y2
which should look familiar, because it is the formula used for defining the variance of
y (s2=sum of squares/degrees of freedom; see p. 51).
This next step is the key to understanding how ANOVA works. Instead of fitting the
overallmeanvalueofythoroughthedata,andlookingatthedeparturesofallthedatapoints
fromtheoverallmean,letusfittheindividualtreatmentmeans(themeanforgardenAand
the mean for garden B in this case), and look at the departures of data points from the
appropriatetreatment mean. Itwill be usefulifwe havedifferent colours forthedifferent
gardens;say,blackforgardenA(col=1)andredforgardenB(col=2).Notetheuseof
as.numerictoturnthefactorlevelsAandBintothenumbers1and2forthepurposesof
selecting the colours (bg) for the plotting symbols.
plot(1:20,ozone,ylim=c(0,8),ylab="y",xlab="order",
pch=21,bg=as.numeric(garden))
abline(h=mean(ozone[garden=="A"]))
abline(h=mean(ozone[garden=="B"]),col="red")
NowitisclearthatthemeanozoneconcentrationingardenB(red)issubstantiallyhigher
thaningardenA.TheaimofANOVAistodeterminewhetheritissignificantlyhigher,or
whether thiskindofdifference couldcomeabout bychancealone,when themean ozone
concentrations in the two gardens were really the same.
ANALYSIS OFVARIANCE 153
Nowwedrawtheresiduals–thedifferencesbetweenthemeasuredozoneconcentrations,
and the mean for the garden involved:
index<-1:length(ozone)
for(iin1:length(index)){
if(garden[i]=="A")
lines(c(index[i],index[i]),c(mean(ozone[garden=="A"]),ozone
[i]))
else
lines(c(index[i],index[i]),c(mean(ozone[garden=="B"]),ozone
[i]),col="red")
}
Nowhere’sthequiz.Ifthemeansinthetwogardensarenotsignificantlydifferent,what
shouldbethedifferenceinthelengthsoftheredandblackresiduallinesinthisfigureandthe
greenlinesinthefirstfigurewedrew?Afterabitofthought,youshouldseethatifthemeans
werethesame,thentheredandblackhorizontallinesinthisfigurewouldbeinthesameplace,
andhencethelengthsoftheresiduallineswouldbethesameasinthepreviousfigure.
We arehalf way there.Now, suppose that mean ozoneconcentrationis different in the
two gardens. Would the residual lines be bigger or smaller when we compute them from
theindividualtreatmentmeans(theredandblacklines,asabove),orfromtheoverallmean
(the green lines inthe previousfigure)?Theywould besmaller when computedfrom the
individual treatment means if the individual treatment means were different.
So there it is.Thatis how ANOVAworks. When the means are significantly different,
thenthesumofsquarescomputedfromtheindividualtreatmentmeanswillbesmallerthan
the sum of squares computed from the overall mean. We judge the significance of the
difference between the two sums of squares using analysis of variance.
The analysis is formalized by defining this new sum of squares: it is the sum of the
squaresofthedifferencesbetweentheindividualyvaluesandtherelevanttreatmentmean.
154 STATISTICS:ANINTRODUCTIONUSINGR
WeshallcallthisSSE,theerrorsumofsquares(therehasbeennoerrorinthesenseofa
mistake;‘error’isusedhereasasynonymof‘residual’).Itisthesumofthesquaresofthe
lengthsoftheverticalredlinesaddedtothesumofthesquaresofthelengthsofthevertical
black lines in the graphic above:
Xk X
SSE  y y2
j
j1
We compute the mean for the jth level of the factor in advance, and then add up the
squares of the differences. Given that we worked it out this way, can you see how many
degreesoffreedomshouldbeassociatedwithSSE?Supposethattherewerenreplicatesin
each treatment (n=10 in our example). And suppose that there are k levels of the factor
(k=2inourexample).Ifyouestimatekparametersfromthedatabeforeyoucanworkout
SSE,thenyoumusthavelostkdegreesoffreedomintheprocess.Sinceeachoftheklevels
of the factor has n replicates, there must be k×n numbers in the whole experiment
(2×10=20 in our example). So the degrees of freedom associated with SSE is kn k=
k(n 1).Anotherwayofseeingthisistosaythattherearenreplicatesineachtreatment,and
hence n 1 degrees of freedom for error in each treatment (because 1 d.f. is lost in
estimatingeachtreatmentmean).Therearektreatments(i.e.klevelsofthefactor)andhence
there are k(n 1) d.f. for error in the experiment as a whole.
Nowwecometothe‘analysis’partoftheanalysisofvariance.Thetotalsumofsquaresin
y,SSY,is broken up(analysed) into components.The unexplained part ofthe variation is
calledtheerrorsum ofsquares, SSE.The componentofthevariation thatisexplainedby
differences between the treatment means is called the treatment sum of squares, and is
traditionally denoted by SSA. This is because in two-way ANOVA, with two different
categoricalexplanatoryvariables,weshalluseSSBtodenotethesumofsquaresattributable
todifferencesbetweenthemeansofthesecondfactor.AndSSCtodenotethesumofsquares
attributable to differences between the means of the third factor. And so on.
Analysisofvariance,therefore,isbasedonthenotionthatwebreakdownthetotalsumof
squares, SSY, into useful and informative components.
SSA
SSY
SSE
Typically, we compute all but one of the components, then find the value of the last
componentbysubtractionoftheothersfromSSY.WealreadyhaveaformulaforSSE,sowe
couldobtainSSAbydifference:SSASSY  SSE.StartingwithSSY,wecalculatethesum
of the squares of the differences between the y values and the overall mean:
SSY<-sum((ozone-mean(ozone))^2)
SSY
[1]44
The question now is: ‘How much of this 44 is attributable to differences between the
means of gardens A and B (SSA=explained variation) and how much is sampling error
ANALYSIS OFVARIANCE 155
(SSE=unexplained variation)?’ We have a formula defining SSE; it is the sum of the
squaresoftheresidualscalculatedseparatelyforeachgarden, usingtheappropriatemean
value. For garden A, we get
sum((ozone[garden=="A"]-mean(ozone[garden=="A"]))^2)
[1]12
and for garden B
sum((ozone[garden=="B"]-mean(ozone[garden=="B"]))^2)
[1]12
sotheerrorsumofsquaresisthetotalofthesecomponentsSSE=12+12=24.Finally,we
can obtain the treatment sum of squares, SSA, by difference:
SSA44 2420
At this point, we can fill in the ANOVA table (see p. 128):
Source Sumofsquares Degreesoffreedom Meansquare Fratio
Garden 20.0 1 20.0 15.0
Error 24.0 18 s2=1.3333
Total 44.0 19
WeneedtotestwhetheranFratioof15.0islargeorsmall.Todothiswecompareitwith
thecriticalvalueofFfromquantilesoftheFdistribution,qf.Wehave1degreeoffreedom
inthe numerator, and18degrees offreedom inthedenominator,and we want towork at
95% certainty (α0:05):
qf(0.95,1,18)
[1]4.413873
Thecalculatedvalueoftheteststatisticof15.0ismuchgreaterthanthecriticalvalueof
F=4.41,sowecanrejectthenullhypothesis(equalityofthemeans)andacceptthealternative
hypothesis(thetwomeansaresignificantlydifferent).Weusedaone-tailedFtest(0.95rather
than0.975intheqffunction)becauseweareonlyinterestedinthecasewherethetreatment
varianceislargerelativetotheerrorvariance.Butthisapproachisratherold-fashioned;the
modern view is to calculate the effect size (the difference between the means is 2.0 pphm
ozone)andtostatetheprobabilitythatsuchadifferencewouldarisebychancealonewhenthe
nullhypothesiswastrueandthemeanozoneconcentrationwasthesameinthetwogardens.
ForthisweusecumulativeprobabilitiesoftheFdistribution,ratherthanquantiles,likethis:
1-pf(15.0,1,18)
[1]0.001114539
156 STATISTICS:ANINTRODUCTIONUSINGR
Sotheprobabilityofobtainingdataasextremeasours(ormoreextreme)ifthetwomeans
really were the same is roughly 0.1%.
That was quite a lot of work. Here is the whole analysis in R in a single line:
summary(aov(ozone∼garden))
DfSum Sq MeanSq F value Pr(>F)
garden 1 20 20.000 15 0.00111**
Residuals 18 24 1.333
Thefirstcolumnshowsthesourcesofvariation(SSAandSSE,respectively);notethatR
leavesoffthebottomrowthatweincludedfortotalvariation,SSY.Thenextcolumnshows
degrees of freedom: there aretwo levelsofgarden (A and B)so there is2 1=1 d.f.for
garden,andthereare10replicatespergarden,so10 1=9d.f.pergardenandtwogardens,
so error d.f.=2×9=18. The next column shows the sums of squares: SSA=20 and
SSE=24.Thefourthcolumngivesthemeansquares(sumsofsquaresdividedbydegreesof
freedom); the treatment mean square is 20.0 and the error variance, s2 (synonym of the
residualmeansquare)is24/18=1.3333.TheFratiois20/1.333=15,andtheprobability
thatthis(oraresultmoreextremethanthis)wouldarisebychancealoneifthetwomeans
really were the same is 0.001115 (just as we calculated long-hand, above).
We finish by carrying out graphical checks of the assumptions of the model: namely,
constancy of variance and normality of errors.
plot(aov(ozone∼garden))
ANALYSIS OFVARIANCE 157
Thefirstplotshowsthatthevariancesareidenticalinthetwotreatments(thisisexactly
whatwewanttosee).Thesecondplotshowsareasonablystraightlinerelationshiponthe
normal quantile–quantile plot (especially since, in this example, the y values are whole
numbers), so we can be confident that non-normality of errors is not a major problem.
The third plot shows the residuals against the fitted values on a different scale (constant
variance again), and the fourth plot shows Cook’s statistics, drawing attention to the fact
that points 8, 17 and 14 have large residuals.
ShortcutFormulas
Intheunlikelyeventthatyoueverneedtodoanalysisofvarianceusingacalculator,thenit
isusefultoknowtheshort-cutformulaforcalculatingSSA.Wecalculateditbydifference,
above,havingworkedoutSSElong-hand.ToworkoutSSAfromfirstprinciples,thething
youneedtounderstandiswhatwemeanbya‘treatmenttotal’.Thetreatmenttotalissimply
the sum of the y values within a particular factor level. For our two gardens we have:
cbind(ozone[garden=="A"],ozone[garden=="B"])
[,1][,2]
[1,] 3 5
[2,] 4 5
[3,] 4 6
[4,] 3 7
[5,] 2 4
[6,] 3 4
[7,] 1 3
[8,] 3 5
[9,] 5 6
[10,] 2 5
tapply(ozone,garden,sum)
A B
3050
ThetotalsforgardensAandBare30and50respectively,andwewillcalltheseT and
1
T . The shortcut formula for SSA (Box 8.1) is then:
2
P (cid:2)P (cid:3)
T2 y 2
SSA i  
n kn
We should confirm that this really does give SSA:
302502 802 3400 6400
SSA      340 32020
10 210 10 20
whichchecksout.Inallsortsofanalysisofvariance,thekeypointtorealizeisthatthesum
of the subtotals squared is always divided by the number of numbers that were added
togethertogeteachsubtotal.Thatsoundscomplicated,buttheideaissimple.Inourcasewe
158 STATISTICS:ANINTRODUCTIONUSINGR
squaredthesubtotalsT andT andaddedtheresultstogether.Wedividedby10because
1 2
T and T were each the sum of 10 numbers.
1 2
Box8.1. Correctedsumsofsquaresinone-wayANOVA
Thetotalsumofsquares,SSY,isdefinedasthesumofthesquaresofthedifferences
between all the data points, y, and the overall mean, y:
Xk X
SSY  y y2
i1
P
where the inner Pmeans the sum over the n replicates within each of the k factor
levelsandtheouter meansaddthesubtotalsacrosseachofthefactorlevels.The
errorsumofsquares,SSE,isthesumofthesquaresofthedifferencesbetweenthedata
points, y, and their individual treatment means, y:
i
Xk X
SSE  y y2
i
i1
The treatment sum of squares, SSA, is the sum of the squares of the differences
between the individual treatment means, y, and the overall mean, y:
i
Xk Xn Xk
SSA y  y2 n y  y2
i i
i1 j1 i1
Squaring the bracketed term and applying summation gives
X X
y2 2y y k:y 2
i i
Now replace y by T =n (where T is our conventional name for the k individual
i i Pi
treatment totals) and replace y by y=k:n to get
P P P P P
k T2 y k T y y
i1 i  2 i1 ik
n2 n:k:n k:n:k:n
P P P
Notethat k T  j n y sotheright-handpositiveandnegativetermsboth
i1(cid:2)P i (cid:3) i1 j1 ij
have the form y
2=k:n2.
Finally, multiplying through by n gives
P (cid:2)P (cid:3)
T2 y 2
SSA  
n k:n
As an exercise, you should prove that SSY=SSA+SSE (for a hint, see Box 7.4).
ANALYSIS OFVARIANCE 159
EffectSizes
Sofarwehaveconcentratedonhypothesistesting,usingsummary.aov.Itisusuallymore
informativetoinvestigatetheeffectsofthedifferentfactorlevels,usingsummary.lmlikethis:
summary.lm(aov(ozone∼garden))
Itwaseasytointerpretthiskindofoutputinthecontextofaregression,wheretherows
representparametersthatareintuitive:namely,theinterceptandtheslope.Inthecontextof
analysisofvariance,ittakesafairbitofpracticebeforethemeaningofthiskindofoutput
is transparent.
Coefficients:
EstimateStd. Error t value Pr(>|t|)
(Intercept) 3.0000 0.3651 8.216 1.67e-07***
gardenB 2.0000 0.5164 3.873 0.00111**
Residualstandarderror:1.155on18degreesoffreedom
MultipleR-squared: 0.4545, AdjustedR-squared: 0.4242
F-statistic: 15on1and18DF, p-value:0.001115
Therowsarelabelled(Intercept)andgardenB.Butwhatdotheparameterestimates
3.0and2.0actuallymean?Andwhyarethestandarderrorsdifferentinthetworows(0.3651
and 0.5164)? After all, the variances in the two gardens were identical.
Tounderstandtheanswerstothesequestions,weneedtoknowhowtheequationforthe
explanatory variables is structured when the explanatory variable, as here, is categorical.
To recap, the linear regression model is written as
lm(y∼x)
which R interprets as the two-parameter linear equation
yabx
inwhichthevaluesoftheparametersaandbaretobeestimatedfromthedata.Butwhat
about ouranalysisofvariance? Wehaveoneexplanatoryvariable,x=‘garden’,withtwo
levels, A and B. The aov model is exactly analogous to the regression model
aov(y∼x)
But what is the associated equation? Let us look at the equation first, then try to
understand it:
yabx cx
1 2
Thislooksjustlikeamultipleregression,withtwoexplanatoryvariables,x andx .The
1 2
keypointtounderstandisthatx andx arethelevelsofthefactorcalledx.If‘garden’werea
1 2
160 STATISTICS:ANINTRODUCTIONUSINGR
four-levelfactor,thentheequationwouldhavefourexplanatoryvariablesinit,x , . . . ,x .
1 4
With a categorical explanatory variable, the levels are all coded as 0 except for the level
associated with the y value in question, which is coded as 1. You will find this hard to
understand without a good deal of practice. Let us look at the first row of data in our
dataframe:
garden[1]
[1]A
SothefirstozonevalueinthedataframecomesfromgardenA.Thismeansthatx =1
1
and x =0. The equation for the first row therefore looks like this:
2
yab1c0ab1ab
What about the second row of the dataframe?
garden[2]
[1]B
BecausethisrowreferstogardenB,x iscodedas0andx iscodedas1sotheequation
1 2
for the second row is:
yab0c1ac1ac
So what does this tell us about the parameters a, b and c? And why do we have three
parameters,whentheexperimentgeneratesonlytwomeans?Thesearethecrucialquestions
for understanding the summary.lm output from an analysis of variance. The simplest
interpretationofthethree-parametercasethatwehavedealtwithsofaristhattheintercept
a is the overall mean from the experiment:
mean(ozone)
[1]4
Then,ifaistheoverallmean,soa+bmustbethemeanforgardenAanda+cmustbe
themeanforgardenB(seetheequationsabove).Ifthatistrue,thenbmustbethedifference
betweenthemeanofgardenAandtheoverallmean.Andcmustbethedifferencebetween
the mean of garden B and the overall mean. Thus, the intercept is a mean, and the other
parameters are differences between means. This explains why the standard errors are
differentinthedifferentrowsofthetable:thestandarderroroftheinterceptisthestandard
error of a mean
sffiffiffiffiffiffi
s2
SE  A
y n
A
ANALYSIS OFVARIANCE 161
whereasthestandarderrorsontheotherrowsarestandarderrorsofthedifferencebetween
two means: sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
s2 s2
SE  A  B
diff n n
A B
pffiffiffi
whichisabiggernumber(biggerbyafactorof1:4142 2if,ashere,thesamplesizes
and variances are equal).
Withthreeparameters,then,weshouldhavebequaltothemeanozoneconcentrationin
garden A minus 4 and c equal to the mean ozone concentration in garden B minus 4.
mean(ozone[garden=="A"])-mean(ozone)
[1]-1
mean(ozone[garden=="B"])-mean(ozone)
[1]1
Thatwouldbeaperfectlyreasonablewaytoparameterizethemodelforthisanalysisof
variance. But it suffers from the fact that there is a redundant parameter. The experiment
produces only two means (one for each garden), and so there is no point in having three
parameterstorepresenttheoutputoftheexperiment.Oneofthethreeparametersissaidto
be‘aliased’(seep.16).Therearelotsofwaysroundthisdilemma,asexplainedindetailin
Chapter12oncontrasts.HereweadopttheconventionthatisusedasthedefaultinRusing
so-calledtreatmentcontrasts.Underthisconvention,wedispensewiththeoverallmean,a.
Sonowweareleftwiththerightnumberofparameters(bandc).Intreatmentcontrasts,the
factorlevelthatcomesfirstinthealphabetissetequaltotheintercept.Theotherparameters
areexpressedasdifferencesbetweenthismeanandtheotherrelevantmeans.So,inourcase,
the mean of garden A becomes the intercept
mean(ozone[garden=="A"])
[1]3
andthedifferencebetweenthemeans ofgardenBandgardenAisthesecondparameter:
mean(ozone[garden=="B"])-mean(ozone[garden=="A"])
[1]2
Let us revisit our summary.lm table and see if it now makes sense:
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 3.0000 0.3651 8.216 1.67e-07***
gardenB 2.0000 0.5164 3.873 0.00111**
The intercept is 3.0 which is the mean for garden A (because the factor level A comes
beforelevelBinthealphabet).TheestimateforgardenBis2.0.Thistellsusthatthemean
ozoneconcentrationingardenBis2pphmgreaterthaningardenA(greaterbecausethereis
162 STATISTICS:ANINTRODUCTIONUSINGR
nominussign).WewouldcomputethemeanforgardenBas3.0+2.0=5.0.Inpractice,
we would not obtain the means like this, but by using tapply, instead:
tapply(ozone,garden,mean)
A B
3 5
There is more about these issues in Chapter 12.
PlotsforInterpretingOne-WayANOVA
There are two traditional ways of plotting the results of ANOVA:
(cid:129) box-and-whisker plots
(cid:129) barplots with error bars
Here is an example to compare the two approaches. We have an experiment on plant
competition where the response variable is biomass and we have one factor with five
levels. The factor is called clipping and the levels are control (i.e. unclipped), two
intensities of shoot pruning and two intensities of root pruning:
comp<-read.csv("c:\\temp\\competition.csv")
attach(comp)
names(comp)
[1]"biomass" "clipping"
This is what the data look like:
plot(clipping,biomass,xlab="Competitiontreatment",
ylab="Biomass",col="lightgrey")
ANALYSIS OFVARIANCE 163
The box-and-whisker plot is good at showing the nature of the variation within each
treatment,andalsowhetherthereisskewwithineachtreatment(e.g.forthecontrolplots,
there is a wider range of values between the median and upper quartile than between the
lower quartile and median). No outliers are shown above the whiskers, so the tops and
bottomsofthebarsarethemaximaandminimawithineachtreatment.Themediansforthe
competitiontreatmentsareallhigherthantheupperquartileofthecontrols,suggestingthat
theymaybesignificantlydifferentfromthecontrols,butthereislittletosuggestthatanyof
the competition treatments are significantly different from one another (see below for the
analysis).
Barplots with error bars are the style preferred by many journal editors, and some
people think that they make hypothesis testing easier. We shall see. Unlike S-PLUS, R
does not have a built-in function called error.bar so we shall have to write our own.
First, the barploton its own. We need to calculate the heights of the bars to represent
themeanbiomassfromeachofthefivetreatments,andthesimplestwaytodothisisto
use tapply:
heights<-tapply(biomass,clipping,mean)
Nowdrawthebarplot,makingsurethattheyaxisislongenoughtoaccommodatethe
tops of the error bars that we intend to add later:
barplot(heights,col="green",ylim=c(0,700),
ylab="meanbiomass",xlab="competitiontreatment")
164 STATISTICS:ANINTRODUCTIONUSINGR
Thisisfineasfarasitgoes,butitgivesusnoimpression oftheuncertaintyassociated
with the estimated heights of the bars.
Hereisaverysimplefunctionwithoutanybellsorwhistles.Weshallcalliterror.bars
todistinguishitfromthemuchmoregeneralS-PLUSfunction.Weshallpasstwoarguments
to the function; the heights of the bars y and the lengths of the error bars, z like this:
error.bars<-function(y,z){
x<-barplot(y,plot=F)
n<-length(y)
for(iin1:n)
arrows(x[i],y[i]-z,x[i],y[i]+z,code=3,angle=90,length=0.15)
}
Thesecondlineworksoutthexcoordinatesofthecentresofthebarswithoutredrawing
thebarplot(plot=FALSE).Thenextlineworksouthowmanyerrorbarsaretobedrawn(n
<-length(y),whichis5inthisexample).Thetrickhereistomodifythearrowsfunction
todrawtheerrorsbarswithheadsatrightangles(angle=90)andheadsatbothendsofthe
arrow(code=3).Thedefaultheadlengthlooksabitclunky,sowereducethewidthofthe
head to 0.15 inches. We use a for loop to draw the n error bars separately.
Tousethisfunctionweneedtodecidewhatkindofvalues(z)touseforthelengthsofthe
bars.Letususeonestandarderrorofthemeanbasedonthepoolederrorvariancefromthe
ANOVA, then return to a discussion of the pros and cons of different kinds of error bars
later. Here is the one-way ANOVA:
model<-aov(biomass∼clipping)
summary(model)
Df SumSq MeanSq Fvalue Pr(>F)
clipping 4 85356 21339 4.3015 0.008752**
Residuals 25 124020 4961
FromtheANOVAtablewecanseethatthepoolederrorvariances2=4961.Nowwe
needtoknowhowmanynumberswereusedinthecalculationofeachofthefivemeans:
table(clipping)
clipping
control n25 n50 r10 r5
6 6 6 6 6
Therewasequalreplication(whichmakeslpifeffiffiffieffiffiaffiffisffiffiier)p,affinffiffidffiffiffiffieffiffiaffifficffiffihffiffi meanwasbasedonsix
replicates,sothestandarderrorofthemeanis s2=n 4961=628:75.Weshalldraw
an error bar up 28.75 from each mean and down by the same distance, so we need five
values, one for each bar, each of 28.75:
se<-rep(28.75,5)
Now we can use the new function to add the error bars to the plot:
error.bars(heights,se)
ANALYSIS OFVARIANCE 165
Wedonotgetthesamefeelforthedistributionofthevalueswithineachtreatmentaswas
obtainedbythebox-and-whiskerplot,butwecancertainlyseeclearlywhichmeansarenot
significantlydifferent.If,ashere,weuse±1standarderrorasthelengthoftheerrorbars,
thenwhenthebarsoverlapthisimpliesthatthetwomeansarenotsignificantlydifferent.
Remembertheruleofthumbfort:significancerequires2ormorestandarderrors,andifthe
barsoverlapitmeansthatthedifferencebetweenthemeansislessthan2standarderrors.
Thisshowsclearlythatnoneofthemeansfortheclippedplants(n25,n50,r10orr5)is
significantlydifferentfromanyother(thetopofthebarforn25overlapsthebottomofthe
bar for r10).
Thereisanotherissue,too.Forcomparingmeans,weshouldbeusingthestandarderror
ofthedifferencebetweentwomeans(notthestandarderrorofonemean)inourtests(see
p.91);thesebarswouldbeabout1.4timesaslongasthebarswehavedrawnhere.Sowhile
wecanbesurethatthepruningtreatmentsarenotsignificantlydifferentfromoneanother,
wecannotconcludefromthisplotthatthecontrolshavesignificantlylowerbiomassthan
the rest (because the error bars are not the correct length for testing differences between
means).
An alternative graphical method is to use 95% confidence intervals for the lengths of
thebars,ratherthanstandarderrorsofmeans.Thisiseasytodo:wemultiplyourstandard
errors by Student’s t, qt(.975,5)=2.570582, to get the lengths of the confidence
intervals:
ci<-se*qt(.975,5)
barplot(heights,col="green",ylim=c(0,700),
ylab="meanbiomass",xlab="competitiontreatment")
error.bars(heights,ci)
166 STATISTICS:ANINTRODUCTIONUSINGR
Now,alloftheerrorbarsoverlap,implyingvisuallythattherearenosignificantdifferences
betweenthemeans.Butweknowthatthisisnottruefromouranalysisofvariance,inwhich
werejectedthenullhypothesisthatallthemeanswerethesameatp=0.00875.Ifitwerethe
casethatthebarsdidnotoverlapwhenweareusingconfidenceintervals(ashere),thenthat
wouldimplythatthemeansdifferedbymorethan4standarderrors,andthisismuchgreater
thanthedifferencerequiredforsignificance.Sothisisnotperfecteither.Withstandarderrors
wecouldbesurethatthemeanswerenotsignificantlydifferentwhenthebarsdidoverlap.
Andwithconfidenceintervalswecanbesurethatthemeansaresignificantlydifferentwhen
thebarsdonotoverlap.Butthealternativecasesarenotclear-cutforeithertypeofbar.Can
wesomehowgetthebestofbothworlds,sothatthemeansaresignificantlydifferentwhenthe
barsdonotoverlap,andthemeansarenotsignificantlydifferentwhenthebarsdooverlap?
The answer is yes, we can, if we use LSD bars (LSD stands for ‘least significant
difference’). Let us revisit the formula for Student’s t test:
a difference
t 
standard error of the difference
Wesaythatthedifferenceissignificantwhent>2(bytheruleofthumb,ort>qt(0.975,df)
ifwewanttobemoreprecise).Wecanrearrangethisformulatofindthesmallestdifference
thatwewouldregardasbeingsignificant.Wecancallthistheleastsignificantdifference:
LSD=qt(0.975,df)×standard error of a difference 2SE
diff
In our present example this is
qt(0.975,10)*sqrt(2*4961/6)
[1]90.60794
ANALYSIS OFVARIANCE 167
becauseadifferenceisbasedon12–2=10degreesoffreedom.Whatwearesayingisthat
thetwomeanswouldbesignificantlydifferentiftheydifferedby90.61ormore.Howcan
weshowthisgraphically?Wewantoverlappingbarstoindicateadifferencelessthan90.61,
andnon-overlappingbarstorepresentadifferencegreaterthan90.61.Withabitofthought
youwillrealizethatweneedtodrawbarsthatareLSD/2inlength,upanddownfromeach
mean. Let us try it with our current example:
lsd<-qt(0.975,10)*sqrt(2*4961/6)
lsdbars<-rep(lsd,5)/2
barplot(heights,col="green",ylim=c(0,700),
ylab="meanbiomass",xlab="competitiontreatment")
error.bars(heights,lsdbars)
Now we can interpret the significant differences visually. The control biomass is
significantly lower than any of the four treatments, but none of the four treatments is
significantlydifferentfromanyother.Thestatisticalanalysisofthiscontrastisexplainedin
detailinChapter12.Sadly,mostjournaleditorsinsistonerrorbarsof1standarderror.Itis
truethattherearecomplicatingissuestodowithLSDbars(notleastthevexedquestionof
multiplecomparisons;seep.17),butatleastLSD/2barsdowhatwasintendedbytheerror
plot (i.e. overlapping bars means non-significance and non-overlapping bars means
significance).Neitherstandarderrorsnorconfidenceintervalscansaythat.Abetteroption
might be to use box-and-whisker plots with the notch=T option to indicate significance
(see p. 93).
168 STATISTICS:ANINTRODUCTIONUSINGR
FactorialExperiments
A factorial experiment has two or more factors, each with two or more levels, plus
replication for each combination of factor levels. This means that we can investigate
statisticalinteractions,inwhichtheresponsetoonefactordependsonthelevelofanother
factor.Ourexamplecomesfromafarm-scaletrialofanimaldiets.Therearetwofactors:diet
andsupplement.Dietisafactorwiththreelevels:barley,oatsandwheat.Supplementisa
factorwithfourlevels:agrimore,control,supergainandsupersupp.Theresponsevariableis
weight gain after 6 weeks.
weights<-read.csv("c:\\temp\\growth.csv")
attach(weights)
Datainspectioniscarriedoutusingbarplot(notetheuseofbeside=Ttogetthebarsin
adjacent clusters rather than vertical stacks; we shall add the error bars later):
barplot(tapply(gain,list(diet,supplement),mean),beside=T)
Notethatthesecondfactorinthelist(supplement)appearsasgroupsofbarsfromleftto
rightinalphabeticalorderbyfactorlevel,from‘agrimore’to‘supersupp’.Thefirstfactor
(diet) appears as three levels within each group of bars: dark=barley, mid=oats, light=
wheat,againinalphabeticalorderbyfactorlevel.Weshouldreallyaddalegendtoexplain
thelevelsofdiet.Weuselevelstoextractthenamesofthedietstouseaslabelsinthekey:
labels<-levels(diet)
ANALYSIS OFVARIANCE 169
Wehaveusedcolourssofar,butitiseasytousegrayscalesinR.Thecontinuumgoes
from 0=black to 1=white, so we could use 0.2, 0.6 and 0.9 here:
shade<-c(0.2,0.6,0.9)
Thetrickybitislocatingthekeyontheplotsothatitdoesnotoverlapanyofthebars.You
needtounderstandthatRlocatesthelegendontheplotusingthecoordinatesofthetop
left-handcorneroftheboxthatsurroundsthekey.Soyouneedtofindsomespacebelow
andtotherightofasuitablepoint.Thenmovethecursortothispointandclick.Rwilldraw
the legend at that point.
barplot(tapply(gain,list(diet,supplement),mean),beside=T,
ylab="weightgain",xlab="supplement",ylim=c(0,30))
legend(locator(1),labels,gray(shade))
There are clearly substantial differences between the three diets, but the effects of
supplement are less obvious. We inspect the mean values using tapply as usual:
tapply(gain,list(diet,supplement),mean)
agrimore control supergain supersupp
barley 26.34848 23.29665 22.46612 25.57530
oats 23.29838 20.49366 19.66300 21.86023
wheat 19.63907 17.40552 17.01243 19.66834
170 STATISTICS:ANINTRODUCTIONUSINGR
NowweuseaovorlmtofitafactorialANOVA(thechoiceaffectsonlywhetherweget
anANOVAtableoralistofparametersestimatesasthedefaultoutputfromsummary).We
estimateparametersforthemaineffectsofeachlevelofdietandeachlevelofsupplement,
plustermsfortheinteractionbetweendietandsupplement.Interactiondegreesoffreedom
aretheproductofthedegreesoffreedomofthecomponentterms(i.e.(3 1)×(4 1)=6).
The model is
gain∼diet+supplement+diet:supplement
but this can be simplified using the asterisk notation like this:
model<-aov(gain∼diet*supplement)
summary(model)
Df SumSq MeanSq Fvalue Pr(>F)
diet 2 287.17 143.59 83.52 3.00e-14***
supplement 3 91.88 30.63 17.82 2.95e-07***
diet:supplement 6 3.41 0.57 0.33 0.917
Residuals 36 61.89 1.72
The ANOVA table shows that there is no hint of any interaction between the two
explanatoryvariables(p=0.9166);evidentlytheeffectsofdietandsupplementareadditive
butbotharehighlysignificant.Nowthatweknowthattheerrorvariances2=1.72wecan
add the error bars to the plot. For bars showing standard errors, we need to know the
replication for each combination of factor levels:
tapply(gain,list(diet,supplement),length)
agrimore control supergain supersupp
barley 4 4 4 4
oats 4 4 4 4
wheat 4 4 4 4
pffiffiffiffiffiffiffiffiffiffiffiffiffiffi
sotheappropriatestandarderroris 1:72=40:656.Theerror.bars functionthatwe
wrote earlier will not work for grouped bars, so we need to modify the code like this:
x<-as.vector(barplot(tapply(gain,list(diet,supplement),mean),
beside=T,ylim=c(0,30)))
y<-as.vector(tapply(gain,list(diet,supplement),mean))
z<-rep(0.656,length(x))
for(iin1:length(x))
arrows(x[i],y[i]-z[i],x[i],y[i]+z[i],length=0.05,code=3,angle=90)
Asusual,weneedtomakeroomontheyaxisforthetopofthehighesterrorbars.Notethe
useofas.vectortoconvertthetabularoutputoftapplyintoaformsuitableforplotting.
Don’t forget to add the legend:
legend(locator(1),labels,gray(shade))
ANALYSIS OFVARIANCE 171
The disadvantage of the ANOVA table is that it does not show us the effect sizes, and
doesnotallowustoworkouthowmanylevelsofeachofthetwofactorsaresignificantly
different.Asapreliminarytomodelsimplification,summary.lmisoftenmoreusefulthan
summary.aov:
summary.lm(model)
Coefficients:
Estimate Std.Error t value Pr(>|t|)
(Intercept) 26.3485 0.6556 40.191 <2e-16***
dietoats -3.0501 0.9271 -3.2900.002248**
dietwheat -6.7094 0.9271 -7.2371.61e-08***
supplementcontrol -3.0518 0.9271 -3.2920.002237**
supplementsupergain -3.8824 0.9271 -4.1870.000174***
supplementsupersupp -0.7732 0.9271 -0.8340.409816
dietoats:supplementcontrol 0.2471 1.3112 0.1880.851571
dietwheat:supplementcontrol 0.8183 1.3112 0.6240.536512
dietoats:supplementsupergain 0.2470 1.3112 0.1880.851652
dietwheat:supplementsupergain 1.2557 1.3112 0.9580.344601
dietoats:supplementsupersupp -0.6650 1.3112 -0.5070.615135
dietwheat:supplementsupersupp 0.8024 1.3112 0.6120.544381
Residualstandarderror:1.311on36degreesoffreedom
MultipleR-squared: 0.8607, AdjustedR-squared: 0.8182
F-statistic:20.22on11and36DF, p-value:3.295e-12
172 STATISTICS:ANINTRODUCTIONUSINGR
Thisisarathercomplexmodel,becausethereare12estimatedparameters(thenumberof
rowsinthetable):6maineffectsand6interactions.Theoutputre-emphasizesthatnoneof
the interaction terms is significant, but it suggests that the minimal adequate model will
require five parameters:an intercept,a difference duetooats,a difference duetowheat, a
difference due to control and difference due to supergain (these are the five rows with
significancestars).Thisdrawsattentiontothemainshortcomingofusingtreatmentcontrasts
asthedefault.Ifyoulookcarefullyatthetable,youwillseethattheeffectsizesoftwoofthe
supplements, control and supergain, are not significantly different from one another. You
needlotsofpracticeatdoingttestsinyourhead,tobeabletodothisquickly.Ignoringthe
signs (because the signs are negative for both of them) we have 3.05 versus 3.88, a
differenceof0.83.Butlookattheassociatedstandarderrors(both0.927);thedifferenceis
onlyabout1standarderrorofthedifferencebetweentwomeans.Forsignificance,wewould
needroughly2 standarderrors(remembertheruleofthumb,inwhicht 2 issignificant;
see p. 83). The rows get starred in the significance column because treatment contrasts
compareallthemaineffectsintherowswiththeintercept(whereeachfactorissettoitsfirst
levelin the alphabet, namelyagrimoreandbarley in this case).When, as here, several
factorlevelsaredifferentfromtheintercept,butnotdifferentfromoneanother,theyallget
significance stars. This means that you cannot count up the number of rows with stars in
order to determine the number of significantly different factor levels.
We begin model simplification by leaving out the interaction terms:
model<-lm(gain∼diet+supplement)
summary(model)
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 26.1230 0.4408 59.258 <2e-16***
dietoats -3.0928 0.4408 -7.016 1.38e-08***
dietwheat -5.9903 0.4408 -13.589 <2e-16***
supplementcontrol -2.6967 0.5090 -5.298 4.03e-06***
supplementsupergain -3.3815 0.5090 -6.643 4.72e-08***
supplementsupersupp -0.7274 0.5090 -1.429 0.16
Residualstandarderror:1.247on42degreesoffreedom
MultipleR-squared: 0.8531, AdjustedR-squared: 0.8356
F-statistic:48.76on5and42DF, p-value:<2.2e-16
Itisclearthatweneedtoretainallthreelevelsofdietbecauseoatsdifferfromwheatby
5.99 3.10=2.89withastandarderrorof0.44(t 2).Itisnotclearthatweneedallfour
levels of supplement, however. Supersupp is not obviously different from the agrimore
( 0.727 with standard error 0.509). Nor is supergain obviously different from the unsup-
plementedcontrolanimals(3.38 2.70=0.68).Weshalltryanewtwo-levelfactortoreplace
thefour-levelsupplement,andseeifthissignificantlyreducesthemodel’sexplanatorypower.
Agrimoreandsupersupparerecodedas‘best’andcontrolandsupergainas‘worst’:
supp2<-factor(supplement)
levels(supp2)
[1]"agrimore" "control" "supergain""supersupp"
ANALYSIS OFVARIANCE 173
levels(supp2)[c(1,4)]<-"best"
levels(supp2)[c(2,3)]<-"worst"
levels(supp2)
[1]"best" "worst"
Now we fit the simpler model, then compare the two models:
model2<-lm(gain∼diet+supp2)
anova(model,model2)
AnalysisofVarianceTable
Model1: gain∼diet+supplement
Model2: gain∼diet+supp2
Res.Df RSSDfSumofSq FPr(>F)
1 42 65.296
2 44 71.284-2 -5.98761.92570.1584
Thesimplermodel2hassaved2degreesoffreedomandisnotsignificantlyworsethan
the more complex model (p=0.158). This is theminimal adequate model:now all of the
parameters are significantly different from zero and from one another:
summary(model2)
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 25.7593 0.3674 70.106 <2e-16***
dietoats -3.0928 0.4500 -6.873 1.76e-08***
dietwheat -5.9903 0.4500 -13.311 <2e-16***
supp2worst -2.6754 0.3674 -7.281 4.43e-09***
Residualstandarderror:1.273on44degreesoffreedom
MultipleR-squared: 0.8396, AdjustedR-squared: 0.8286
F-statistic:76.76on3and44DF, p-value:<2.2e-16
Modelsimplificationhasreducedourinitial12-parametermodeltoamuchmoretractable
four-parametermodelthatisfareasiertocommunicatetoreaders.Ifmaximumweightgainis
yourobjective,thenadietofbarleyandasupplementofeitheragrimoreorsupersuppis
indicated.Notethattherecommendationsofthemodelmightbedifferentifprofitwerethe
responsevariableratherthanweightgainbecausewehavenottakenthecostsintoaccount.
Pseudoreplication:NestedDesignsandSplitPlots
Themodel-fittingfunctionsaov,lmeandlmerhavethefacilitytodealwithcomplicated
errorstructures.Detailedanalysisofthesetopicsisbeyondthescopeofthisbook(seeTheR
Book (Crawley, 2013) for worked examples), but it is important that you can recognize
them, and hence avoid the pitfalls of pseudoreplication. There are two general cases:
(cid:129) nestedsampling,aswhenrepeatedmeasurementsaretakenfromthesameindividual,or
observationalstudiesareconductedatseveraldifferentspatialscales(mostorallofthe
factors are random effects)
174 STATISTICS:ANINTRODUCTIONUSINGR
(cid:129) split-plot analysis, as when designed experiments are carried out with different
treatments applied to plots of different sizes (most of the factors are fixed effects)
Split-PlotExperiments
Inasplit-plotexperiment,differenttreatmentsareappliedtoplotsofdifferentsizes.Each
differentplotsizeisassociatedwithitsownerrorvariance,soinsteadofhavingoneerror
variance(asinalltheANOVAtablesuptothispoint),wehaveasmanyerrortermsasthere
aredifferentplotsizes.TheanalysisispresentedasaseriesofcomponentANOVAtables,
oneforeachplotsize,inahierarchyfromthelargestplotsizewiththelowestreplicationat
the top, down to the smallest plot size with the greatest replication at the bottom.
The example refers to a designed field experiment on crop yield with three treatments:
irrigation (with two levels, irrigated and not), sowing density (with three levels, low,
medium and high), and fertilizer application (with three levels, low, medium and high):
yields<-read.csv("c:\\temp\\splityield.csv")
attach(yields)
names(yields)
[1]"yield" "block" "irrigation" "density" "fertilizer"
Thelargestplotswerethefourwholefields(block),eachofwhichwassplitinhalf,and
irrigationwasallocatedatrandomtoonehalfofthefield.Eachirrigationplotwassplitinto
three,andoneofthreedifferentseed-sowingdensities(low,mediumorhigh)wasallocated
atrandom(independentlyforeachlevelofirrigationandeachblock).Finally,eachdensity
plotwasdividedintothreeandoneofthreefertilizernutrienttreatments(N,P,orNandP
together)wasallocatedatrandom.Themodelformulaisspecifiedasa factorial,usingthe
asterisknotation.TheerrorstructureisdefinedintheError()term,withtheplotsizeslisted
fromlefttoright,fromlargesttosmallest,witheachvariableseparatedbytheslashoperator/.
Note that the smallest plot size, fertilizer, does not need to appear in the Error() term:
model<-
aov(yield∼irrigation*density*fertilizer+Error(block/irrigation/
density))
summary(model)
Error:block
DfSumSqMeanSqFvaluePr(>F)
Residuals 3 194.4 64.81
Error:block:irrigation
Df Sum Sq Mean Sq F value Pr(>F)
irrigation 1 8278 8278 17.59 0.0247*
Residuals 3 1412 471
Error:block:irrigation:density
Df Sum Sq Mean Sq Fvalue Pr(>F)
density 2 1758 879.2 3.784 0.0532.
irrigation:density 2 2747 1373.5 5.912 0.0163*
Residuals 12 2788 232.3
ANALYSIS OFVARIANCE 175
Error:Within
Df Sum Sq Mean Sq F value Pr(>F)
fertilizer 2 1977.4 988.7 11.449 0.000142***
irrigation:fertilizer 2 953.4 476.7 5.520 0.008108**
density:fertilizer 4 304.9 76.2 0.883 0.484053
irrigation:density:fertilizer 4 234.7 58.7 0.680 0.610667
Residuals 36 3108.8 86.4
Here you see the four ANOVA tables, one for each plot size: blocks are the biggest
plots,halfblocksgettheirrigationtreatment,onethirdofeachhalfblockgetsasowing
density treatment, and one third of a sowing density treatment gets each fertilizer
treatment. Each plot size has a different error variance. Note that the non-significant
main effect for density (p=0.053) does not mean that density is unimportant, because
densityappearsinasignificantinteractionwithirrigation(thedensitytermscancelout
when averaged over the two irrigation treatments; see below). The best way to
understand the two significant interaction terms is to plot them using interac-
tion.plot like this:
interaction.plot(fertilizer,irrigation,yield)
Irrigation increases yield proportionately more on the P-fertilized plots than on the
N-fertilized plots. The irrigation:density interaction is more complicated:
interaction.plot(density,irrigation,yield)
176 STATISTICS:ANINTRODUCTIONUSINGR
Ontheirrigatedplots,yieldisminimalonthelow-densityplots,butoncontrolplotsyield
is minimal on the high-density plots.
RandomEffectsandNestedDesigns
Mixedeffectsmodelsaresocalledbecausetheexplanatoryvariablesareamixtureoffixed
effects and random effects:
(cid:129) fixed effects influence only the mean of y
(cid:129) random effects influence only the variance of y
A random effect should be thought of as coming from a population of effects: the
existence of this population is an extra assumption. We speak of prediction of random
effects,ratherthanestimation:weestimatefixedeffectsfromdata,butweintendtomake
predictionsaboutthepopulationfromwhichourrandomeffectsweresampled.Fixedeffects
areunknownconstantstobeestimatedfromthedata.Randomeffectsgovernthevariance–
covariance structure of the response variable. The fixed effects are often experimental
treatments that were applied under our direction, and the random effects are either
categorical or continuous variables that are distinguished by the fact that we are typically
not interested in the parameter values, but only in the variance they explain.
One or more of the explanatory variables represents grouping in time or in space.
Randomeffectsthatcomefromthesamegroupwillbecorrelated,andthiscontravenesone
of the fundamental assumptions of standard statistical models: independence of errors.
Mixed effects models take care of this non-independence of errors by modelling the
covariance structure introduced by the grouping of the data. A major benefit of random
effectsmodelsisthattheyeconomizeonthenumberofdegreesoffreedomusedupbythe
ANALYSIS OFVARIANCE 177
factorlevels.Insteadofestimatingameanforeverysinglefactorlevel,therandomeffects
model estimates the distribution of the means (usually as the standard deviation of the
differences of the factor-level means around an overall mean). Mixed effects models are
particularly useful in cases where there is temporal pseudoreplication (repeated measure-
ments) and/or spatial pseudoreplication (e.g. nested designs or split-plot experiments).
These models can allow for:
(cid:129) spatial autocorrelation between neighbours
(cid:129) temporal autocorrelation across repeated measures on the same individuals
(cid:129) differences in the mean response between blocks in a field experiment
(cid:129) differences between subjects in a medical trial involving repeated measures
Thepointisthatwereallydonotwanttowastepreciousdegreesoffreedominestimating
parameters for each of the separate levels of the categorical random effects. On the other
hand,wedowanttomakeuseoftheallmeasurementswehavetaken,butbecauseofthe
pseudoreplication we want to take account of both the:
(cid:129) correlationstructure,usedtomodelwithin-groupcorrelationassociatedwithtemporal
and spatial dependencies, using correlation
(cid:129) variancefunction,usedtomodelnon-constantvarianceinthewithin-grouperrorsusing
weights
FixedorRandomEffects?
It is difficult without lots of experience to know when to use categorical explanatory
variables as fixed effects and when as random effects. Some guidelines are:
(cid:129) Am I interested in the effect sizes? Yes means fixed effects
(cid:129) Isitreasonabletosupposethatthefactorlevelscomefromapopulationoflevels?Yes
means random effects
(cid:129) Are there enough levels of the factor in the data on which to base an estimate of the
variance of the population of effects? No means fixed effects
(cid:129) Are the factor levels informative? Yes means fixed effects
(cid:129) Are the factor levels just numeric labels? Yes means random effects
(cid:129) AmImostlyinterestedinmakinginferencesaboutthedistributionofeffects,basedon
therandomsampleofeffectsrepresentedinthedataframe?Yesmeansrandomeffects.
(cid:129) Is there hierarchical structure? Yes means you need to ask whether the data are
experimental or observations
(cid:129) Isitahierarchicalexperiment,wherethefactorlevelsareexperimentalmanipulations?
Yes means fixed effects in a split-plot design (see p. 173)
178 STATISTICS:ANINTRODUCTIONUSINGR
(cid:129) Isitahierarchicalobservationalstudy?Yesmeansrandomeffects,perhapsinavariance
components analysis (see p. 183)
(cid:129) When your model contains both fixed and random effects, use mixed effects models
(cid:129) If your model structure is linear, use linear mixed effects, lmer
(cid:129) Otherwise, specify the model equation and use non-linear mixed effects, nlme
RemovingthePseudoreplication
The extreme response to pseudoreplication in a data set is simply to eliminate it. Spatial
pseudoreplicationcanbeaveragedawayandtemporalpseudoreplicationcanbedealtwith
bycarryingoutcarryingoutseparateANOVAs,oneforeachtimeperiod.Thisapproachhas
two major weaknesses:
(cid:129) it cannot address questions about treatment effects that relate to the longitudinal
development of the mean response profiles (e.g. differences in growth rates between
successive times)
(cid:129) inferences made with each of the separate analyses are not independent, and it is not
always clear how they should be combined
AnalysisofLongitudinalData
The key feature of longitudinal data is that the same individuals are measured repeatedly
through time. This would represent temporal pseudoreplication if the data were used
uncriticallyinregressionorANOVA.Thesetofobservationsononeindividualsubjectwill
tend to be positively correlated, and this correlation needs to be taken into account in
carryingouttheanalysis.Thealternativeisacross-sectionalstudy,withallthedatagathered
at a single point in time, in which each individual contributes a single data point. The
advantage of longitudinal studies is that they are capable of separating age effects from
cohort effects; these are inextricably confounded in cross-sectional studies. This is
particularly important when differences between years mean that cohorts originating at
different times experience different conditions, so that individuals of the same age in
differentcohortswouldbeexpectedtodiffer.Therearetwoextremecasesinlongitudinal
studies:
(cid:129) a few measurements on a large number of individuals
(cid:129) a large number of measurements on a few individuals
In thefirst case it isdifficult tofitan accuratemodelfor changewithinindividuals,but
treatmenteffectsarelikelytobetestedeffectively.Inthesecondcase,itispossibletogetan
accuratemodelofthewayindividualschangethoughtime,butthereislesspowerfortesting
thesignificanceoftreatmenteffects,especiallyifvariationfromindividualtoindividualis
large.Inthefirstcase,lessattentionwillbepaidtoestimatingthecorrelationstructure,whilein
thesecondcasethecovariancemodelwillbetheprincipalfocusofattention.Theaimsare:
ANALYSIS OFVARIANCE 179
(cid:129) to estimate the average time course of a process
(cid:129) tocharacterizethedegreeofheterogeneityfromindividualtoindividualintherateofthe
process
(cid:129) identify the factors associated with both of these, including possible cohort effects
Theresponseisnottheindividualmeasurement,butthesequenceofmeasurementsonan
individualsubject.Thisenablesustodistinguishbetweenageeffectsandyeareffects(see
Diggle, Liang and Zeger, 1994, for details).
DerivedVariableAnalysis
Theideahereistogetridofthepseudoreplicationbyreducingtherepeatedmeasuresintoa
set of summary statistics (slopes, intercepts or means), and then analyse these summary
statistics using standard parametric techniques such as ANOVA or regression. The
technique is weak when the values of the explanatory variables change through time.
Derived variable analysis makes most sense when it is based on the parameters of
scientifically interpretable non-linear models from each time sequence. However, the
best model from a theoretical perspective may not be the best model from the statistical
point of view.
There are three qualitatively different sources of random variation:
(cid:129) random effects: experimental units differ (e.g. genotype, history, size, physiological
condition) so that there are intrinsically high responders and other low responders
(cid:129) serial correlation: there may be time-varying stochastic variation within a unit (e.g.
marketforces,physiology,ecologicalsuccession,immunity)sothatcorrelationdepends
onthetimeseparationofpairsofmeasurementsonthesameindividual,withcorrelation
weakening with the passage of time
(cid:129) measurementerror:theassaytechniquemayintroduceanelementofcorrelation(e.g.
shared bioassay of closely spaced samples; different assay of later specimens)
DealingwithPseudoreplication
Forrandomeffectsweareoftenmoreinterestedinthequestionofhowmuchofthevariation
intheresponsevariablecanbeattributedtoagivenfactorthanweareinestimatingmeansor
assessingthesignificanceofdifferencesbetweenmeans.Thisprocedureiscalledvariance
components analysis.
rats<-read.csv("c:\\temp\\rats.csv")
attach(rats)
names(rats)
[1]"Glycogen" "Treatment""Rat" "Liver"
This classic example of pseudoreplication comes from Snedecor and Cochran (1980).
Threeexperimentaltreatmentswereadministeredtorats,andtheglycogencontentsofthe
rats’liverswereanalysedastheresponsevariable.Thiswastheset-up:thereweretworats
180 STATISTICS:ANINTRODUCTIONUSINGR
pertreatment,sothetotalsamplewasn=3×2=6.Thetrickybitwasthis:aftereachratwas
killed,itsliverwascutupintothreepieces:aleft-handbit,acentralbitandaright-handbit.
Sonowtherearesixratseachproducingthreebitsofliver,foratotalof6×3=18numbers.
Finally,twoseparatepreparationsweremadefromeachmaceratedbitofliver,toassessthe
measurement error associated with the analytical machinery. At this point there are
2×18=36 numbers in the dataframe as a whole. The factor levels are numbers, so we
need to declare the explanatory variables to be categorical before we begin:
Treatment<-factor(Treatment)
Rat<-factor(Rat)
Liver<-factor(Liver)
Here is the analysis done the wrong way:
model<-aov(Glycogen∼Treatment)
summary(model)
Df SumSq Mean Sq F value Pr(>F)
Treatment 2 1558 778.8 14.5 3.03e-05***
Residuals 33 1773 53.7
Treatmenthasahighlysignificanteffectonliverglycogencontent(p=0.00003).Wrong!
We have committed a classic error of pseudoreplication. Look at the error line in the
ANOVAtable:itsaystheresidualshave33degreesoffreedom.Buttherewereonlysixrats
inthewholeexperiment,sotheerrordegrees offreedommustbe6 1 23(not33)!
Here is the analysis of variance done properly, averaging away the pseudoreplication.
First reduce the dataframe from 36 numbers to just six numbers: one mean value per rat:
yv<-tapply(Glycogen,list(Treatment,Rat),mean)
yv
1 2
1 132.5000 148.5000
2 149.6667 152.3333
3 134.3333 136.0000
Turn this object into a vector to act as our new response variable:
(yv<-as.vector(yv))
[1] 132.5000 149.6667 134.3333 148.5000 152.3333 136.0000
Fromthetapplyoutputyoucanseethatthedataappearcolumnwise.Thecolumns(1
and2)arethereplicaterats,andtherows(1,2and3)arethetreatments(control,supplement
and supplement plus sugar). We need to produce a new factor of length 6 to contain the
treatment in the correct order, 1, 2, 3 then 1, 2, 3 (make sure you understand this):
treatment<-factor(c(1,2,3,1,2,3))
ANALYSIS OFVARIANCE 181
Now we can fit the non-pseudoreplicated model:
model<-aov(yv∼treatment)
summary(model)
Df Sum Sq MeanSq F valuePr(>F)
treatment 2 259.6 129.80 2.929 0.197
Residuals 3 132.9 44.31
As you can see, the error degrees of freedom are correct (d.f.=3, not 33), and the
interpretationiscompletelydifferent:therearenosignificantdifferencesinliverglycogen
under the three experimental treatments (p=0.197).
TherearetwodifferentwaysofdoingtheanalysisproperlyinR:ANOVAwithmultiple
errorterms(aov)orlinearmixedeffectsmodels(lmer).Theproblemisthatthebitsofthe
same liver are pseudoreplicates because they are spatially correlated (they come from
thesamerat);theyarenotindependent,asrequirediftheyaretobetruereplicates.Likewise,
thetwopreparationsfromeachliverbitareveryhighlycorrelated(theliversweremacerated
beforethepreparationsweretaken,sotheyareessentiallythesamesample,andcertainlynot
independent replicates of the experimental treatments).
Hereisthecorrectanalysisusingaovwithmultipleerrorterms.Intheerrortermwestart
with the largest scale (treatment), then rats within treatments, then liver bits within rats
withintreatments.Finally,therewerereplicatedmeasurements(twopreparations)madefor
each bit of liver (see Box 8.2):
model2<-aov(Glycogen∼Treatment+Error(Treatment/Rat/Liver))
summary(model2)
Error:Treatment
Df Sum Sq Mean Sq
Treatment 2 1558 778.8
Error:Treatment:Rat
Df Sum Sq Mean Sq FvaluePr(>F)
Residuals 3 797.7 265.9
Error:Treatment:Rat:Liver
Df Sum Sq Mean Sq FvaluePr(>F)
Residuals12 594 49.5
Error:Within
Df Sum Sq Mean Sq F valuePr(>F)
Residuals 18 381 21.17
Youcandothecorrect,non-pseudoreplicatedanalysisofvariancefromthisoutput.TheF
testinvolvesthetreatmentvariance(778.8)dividedbytheerrorvarianceatthespatialscale
immediately below (i.e. rats within treatments 265.9). This means that the test statistic
F=778.8/265.9=2.928921isnotsignificant(comparewiththewronganalysisonp.180).
182 STATISTICS:ANINTRODUCTIONUSINGR
NotethatthecurrentversionofRdoesnotprinttheFvaluesorpvalues.Thecriticalvalueof
F would need to be
qf(0.95,2,3)
[1]9.552094
Box8.2. Sumsofsquaresinhierarchicaldesigns
The trick to understanding these sums of squares is to appreciate that with nested
categorical explanatory variables (random effects) the correction factor, which is
(cid:2)P (cid:3)
subtracted from the sum of squared subtotals, is not the conventional y
2=kn.
Instead, the correction factor is the uncorrected sum of squared subtotals from the
levelinthehierarchyimmediatelyabovethelevelinquestion.Thisisveryhardtosee
without lots of practice. The total sum of squares, SSY, and the treatment sum of
squares, SSA, are computed in the usual way (see Box 8.1):
(cid:2)P (cid:3)
X 2
y
SSY  y2 
n
P (cid:2)P (cid:3)
k C2 y 2
SSA i1 i  
n kn
Theanalysisiseasiesttounderstandinthecontextofanexample.Fortheratsdata,the
treatment totals were based on 12 numbers (2 rats, 3 liver bits per rat and 2
preparations per liver bit). In this case, in the formula for SSA, above, has n=12
andkn=36.Weneedtocalculatesumsofsquaresforratswithintreatments,SS ,
Rats
liver bits within rats within treatments, SS , and preparations within liver bits
Liverbits
within rats within treatments, SS :
Preparations
P P
R2 C2
SS   
Rats
6 12
P P
L2 R2
SS   
Liverbits
2 6
P P
y2 L2
SS   
Preparations 1 2
The correction factor at any level is the uncorrected sum of squares from the level
above. The last sum of squares could have been computed by difference:
SS SSY  SSA SS  SS
Preparations Rats Liverbits
ANALYSIS OFVARIANCE 183
VarianceComponentsAnalysis(VCA)
ToturnthisnestedANOVAtableintoavariancecomponentsanalysisweneedtodoalittle
work.Thethingtounderstandisthateachrowofthetableincludesthevariabilityfromthe
levelbelowplusthenewvariabilityintroducedatthatlevel.Soatthebottom,thevarianceof
21.17 represents measurement error (differences between readings produced by similar
samples put through the same machine). The next level up reflects heterogeneity within
individualrats’livers(e.g.ifthemiddleoftheliverwasdifferentinglycogencontentthan
theleft-orright-handendsoftheliver;physiologicaldifferences,ifyoulike).Thenextlevel
up reflects differences between rats; these will include gender effects, genetic effects,
nutritionaleffectsandsuchlike(althoughwewouldhopethatthesehadbeencontrolledin
the experimental design).
We are interested in discovering the variance entering the system at each level in the
hierarchy. To find this out we begin by calculating the difference in variance between
adjacent levels:
Level 2 versus level 1: 49.5 21.17=28.33 reflecting liver bit difference
Level 3 versus level 2: 265.9 49.5=216.4 reflecting rat differences
Westopatthispoint,becausethenextlevelupisafixedeffect(treatment)notarandom
effect (like rat, or liver bit within rat).
Thenextstepistodividethesedifferencesbythenumberofpseudoreplicatesinthelower
ofthetwolevels(2preparationsperliverbit;6preparationsperrat,2fromeachof3liver
bits). So now we have:
Residuals=preparations within liver bits: unchanged=21.17
Liver bits within rats within treatments: (49.5 21.17)/2=14.165
Rats within treatments: (265.89 49.5)/6=36.065
These numbers are the variance components. You can see straight away that most
variation enters this experiment as a result of differences between individual rats in their
meanliverglycogencontents(36.065).Leastvariationisintroducedbycuttingeachliver
into three pieces (14.165).
Theresultsofavariancecomponentsanalysisaretraditionallyexpressedaspercentages.
We add the three components together to do this:
vc<-c(21.17,14.165,36.065)
100*vc/sum(vc)
[1]29.6498619.8389450.51120
That’s all there is to it. An analysis like this can be very useful in planning future
experiments.Sincemorethan50%ofthevarianceisattributabletodifferencesbetweenthe
rats,thenincreasingthenumberofratswillhavethebiggestpositiveeffectsonthepowerof
thenextexperiment(youmightalsoconsidercontrollingtheratsmorecarefullybyusingthe
184 STATISTICS:ANINTRODUCTIONUSINGR
same age and same gender of the same genotype, for instance). There is no real point in
splittingupeachliver:itmakesforthreetimestheworkandexplainslessthan20%ofthe
variance.
References
Crawley,M.J. (2013) TheR Book, 2ndedn,JohnWiley &Sons, Chichester.
Diggle,P.J.,Liang,K.-Y.andZeger,S.L.(1994) AnalysisofLongitudinal Data,ClarendonPress,
Oxford.
Snedecor,G.W.andCochran,W.G.(1980)StatisticalMethods,Ames,IowaStateUniversityPress.
FurtherReading
Pinheiro, J.C. and Bates, D.M. (2000) Mixed-Effects Models in S and S-PLUS, Springer-Verlag,
NewYork.
9
Analysis of Covariance
Analysis of covariance (ANCOVA) involves a combination of regression and analysis of
variance.Theresponsevariableiscontinuous,andthereisatleastonecontinuousexplanatory
variable and at least one categorical explanatory variable. Typically, the maximal model
involvesestimatingaslopeandanintercept(theregressionpartoftheexercise)foreachlevel
of the categorical variable(s) (the ANOVA part of the exercise). Let us take a concrete
example.Supposewearemodellingweight(theresponsevariable)asafunctionofsexand
age.Sexisafactorwithtwolevels(maleandfemale)andageisacontinuousvariable.The
maximalmodelthereforehasfourparameters:twoslopes(aslopeformalesandaslopefor
females)andtwointercepts(oneformalesandoneforfemales)likethis:
weight a b age
male male male
weight a b age
female female female
ModelsimplificationisanessentialpartofANCOVA,becausetheprincipleofparsimony
requiresthat we keepas few parameters inthe model as possible.
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
186 STATISTICS:ANINTRODUCTIONUSINGR
Thereareatleastsixpossiblemodelsinthiscase,andtheprocessofmodelsimplification
beginsbyaskingwhetherweneedallfourparameters(topleft).Perhapswecouldmakedo
with two intercepts and a common slope (top right). Or a common intercept and two
different slopes (centre left). There again, age may have no significant effect on the
response,soweonlyneedtwoparameterstodescribethemaineffectsofsexonweight;this
wouldshowupastwoseparated,horizontallinesintheplot(onemeanweightforeachsex;
centreright).Alternatively,theremaybenoeffectofsexatall,inwhichcaseweonlyneed
twoparameters(oneslopeandoneintercept)todescribetheeffectofageonweight(bottom
left). In the limit, neither the continuous nor the categorical explanatory variables might
haveanysignificanteffectontheresponse,inwhichcasemodelsimplificationwillleadto
the one-parameter null model ^yy (a single, horizontal line; bottom right).
Decisionsaboutmodelsimplificationarebasedontheexplanatorypowerofthemodel:if
thesimplermodeldoesnotexplainsignificantlylessofthevariationintheresponse,then
thesimplermodelispreferred.Testsofexplanatorypowerarecarriedoutusinganovaor
AICtocomparetwomodels:whenusinganovaweshallonlyretainthemorecomplicated
modelifthepvaluefromcomparingthetwomodelsislessthan0.05.WhenusingAICwe
simply prefer the model with the lower value.
Letusseehowthisallworksbyinvestigatingarealisticexample.Thedataframeconcerns
an experiment on a plant’s ability to regrow and produce seeds following grazing. The
initial, pre-grazingsize oftheplant isrecordedasthediameterofthetopofitsrootstock.
Grazingisatwo-levelfactor:grazedorungrazed(protectedbyfences).Theresponseisthe
weightofseedsproducedperplantattheendofthegrowingseason.Ourexpectationisthat
big plants will produce more seeds than small plants and that grazed plants will produce
fewer seeds than ungrazed plants. Let us see what actually happened:
compensation<-read.csv("c:\\temp\\ipomopsis.csv")
attach(compensation)
names(compensation)
[1]"Root" "Fruit" "Grazing"
We begin with data inspection. First: did initial plant size matter?
plot(Root,Fruit,pch=16,col="blue")
ANALYSIS OFCOVARIANCE 187
Yes,indeed.Biggerplantsproducedmoreseedsattheendofthegrowingseason.What
about grazing?
plot(Grazing,Fruit,col="lightgreen")
Thisisnotatallwhatweexpectedtosee.Apparently,thegrazedplantsproducedmore
seeds, not less than the ungrazed plants. Taken at face value, the effect looks to be
statistically significant (p<0.03):
summary(aov(Fruit∼Grazing))
Df Sum Sq Mean Sq F value Pr(>F)
Grazing 1 2910 2910.4 5.309 0.0268*
Residuals 38 20833 548.2
We shall return to this after we have carried out the statistical modelling properly.
Analysisofcovarianceisdoneinthefamiliarway:itisjustthattheexplanatoryvariablesare
amixtureofcontinuousandcategoricalvariables.Westartbyfittingthemostcomplicated
model,withdifferentslopesandinterceptsforthegrazedandungrazedplants.Forthis,weuse
theasteriskoperator:
model<-lm(Fruit∼Root*Grazing)
Animportantthingtorealizeaboutanalysisofcovarianceisthatordermatters.Lookat
the regression sum of squares in the ANOVA table when we fit root first:
summary.aov(model)
Df Sum Sq Mean Sq F value Pr(>F)
Root 1 16795 16795 359.968 <2e-16***
Grazing 1 5264 5264 112.832 1.21e-12***
188 STATISTICS:ANINTRODUCTIONUSINGR
Root:Grazing 1 5 5 0.103 0.75
Residuals 36 1680 47
and when we fit root second:
model<-lm(Fruit∼Grazing*Root)
summary.aov(model)
Grazing 1 2910 2910 62.380 2.26e-09***
Root 1 19149 19149 410.420 <2e-16***
Grazing:Root 1 5 5 0.103 0.75
Residuals 36 1680 47
Inbothcases,theerrorsumofsquares(1680)andtheinteractionsumofsquares(5)are
the same, but the regression sum of squares (labelled Root) is much greater when root is
fittedtothemodelaftergrazing(19,149),thanwhenitisfittedfirst(16,795).Thisisbecause
thedataforANCOVAaretypicallynon-orthogonal.Remember,withnon-orthogonaldata,
order matters (Box 9.1).
Box9.1. Correctedsumsofsquaresinanalysisofcovariance
Thetotalsumofsquares,SSY,andthetreatmentsumsofsquares,SSA,arecalculatedin
thesamewayasinastraightforwardanalysisofvariance(Box8.1).Thesumsofsquares
fortheseparateregressionswithintheindividualfactorlevels,i,arecalculatedasshown
in Box 7.3: SSXY i, SSX i, SSR i, and SSE i are then added up across the factor levels:
X
SSXY  SSXY
total i
X
SSX  SSX
total i
X
SSR  SSR
total i
Thentheoverallregressionsumofsquares,SSR,iscalculatedfromthetotalcorrected
sums of products and the total corrected sums of squares of x:
SSXY 2
SSR total
SSX
total
Thedifferenceinthetwoestimates,SSRandSSR ,iscalledSSR andisameasure
total diff
of the significance of the differences between the regression slopes. Now we can
compute SSE by difference:
SSE SSY  SSA SSR SSR
diff
ANALYSIS OFCOVARIANCE 189
But SSE is defined for the k levels in which the regressions were computed as
Xk X
SSE  y a  bx2
i i
i1
Of course, both methods give the same answer.
Backtotheanalysis.Theinteraction,SSR ,representingdifferencesinslopebetween
diff
the grazed and ungrazed treatments, appears to be insignificant, so we remove it:
model2<-lm(Fruit∼Grazing+Root)
Noticetheuseof+ratherthan*inthemodelformula.Thissays‘fitdifferentintercepts
for grazed and ungrazed plants, but fit the same slope to both graphs’. Does this simpler
model have significantly lower explanatory power? We use anova to find out:
anova(model,model2)
AnalysisofVarianceTable
Model 1: Fruit ∼ Grazing * Root
Model 2: Fruit ∼ Grazing + Root
Res.Df RSS DfSumofSq FPr(>F)
1 36 1679.7
2 37 1684.5 -1 -4.8122 0.1031 0.75
Thesimplermodeldoesnothavesignificantlylowerexplanatorypower(p=0.75),sowe
adopt it. Note that we did not have to do the anova in this case: the p value given in the
summary.aov(model) table gave the correct, deletion p value. Here are the parameter
estimates from our minimal adequate model:
summary.lm(model2)
Coefficients:
EstimateStd. Error tvalue Pr (>|t|)
(Intercept) -127.829 9.664 -13.23 1.35e-15***
GrazingUngrazed 36.103 3.357 10.75 6.11e-13***
Root 23.560 1.149 20.51 <2e-16***
Residualstandarderror:6.747on37degreesoffreedom
MultipleR-squared: 0.9291, AdjustedR-squared: 0.9252
F-statistic:242.3on2and37DF, p-value:<2.2e-16
Themodelhashighexplanatorypower,accountingformorethan90%ofthevariationin
seed production (multiple r2). The hard thing about ANCOVA is understanding what the
parameterestimatesmean.Startingatthetop,thefirstrow,aslabelled,containsanintercept.
It is the intercept for the graph of seed production against initial rootstock size for the
190 STATISTICS:ANINTRODUCTIONUSINGR
grazingtreatmentwhosefactorlevelcomesfirstinthealphabet.Toseewhichonethisis,we
can use levels:
levels(Grazing)
[1]"Grazed" "Ungrazed"
Sotheinterceptisforthegrazedplants.Thesecondrow,labelledGrazingUngrazed,
is a difference between two intercepts. To get the intercept for the ungrazed plants, we
needtoadd36.103totheinterceptforthegrazedplants( 127.829+36.103= 91.726).
Thethirdrow,labelledRoot,isaslope:itisthegradientofthegraphofseedproduction
against initial rootstock size, and it is the same for both grazed and ungrazed plants. If
there had been a significant interaction term, this would have appeared in row 4 as a
difference between two slopes.
Wecannowplotthefittedmodelthroughthescatterplot.Itwillbeusefultohavedifferent
colours for the grazed (red) and ungrazed plants (green)
plot(Root,Fruit,pch=21,bg=(1+as.numeric(Grazing)))
Notetheuseof1+as.numeric(Grazing)toproducethedifferentcolours:2(red)for
Grazed and 3 (green) for ungrazed. Let us add a legend to make this clear:
legend(locator(1),c("grazed","ungrazed"),col=c(2,3),pch=16)
Just position the cursor where you want the top left-hand corner of the key to appear,
then click:
ANALYSIS OFCOVARIANCE 191
Nowitbecomesclearwhywegotthecuriousresultatthebeginning,inwhichgrazing
appearedtoincreaseseedproduction.Clearlywhathashappenedisthatthemajorityofbig
plantsendedupinthegrazedtreatment(redsymbols).Ifyoucomparelikewithlike(e.g.
plants at 7 mm initial root diameter), it is clear that the ungrazed plants (green symbols)
producedmoreseedthanthegrazedplants(36.103more,tobeprecise).Thiswillbecome
clearer when we fit the lines predicted by model2:
abline(-127.829,23.56,col="blue")
abline(-127.829+36.103,23.56,col="blue")
Thisexampleshowsthegreatstrengthofanalysisofcovariance.Bycontrollingforinitial
plantsize,wehavecompletelyreversedtheinterpretation.Thenaïvefirstimpressionwas
that grazing increased seed production:
tapply(Fruit,Grazing,mean)
Grazed Ungrazed
67.9405 50.8805
andthiswassignificantifwewererashenoughtofitgrazingonitsown(p=0.0268aswe
saw earlier). But when we do the correct ANCOVA, we find the opposite result: grazing
significantlyreducesseedproductionforplantsofcomparableinitialsize;forexamplefrom
77.46 to 41.36 at mean rootstock size:
-127.829+36.103+23.56*mean(Root)
[1]77.4619
-127.829+23.56*mean(Root)
[1]41.35889
192 STATISTICS:ANINTRODUCTIONUSINGR
Themoralisclear.Whenyouhavecovariates(likeinitialsizeinthisexample),thenuse
them.Thiscandonoharm,becauseifthecovariatesarenotsignificant,theywilldropout
during modelsimplification. Alsoremember that inANCOVA,order matters.So always
start model simplification by removing the highest-order interaction terms first. In
ANCOVA,theseinteractiontermsaredifferencesbetweenslopesfordifferentfactorlevels
(recallthatinmulti-wayANOVA,theinteractiontermsweredifferencesbetweenmeans).
Other ANCOVAs are described in Chapters 13, 14 and 15 in the context of count data,
proportion data and binary response variables.
FurtherReading
Huitema,B.E.(1980)TheAnalysisofCovarianceandAlternatives,JohnWiley&Sons,NewYork.
10
Multiple Regression
Inmultipleregressionwehaveacontinuousresponsevariableandtwoormorecontinuous
explanatory variables (i.e. there are no categorical explanatory variables). In many
applications, multiple regression is the most difficult of all the statistical models to do
well. There are several things that make multiple regression so challenging:
(cid:129) the studies are often observational (rather than controlled experiments)
(cid:129) we often have a great many explanatory variables
(cid:129) we often have rather few data points
(cid:129) missing combinations of explanatory variables are commonplace
There are several important statistical issues, too:
(cid:129) the explanatory variables are often correlated with one another (non-orthogonal)
(cid:129) there are major issues about which explanatory variables to include
(cid:129) there could be curvature in the response to the explanatory variables
(cid:129) there might be interactions between explanatory variables
(cid:129) the last three issues all tend to lead to parameter proliferation
Thereisa temptationtobecomepersonally attached toaparticularmodel.Statisticians
call this ‘falling in love with your model’. It is as well to remember the following truths
about models:
(cid:129) all models are wrong
(cid:129) some models are better than others
(cid:129) the correct model can never be known with certainty
(cid:129) the simpler the model, the better it is
Fitting models to data is the central function of R. The process is essentially one of
exploration;therearenofixedrulesandnoabsolutes.Theobjectistodetermineaminimal
adequate model from the large set of potential models that might be used to describe the
given set of data. In this book we discuss five types of model:
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
194 STATISTICS:ANINTRODUCTIONUSINGR
(cid:129) the null model
(cid:129) the minimal adequate model
(cid:129) the current model
(cid:129) the maximal model
(cid:129) the saturated model
Thestepwiseprogressionfromthesaturatedmodel(orthemaximalmodel,whicheveris
appropriate)throughaseriesofsimplificationstotheminimaladequatemodelismadeon
thebasisofdeletiontests;theseareFtests,AIC,ttestsorchi-squaredteststhatassessthe
significanceoftheincreaseindeviancethatresultswhenagiventermisremovedfromthe
current model.
Models are representations of reality that should be both accurate and convenient.
However, it is impossible to maximize a model’s realism, generality and holism simulta-
neously,andtheprincipleofparsimony(Occam’srazor;seep.8)isavitaltoolinhelpingto
chooseonemodeloveranother.Thus,wewouldonlyincludeanexplanatoryvariableina
modelifitsignificantlyimprovedthefitofamodel.Justbecausewewenttothetroubleof
measuringsomething,thatdoesnotmeanwehavetohaveitinourmodel.Parsimonysays
that, other things being equal, we prefer:
(cid:129) a model with n – 1 parameters to a model with n parameters
(cid:129) a model with k – 1 explanatory variables to a model with k explanatory variables
(cid:129) a linear model to a model which is curved
(cid:129) a model without a hump to a model with a hump
(cid:129) a model without interactions to a model containing interactions between variables
Other considerations include a preference for models containing explanatory variables
thatareeasytomeasureovervariablesthataredifficultorexpensivetomeasure.Also,we
prefer models that are based on a sound mechanistic understanding of the process over
purely empirical functions.
Parsimony requires that the model should be as simple as possible. This means
that the model should not contain any redundant parameters or factor levels. We
achieve this by fitting a maximal model then simplifying it by following one or more
of these steps:
(cid:129) remove non-significant interaction terms
(cid:129) remove non-significant quadratic or other non-linear terms
(cid:129) remove non-significant explanatory variables
(cid:129) group together factor levels that do not differ from one another
(cid:129) in ANCOVA, set non-significant slopes of continuous explanatory variables to zero
subject,ofcourse,tothecaveatsthatthesimplificationsmakegoodscientificsense,and
do not lead to significant reductions in explanatory power.
MULTIPLE REGRESSION 195
Justasthereisnoperfectmodel,sotheremaybenooptimalscaleofmeasurementfora
model.Suppose,forexample,wehadaprocessthathadPoissonerrorswithmultiplicative
effects amongst the explanatory variables. Then, one must chose between three different
scales, each of which optimizes one of three different properties:
pffiffiffi
1. the scale of y would give constancy of variance
2. the scale of y2=3 would give approximately normal errors
3. the scale of ln(y) would give additivity
Thus,anymeasurementscaleisalwaysgoingtobeacompromise,andyoushouldchoose
the scale that gives the best overall performance of the model.
Model Interpretation
Saturatedmodel Oneparameterforeverydatapoint
Fit:perfect
Degreesoffreedom:none
Explanatorypowerofthemodel:none
Maximalmodel Containsall(p)factors,interactionsandcovariatesthatmightbeof
anyinterest.Manyofthemodel’stermsarelikelytobeinsignificant
Degreesoffreedom:n p 1
Explanatorypowerofthemodel:itdepends
Minimaladequate Asimplifiedmodelwith0p´ pparameters
model Fit:lessthanthemaximalmodel,butnotsignificantlyso
Degreesoffreedom:n p´ 1
Explanatorypowerofthemodel:r2=SSR/SSY
Nullmodel Justoneparameter,theoverallmeany
Fit:none;SSE=SSY
Degreesoffreedom:n 1
Explanatorypowerofthemodel:none
TheStepsInvolvedinModelSimplification
Therearenohardandfastrules,buttheprocedurelaidoutbelowworkswellinpractice.With
large numbers of explanatory variables, and many interactions and non-linear terms, the
processofmodelsimplificationcantakeaverylongtime.Butthisistimewellspentbecauseit
reducestheriskofoverlookinganimportantaspectofthedata.Itisimportanttorealizethat
thereisnoguaranteedwayoffindingalltheimportantstructuresinacomplexdataframe.
Step Procedure Explanation
1 Fitthemaximalmodel Fitallthefactors,interactionsandcovariatesof
interest.Notetheresidualdeviance.Ifyouare
usingPoissonorbinomialerrors,checkfor
overdispersionandrescaleifnecessary
(continued)
196 STATISTICS:ANINTRODUCTIONUSINGR
(Continued)
Step Procedure Explanation
2 Beginmodelsimplification Inspecttheparameterestimatesusingsummary.
Removetheleastsignificanttermsfirst,using
update-,startingwiththehighest-order
interactions
3 Ifthedeletioncausesan Leavethattermoutofthemodel.
insignificantincreasein Inspecttheparametervaluesagain.
deviance Removetheleastsignificanttermremaining
4 Ifthedeletioncausesa Putthetermbackinthemodelusingupdate+.
significantincreaseindeviance Thesearethestatisticallysignificanttermsas
assessedbydeletionfromthemaximalmodel
5 Keepremovingtermsfromthe Repeatsteps3or4untilthemodelcontains
model nothingbutsignificantterms.
Thisistheminimaladequatemodel.
Ifnoneoftheparametersissignificant,thenthe
minimaladequatemodelisthenullmodel
Caveats
Model simplification is an important process, but it should not be taken to extremes.
For example, the interpretation of deviances and standard errors produced with fixed
parameters that have been estimated from the data should be undertaken with caution.
Again, the search for ‘nice numbers’ should not be pursued uncritically. Sometimes
there are good scientific reasons forusing a particular number (e.g.a powerof0.66 in
anallometricrelationshipbetweenrespirationandbodymass).Again,itismuchmore
straightforward, for example, to say that yield increases by 2kg per hectare for every
extra unit of fertilizer, than to say that it increases by 1.947kg. Similarly, it may be
preferable to say that the odds of infection increase 10-fold under a given treatment,
ratherthantosaythatthelogitsincreaseby2.321(withoutmodelsimplificationthisis
equivalent to saying that there is a 10.186-fold increase in the odds). It would be
absurd, however, to fix on an estimate of 6 rather than 6.1 just because 6 is a whole
number.
OrderofDeletion
Remember that when explanatory variables are correlated (as they almost always are in
multipleregression),ordermatters.Ifyourexplanatoryvariablesarecorrelatedwitheach
other, then the significance you attach to a given explanatory variable will depend upon
whetheryoudeleteitfromamaximalmodeloraddittothenullmodel.Ifyoualwaystestby
model simplification then you won’t fall into this trap.
The fact that you have laboured long and hard to include a particular experimental
treatmentdoesnotjustifytheretentionofthatfactorinthemodeliftheanalysisshows
MULTIPLE REGRESSION 197
it to have no explanatory power. ANOVA tables are often published containing a
mixture of significant and non-significant effects. This is not a problem in orthogonal
designs, because sums of squares can be unequivocally attributed to each factor and
interaction term. But as soon as there are missing values or unequal weights, then it is
impossible to tell how the parameter estimates and standard errors of the significant
terms would have been altered if the non-significant terms had been deleted. The best
practice is this
(cid:129) say whether your data are orthogonal or not
(cid:129) present a minimal adequate model
(cid:129) givealistofthenon-significanttermsthatwereomitted,andthedeviancechangesthat
resulted from their deletion
Readers can then judge for themselves the relative magnitude of the non-significant
factors, and the importance of correlations between the explanatory variables.
The temptation to retain terms in the model that are ‘close to significance’ should be
resisted.Thebestwaytoproceedisthis.Ifaresultwouldhavebeenimportantifithadbeen
statisticallysignificant,thenitisworthrepeatingtheexperimentwithhigherreplicationand/
or more efficient blocking, in order to demonstrate the importance of the factor in a
convincing and statistically acceptable way.
CarryingOutaMultipleRegression
Formultipleregression,theapproachrecommendedhereisthatbeforeyoubeginmodelling
in earnest you do two things:
(cid:129) use tree models to investigate whether there are complicated interactions
(cid:129) use generalized additive models to investigate curvature
Let us begin with an example from air pollution studies. How is ozone concentration
related to wind speed, air temperature and the intensity of solar radiation?
ozone.pollution<-read.csv("c:\\temp\\ozone.data.csv")
attach(ozone.pollution)
names(ozone.pollution)
[1]"rad" "temp" "wind" "ozone"
Inmultipleregression,itisalways agoodideatousethepairsfunctiontolookatall
the correlations:
pairs(ozone.pollution,panel=panel.smooth)
198 STATISTICS:ANINTRODUCTIONUSINGR
The response variable, ozone concentration, is shown on the y axis of the bottom
row of panels: there is a strong negative relationship with wind speed, a positive
correlation with temperature and a rather unclear, but possibly humped relationship
with radiation.
Agoodwaytostartamultipleregressionproblemisusingnon-parametricsmoothersina
generalized additive model (gam) like this:
library(mgcv)
par(mfrow=c(2,2))
model<-gam(ozone∼s(rad)+s(temp)+s(wind))
plot(model,col="blue")
MULTIPLE REGRESSION 199
Theconfidenceintervalsaresufficientlynarrowtosuggestthatthecurvatureinallthree
relationships might be real.
Thenextstepmightbetofitatreemodeltoseewhethercomplexinteractionsbetweenthe
explanatory variables are indicated. The older tree function is better for this graphical
search for interactions than is the more modern rpart (but the latter is much better for
statistical modelling):
par(mfrow=c(1,1))
library(tree)
model<-tree(ozone∼.,data=ozone.pollution)
plot(model)
text(model)
200 STATISTICS:ANINTRODUCTIONUSINGR
This shows that temperature is far and away the most important factor affecting ozone
concentration(thelongerthebranchesinthetree,thegreaterthedevianceexplained).Wind
speedisimportantatbothhighandlowtemperatures,withstillairbeingassociatedwithhigher
meanozonelevels(thefiguresattheendsofthebranchesaremeanozoneconcentrations).
Radiationshowsaninteresting,butsubtle effect. Atlowtemperatures, radiationmattersat
relatively high wind speeds (>7.15), whereas at high temperatures, radiation matters at
relativelylowwindspeeds(<10.6);inbothcases,however,higherradiationisassociatedwith
higher mean ozone concentration. The tree model therefore indicates that the interaction
structureofthedataisnotparticularlycomplex(thisisareassuringfinding).
Armed with this background information (likely curvature of responses and a relatively
uncomplicated interaction structure), we can begin the linear modelling. We start with the
mostcomplicatedmodel:this includesinteractionsbetween all threeexplanatoryvariables
plusquadratictermstotestforcurvatureinresponsetoeachofthethreeexplanatoryvariables:
model1<-lm(ozone∼temp*wind*rad+I(rad^2)+I(temp^2)+I(wind^2))
summary(model1)
Coefficients:
Estimate Std. Error tvalue Pr (>|t|)
(Intercept) 5.683e+02 2.073e+02 2.741 0.00725**
temp -1.076e+01 4.303e+00 -2.501 0.01401*
wind -3.237e+01 1.173e+01 -2.760 0.00687**
rad -3.117e-01 5.585e-01 -0.558 0.57799
I(rad^2) -3.619e-04 2.573e-04 -1.407 0.16265
I(temp^2) 5.833e-02 2.396e-02 2.435 0.01668*
I(wind^2) 6.106e-01 1.469e-01 4.157 6.81e-05***
temp:wind 2.377e-01 1.367e-01 1.739 0.08519.
temp:rad 8.403e-03 7.512e-03 1.119 0.26602
wind:rad 2.054e-02 4.892e-02 0.420 0.67552
temp:wind:rad-4.324e-04 6.595e-04 -0.656 0.51358
Residualstandarderror:17.82on100degreesoffreedom
MultipleR-squared: 0.7394, AdjustedR-squared: 0.7133
F-statistic:28.37on10and100DF, p-value:<2.2e-16
Thethree-wayinteractionisclearlynotsignificant,soweremoveittobegintheprocess
of model simplification:
model2<-update(model1,∼.–temp:wind:rad)
summary(model2)
Next,weremovetheleastsignificanttwo-wayinteractionterm,inthiscasewind:rad:
model3<-update(model2,∼.-wind:rad)
summary(model3)
Then we try removing the temperature by wind interaction:
model4<-update(model3,∼.-temp:wind)
summary(model4)
MULTIPLE REGRESSION 201
We shall retain the marginally significant interaction between temp and rad
(p=0.04578) forthetimebeing, butleave outall other interactions. In model4, theleast
significant quadratic term is for rad, so we delete this:
model5<-update(model4,∼.-I(rad^2))
summary(model5)
This deletion hasrendered the temp:rad interactioninsignificant, andcausedthemain
effectofradiationtobecomeinsignificant.Weshouldtryremovingthetemp:radinteraction:
model6<-update(model5,∼.-temp:rad)
summary(model6)
Coefficients:
Estimate Std.Error t value Pr (>|t|)
(Intercept)291.16758 100.87723 2.886 0.00473**
temp -6.33955 2.71627 -2.334 0.02150*
wind -13.39674 2.29623 -5.834 6.05e-08***
rad 0.06586 0.02005 3.285 0.00139**
I(temp^2) 0.05102 0.01774 2.876 0.00488**
I(wind^2) 0.46464 0.10060 4.619 1.10e-05***
Residual standarderror: 18.25 on 105 degreesof freedom
Multiple R-Squared:0.713, AdjustedR-squared:0.6994
F-statistic: 52.18 on5 and 105 DF, p-value: 0
Nowwearemakingprogress.Allthetermsinmodel6aresignificant.Atthisstage,we
should check the assumptions, using plot(model6):
202 STATISTICS:ANINTRODUCTIONUSINGR
Thereisaclearpatternofvarianceincreasingwiththemeanofthefittedvalues.Thisis
badnews(heteroscedasticity).Also,thenormalityplotisdistinctlycurved;again,thisisbad
news.Letustrytransformationoftheresponsevariable.Therearenozerosintheresponse,
so a log transformation is worth trying. We need to repeat the entire process of model
simplificationfromtheverybeginning,becausetransformationaltersthevariancestructure
and the linearity of all the relationships.
model7<-lm(log(ozone)∼temp*wind*rad+I(rad^2)+I(temp^2)+I(wind^2))
We can speed up the model simplification using the step function:
model8<-step(model7)
summary(model8)
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 7.724e-01 6.350e-01 1.216 0.226543
temp 4.193e-02 6.237e-03 6.723 9.52e-10***
wind -2.211e-01 5.874e-02 -3.765 0.000275***
rad 7.466e-03 2.323e-03 3.215 0.001736**
I(rad^2) -1.470e-05 6.734e-06 -2.183 0.031246*
I(wind^2) 7.390e-03 2.585e-03 2.859 0.005126**
Residual standard error: 0.4851on105 degreesoffreedom
MultipleR-squared: 0.7004, AdjustedR-squared: 0.6861
F-statistic: 49.1 on 5 and 105 DF, p-value: <2.2e-16
Thissimplifiedmodelfollowingtransformationhasadifferentstructurethanbefore:all
threemaineffectsaresignificantandtherearestillnointeractionsbetweenvariables,butthe
quadratic term fortemperature has gone and a quadratic term for radiation remains in the
model.Weneedtocheckthattransformationhasimprovedtheproblemsthatwehadwith
non-normality and non-constant variance:
plot(model8)
MULTIPLE REGRESSION 203
Thisshowsthatthevarianceandnormalityarenowreasonablywellbehaved,sowecan
stopatthispoint.Wehavefoundtheminimaladequatemodel.Ourinitialimpressionsfrom
the tree model (no interactions) and the generalized additive model (a good deal of
curvature) have been borne out by the statistical modelling.
ATrickierExample
Inthisexampleweintroducetwonewbuthighlyrealisticdifficulties:moreexplanatoryvariables,
and fewer data points. This is another air pollution dataframe, but the response variable
thistimeissulphurdioxideconcentration.Therearesixcontinuousexplanatoryvariables:
pollute<-read.csv("c:\\temp\\sulphur.dioxide.csv")
attach(pollute)
names(pollute)
[1]"Pollution" "Temp" "Industry" "Population""Wind"
[6]"Rain" "Wet.days"
Here are the 36 paired scatterplots:
pairs(pollute,panel=panel.smooth)
204 STATISTICS:ANINTRODUCTIONUSINGR
This time, let us begin with the tree model rather than the generalized additive model.
par(mfrow=c(1,1))
library(tree)
model<-tree(Pollution∼.,data=pollute)
plot(model)
text(model)
MULTIPLE REGRESSION 205
Thistreemodelismuchmorecomplicatedthanwesawinthepreviousozoneexample.It
is interpreted as follows. The most important explanatory variable is industry, and the
thresholdvalueseparatinglowandhighvaluesofindustryis748.Therighthandbranchof
thetreeindicatesthemeanvalueofairpollutionforhighlevelsofindustry(67.00).Thefact
thatthislimbisunbranchedmeansthatnoothervariablesexplainasignificantamountofthe
variationinpollutionlevelsforhighvaluesofindustry.Theleft-handlimbdoesnotshow
themeanvaluesofpollutionforlowvaluesofindustry,becausethereareothersignificant
explanatory variables. Mean values of pollution are only shown at the extreme ends of
branches. For low values of industry, the tree shows us that population has a significant
impactonairpollution.Atlowvaluesofpopulation(<190)themeanlevelofairpollution
was 43.43. For high values of population, the number of wet days is significant. Low
numbersofwetdays(<108)havemeanpollutionlevelsof12.00,whiletemperaturehasa
significant impact on pollution forplaces where thenumber of wet days is large. At high
temperatures(>59.35°F)themeanpollutionlevelwas15.00,whileatlowertemperatures
therunofwindisimportant.Forstillair(wind<9.65)pollutionwashigher(33.88)thanfor
higher wind speeds (23.00).
The virtues of tree-based models are numerous:
(cid:129) they are easy to appreciate and to describe to other people
(cid:129) the most important variables stand out
(cid:129) interactions are clearly displayed
(cid:129) non-linear effects are captured effectively
(cid:129) the complexity of the behaviour of the explanatory variables is plain to see
206 STATISTICS:ANINTRODUCTIONUSINGR
Weconcludethattheinteractionstructureishighlycomplex.Weshallneedtocarryout
the linear modelling with considerable care.
Start with some elementary calculations. With six explanatory variables, how many
interactionsmightwefit?Well,thereare5+4+3+2+1=15two-wayinteractionsfora
start.Plus20three-way,15four-wayand6five-wayinteractions,and1six-wayinteraction
forgoodluck.Thentherearequadratictermsforeachofthesixexplanatoryvariables.So
wearelookingatabout70parametersthatmightbeestimatedfromthedatawithacomplete
maximal model. But how many data points do we have?
length(Pollution)
[1]41
Oops! We are planning to estimate almost twice as many parameters as there are data
points.That’stakingover-parameterizationtonewheights.Thisleadsustoaverygeneral
andextremelyimportantquestion:howmanydatapointsdoesoneneedperparametertobe
estimated?
Ofcoursetherecanbenohardandfastanswertothis,butperhapstherearesomeuseful
rulesofthumb?Toputthequestionanotherway,giventhatIhave41datapoints,howmany
parameters is it reasonable to expect that I shall be able to estimate from the data?
As an absolute minimum, one would need at least three data points per parameter
(rememberthatanytwopointscanalwaysbejoinedperfectlybyastraightline).Applying
this to our current example means that the absolute maximum we should try to estimate
wouldbe41/3=13parameters(onefortheinterceptplus12slopes).Amoreconservative
rule of thumb might suggest 10 data points per parameter, under which more stringent
criteria we would be allowed to estimate an intercept and just three slopes.
Itisatthisstagethattherealitybites.IfIcannotfitalloftheexplanatoryvariablesIwant,
then how do I choose which ones to fit? This is a problem because:
(cid:129) maineffectscanappeartobenon-significantwhenarelationshipisstronglycurved(see
p.148)soifIdon’tfitaquadraticterm(usingupanextraparameterintheprocess)I
shan’t be confident about leaving a variable out of the model
(cid:129) main effects can appear to be non-significant when interactions between two or more
variables are pronounced (see p. 174)
(cid:129) interactionscanonlybeinvestigatedwhenvariablesappeartogetherinthesamemodel
(interactionsbetweentwocontinuousexplanatoryvariablesaretypicallyincludedinthe
modelasafunctionoftheproductofthetwovariables,andbothmaineffectsshouldbe
included when we do this)
Theidealstrategyistofitamaximalmodelcontainingalltheexplanatoryvariables,with
curvature terms for each variable, along with all possible interactions terms, and then
simplify this maximal model, perhaps using step to speed up the initial stages. In our
present example, however, this is completely out of the question. We have far too many
variables and far too few data points.
MULTIPLE REGRESSION 207
Inthisexample,itisimpossibletofitallcombinationsofvariablessimultaneouslyand
hence it is highly likely that important interaction terms could be overlooked. We know
fromthetreemodelthattheinteractionstructureisgoingtobecomplicated,soweneedto
concentrateonthat.Perhapsagoodplacetostartisbylookingforcurvature,toseeifwecan
eliminatethisasamajorcauseofvariation.Fittingallthevariablesandtheirquadraticterms
requiresaninterceptand12slopestobeestimated,andthisisrightattheextremelimitof
three data points per parameter:
model1<-lm(Pollution∼Temp+I(Temp^2)+Industry+I(Industry^2)+
Population+I(Population^2)+Wind+I(Wind^2)+Rain+I(Rain^2)+Wet.
days+I(Wet.days^2))
summary(model1)
Coefficients:
Estimate Std. Error t value Pr (>|t|)
(Intercept) -6.641e+01 2.234e+02 -0.297 0.76844
Temp 5.814e-01 6.295e+00 0.092 0.92708
I(Temp^2) -1.297e-02 5.188e-02 -0.250 0.80445
Industry 8.123e-02 2.868e-02 2.832 0.00847**
I(Industry^2) -1.969e-05 1.899e-05 -1.037 0.30862
Population -7.844e-02 3.573e-02 -2.195 0.03662*
I(Population^2) 2.551e-05 2.158e-05 1.182 0.24714
Wind 3.172e+01 2.067e+01 1.535 0.13606
I(Wind^2) -1.784e+00 1.078e+00 -1.655 0.10912
Rain 1.155e+00 1.636e+00 0.706 0.48575
I(Rain^2) -9.714e-03 2.538e-02 -0.383 0.70476
Wet.days -1.048e+00 1.049e+00 -0.999 0.32615
I(Wet.days^2) 4.555e-03 3.996e-03 1.140 0.26398
Residualstandarderror:14.98on28degreesoffreedom
MultipleR-squared: 0.7148, AdjustedR-squared: 0.5925
F-statistic:5.848on12and28DF, p-value:5.868e-05
Sothat’sourfirst bitofgoodnews.Inthisunsimplifiedmodel,thereisnoevidenceof
curvature for any of the six explanatory variables. Only the main effects of industry and
populationaresignificantinthis(highlyover-parameterized)model.Letusseewhatstep
makes of this:
model2<-step(model1)
summary(model2)
Coefficients:
Estimate Std. Error t value Pr (>|t|)
(Intercept) 54.468341 14.448336 3.770 0.000604 ***
I(Temp^2) -0.009525 0.003395 -2.805 0.008150 **
Industry 0.065719 0.015246 4.310 0.000126 ***
Population -0.040189 0.014635 -2.746 0.009457 **
I(Wind^2) -0.165965 0.089946 -1.845 0.073488 .
Rain 0.405113 0.211787 1.913 0.063980 .
Residualstandarderror:14.25on35degreesoffreedom
MultipleR-squared: 0.6773, AdjustedR-squared: 0.6312
F-statistic:14.69on5and35DF, p-value:8.951e-08
208 STATISTICS:ANINTRODUCTIONUSINGR
Thesimplermodelshowssignificantcurvatureoftemperatureandmarginallysignificant
curvature for wind. Notice that step has removed the linear terms for Temp and Wind;
normally when we have a quadratic term we retain the linear term even if its slope is not
significantlydifferentfromzero.Letusremovethenon-significanttermsfrommodel2to
see what happens:
model3<-update(model2,∼.-Rain-I(Wind^2))
summary(model3)
Coefficients:
EstimateStd. Error t value Pr (>|t|)
(Intercept) 42.068701 9.993087 4.210 0.000157 ***
I(Temp^2) -0.005234 0.003100 -1.688 0.099752 .
Industry 0.071489 0.015871 4.504 6.45e-05 ***
Population -0.046880 0.015199 -3.084 0.003846 **
Residualstandarderror:15.08on37degreesoffreedom
MultipleR-squared: 0.6183, AdjustedR-squared: 0.5874
F-statistic:19.98on3and37DF, p-value:7.209e-08
Simplification reduces the significance of the quadratic term for temperature. We shall
needtoreturntothis.Themodelsupportsoutinterpretationoftheinitialtreemodel:amain
effect for industry is very important, as is a main effect for population.
Now we need to consider the interaction terms. We shall not fit interaction terms
without both the component main effects, so we cannot fit all the two-way interaction
terms at the same time (that would be 15+6=21 parameters; well above the rule of
thumb maximum value of 13). One approach is to fit the interaction terms in randomly
selected sets. With all six main effects, we can afford to assess 13 6=7 interaction
terms at a time. Let us try this. Make a vector containing the names of the 15 two-way
interactions:
interactions<-c("ti","tp","tw","tr","td","ip","iw",
"ir","id","pw","pr","pd","wr","wd","rd")
Now shuffle the interactions into random order using sample without replacement:
sample(interactions)
[1]"wr""wd""id""ir""rd""pr""tp""pw""ti"
[10]"iw""tw""pd""tr""td""ip"
Itwouldbepragmatictotestthetwo-wayinteractionsinthreemodelseachcontainingall
the main effects, plus two-way interaction terms five-at-a-time:
model4<-lm(Pollution∼Temp+Industry+Population+Wind+Rain+Wet.
days+Wind:Rain+Wind:Wet.days+Industry:Wet.days+Industry:Rain+Rain:
Wet.days)
model5<-lm(Pollution∼Temp+Industry+Population+Wind+Rain+Wet.
days+Population:Rain+Temp:Population+Population:Wind+Temp:
Industry+Industry:Wind)
MULTIPLE REGRESSION 209
model6<-lm(Pollution∼Temp+Industry+Population+Wind+Rain+Wet.
days+Temp:Wind+Population:Wet.days+Temp:Rain+Temp:Wet.
days+Industry:Population)
Extracting only the interaction terms from the three models, we see:
Industry:Rain -1.616e-04 9.207e-04 -0.176 0.861891
Industry:Wet.days 2.311e-04 3.680e-04 0.628 0.534949
Wind:Rain 9.049e-01 2.383e-01 3.798 0.000690 ***
Wind:Wet.days -1.662e-01 5.991e-02 -2.774 0.009593 **
Rain:Wet.days 1.814e-02 1.293e-02 1.403 0.171318
Temp:Industry -1.643e-04 3.208e-03 -0.051 0.9595
Temp:Population 1.125e-03 2.382e-03 0.472 0.6402
Industry:Wind 2.668e-02 1.697e-02 1.572 0.1267
Population:Wind -2.753e-02 1.333e-02 -2.066 0.0479 *
Population:Rain 6.898e-04 1.063e-03 0.649 0.5214
Temp:Wind 1.261e-01 2.848e-01 0.443 0.66117
Temp:Rain -7.819e-02 4.126e-02 -1.895 0.06811 .
Temp:Wet.days 1.934e-02 2.522e-02 0.767 0.44949
Industry:Population 1.441e-06 4.178e-06 0.345 0.73277
Population:Wet.days 1.979e-05 4.674e-04 0.042 0.96652
Thenextstepmightbetoputallofthesignificantorclose-to-significantinteractionsinto
the same model, and see which survive:
model7<-lm(Pollution∼Temp+Industry+Population+Wind+Rain+Wet.
days+Wind:Rain+Wind:Wet.days+Population:Wind+Temp:Rain)
summary(model7)
Coefficients:
Estimate Std.Error t value Pr (>|t|)
(Intercept) 323.054546 151.458618 2.133 0.041226 *
Temp -2.792238 1.481312 -1.885 0.069153 .
Industry 0.073744 0.013646 5.404 7.44e-06 ***
Population 0.008314 0.056406 0.147 0.883810
Wind -19.447031 8.670820 -2.243 0.032450 *
Rain -9.162020 3.381100 -2.710 0.011022 *
Wet.days 1.290201 0.561599 2.297 0.028750 *
Temp:Rain 0.017644 0.027311 0.646 0.523171
Population:Wind -0.005684 0.005845 -0.972 0.338660
Wind:Rain 0.997374 0.258447 3.859 0.000562 ***
Wind:Wet.days -0.140606 0.053582 -2.624 0.013530 *
We certainly don’t need Temp:Rain
model8<-update(model7,∼.-Temp:Rain)
summary(model8)
or Population:Wind
210 STATISTICS:ANINTRODUCTIONUSINGR
model9<-update(model8,∼.-Population:Wind)
summary(model9)
Coefficients:
Estimate Std. Error t value Pr (>|t|)
(Intercept) 290.12137 71.14345 4.078 0.000281 ***
Temp -2.04741 0.55359 -3.698 0.000811 ***
Industry 0.06926 0.01268 5.461 5.19e-06 ***
Population -0.04525 0.01221 -3.707 0.000793 ***
Wind -20.17138 5.61123 -3.595 0.001076 **
Rain -7.48116 1.84412 -4.057 0.000299 ***
Wet.days 1.17593 0.54137 2.172 0.037363 *
Wind:Rain 0.92518 0.20739 4.461 9.44e-05 ***
Wind:Wet.days -0.12925 0.05200 -2.486 0.018346 *
Residualstandarderror:11.75on32degreesoffreedom
MultipleR-squared: 0.7996, AdjustedR-squared: 0.7495
F-statistic:15.96on8and32DF, p-value:3.51e-09Allthetermsin
model7aresignificant. Timeforacheckonthebehaviourofthemodel:
Therearetwosignificanttwo-wayinteractions,Wind:RainandWind:Wet.days.Itis
time to check the assumptions:
plot(model9)
MULTIPLE REGRESSION 211
ThevarianceisOK,butthereisastrongindicationofnon-normalityoferrors.Butwhat
aboutthehigher-orderinteractions?Onewaytoproceedistospecifytheinteractionlevel
using^3inthemodelformula,butifwedothis,we’llrunoutofdegreesoffreedomstraight
away.Asensibleoptionistofitthree-waytermsforthevariablesthatalreadyappearintwo-
way interactions: in our case, that is just one term, Wind:Rain:Wet.days:
model10<-update(model9,∼.+Wind:Rain:Wet.days)
summary(model10)
Coefficients:
Estimate Std. Error t value Pr (>|t|)
(Intercept) 278.464474 68.041497 4.093 0.000282 ***
Temp -2.710981 0.618472 -4.383 0.000125 ***
Industry 0.064988 0.012264 5.299 9.11e-06 ***
Population -0.039430 0.011976 -3.293 0.002485 **
Wind -7.519344 8.151943 -0.922 0.363444
Rain -6.760530 1.792173 -3.772 0.000685 ***
Wet.days 1.266742 0.517850 2.446 0.020311 *
Wind:Rain 0.631457 0.243866 2.589 0.014516 *
Wind:Wet.days -0.230452 0.069843 -3.300 0.002440 **
Wind:Rain:Wet.days 0.002497 0.001214 2.056 0.048247 *
Residualstandarderror:11.2on31degreesoffreedom
MultipleR-squared: 0.8236, AdjustedR-squared: 0.7724
F-statistic:16.09on9and31DF, p-value:2.231e-09
Thereisindeedamarginallysignificantthree-wayinteraction.Youshouldconfirmthat
there is no place for quadratic terms for either Temp or Wind in this model.
That’senoughfornow.I’msureyougettheidea.Multipleregressionisdifficult,time-
consuming,andalwaysvulnerabletosubjectivedecisionsaboutwhattoincludeandwhatto
leaveout.Thelinearmodellingconfirmstheearlyimpressionfromthetreemodel:forlow
levelsofindustry,theSO leveldependsinasimplewayonpopulation(peopletendtowant
2
tolivewheretheairisclean)andinacomplicatedwayondailyweather(asreflectedbythe
three-way interaction between wind, total rainfall and the number of wet days (i.e. on
rainfall intensity)).
FurtherReading
Claeskens,G.andHjort,N.L.(2008)ModelSelectionandModelAveraging,CambridgeUniversity
Press, Cambridge.
Draper,N.R. andSmith,H.(1981) Applied RegressionAnalysis,JohnWiley& Sons, NewYork.
Fox, J.(2002) AnR andS-Plus Companion toAppliedRegression,Sage,Thousand Oaks,CA.
Mosteller,F.andTukey,J.W.(1977)DataAnalysisandRegression,Addison-Wesley,Reading,MA.
11
Contrasts
Oneofthehardestthingsaboutlearningtodostatisticsisknowinghowtointerprettheoutput
produced by the model that you have fitted to data. The reason why the output is hard to
understandisthatitisbasedoncontrasts,andtheideaofcontrastsmaybeunfamiliartoyou.
Contrastsaretheessenceofhypothesistestingandmodelsimplification.Theyareusedto
comparemeansorgroupsofmeanswithothermeansorgroupsofmeans,inwhatareknown
assingledegreeoffreedomcomparisons.Therearetwosortsofcontrastswemightwantto
carry out:
(cid:129) contrastswehadplannedtocarryoutattheexperimentaldesignstage(thesearereferred
to as a priori contrasts)
(cid:129) contraststhatlookinterestingafterwehaveseentheresults(thesearereferredtoasa
posteriori contrasts)
Somepeopleareverysnootyaboutaposterioricontrasts,onthegroundsthattheywere
unplanned.Youarenotsupposedtodecidewhatcomparisonstomakeafteryouhaveseen
the analysis, but scientists do this all the time. The key point is that you should only do
contrastsaftertheANOVAhasestablishedthattherereallyaresignificantdifferencestobe
investigated.Itisbadpracticetocarryoutteststocomparethelargestmeanwiththesmallest
mean, if the ANOVA fails to reject the null hypothesis (tempting though this may be).
There are two important points to understand about contrasts:
(cid:129) there is a huge number of possible contrasts
(cid:129) there are only k 1 orthogonal contrasts
where k is the number of factor levels. Two contrasts are said to be orthogonal to one
another if the comparisons are statistically independent. Technically, two contrasts are
orthogonaliftheproductsoftheircontrastcoefficientssumtozero(weshallseewhatthis
means in a moment).
Let’stakeasimpleexample.Supposewehaveonefactorwithfivelevelsandthefactor
levelsarecalleda,b,c,dande.Letusstartwritingdownthepossiblecontrasts.Obviously
we could compare each mean singly with every other:
avs:b;avs:c;avs:d;avs:e;bvs:c;bvs:d;bvs:e;cvs:d;cvs:e;dvs:e
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
CONTRASTS 213
but we could also compare pairs of means:
fa;bgvs:fc;dg;fa;bgvs:fc;eg;fa;bgvs:fd;eg;fa;cgvs:fb;dg;fa;cgvs:fb;eg;:::
or triplets of means:
fa;b;cgvs:d;fa;b;cgvs:e;fa;b;dgvs:c;fa;b;dgvs:e;fa;c;dgvs:b;:::
or groups of four means:
fa;b;c;dgvs:e;fa;b;c;egvs:d;fb;c;d;egvs:a;fa;b;d;egvs:c;fa;b;c;egvs:d
I think you get the idea. There are absolutely masses of possible contrasts. In practice,
however, we should only compare things once, either directly or implicitly. So the two
contrastsavs.bandavs.cimplicitlycontrastbvs.c.Thismeansthatifwehavecarriedout
the two contrasts a vs. b and a vs. c then the third contrast b vs. c is not an orthogonal
contrastbecauseyouhavealreadycarrieditout,implicitly.Whichparticularcontrastsare
orthogonaldependsverymuchonyourchoiceofthefirstcontrasttomake.Supposethere
weregoodreasonsforcomparing{a,b,c,e}vs.d.Forexample,dmightbetheplaceboand
theotherfourmightbedifferentkindsofdrugtreatment,sowemakethisourfirstcontrast.
Becausek 1=4 we onlyhave three possiblecontrasts that are orthogonal to this. There
maybeapriorireasonstogroup{a,b}and{c,e},sowemakethisoursecondorthogonal
contrast.Thismeansthatwehavenodegreesoffreedominchoosingthelasttwoorthogonal
contrasts:theyhavetobeavs.bandcvs.e.Justrememberthatwithorthogonalcontrasts
you compare things only once.
ContrastCoefficients
Contrastcoefficientsareanumericalwayofembodyingthehypothesiswewanttotest.The
rules for constructing contrast coefficients are straightforward:
(cid:129) treatments to be lumped together get the same sign (plus or minus)
(cid:129) groups of means to be contrasted get opposite sign
(cid:129) factor levels to be excluded get a contrast coefficient of 0
(cid:129) the contrast coefficients must add up to 0
Supposethatwithourfive-levelfactorfa; b; c; d; egwewanttobeginbycomparingthe
fourlevels{a,b,c,e}withthesingleleveld.Alllevelsenterthecontrast,sononeofthe
coefficientsis0.Thefourterms{a,b,c,e}aregroupedtogethersotheyallgetthesamesign
(minus,forexample,althoughitmakesnodifferencewhichsignischosen).Theyaretobe
comparedtod,soitgetstheoppositesign(plus,inthiscase).Thechoiceofwhatnumeric
valuestogivethecontrastcoefficientsisentirelyuptoyou.Mostpeopleusewholenumbers
ratherthanfractions,butitreallydoesn’tmatter.Allthatmattersisthatthevalueschosen
sum to0.The positiveandnegativecoefficients have toaddup tothesame value. Inour
example,comparingfourmeanswithonemean,anaturalchoiceofcoefficientswouldbe
214 STATISTICS:ANINTRODUCTIONUSINGR
 1foreachof{a,b,c,e}and+4ford.Alternatively,wecouldhaveselected+0.25foreach
of {a, b, c, e} and  1 for d.
factorlevel: a b c d e
contrast1coefficients,c: -1 -1 -1 4 -1
Suppose the second contrast is to compare {a, b} with {c, e}. Because this contrast
excludesd,wesetitscontrastcoefficientto0.{a,b}getthesamesign(say,plus)and{c,e}
gettheoppositesign.Becausethenumberoflevelsoneachsideofthecontrastisequal(two
inbothcases)wecanusethesamenumericvalueforallthecoefficients.Thevalue1isthe
most obvious choice (but you could use 13.7 if you wanted to be perverse):
factorlevel: a b c d e
contrast2coefficients,c: 1 1 -1 0 -1
Thereareonlytwopossibilitiesfortheremainingorthogonalcontrasts:avs.bandcvs.e:
factorlevel: a b c d e
contrast3coefficients,c: 1 -1 0 0 0
contrast4coefficients,c: 0 0 1 0 -1
AnExampleofContrastsinR
TheexamplecomesfromthecompetitionexperimentweanalysedinChapter9inwhichthe
biomassofcontrolplantsiscomparedtothebiomassofplantsgrowninconditionswhere
competitionwasreducedinoneoffourdifferentways.Therearetwotreatmentsinwhich
the roots of neighbouring plants were cut (to 5cm depth or 10cm) and two treatments in
whichtheshootsofneighbouringplantswereclipped(25%or50%oftheneighbourscut
back to ground level; see p. 162).
comp<-read.csv("c:\\temp\\competition.csv")
attach(comp)
names(comp)
[1]"biomass" "clipping"
We start with the one-way ANOVA:
model1<-aov(biomass∼clipping)
summary(model1)
Df SumSq Mean Sq F value Pr(>F)
clipping 4 85356 21339 4.302 0.00875**
Residuals 25 124020 4961
Clipping treatment has a highly significant effect on biomass. But have we fully
understood the result of this experiment? I don’t think so. For example, which factor
levels had the biggest effect on biomass? Were all of the competition treatments signifi-
cantlydifferentfromthecontrols?Toanswerthesequestions,weneedtousesummary.lm:
CONTRASTS 215
summary.lm(model1)
Coefficients:
EstimateStd.Error t value Pr(>|t|)
(Intercept) 465.17 28.75 16.177 9.4e-15 ***
clippingn25 88.17 40.66 2.168 0.03987 *
clippingn50 104.17 40.66 2.562 0.01683 *
clippingr10 145.50 40.66 3.578 0.00145 **
clippingr5 145.33 40.66 3.574 0.00147 **
Residualstandarderror:70.43on25degreesoffreedom
MultipleR-squared: 0.4077, AdjustedR-squared: 0.3129
F-statistic:4.302on4and25DF, p-value:0.008752
Itlooksasifweneedtokeepallfiveparameters,becauseallfiverowsofthesummary
table have one or more significance stars. If fact, this is not the case. This example
highlights the major shortcoming of treatment contrasts (the default contrast method in
R):theydonotshowhowmanysignificantfactorlevelsweneedtoretainintheminimal
adequate model.
APrioriContrasts
In this experiment, there are several planned comparisons we should like to make. The
obviousplacetostartisbycomparingthecontrolplantsthatwereexposedtothefullrigours
of competition, with all of the other treatments.
levels(clipping)
[1] "control" "n25" "n50" "r10" "r5"
Thatistosay,wewanttocontrastthefirstlevelofclippingwiththeotherfourlevels.The
contrastcoefficients,therefore,wouldbe4, 1, 1, 1, 1.Thenextplannedcomparison
mightcontrasttheshoot-prunedtreatments(n25andn50)withtheroot-prunedtreatments
(r10andr5).Suitablecontrastcoefficientsforthiswouldbe0,1,1, 1, 1(becauseweare
ignoringthecontrolinthiscontrast).Athirdcontrastmightcomparethetwodepthsofroot-
pruning;0,0,0,1, 1.Thelastorthogonalcontrastwouldthereforehavetocomparethetwo
intensitiesofshoot-pruning:0,1, 1,0,0.Becausetheclippingfactorhasfivelevelsthere
are only 5 1=4 orthogonal contrasts.
Risoutstandinglygoodatdealingwithcontrasts,andwecanassociatethesefouruser-
specified a priori contrasts with the categorical variable called clipping like this:
contrasts(clipping)<-
cbind(c(4,-1,-1,-1,-1),c(0,1,1,-1,-1),c(0,0,0,1,-1),c(0,1,-1,0,0))
216 STATISTICS:ANINTRODUCTIONUSINGR
We can check that this has done what we wanted by typing
contrasts(clipping)
[,1] [,2] [,3] [,4]
control 4 0 0 0
n25 -1 1 0 1
n50 -1 1 0 -1
r10 -1 -1 1 0
r5 -1 -1 -1 0
which produces the matrix of contrast coefficients that we specified. Note that all the
columnsaddtozero(i.e.eachsetofcontrastcoefficientsiscorrectlyspecified).Notealso
thattheproductsofanytwoofthecolumnssumtozero(thisshowsthatallthecontrastsare
orthogonal, as intended): for example comparing contrasts 1 and 2 gives products
0+( 1)+( 1)+1+1=0.
Nowwecanrefitthemodelandinspecttheresultsofourspecifiedcontrasts,ratherthan
the default treatment contrasts:
model2<-aov(biomass∼clipping)
summary.lm(model2)
Coefficients:
Estimate Std.Errortvalue Pr (>|t|)
(Intercept) 561.80000 12.85926 43.688 <2e-16 ***
clipping1 -24.15833 6.42963 -3.757 0.000921 ***
clipping2 -24.62500 14.37708 -1.713 0.099128 .
clipping3 0.08333 20.33227 0.004 0.996762
clipping4 -8.00000 20.33227 -0.393 0.697313
Residualstandarderror:70.43on25degreesoffreedom
MultipleR-squared: 0.4077, AdjustedR-squared: 0.3129
F-statistic:4.302on4and25DF, p-value:0.008752
Insteadofrequiringfiveparameters(assuggestedbyourinitialtreatmentcontrasts),this
analysisshowsthatweneedonlytwoparameters:theoverallmean(561.8)andthecontrast
between the controls and the four competition treatments (p=0.000921). All the other
contrasts are non-significant. Notice than the rows are labelled by the variable name
clipping pasted to the contrast number (1 to 4).
TreatmentContrasts
We have seen how to specify our own customized contrasts (above) but we need to
understandthedefaultbehaviourofR.Thisbehaviouriscalledtreatmentcontrastsanditis
set like this:
options(contrasts=c("contr.treatment","contr.poly"))
The first argument, "contr.treatment", says we want to use treatment contrasts
(ratherthanHelmertcontrastsorsumcontrasts),whilethesecond,"contr.poly",setsthe
CONTRASTS 217
default method for comparing ordered factors (where the levels are semi-quantitative like
‘low’,‘medium’and‘high’).Weshallseewhattheseunfamiliartermsmeaninamoment.
Becausewehadassociatedourexplanatoryvariableclippingwithasetofuser-defined
contrasts(above),weneedtoremovethesetogetbacktothedefaulttreatmentcontrasts,like
this:
contrasts(clipping)<-NULL
Now let us refit the model and work out exactly what the summary is showing:
model3<-lm(biomass∼clipping)
summary(model3)
Coefficients:
EstimateStd. Error tvalue Pr(>|t|)
(Intercept) 465.17 28.75 16.177 9.4e-15 ***
clippingn25 88.17 40.66 2.168 0.03987 *
clippingn50 104.17 40.66 2.562 0.01683 *
clippingr10 145.50 40.66 3.578 0.00145 **
clippingr5 145.33 40.66 3.574 0.00147 **
Residualstandarderror:70.43on25degreesoffreedom
MultipleR-squared: 0.4077, AdjustedR-squared: 0.3129
F-statistic:4.302on4and25DF, p-value:0.008752
First things first. There are five rows in the summary table, labelled (Intercept),
clippingn25, clippingn50, clippingr10 and clippingr55. Why are there five
rows? There are five rows in the summary table because this model has estimated five
parametersfromthedata(oneforeachfactorlevel).Thisisaveryimportantpoint.Different
modelswillproducesummarytableswithdifferentnumbersofrows.Youshouldalwaysbe
awareofhowmanyparametersyourmodelisestimatingfromthedata,soyoushouldnever
be taken by surprise by the number of rows in the summary table.
Nextweneedtothinkwhattheindividualrowsaretellingus.Letusworkouttheoverall
mean value of biomass, to see if that appears anywhere in the table:
mean(biomass)
[1]561.8
No.Ican’tseethatnumberanywhere.Whataboutthefivetreatmentmeans?Weobtain
these with the extremely useful tapply function:
tapply(biomass,clipping,mean)
control n25 n50 r10 r5
465.1667 553.3333 569.3333 610.6667 610.5000
Thisisinteresting.Thefirsttreatmentmean(465.1667forthecontrols)doesappear(inrow
number1),butnoneoftheothermeansareshown.Sowhatdoesthevalueof88.17inrow
218 STATISTICS:ANINTRODUCTIONUSINGR
number2ofthetablemean?Inspectionoftheindividualtreatmentmeansletsyouseethat
88.17 is the difference between the mean biomass for n25 and the mean biomass for the
control.Whatabout104.17?Thatturnsouttobethedifferencebetweenthemeanbiomass
for n50 and the mean biomass for the control. Treatment contrasts are expressed as the
differencebetweenthemeanforaparticularfactorlevelandthemeanforthefactorlevelthat
comesfirstinthealphabet.Nowondertheyarehardtounderstandonfirstencounter.
Whydoesitallhavetobesocomplicated?What’swrongwithjustshowingthefivemean
values?Theanswerhastodowithparsimonyandmodelsimplification.Wewanttoknow
whether the differences between the means that we observe (e.g. using tapply) are
statisticallysignificant.Toassessthis,weneedtoknowthedifferencebetweenthemeans
and (crucially) the standard error of the difference between two means (see p. 91 for a
refresher on this). The summary.lm table is designed to make this process as simple as
possible.Itdoesthisbyshowingusjustonemean(theintercept)andalltheotherrowsare
differencesbetweenmeans.Likewise,inthesecondcolumn,itshowsusjustonestandard
error (the standard error of the mean for the intercept – the control treatment in this
example), and all ofthe other values arethe standard error of the difference between two
means.Sothevalueinthetoprow(28.75)ismuchsmallerthanthevaluesintheotherfour
rows (40.66) because the standard error of a mean is
rffiffiffiffi
s2
n
while the standard error of the difference between two means is
sffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi rffiffiffiffiffiffiffi
s2 s2 2s2
A  B 
n n n
A B
Youcanseethatforthesimplestcasewhere(ashere)wehaveequalreplicationinallfive
factorlevels(n=6)andweintendtopusffieffiffithepoolederrorvariances2,thestandarderrorof
thedifferencebetweentwomeansis 21:414 timesthesizeofthestandarderrorofa
mean. Let us check this out for our example
28.75*sqrt(2)
[1]40.65864
Sothatchecksout.Whatifitisnotconvenienttocomparethemeanswiththefactorlevel
thathappenstocomefirstinthealphabet?Thenallweneedtodoischangetheorderofthe
levels with a new factor declaration, in which we specify the order in which the factor
levelsaretobeconsidered.Youcanseeanexampleofthisonp.242.Alternatively,youcan
define the contrasts attributes of the factor, as we saw earlier in this chapter.
ModelSimplificationbyStepwiseDeletion
Model simplification with treatment contrasts requires that we aggregate non-significant
factorlevelsinastepwiseaposterioriprocedure.Theresultswegetbydoingthisareexactly
CONTRASTS 219
thesameasthoseobtainedbydefiningthecontrastattributesoftheexplanatoryvariableas
we did earlier.
Lookingdownthelistofparameterestimates,weseethatthemostsimilareffectsizesare
forr5andr10(theeffectsofrootpruningto5and10cm):145.33and145.5,respectively.
We shall begin by simplifying these to a single root-pruning treatment called root. The
trick is to use ‘levels gets’ to change the names of the appropriate factor levels. Start by
copying the original factor name:
clip2<-clipping
Now inspect the level numbers of the various factor level names:
levels(clip2)
[1]"control""n25" "n50" "r10" "r5"
Theplanistolumptogetherr10andr5underthesamename,‘root’.Thesearethefourth
and fifth levels of clip2 (count them from the left), so we write:
levels(clip2)[4:5]<- "root"
and to see what has happened we type:
levels(clip2)
[1]"control""n25" "n50" "root"
Weseethat"r10"and"r5"havebeenmergedandreplacedby"root".Thenextstepis
tofitanewmodelwithclip2inplaceofclipping,andtouseanovatotestwhetherthe
newsimplermodelwithonlyfourfactorlevelsissignificantlyworseasadescriptionof
the data:
model4<-lm(biomass∼clip2)
anova(model3,model4)
AnalysisofVarianceTable
Model1:biomass∼clipping
Model2:biomass∼clip2
Res.Df RSSDf SumofSq FPr(>F)
1 25124020
2 26124020-1-0.0833333 0.00001680.9968
220 STATISTICS:ANINTRODUCTIONUSINGR
As weexpected,thismodelsimplification wascompletelyjustified.Thenextstep isto
investigate the effects using summary:
summary(model4)
Coefficients:
EstimateStd.Error tvalue Pr(>|t|)
(Intercept) 465.17 28.20 16.498 2.72e-15 ***
clip2n25 88.17 39.87 2.211 0.036029 *
clip2n50 104.17 39.87 2.612 0.014744 *
clip2root 145.42 34.53 4.211 0.000269 ***
Residualstandarderror:69.07on26degreesoffreedom
MultipleR-squared: 0.4077, AdjustedR-squared: 0.3393
F-statistic:5.965on3and26DF, p-value:0.003099
Next, we want to compare the two shoot-pruning treatments with their effect sizes of
88.17and104.17.Theydifferby16.0(muchmorethantheroot-pruningtreatmentsdid),but
youwillnoticethatthestandarderrorofthedifferenceif39.87.Youwillsoongetusedto
doingttestsinyourheadandusingtheruleofthumbthattneeds tobebiggerthan 2for
significance. In this case, the difference would need to be at least 2×39.87=79.74 for
significance,sowepredictthatmergingthetwoshootpruningtreatmentswillnotcausea
significant reduction in the explanatory power of the model. We can lump these together
into a single shoot-pruning treatment as follows:
clip3<-clip2
levels(clip3)[2:3]<-"shoot"
levels(clip3)
[1]"control""shoot" "root"
Then we fit a new model with clip3 in place of clip2:
model5<-lm(biomass∼clip3)
anova(model4,model5)
AnalysisofVarianceTable
Model1:biomass∼clip2
Model2:biomass∼clip3
Res.Df RSSDfSumofSq FPr(>F)
1 26124020
2 27124788-1 -7680.1610.6915
Again, this simplification was fully justified. Do the root and shoot competition
treatments differ?
CONTRASTS 221
summary(model5)
Coefficients:
EstimateStd. Error tvalue Pr(>|t|)
(Intercept) 465.17 27.75 16.760 8.52e-16 ***
clip3shoot 96.17 33.99 2.829 0.008697 **
clip3root 145.42 33.99 4.278 0.000211 ***
Residualstandarderror:67.98on27degreesoffreedom
MultipleR-squared: 0.404, AdjustedR-squared: 0.3599
F-statistic:9.151on2and27DF, p-value:0.0009243
The two effect sizes differ by 145.42 96.17=49.25 with a standard error of 33.99
(givingt<2)soweshallbesurprisedifmergingthetwofactorlevelscausesasignificant
reduction in the model’s explanatory power:
clip4<-clip3
levels(clip4)[2:3]<-"pruned"
levels(clip4)
[1]"control""pruned"
Now we fit a new model with clip4 in place of clip3:
model6<-lm(biomass∼clip4)
anova(model5,model6)
AnalysisofVarianceTable
Model1:biomass∼clip3
Model2:biomass∼clip4
Res.Df RSSDfSumofSq F Pr(>F)
1 27124788
2 28139342-1 -145533.14890.08726.
Thissimplificationwasclosetosignificant,butweareruthless(p>0.05),soweaccept
the simplification. Now we have the minimal adequate model:
summary(model6)
Coefficients:
EstimateStd.Error tvalue Pr (>|t|)
(Intercept) 465.2 28.8 16.152 1.01e-15 ***
clip4pruned 120.8 32.2 3.751 0.000815 ***
Residualstandarderror:70.54on28degreesoffreedom
MultipleR-squared: 0.3345, AdjustedR-squared: 0.3107
F-statistic:14.07on1and28DF, p-value:0.0008149
Ithasjusttwoparameters:themeanforthecontrols(465.2)andthedifferencebetween
the control mean and the four other treatment means (465.2+120.8=586.0):
222 STATISTICS:ANINTRODUCTIONUSINGR
tapply(biomass,clip4,mean)
control pruned
465.1667585.9583
We know from the p value=0.000815 (above) that these two means are significantly
different, but just to show how it is done, we can make a final model7 that has no
explanatoryvariableatall(itfitsonlytheoverallmean).Thisisachievedbywritingy∼1in
the model formula (parameter 1, you will recall, is the intercept in R):
model7<-lm(biomass∼1)
anova(model6,model7)
AnalysisofVarianceTable
Model1:biomass∼clip4
Model2:biomass∼1
Res.Df RSSDfSumofSq F Pr(>F)
1 28139342
2 29209377-1 -7003514.0730.0008149***
Notethatthepvalueisexactlythesameasinmodel6.ThepvaluesinRarecalculated
such that they avoid the need for this final step in model simplification: they are ‘p on
deletion’ values.
It isinformative tocompare what we havejust doneby stepwise factor-level reduction
with what we did earlier by specifying the contrast attribute of the factor.
ContrastSumsofSquaresbyHand
ThekeypointtounderstandisthatthetreatmentsumofsquaresSSAisthesumofallk 1
orthogonal sums of squares. It is very useful to know which of the contrasts contributes
most to SSA, and to work this out, we compute a set of contrast sums of squares SSC
as follows:
(cid:3) (cid:4)
PcT 2
i i
n
SSC  i
Pc2
i
n
i
or, when all the sample sizes are the same for each factor level:
(cid:3) (cid:4)
1P 2
n
ciTi
SSC  X
1
c2
n i
The significance of a contrast is judged in the usual way by carrying out an F test to
compare the contrast variance with the pooled error variance, s2 from the ANOVA table.
CONTRASTS 223
Sinceallcontrastshaveasingledegreeoffreedom,thecontrastvarianceisequaltoSSC,so
the F test is just
SSC
F 
s2
The contrast is significant (i.e. the two contrasted groups have significantly different
means)ifthecalculatedvalueoftheteststatisticislargerthanthecriticalvalueofFwith1
andk(n 1)degrees offreedom.Wedemonstrate theseideas by continuingourexample.
Whenwedidfactor-levelreductionintheprevioussection,weobtainedthefollowingsums
ofsquaresforthefourorthogonalcontraststhatwecarriedout(thefigurescomefromthe
model-comparison ANOVA tables, above):
Contrast Sumofsquares
Controlvs.therest 70035
Shootpruningvs.rootpruning 14553
Intensevs.lightshootpruning 768
Deepvs.shallowrootpruning 0.083
Total
What we shall do now iscarry outthe calculations involved in the a priori contrasts in
ordertodemonstratethattheresultsareidentical.Thenewquantitiesweneedtocompute
thecontrastsumsofsquaresSSCarethetreatmenttotals,T ,intheformulaaboveandthe
i
contrastcoefficientsc.Itisworthwritingdownthecontrastcoefficientsagain(seep.214,
i
above):
Contrast Contrastcoefficients
Controlvs.therest 4  1  1  1  1
Shootpruningvs.rootpruning 0 1 1  1  1
Intensevs.lightshootpruning 0 1  1 0 0
Deepvs.shallowrootpruning 0 0 0 1  1
We use tapply to obtain the five treatment totals:
tapply(biomass,clipping,sum)
control n25 n50 r10 r5
2791 3320 3416 3664 3663
224 STATISTICS:ANINTRODUCTIONUSINGR
Forthefirstcontrast,thecontrastcoefficientsare4, 1, 1, 1,and 1.Theformulafor
SSC is therefore
(cid:5) (cid:6)
1 2
f 42791  13320  13416  13664  13663g
6
(cid:7) (cid:8)
1
 42  12  12  12  12
6
(cid:3) (cid:4)
1 2
 2899
6 233450
  70035
20 3:33333
6
In the second contrast, the coefficient for the control is zero, so we leave this out:
(cid:5) (cid:6) (cid:3) (cid:4)
1 2 1 2
f 13320 13416  13664  13663g  591
6 6

(cid:7) (cid:8)
1 4
  12  12  12  12
6 6
9702:25
 14553
0:666666
The third contrast ignores the root treatments, so:
(cid:5) (cid:6) (cid:3) (cid:4)
1 2 1 2
f 13320  13416g  96
6 6 256
1 (cid:7) (cid:8)

2

0:33333
768
  12 12
6 6
Finally, we compare the two root pruning treatments:
(cid:5) (cid:6) (cid:3) (cid:4)
1 2 1 2
f 13664  13663g 1
6 6 0:027778
1 (cid:7) (cid:8)

2

0:333333
0:083333
  12 12
6 6
Youwillseethatthesesumsofsquaresareexactlythesameasweobtainedbystepwise
factor-levelreduction.Sodon’tletanyonetellyouthattheseproceduresaredifferentorthat
one of them is superior to the other.
TheThreeKindsofContrastsCompared
YouareveryunlikelytoneedtouseHelmertcontrastsorsumcontrastsinyourwork,but
ifyouneedtofindoutaboutthem,theyareexplainedindetailinTheRBook(Crawley,
2013).
CONTRASTS 225
Reference
Crawley, M.J.(2013) The RBook,2ndedn,JohnWiley & Sons,Chichester.
FurtherReading
Ellis, P.D. (2010) The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the
Interpretation ofResearch Results, CambridgeUniversity Press, Cambridge.
Miller, R.G. (1997) Beyond ANOVA:Basicsof AppliedStatistics,Chapman& Hall, London.
12
Other Response Variables
Uptonow,theresponsevariablehasbeenacontinuousrealnumbersuchasaweight,length
or concentration. We made several important assumptions about the behaviour of the
response variable, and it is worth reiterating those assumptions here, ranked in order of
importance:
(cid:129) random sampling
(cid:129) constant variance
(cid:129) normal errors
(cid:129) independent errors
(cid:129) additive effects
Sofar,whenwefoundthatoneormoreoftheassumptionswaswrong,ourtypicalresort
wastransformationoftheresponsevariable,coupledperhapswithtransformationofoneor
more of the explanatory variables.
Inourlineofwork,weoftenmeetwithresponsevariableswherethekeyassumptionsare
rarelyifevermet.Inthesecases,itissensibletolookforalternativestotransformationthat
might improve our ability to model these systems effectively. In this book we cover four
new kinds of response variable that are very common in practice:
(cid:129) count data
(cid:129) proportion data
(cid:129) binary response data
(cid:129) age-at-death data
allofwhichroutinelyfailtheassumptionsaboutconstancyofvarianceandnormalityof
errors.Itturnsoutthatthesedifferentkindsofresponsevariableshavemoreincommonthan
youmightfirstimagine,andthattheycanallbedealtwithintheframeworkofgeneralized
linear models (GLMs). These models allow variance to be non-constant and errors to be
non-normally distributed. It is worth noting, however, that they still assume random
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
OTHERRESPONSE VARIABLES 227
samplingandindependenceoferrors. Effects inGLMsmaybeadditiveormultiplicative,
depending on the circumstances.
Webeginbythinkingaboutvariance.Countdataarewholenumbers(integers)soyoucan
seeimmediatelythatwhenthemeanislow,thedataarelikelytoconsistonlyofzeros,ones
andtwos,withtheoddthreeoffourthrownin.Thisbeingthecase,thevarianceofcount
dataisboundtobelowwhenthemeanislow(recallthatvarianceisthesumofthesquares
of the departures of the counts from the mean count, divided by the degrees of freedom).
However,whenthemeanofcountdataishigh,therangeofindividualcountscanbefrom
zerotopotentiallyverylargenumbers,andwhenweworkouttheresidualsandsquarethem
wecanexpect toobtainverylargenumbers,andaconsequentlyhighvariance.Forcount
data,therefore,thevarianceisexpectedtoincreasewiththemean,ratherthanbeingconstant
asassumedinlinearmodels.Forthekindofcountdatawearetalkingabouthere,weknow
thenumberoftimesthatsomethinghappened(lightningstrikes,cellsonamicroscopeslide,
insects on a leaf), but not the number of times it did not happen.
Forproportiondatabasedwetypicallyhaveacountofthenumberofindividualsdoing
onethingandanothercountofthenumbernotdoingthatthing.Boththesecountsarefreeto
varyandbothcountsareimportantinanalysingthedataproperly.Ingeneralthenumbers
doingthethinginquestionareknownassuccessesandthenumbersnotdoingitareknown
as failures. This can be rather macabre, as when the response variable is the number of
people dying in a medical trail. Examples of proportion data based on counts are:
Successes Failures
dead alive
female male
diseased healthy
occupied unoccupied
pollinated notpollinated
adult juvenile
Youcanseethatthiskindofproportiondatabasedoncountsarisesinagreatmanykinds
ofcircumstances.Letusthinkaboutthevarianceofproportiondatalikethis.Ifthesuccess
rateis100%thenalltheindividualsarealikeandthevarianceiszero.Again,ifthesuccess
rate is zero, then all the individuals are alike and the variance is zero. If, however, the
success rate is intermediate (50%, say) then some of the individuals are in one class and
someareintheother,sovarianceishigh.Thismeansthatunlikecountdata(above)where
the variance increased monotonically with the mean, for proportion data the variance isa
humpedfunctionofthemean.Thebinomialdistributionisanimportantexampleofthekind
ofdistributionusedintheanalysisofproportiondata:ifpistheprobabilityofsuccessandn
is the number of trails, then the mean number of successes is np and the variance in the
numberofsuccessesisnp(1 p).Asyoucansee,thevarianceis0whenp=1and0when
p=0, reaching a peak when p=0.5.
228 STATISTICS:ANINTRODUCTIONUSINGR
Anothersortofproportiondata(suchaspercentagecoverdatafromplantecology),isnot
basedoncounts,butinvolvescontinuousnumbersthatareboundedbothaboveandbelow
(for instance, you cannot have negative percentage cover, or cover values that are greater
than100%).Thesekindsofproportiondataarearcsinetransformedpriortoanalysis,then
analysed using linear models (see p. 257).
Averyparticularkindofresponsevariableisanalysedbymedicalresearchers:thisisthe
age at death. The research questions centre on the effect of a particular treatment on
(hopefully)increasingtheageatdeathofthepatientsreceivingthistreatmentcomparedto
patientsreceivingtheplacebo(thecontrols).Age-at-deathdataarenotoriousfortheirnon-
constantvariance.Wesawthatforcountdatathevarianceincreaseswiththemean,butfor
age-at-deathdatathesituationisevenmoreextremethanthis:thevarianceincreaseswith
the square of the mean.
Herearegraphsofthevarianceasafunctionofthemeanforfourcontrastingkindsof
response variable: data suitable for analysis using linear models (constant variance, top
left);countdata(linearlyincreasingvariance,topright);proportiondata(humpedvariance
meanrelationship,bottomleft)andage-at-deathdata(quadraticallyincreasingvariance,
bottom right).
IntroductiontoGeneralizedLinearModels
A generalized linear model has three important properties:
(cid:129) the error structure
(cid:129) the linear predictor
(cid:129) the link function
OTHERRESPONSE VARIABLES 229
Thesearealllikely tobeunfamiliar concepts.The ideasbehindthem arestraightforward,
however, and it is worth learning what each of the concepts involves.
TheErrorStructure
Up to this point, we have dealt with the statistical analysis of data with normal errors. In
practice, however, many kinds of data have non-normal errors:
(cid:129) errors that are strongly skewed
(cid:129) errors that are kurtotic
(cid:129) errors that are strictly bounded (as in proportions)
(cid:129) errors that cannot lead to negative fitted values (as in counts)
Inthepast,theonlytoolsavailabletodealwiththeseproblemsweretransformationofthe
response variable or the adoption of non-parametric methods. A GLM allows the specifi-
cation of a variety of different error distributions:
(cid:129) Poisson errors, useful with count data
(cid:129) binomial errors, useful with data on proportions
(cid:129) gamma errors, useful with data showing a constant coefficient of variation
(cid:129) exponential errors, useful with data on time to death (survival analysis)
Theerrorstructureisdefinedbymeansofthefamilydirective,usedaspartofthemodel
formula like this:
glm(y∼z,family=poisson)
which means that the response variable y has Poisson errors. Or
glm(y∼z,family=binomial)
which means that the response is binary, and the model has binomial errors. As with
previous models, the explanatory variable z can be continuous (leading to a regression
analysis)orcategorical(leadingtoanANOVA-likeprocedurecalledanalysisofdeviance,
as described below).
TheLinearPredictor
Thestructureofthemodelrelateseachobservedyvaluetoapredictedvalue.Thepredicted
value is obtained by transformation of the value emerging from the linear predictor. The
linearpredictor,η(eta),isalinearsumoftheeffectsofoneormoreexplanatoryvariables,x j,
and is what we see when we ask for summary.lm:
Xp
η  x β
i ib j
j1
230 STATISTICS:ANINTRODUCTIONUSINGR
where the xs are the values of the p different explanatory variables, and the βs are the
(usually) unknown parameters to be estimated from the data. The right-hand side of the
equation is called the linear structure.
Thereareasmanytermsinthelinearpredictorasthereareparameters,p,tobeestimated
fromthedata. Thuswith asimpleregression, thelinear predictor isthesum oftwoterms
whose parameters are the intercept and the slope. With a one-way ANOVA with four
treatments,thelinearpredictoristhesumoffourtermsleadingtotheestimationofthemean
foreachlevelofthefactor.Iftherearecovariatesinthemodel,theyaddonetermeachtothe
linearpredictor(theslopeofeachrelationship).InteractiontermsinafactorialANOVAadd
oneormoreparameterstothelinearpredictor,dependinguponthedegreesoffreedomof
each factor (e.g. there would be three extra parameters for the interaction between a two-
level factor and a four-level factor, because (2 1)×(4–1)=3).
Thelinearpredictorcanbeinspectedbytypingsummary.lm:thereareasmanyrowsas
thereareparametersinyourmodel,andforeveryparameteryougettheeffectsizeandthe
standarderroroftheeffectinthefirsttwocolumns.Theothercolumnsarelessimportant
because you could easily work them out yourself: the t value, the p value and the
significance stars.
Whatyouwillfinddifficultatfirstisknowingexactlywhateffectsarebeingshownon
eachrow.Inananalysisofcovariance,forexample,thetoprowwillcontainaninterceptand
the second row might contain a slope, but the other rows will all be differences between
interceptsand/ordifferencesbetweenslopes(i.e.thereisonlyoneslopeinthetableandonly
one intercept, no matter how many slopes or intercepts there are in the fitted model).
FittedValues
OneofthegreatstrengthsofGLMsisthatdifferentmodelscanbecomparedonthesame
scale of measurement as the response variable. Up to now, we have used variance to
measurethelackoffitbetweenthemodelandthedata:thiswasthesumofthesquaresofthe
differencPebetweentheresponsevariableyandthefittedvaluespredictedbythemodel^y,
whichis y ^y2dividedbythedegreesoffreedom.Nowifyoufitadifferentmodel,say
Plog(y)=a+bx, then obviously the variance is completely different because it is based on
log y ^y2. This makes model comparison difficult, because there is no common
currencyformeasuringthefitofthetwomodelstothedata.InaGLM,however,wealways
compare y and^y on the same scale on which the response was measured (as a count, for
instance,orasaproportionbasedontwocounts).Thismakesmodelcomparisonmuchmore
straightforward.
AGeneralMeasureofVariability
The difference is that the measure of lack of fit of the model to the data depends on the
context.Wegiveitthenew,moregeneralnameofdeviance.Thetechnicaldefinitionwon’t
mean much to you at this stage, but here it is:
deviance 2loglikelihood
where the log likelihood depends on the model given the data. We don’t need to unlearn
anything, because deviance is the same as variance when we have constant variance and
OTHERRESPONSE VARIABLES 231
normalerrors(asinlinearregression,ANOVAorANCOVA).Butforcountdatawedoneeda
differentmeasureoflackoffit(itisbasedonylog y=^yratherthanon y ^y2)andweneeda
different definition of deviance for proportion data, and so on. But the point is that for
measuringthefit,wecompareyand^y onthesameoriginaluntransformedscale.Weshall
discussthevariousdeviancemeasuresinmoredetailinthefollowingchapters,butsothatyou
cancomparethemwithoneanother,herearethemainmeasuresoflackoffitside-by-side:
Model Deviance Error Link
X
linear y  ^y2 Gaussian identity
(cid:2) (cid:3)
loglinear X y Poisson log
2 ylog
^y
(cid:2) (cid:3) (cid:2) (cid:3)
logistic X y n y binomial logit
2 ylog  n ylog
^y n ^y
(cid:2) (cid:3)
gamma X y ^y y gamma reciprocal
2  log
y ^y
Todeterminethefitofagivenmodel,aGLMevaluatesthelinearpredictorforeachvalue
of the response variable, then back-transforms the predicted value to compare it with
the observed value of y. The parameters are then adjusted, and the model refitted on the
transformedscaleinaniterativeprocedureuntilthefit stopsimproving.Itwilltake some
time before you understand what is going on here, and why it is so revolutionary. Don’t
worry; it will come with practice.
TheLinkFunction
OneofthedifficultthingstograspaboutGLMsistherelationshipbetweenthevaluesofthe
responsevariable(asmeasuredinthedataandpredictedbythemodelasfittedvalues)and
the linear predictor.
The transformation to be employed is specified in the link function. The fitted value is
computedbyapplyingthereciprocalofthelinkfunction,inordertogetbacktotheoriginal
scaleofmeasurementoftheresponsevariable.Thus,withaloglink,thefittedvalueisthe
antilogofthelinearpredictor,andwiththereciprocallink,thefittedvalueisthereciprocal
of the linear predictor.
Thethingtorememberisthat thelinkfunctionrelates themean value ofytoitslinear
predictor. In symbols, this means that:
ηg μ
whichissimple,butneedsthinkingabout.Thelinearpredictor,η,emergesfromthelinear
modelasasumofthetermsforeachofthepparameters.Thisisnotavalueofy(exceptin
the special case of the identity link that we have been using (implicitly) up to now). The
valueofηisobtainedbytransformingthevalueofybythelinkfunction,andthepredicted
value of y is obtained by applying the inverse link function to η.
232 STATISTICS:ANINTRODUCTIONUSINGR
The most frequently used link functions are shown below. An important criterion in
the choice of link function is to ensure that the fitted values stay within reasonable
bounds. We would want to ensure, for example, that counts were all greater than or
equal to zero (negative count data would be nonsense). Similarly, if the response
variablewastheproportionofindividualswhodied,thenthefittedvalueswouldhaveto
liebetween0and1(fittedvaluesgreaterthan1orlessthan0wouldbemeaningless).In
thefirstcase,aloglinkisappropriatebecausethefittedvaluesareantilogsofthelinear
predictor, and all antilogs are greater than or equal to zero. In the second case, the logit
linkisappropriatebecausethefittedvaluesarecalculatedastheantilogsofthelogodds,
log(p/q).
Byusingdifferentlinkfunctions,theperformanceofavarietyofmodelscanbecompared
directly. The total deviance is the same in each case, and we can investigate the conse-
quences of altering our assumptions about precisely how a given change in the linear
predictorbringsaboutaresponseinthefittedvalueofy.Themostappropriatelinkfunction
is the one which produces the minimum residual deviance.
CanonicalLinkFunctions
The canonical link functions are the default options employed when a particular error
structure is specified in the family directive in the model formula. Omission of a link
directive means that the following settings are used:
Error Canonicallink
gaussian identity
poisson log
binomial logit
Gamma reciprocal
You should try to memorize these canonical links and to understand why each is
appropriatetoitsassociatederrordistribution.Notethatonlygammaerrorshaveacapital
initial letter in R.
Choosing between using a link function (e.g. log link) in a GLM and transforming the
responsevariable(i.e.havinglog(y)astheresponsevariableratherthany)andusingalinear
model takes a certain amount of experience.
As you read thefollowingfour chapters youwill graduallybecome more familiarwith
thesenewconcepts.Ofcoursetheyarehardtounderstandatfirst,anditwouldbewrongto
pretendotherwise.Buttheydobecomeeasierwithpractice.Thekeyistounderstandthat
whenvarianceisnotconstantandwhentheerrorsarenotnormallydistributed,wehaveto
do something about it. And using a generalized linear model instead of a linear model is
often the best solution. Learning about deviance, link functions and linear predictors is a
small price to pay.
OTHERRESPONSE VARIABLES 233
Akaike’sInformationCriterion(AIC)asaMeasureoftheFitofaModel
Unexplainedvariationisboundtogodownwitheveryparameteraddedtothemodel.The
more parameters that there are in the model, the better will be the fit. You could obtain a
perfectfitifyouhadaseparateparameterforeverydatapoint(thesaturatedmodel;p.195),
butthismodelwouldhaveabsolutelynoexplanatorypower.Thereisalwaysgoingtobea
trade-offbetweenthegoodnessoffitandthenumberofparametersrequiredbyparsimony.
Whatwewouldliketodoistojudgetheusefulnessofeachparameter.Asimplewayof
doingthisistopenalizeeachparameter,andonlyallowtheparametertostayinthemodelif
it more than pays for itself (i.e. if the unexplained variation does down by more than the
penalty). When comparing two models, the smaller the AIC, the better the fit. This is the
basisofautomatedmodelsimplificationusingstep.AICusesapenaltyof2perparameter,
so for a given model AIC is calculated as
AICdeviance2p
andwhenthedeviancegoesdownbylessthan2,theninclusionoftheextraparameterisnot
justified. Other systems (like the Bayesian Information Criterion, BIC) employ stiffer
penaltiesandsotypicallyleadtofewerparametersbeingincludedintheminimaladequate
model(e.g.BICpenalizeseachparameterbylog(n)wherenisthenumberofobservations).
FurtherReading
Aitkin,M.,Francis,B.,Hinde,J.andDarnell,R.(2009)StatisticalModellinginR,ClarendonPress,
Oxford.
McCullagh, P. and Nelder, J.A. (1989) Generalized Linear Models, 2nd edn, Chapman & Hall,
London.
McCulloch,C.E.andSearle,S.R.(2001)Generalized,LinearandMixedModels,JohnWiley&Sons,
NewYork.
13
Count Data
Up to this point, the response variables have all been continuous measurements such as
weights,heights,lengths,temperaturesandgrowthrates.Agreatdealofthedatacollectedby
scientists, medical statisticians and economists, however, is in the form of counts (whole
numbersorintegers).Thenumberofindividualswhodied,thenumberoffirmsgoingbankrupt,
thenumberofdaysoffrost,thenumberofredbloodcellsonamicroscopeslide,orthenumber
ofcratersinasectoroflunarlandscapeareallpotentiallyinterestingvariablesforstudy.With
count data, the number 0 often appears as a value of the response variable (consider, for
example,whata0wouldmeaninthecontextoftheexamplesjustlisted).Inthischapterwedeal
withdataonfrequencies,wherewecounthowmanytimessomethinghappened,butwehave
nowayofknowinghowoftenitdidnothappen(e.g.lighteningstrikes,bankruptcies,deaths,
births).Thisisincontrastwithcountdataonproportions,whereweknowthenumberdoinga
particularthing,butalsothenumbernotdoingthatthing(e.g.theproportiondying,sexratiosat
birth,proportionsofdifferentgroupsrespondingtoaquestionnaire).
Straightforward linear regression methods (assuming constant variance and normal
errors) are not appropriate for count data for four main reasons:
(cid:129) the linear model might lead to the prediction of negative counts
(cid:129) the variance of the response variable is likely to increase with the mean
(cid:129) the errors will not be normally distributed
(cid:129) zeros are difficult to handle in transformations
InR,countdataarehandledveryelegantlyinageneralizedlinearmodelbyspecifying
family=poissonwhichusesPoissonerrorsandtheloglink(seeChapter12).Theloglink
ensuresthatallthefittedvaluesarepositive,whilethePoissonerrorstakeaccountofthefact
that the data are integer and have variances that are equal to their means.
ARegressionwithPoissonErrors
Thisexamplehasacount(thenumber ofreported cancer cases peryear perclinic) asthe
responsevariable,andasinglecontinuousexplanatoryvariable(thedistancefromanuclear
plant to the clinic in kilometres). The question is whether or not proximity to the reactor
affects the number of cancer cases.
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
clusters<-read.csv("c:\\temp\\clusters.csv")
attach(clusters)
names(clusters)
[1]"Cancers" "Distance"
plot(Distance,Cancers,pch=21,bg="lightblue")
0 20 40
Distance
srecnaC
6
5
4
3
2
1
0
COUNT DATA 235
60 80 100
Asistypicalofcountdata,thevaluesoftheresponsedon’tclusteraroundaregression
line,buttheyaredispersedindiscreterowswithinaroughlytriangulararea.Asyoucansee
therearelotsofzerosallthewayfromdistancesof0to100km.Theonlyreallylargecount
(6)wasveryclosetothesource.Thereseemstobeadownwardtrendincancercaseswith
distance. But is the trend significant? To find out, we do a regression of cases against
distance, using a GLM with Poisson errors:
model1<-glm(Cancers∼Distance,poisson)
summary(model1)
Coefficients:
Estimate Std.Error z value Pr(>|z|)
(Intercept) 0.186865 0.188728 0.990 0.3221
Distance -0.006138 0.003667 -1.674 0.0941 .
(Dispersionparameterforpoissonfamilytakentobe1)
Nulldeviance:149.48 on93 degreesoffreedom
Residualdeviance:146.64 on92 degreesoffreedom
AIC:262.41
NumberofFisherScoringiterations:5
236 STATISTICS:ANINTRODUCTIONUSINGR
The trend does not look to be significant, but we need first to look at the residual
deviance. It is assumed that this is the same as the residual degrees of freedom. The
fact that residual deviance is larger than residual degrees of freedom indicates that
we have overdispersion (extra, unexplained variation in the response). We compen-
sate for the overdispersion by refitting the model using quasipoisson rather than
Poisson errors:
model2<-glm(Cancers∼Distance,quasipoisson)
summary(model2)
Coefficients:
Estimate Std.Error t value Pr(>|t|)
(Intercept) 0.186865 0.235364 0.794 0.429
Distance -0.006138 0.004573 -1.342 0.183
(Dispersionparameterforquasipoissonfamilytakentobe1.555271)
Nulldeviance:149.48 on93 degreesoffreedom
Residualdeviance:146.64 on92 degreesoffreedom
AIC:NA
NumberofFisherScoringiterations:5
Compensating for the overdispersion has increased the p value to 0.183, so there is no
compellingevidence tosupport theexistence of atrend incancer incidence with distance
fromthenuclearplant.Todrawthefittedmodelthroughthedata,youneedtounderstand
that the GLM with Poisson errors uses the log link, so the parameter estimates and the
predictions from the model (the ‘linear predictor’ above) are in logs, and need to be
antilogged before the (non-significant) fitted line is drawn.
We want the x values to go from 0 to 100, so we can put:
xv<-0:100
Todrawattentiontotherelationshipbetweenthelinearpredictorandthevaluesofy
we want to draw on our scatterplot, we shall work out the y values long-hand. The
intercept is 0.186865 and the slope is  0.006138 (see above), so the linear predictor
gives yv as:
yv<-0.186865-0.006138*xv
Theimportantpointtobearinmindisthatyvisonalogarithmicscale.Wewanttoploty
(the raw numbers of cases, not their logarithms) so we need to take antilogs (to back-
transform, in other words):
y<-exp(yv)
lines(xv,y,col="red")
COUNT DATA 237
The red fitted line is curved, but very shallow. The trend is not significant (p>0.18).
Once youhaveunderstoodtherelationshipbetween thelinearpredictor,thelink function
and the predicted count data, y, you can speed up the curve drawing with the predict
function, because you can specify type="response", which carries out the back-
transformation automatically:
y<-predict(model2,list(Distance=xv),type="response")
lines(xv,y,col="red")
AnalysisofDeviancewithCountData
The response variable is a count of infected blood cells per mm2 on microscope slides
prepared from randomly selected individuals. The explanatory variables are smoker
(alogicalvariablewithvalues‘yes’or‘no’),age(threelevels:under20,21to59,60and
over), sex (male or female) and a body mass score (three levels: normal, overweight,
obese).
count<-read.csv("c:\\temp\\cells.csv")
attach(count)
names(count)
[1] "cells" "smoker" "age" "sex" "weight"
Itisalwaysagoodideawithcountdatatogetafeelfortheoverallfrequencydistribution
of counts using table:
table(cells)
0 1 2 3 4 5 6 7
314 75 50 32 18 13 7 2
238 STATISTICS:ANINTRODUCTIONUSINGR
Most subjects (314 of them) showed no damaged cells, and the maximum of 7 was
observed in just two patients. We begin data inspection by tabulating the main effect
means:
tapply(cells,smoker,mean)
FALSE TRUE
0.5478723 1.9111111
tapply(cells,weight,mean)
normal obese over
0.5833333 1.2814371 0.9357143
tapply(cells,sex,mean)
female male
0.6584507 1.2202643
tapply(cells,age,mean)
mid old young
0.8676471 0.7835821 1.2710280
It looks as if smokers have a substantially higher mean count than non-smokers,
that overweight and obese subjects had higher counts than those of normal weight,
males had a higher count that females, and young subjects had a higher mean count
than middle-aged or older people. We need to test whether any of these differences
are significant and to assess whether there are any interactions between the
explanatory variables:
model1<-glm(cells∼smoker*sex*age*weight,poisson)
summary(model1)
You should scroll down to the bottom of the (voluminous) output to find the
residual deviance and residual degrees of freedom, because we need to test for
overdispersion:
Nulldeviance:1052.95 on510 degreesoffreedom
Residualdeviance: 736.33 on477 degreesoffreedom
AIC:1318
NumberofFisherScoringiterations:6
COUNT DATA 239
The residual deviance (736.33) is much greater than the residual degrees of freedom
(477), indicating substantial overdispersion, so before interpreting any of the effects, we
should refit the model using quasipoisson errors:
model2<-glm(cells∼smoker*sex*age*weight,quasipoisson)
summary(model2)
Coefficients:(2notdefinedbecauseofsingularities)
EstimateStd. Error t value Pr(>|t|)
(Intercept) -0.8329 0.4307 -1.934 0.0537 .
smokerTRUE -0.1787 0.8057 -0.222 0.8246
sexmale 0.1823 0.5831 0.313 0.7547
ageold -0.1830 0.5233 -0.350 0.7267
ageyoung 0.1398 0.6712 0.208 0.8351
weightobese 1.2384 0.8965 1.381 0.1678
weightover -0.5534 1.4284 -0.387 0.6986
smokerTRUE:sexmale 0.8293 0.9630 0.861 0.3896
smokerTRUE:ageold -1.7227 2.4243 -0.711 0.4777
smokerTRUE:ageyoung 1.1232 1.0584 1.061 0.2892
sexmale:ageold -0.2650 0.9445 -0.281 0.7791
sexmale:ageyoung -0.2776 0.9879 -0.281 0.7788
smokerTRUE:weightobese 3.5689 1.9053 1.873 0.0617 .
smokerTRUE:weightover 2.2581 1.8524 1.219 0.2234
sexmale:weightobese -1.1583 1.0493 -1.104 0.2702
sexmale:weightover 0.7985 1.5256 0.523 0.6009
ageold:weightobese -0.9280 0.9687 -0.958 0.3386
ageyoung:weightobese -1.2384 1.7098 -0.724 0.4693
ageold:weightover 1.0013 1.4776 0.678 0.4983
ageyoung:weightover 0.5534 1.7980 0.308 0.7584
smokerTRUE:sexmale:ageold 1.8342 2.1827 0.840 0.4011
smokerTRUE:sexmale:ageyoung -0.8249 1.3558 -0.608 0.5432
smokerTRUE:sexmale:weightobese -2.2379 1.7788 -1.258 0.2090
smokerTRUE:sexmale:weightover -2.5033 2.1120 -1.185 0.2365
smokerTRUE:ageold:weightobese 0.8298 3.3269 0.249 0.8031
smokerTRUE:ageyoung:weightobese -2.2108 1.0865 -2.035 0.0424 *
smokerTRUE:ageold:weightover 1.1275 1.6897 0.667 0.5049
smokerTRUE:ageyoung:weightover -1.6156 2.2168 -0.729 0.4665
sexmale:ageold:weightobese 2.2210 1.3318 1.668 0.0960 .
sexmale:ageyoung:weightobese 2.5346 1.9488 1.301 0.1940
sexmale:ageold:weightover -1.0641 1.9650 -0.542 0.5884
sexmale:ageyoung:weightover -1.1087 2.1234 -0.522 0.6018
smokerTRUE:sexmale:ageold:weightobese -1.6169 3.0561 -0.529 0.5970
smokerTRUE:sexmale:ageyoung:weightobese NA NA NA NA
smokerTRUE:sexmale:ageold:weightover NA NA NA NA
smokerTRUE:sexmale:ageyoung:weightover 2.4160 2.6846 0.900 0.3686
(Dispersionparameterforquasipoissonfamilytakentobe1.854815)
Nulldeviance:1052.95 on510 degreesoffreedom
Residualdeviance: 736.33 on477 degreesoffreedom
AIC:NA
NumberofFisherScoringiterations:6
The first thing to notice is that there are NA (missing value) symbols in the table
of the linear predictor (the message reads Coefficients: (2 not defined
because of singularities). This is the first example we have met of aliasing
240 STATISTICS:ANINTRODUCTIONUSINGR
(p. 16): these symbols indicate that there are no data in the dataframe from which to
estimate two of the terms in the four-way interaction between smoking, sex, age and
weight.
It does look as if there might be three-way interaction between smoking, age and
obesity (p=0.0424). With a complicated model like this, it is a good idea to speed up
the early stages of model simplification by using the step function, but this is not
available with quasipoisson errors, so we need to work through the analysis long-
land.Startbyremovingthealiasedfour-wayinteraction,thentryremovingwhatlooks
(fromthepvalues)tobetheleastsignificantthree-wayinteraction,thesex–age–weight
interaction:
model3<-update(model2,∼.-smoker:sex:age:weight)
model4<-update(model3,∼.-sex:age:weight)
anova(model4,model3,test="F")
AnalysisofDevianceTable
Resid.Df Resid. Dev Df Deviance FPr(>F)
1 483 745.31
2 479 737.87 4 7.4416 1.0067 0.4035
That model simplification was not significant (p=0.4035) so we leave it out and try
deleting the next least significant interaction
model5<-update(model4,∼.-smoker:sex:age)
anova(model5,model4,test="F")
Again, that was not significant, so we leave it out.
model6<-update(model5,∼.-smoker:age:weight)
anova(model6,model5,test="F")
Despite one-star significance for one of the interaction terms, this was not significant
either, so we leave it out.
model7<-update(model6,∼.-smoker:sex:weight)
anova(model7,model6,test="F")
That is the last of the three-way interactions, so we can start removing the two-way
interactions, starting, as usual, with the least significant:
model8<-update(model7,∼.-smoker:age)
anova(model8,model7,test="F")
Not significant. Next:
model9<-update(model8,∼.-sex:weight)
anova(model9,model8,test="F")
COUNT DATA 241
Not significant. Next:
model10<-update(model9,∼.-age:weight)
anova(model10,model9,test="F")
Not significant. Next:
model11<-update(model10,∼.-smoker:sex)
anova(model11,model10,test="F")
Not significant. Next:
model12<-update(model11,∼.-sex:age)
anova(model12,model11,test="F")
AnalysisofDevianceTable
Resid.DfResid. Dev Df Deviance F Pr(>F)
1 502 791.59
2 500 778.69 2 12.899 3.48050.03154*
Thatissignificantsothesex–ageinteractionneedstostayintthemodel.Whataboutthe
smoker–weight interaction? We need to compare it with model11 (not model12):
model13<-update(model11,∼.-smoker:weight)
anova(model13,model11,test="F")
AnalysisofDevianceTable
Resid. Df Resid. Dev Df Deviance F Pr(>F)
1 502 790.08
2 500 778.69 2 11.395 3.0747 0.04708*
Thatissignificanttoo,soweretainit.Itappearsthatmodel11istheminimaladequate
model.Therearetwo-wayinteractionsbetweensmokingandweightandbetweensexand
age,soallfouroftheexplanatoryvariablesneedtoremaininthemodelasmaineffects(you
mustnotdeletemaineffectsfromamodel,eveniftheylooktobenon-significant(likesex
and age in this case) when they appear in significant interaction terms. The biggest main
effect (to judge by the p values) is smoking. The significance of the intercept is not
interestinginthiscase(itjustsaysthemeannumberofcellsfornon-smoking,middle-aged
females of normal weight is greater than zero – but it is a count, so it would be).
Coefficients:
Estimate Std.Error t value Pr(>|t|)
(Intercept) -1.09888 0.33330 -3.297 0.00105 **
smokerTRUE 0.79483 0.26062 3.050 0.00241 **
sexmale 0.32917 0.34468 0.955 0.34004
ageold 0.12274 0.34991 0.351 0.72590
ageyoung 0.54004 0.36558 1.477 0.14025
weightobese 0.49447 0.23376 2.115 0.03490 *
242 STATISTICS:ANINTRODUCTIONUSINGR
weightover 0.28517 0.25790 1.106 0.26937
sexmale:ageold 0.06898 0.40297 0.171 0.86414
sexmale:ageyoung -0.75914 0.41819 -1.815 0.07007 .
smokerTRUE:weightobese 0.75913 0.31421 2.416 0.01605 *
smokerTRUE:weightover 0.32172 0.35561 0.905 0.36606
(Dispersionparameterforquasipoissonfamilytakentobe1.853039)
Nulldeviance:1052.95 on510 degreesoffreedom
Residualdeviance: 778.69 on500 degreesoffreedom
Thismodelshowsasignificantinteractionbetweensmokingandweightindetermining
the number of damaged cells (p=0.05):
tapply(cells,list(smoker,weight),mean)
normal obese over
FALSE 0.4184397 0.689394 0.5436893
TRUE 0.9523810 3.514286 2.0270270
and an interaction between sex and age (p=0.03):
tapply(cells,list(sex,age),mean)
mid old young
female 0.4878049 0.5441176 1.435897
male 1.0315789 1.5468750 1.176471
Thesummarytableshowsthatthegendereffectismuchsmallerforyoungpeople(where
thecountwasslightlyhigherinfemales) thanformiddle-agedorolderpeople(wherethe
count was much higher in males).
Withcomplicatedinteractionslikethis,itisoftenusefultoproducegraphsoftheeffects.
The relationship between smoking and weight is shown like this:
barplot(tapply(cells,list(smoker,weight),mean),beside=T)
ThisisOK,butthebarsarenotinthebestorder;theobesecategoryshouldbeontheright
of the figure. To correct this we need to change the order of the factor levels for weight
awayfromthedefault(alphabeticalorder)toauser-specifiedorder,withnormaloftheleft
and obese on the right. We do this using the function called factor:
weight<-factor(weight,c("normal","over","obese"))
barplot(tapply(cells,list(smoker,weight),mean),beside=T)
COUNT DATA 243
That’s more like it. Finally, we need to add a legend showing which bars are the
smokers:
barplot(tapply(cells,list(smoker,weight),mean),beside=T)
legend(locator(1),c("nonsmoker","smoker"),fill=gray(c(0.2,0.8)))
Justclickwhenthecursorisoverthespotonthegraphwhereyouwantthetopleft-hand
corner of the legend to appear:
244 STATISTICS:ANINTRODUCTIONUSINGR
TheDangerofContingencyTables
WehavealreadydealtwithsimplecontingencytablesandtheiranalysisusingFisher’sexact
testorPearson’schi-squaredtest(seeChapter6).Butthereisanimportantfurtherissueto
be dealt with. In observational studies we quantify only a limited number of explanatory
variables. It is inevitable that we shall fail to measure a number of factors that have an
importantinfluenceonthebehaviourofthesysteminquestion.That’slife,andgiventhat
we make every effort to note the important factors, there’s little we can do about it. The
problem comes when we ignore factors that have an important influence on the response
variable. This difficulty can be particularly acute if we aggregate data over important
explanatory variables. An example should make this clear.
Supposewearecarryingoutastudyofinduceddefencesintrees.Apreliminarytrialhas
suggested that early feeding on a leaf by aphids may cause chemical changes in the leaf
whichreducetheprobabilityofthatleafbeingattackedlaterintheseasonbyhole-making
caterpillars. To this end we mark a large cohort of leaves, then score whether they were
infested by aphids early intheseason andwhetherthey were holed byinsectslaterinthe
year. The work was carried out on two different trees and the results were as follows:
Tree Aphids Holed Intact Totalleaves Proportionholed
Tree1 Without 35 1750 1785 0.0196
With 23 1146 1169 0.0197
Tree2 Without 146 1642 1788 0.0817
With 30 333 363 0.0826
There are four variables: the response variable, count, with eight values (highlighted
above),atwo-levelfactorforlateseasonfeedingbycaterpillars(holedorintact),atwo-level
factorforearlyseasonaphidfeeding(withorwithoutaphids)andatwo-levelfactorfortree
(the observations come from two separate trees, imaginatively named Tree1 and Tree2).
induced<-read.csv("c:\\temp\\induced.csv")
attach(induced)
names(induced)
[1]"Tree" "Aphid" "Caterpillar" "Count"
Webeginbyfittingwhatisknownasasaturatedmodel.Thisisacuriousthing,whichhasas
manyparametersastherearevaluesoftheresponsevariable.Thefitofthemodelisperfect,so
therearenoresidualdegreesoffreedomandnoresidualdeviance.Thereasonwhywefita
saturated model is that it is always the best place to start modelling complex contingency
tables. If we fit the saturated model, then there is no risk that we inadvertently leave out
importantinteractionsbetweentheso-called‘nuisancevariables’.Thesearetheparameters
thatneedtobeinthemodeltoensurethatthemarginaltotalsareproperlyconstrained.
model<-glm(Count∼Tree*Aphid*Caterpillar,family=poisson)
COUNT DATA 245
The asterisk notation ensures that the saturated model is fitted, because all of the main
effectsandtwo-wayinteractionsarefitted,alongwiththethree-way,tree–aphid–caterpillar
interaction.Themodel fitinvolvestheestimation of2×2×2=8parameters,andexactly
matches the eight values of the response variable, Count. There is no point looking at
the saturated model in any detail, because the reams of information it contains are all
superfluous.Thefirstrealstepinthemodellingistouseupdatetoremovethethree-way
interaction from the saturated model, and then to use anova to test whether the three-
way interaction is significant or not:
model2<-update(model, ∼.-Tree:Aphid:Caterpillar)
Thepunctuationhereisveryimportant(itiscomma,tilde,dot,minus)andnotetheuseof
colons rather than asterisks to denote interaction terms rather than main effects plus
interaction terms. Now we can see whether the three-way interaction was significant by
specifying test="Chi" like this:
anova(model,model2,test="Chi")
AnalysisofDevianceTable
Model1:Count∼Tree*Aphid*Caterpillar
Model2:Count∼Tree+Aphid+Caterpillar+Tree:Aphid+
Tree:Caterpillar+ Aphid:Caterpillar
Resid.Df Resid.Dev Df Deviance Pr(>Chi)
1 0 0.00000000
2 1 0.00079137 -1 -0.00079137 0.9776
Thisshowsclearlythattheinteractionbetweencaterpillarattackandleafholingdoesnot
differfromtreetotree(p=0.9776).Notethatifthisinteractionhadbeensignificant,then
we would have stopped the modelling at this stage. But it wasn’t, so we leave it out and
continue.Whataboutthemainquestion?Isthereaninteractionbetweencaterpillar attack
andleafholing?Totestthiswedeletetheaphid–caterpillarinteractionfromthemodel,and
assess the results using anova:
model3<-update(model2, ∼.-Aphid:Caterpillar)
anova(model3,model2,test="Chi")
AnalysisofDevianceTable
Model1:Count∼Tree+Aphid+Caterpillar+Tree:Aphid+
Tree:Caterpillar
Model2:Count∼Tree+Aphid+Caterpillar+Tree:Aphid+
Tree:Caterpillar+ Aphid:Caterpillar
Resid.Df Resid.Dev DfDeviance Pr(>Chi)
1 2 0.0040853
2 1 0.0007914 10.003294 0.9542
246 STATISTICS:ANINTRODUCTIONUSINGR
Thereisabsolutelynohintofaninteraction(p=0.954).Theinterpretationisclear:this
work provides no evidence for induced defences caused by early season caterpillar
feeding.
But look what happens when we do the modelling the wrong way. Suppose we went
straight for the aphid–caterpillar interaction of interest. We might proceed like this:
wrong<-glm(Count∼Aphid*Caterpillar,family=poisson)
wrong1<-update(wrong,∼.-Aphid:Caterpillar)
anova(wrong,wrong1,test="Chi")
AnalysisofDevianceTable
Model1:Count∼Aphid*Caterpillar
Model2:Count∼Aphid+Caterpillar
Resid.Df Resid.Dev Df Deviance Pr(>Chi)
1 4 550.19
2 5 556.85 -1 -6.6594 0.009864**
The aphid–caterpillar interaction is highly significant (p<0.01) providing strong evi-
denceforinduceddefences.Wrong!Byfailingtoincludethetreevariableinthemodelwe
haveomittedanimportantexplanatoryvariable.Asitturnsout,andweshouldreallyhave
determined by more thorough preliminary analysis, the trees differ enormously in their
average levels of leaf holing:
tapply(Count,list(Tree,Caterpillar),sum)
holed not
Tree1 58 2896
Tree2 176 1975
TheproportionofleaveswithholesonTree1was58/(58+2896)=0.0196,butonTree2
was176/(176+1975)=0.0818.Tree2hasmorethanfourtimestheproportionofitsleaves
holedbycaterpillars.Ifwehadbeenpayingmoreattentionwhenwedidthemodellingthe
wrongway,weshouldhavenoticedthatthemodelcontainingonlyaphidandcaterpillarhad
massive overdispersion, and this should have alerted us that all was not well.
The moral is simple and clear. Always fit a saturated model first, containing all the
variables of interest and all the interactions involving the nuisance variables (tree in this
case). Only delete from the model those interactions that involve the variables of interest
(aphidandcaterpillarinthiscase).Maineffectsaremeaninglessincontingencytables,as
arethemodelsummaries.Alwaystestforoverdispersion.Itwillneverbeaproblemifyou
followtheadviceofsimplifyingdownfromasaturatedmodel,becauseyouonlyeverleave
out non-significant terms, and you never delete terms involving any of the nuisance
variables, except for the highest-order interaction at the first step in model simplification
(Tree:Aphid:Caterpillar in this example).
COUNT DATA 247
AnalysisofCovariancewithCountData
In this example the response is a count of the number of plant species on plots that have
different biomass (a continuous explanatory variable) and different soil pH (a categorical
variable with three levels: high, mid and low):
species<-read.csv("c:\\temp\\species.csv")
attach(species)
names(species)
[1]"pH" "Biomass""Species"
We start by plotting the data, using different colours for each of the three pH classes:
plot(Biomass,Species,pch=21,bg=(1+as.numeric(pH)))
Nowwefitastraightforwardanalysisofcovarianceanduseablinetodrawlinesofthe
appropriate colour through the scatterplot:
model<-lm(Species∼Biomass*pH)
summary(model)
Call:
lm(formula=Species∼Biomass*pH)
Residuals:
Min 1Q Median 3Q Max
-9.290 -2.554 -0.124 2.208 15.677
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 40.60407 1.36701 29.703 <2e-16***
Biomass -2.80045 0.23856 -11.739 <2e-16***
pHlow -22.75667 1.83564 -12.397 <2e-16***
pHmid -11.57307 1.86926 -6.191 2.1e-08***
Biomass:pHlow -0.02733 0.51248 -0.053 0.958
Biomass:pHmid 0.23535 0.38579 0.610 0.543
Residualstandarderror:3.818on84degreesoffreedom
MultipleR-squared: 0.8531, AdjustedR-squared: 0.8444
F-statistic:97.58on5and84DF, p-value:<2.2e-16
Thereisnoevidenceinthelinearmodelforanydifferencebetweentheslopesatdifferent
levelsofpH.MakesureyouunderstandhowIgottheinterceptsandslopesforablineout
of the model summary:
abline(40.60407,-2.80045,col="red")
abline(40.60407-22.75667,-2.80045-0.02733,col="green")
abline(40.60407-11.57307,-2.80045+0.23535,col="blue")
248 STATISTICS:ANINTRODUCTIONUSINGR
Itisclearthatspeciesdeclineswithbiomass,andthatsoilpHhasabigeffectonspecies,
but does the slope of the relationship between species and biomass depend on pH? The
biggestproblemwiththislinearmodelisthatitpredictsnegativevaluesforspeciesrichness
abovebiomassvaluesof6forthelowpHplots.Countdataarestrictlyboundedbelow,and
our model should really take account of this.
Let us refit the model using a GLM with Poisson errors instead of a linear model:
model<-glm(Species∼Biomass*pH,poisson)
summary(model)
Coefficients:
Estimate Std.Error z value Pr(>|z|)
(Intercept) 3.76812 0.06153 61.240 <2e-16 ***
Biomass -0.10713 0.01249 -8.577 <2e-16 ***
pHlow -0.81557 0.10284 -7.931 2.18e-15 ***
pHmid -0.33146 0.09217 -3.596 0.000323 ***
Biomass:pHlow -0.15503 0.04003 -3.873 0.000108 ***
Biomass:pHmid -0.03189 0.02308 -1.382 0.166954
(Dispersionparameterforpoissonfamilytakentobe1)
Nulldeviance:452.346 on89 degreesoffreedom
Residualdeviance: 83.201 on84 degreesoffreedom
AIC:514.39
NumberofFisherScoringiterations:4
Theresidualdevianceisnotlargerthantheresidualdegreesoffreedom,sowedon’tneedto
correctforoverdispersion.Doweneedtoretaintheinteractionterm?Wetestthisbydeletion:
model2<-glm(Species∼Biomass+pH,poisson)
anova(model,model2,test="Chi")
COUNT DATA 249
AnalysisofDevianceTable
Model1:Species∼Biomass*pH
Model2:Species∼Biomass+pH
Resid.Df Resid. Dev Df Deviance Pr(>Chi)
1 84 83.201
2 86 99.242 -2 -16.04 0.0003288***
Yes,wedo.Thereisahighlysignificantdifferencebetweentheslopesatdifferentlevels
of pH. So our first model is minimal adequate.
Finally,wedrawthefittedcurvesthroughthescatterplot,usingpredict.Thetrickisthat
we need to draw three separate curves; one for each level of soil pH.
plot(Biomass,Species,pch=21,bg=(1+as.numeric(pH)))
xv<-seq(0,10,0.1)
length(xv)
[1]101
acidity<-rep("low",101)
yv<-predict(model,list(Biomass=xv,pH=acidity),type="response")
lines(xv,yv,col="green")
acidity<-rep("mid",101)
yv<-predict(model,list(Biomass=xv,pH=acidity),type="response")
lines(xv,yv,col="blue")
acidity<-rep("high",101)
yv<-predict(model,list(Biomass=xv,pH=acidity),type="response")
lines(xv,yv,col="red")
250 STATISTICS:ANINTRODUCTIONUSINGR
Note the use of type="response" in the predict function. This ensures that the
responsevariableiscalculatedasspeciesratherthanlog(species),andmeanswedonotneed
toback-transformusingantilogsbeforedrawingthelines.YoucouldmaketheRcodemore
elegantbywritingafunctiontoplotanynumberoflines,dependingonthenumberoflevels
of the factor (three levels of pH in this case).
FrequencyDistributions
Givendataonthenumbersofbankruptciesin80districts,wewanttoknowwhetherthereis
any evidence that some districts show greater than expected numbers of cases.
What would we expect? Of course we should expect some variation. But how much,
exactly?Well,thatdependsonourmodeloftheprocess.Perhapsthesimplestmodelisthat
absolutely nothing is going on, and that every single bankruptcy case is absolutely
independent of every other. That leads to the prediction that the numbers of cases per
district will follow a Poisson process: a distribution in which the variance is equal to the
mean (Box 13.1).
Box13.1. ThePoissondistribution
The Poisson distribution is widely used for the description of count data. We know
how many times something happened (e.g. kicks from cavalry horses, lightening
strikes,bombhits),butwehavenowayofknowinghowmanytimesitdidnothappen.
The Poisson is a one-parameter distribution, specified entirely by the mean. The
variance is identical to the mean (λ), so the variance–mean ratio is equal to 1. The
probability of observing a count of x is given by
e λλx
P x
x!
This can be calculated very simply on a hand calculator because:
λ
P xP x 1
x
which means that if you start with the zero term
P 0e λ
theneachsuccessiveprobabilityisobtainedsimplybymultiplyingbythemeanand
dividing by x.
COUNT DATA 251
Let us see what the data show:
case.book<-read.csv("c:\\temp\\cases.csv")
attach(case.book)
names(case.book)
[1]"cases"
Firstweneedtocountthenumbersofdistrictswithnocases,onecase,twocases,andso
on. The R function that does this is called table:
frequencies<-table(cases)
frequencies
cases
0 1 2 3 4 5 6 7 8 9 10
34 14 10 7 4 5 2 1 1 1 1
There were no cases at all in 34 districts, but one district had 10 cases. A good way to
proceedistocompareourdistribution(calledfrequencies)withthedistributionthatwould
be observed if the data really did come from a Poisson distribution as postulated by our
model.WecanusetheRfunctiondpoistocomputetheprobabilitydensityofeachofthe
11 frequencies from 0 to10 (we multiply theprobability producedbydpois by thetotal
sampleof80toobtainthepredictedfrequencies).Weneedtocalculatethemeannumberof
cases per district – this is the Poisson distribution’s only parameter:
mean(cases)
[1]1.775
The plan is to draw two distributions side by side, so we set up the plotting region:
windows(7,4)
par(mfrow=c(1,2))
Now we plot the observed frequencies in the left-hand panel:
barplot(frequencies,ylab="Frequency",xlab="Cases",
col="red",main="observed")
and the predicted, Poisson frequencies in the right-hand panel:
barplot(dpois(0:10,1.775)*80,names=as.character(0:10),
ylab="Frequency",xlab="Cases",col="blue",main="expected")
252 STATISTICS:ANINTRODUCTIONUSINGR
Thedistributionsareverydifferent:themodeoftheobserveddataiszero,butthemodeof
thePoissonwiththesamemeanis1;theobserveddatacontainedexamplesof8,9and10
cases,butthesewouldbehighlyunlikelyunderaPoissonprocess.Wewouldsaythatthe
observed data are highly aggregated; they have a variance–mean ratio of nearly 3 (the
Poisson, of course, has a variance–mean ratio of 1):
var(cases)/mean(cases)
[1] 2.99483
So, if the data are not Poisson distributed, how are they distributed? A good candidate
distribution where the variance–mean ratio is this big (c. 3.0) is the negative binomial
distribution (Box 13.2).
Box13.2. Thenegativebinomialdistribution
This discrete, two-parameter distribution is useful for describing the distribution of
count data, where the variance is often much greater than the mean. The two
parameters are the mean μ and the clumping parameter k. The smaller the value of
k, the greater the degree of clumping. The density function is
(cid:4) (cid:5)
(cid:1) μ (cid:3)  kΓ kx μ x
p x 1
k x!Γ k μk
where Γ is the gamma function. The zero term is found by setting x = 0 and
simplifying:
(cid:1) (cid:3)
μ  k
p 0 1
k
Successive terms in the distribution can be computed iteratively from
(cid:4) (cid:5)(cid:4) (cid:5)
kx 1 μ
p xp x 1
x μk
An initial estimate of the value of k can be obtained from the sample mean and
variance:
μ2
k 
s2 μ
Sincekcannotbenegative,itisclearthatthenegativebinomialdistributionshouldnot
befittedtodatawherethevarianceislessthanthemean(useabinomialdistribution
instead). The precise maximum likelihood estimate of k is found numerically, by
COUNT DATA 253
iteratingprogressivelymorefine-tunedvaluesofkuntiltheleft-andright-handsides
of the following equation are equal:
(cid:4) (cid:5)
(cid:1) μ (cid:3) Xmax A
nln 1  x
k kx
x0
where the vector A(x) contains the total frequency of values greater than x.
You could create a function to work out the probability densities like this:
negbin<-function(x,u,k)
(1+u/k)^(-k)*(u/(u+k))^x*gamma(k+x)/(factorial(x)*gamma(k))
then use the function to produce a barplot of probability densities for a range of x
values(say0to10,foradistributionwithspecifiedmeanandaggregationparameter
(say μ0:8;k 0:2) like this:
xf<-numeric(11)
for(iin0:10)xf[i+1]<-negbin(i,0.8,0.2)
barplot(xf)
AsdescribedinBox13.2,thisatwo-parameterdistribution.Wehavealreadyworkedout
themeannumberofcases,whichis1.775.Weneedanestimateoftheclumpingparameter,
k,toestablishthedegreeofaggregationinthedata(asmallvalueofk(suchask<1)would
showhighaggregation,whilealargevalue(suchask>5)wouldindicaterandomness).We
can get an approximate estimate of magnitude using the formula in Box 13.2:
μ2
k 
s2 μ
We have:
mean(cases)^2/(var(cases)-mean(cases))
[1]0.8898003
so we shall work with k=0.8898.
How do we compute the expected frequencies? The density function for the negative
binomial distribution is dnbinomand it has three arguments: thefrequency for which we
wanttheprobability(inourcase0to10),theclumpingparameter(size=0.8898),andthe
meannumberofcases(mu=1.775);wemultiplybythetotalnumberofcases(80)toobtain
the expected frequencies
expected<-dnbinom(0:10,size=0.8898,mu=1.775)*80
254 STATISTICS:ANINTRODUCTIONUSINGR
The plan istodrawasinglefigureinwhichtheobservedandexpectedfrequenciesare
drawnsidebyside.Thetrickistoproduceanewvector(calledboth)whichistwiceaslong
as the observed and expected frequency vectors (2×11=22). Then we put the observed
frequencies in the odd numbered elements (using modulo 2 to calculate the values of the
subscripts), and the expected frequencies in the even numbered elements:
both<-numeric(22)
both[1:22%%2!=0]<-frequencies
both[1:22%%2==0]<-expected
On the x axis, we intend to label only every other bar:
labels<-character(22)
labels[1:22%%2==0]<-as.character(0:10)
Nowwecanproducethebarplot,usinglightgreyfortheobservedfrequenciesanddark
grey for the negative binomial frequencies:
barplot(both,col=rep(c("lightgray","darkgray"),11),names=labels,
ylab="Frequency",xlab="Cases")
Weneedtoaddalegendtoshowwhatthetwocoloursofthebarsmean.Youcanlocate
the legend by trial and error, or by left-clicking mouse when the cursor is in the correct
position, using the locator(1) function (see p. 169):
legend(locator(1),c("Observed","Expected"),
fill=c("lightgray","darkgray"))
COUNT DATA 255
ThefittothenegativebinomialdistributionismuchbetterthanitwaswiththePoisson
distribution,especiallyintheright-handtail.Buttheobserveddatahavetoomanyzerosand
toofewonestoberepresentedperfectlybyanegativebinomialdistribution.Ifyouwantto
quantifythelackoffitbetweenPtheobservedandexpectedfrequencydistributions,youcan
calculatePearson’schisquare O E2=Ebasedonthenumberofcomparisonsthathave
expected frequency greater than 4.
expected
[1]30.1449097 17.8665264 11.2450066 7.2150606 4.6734866 3.0443588
[7] 1.9905765 1.3050321 0.8572962 0.5640455 0.3715655
Ifweaccumulatetherightmostsixfrequencies,thenallthevaluesofexpectedwillbe
bigger than 4. The degrees of freedom are then given by the number of comparisons (6)
minus the number of parameters estimated from the data (2 in our case; the mean and k)
minus 1 for contingency (because the total frequency must add up to 80), so there are 3
degreesoffreedom.Weuse‘levelsgets’toreducethelengthsoftheobservedandexpected
vectors, creating an upper interval called ‘5+’ for ‘5 or more’:
cs<-factor(0:10)
levels(cs)[6:11]<-"5+"
levels(cs)
[1]"0" "1" "2" "3" "4" "5+"
Nowmakethetwoshortervectorsofandef(for observedandexpectedfrequencies):
ef<-as.vector(tapply(expected,cs,sum))
of<-as.vector(tapply(frequencies,cs,sum))
Finally, we can compute the chi-squared value measuring the difference between the
observedandexpectedfrequencydistributions,anduse1-pchisqtoworkoutthepvalue:
sum((of-ef)^2/ef)
[1]2.581842
1-pchisq(2.581842,3)
[1]0.4606818
We conclude that a negative binomial description of these data is reasonable (the
observed and expected distributions are not significantly different; p=0.46).
FurtherReading
Agresti, A.(1990) Categorical DataAnalysis,JohnWiley &Sons, NewYork.
Santer, T.J. and Duffy, D.E. (1990) The Statistical Analysis of Discrete Data, Springer-Verlag,
NewYork.
14
Proportion Data
An important class of problems involves data on proportions:
(cid:129) studies on percentage mortality
(cid:129) infection rates of diseases
(cid:129) proportion responding to clinical treatment
(cid:129) proportion admitting to particular voting intentions
(cid:129) sex ratios
(cid:129) data on proportional response to an experimental treatment
Thesearecountdata,butwhattheyhaveincommonisthatweknowhowmanyofthe
experimental objects are in one category (dead, insolvent, male or infected) and we also
know how many are in another (alive, solvent, female or uninfected). This differs from
Poissoncountdata,whereweknewhowmanytimesaneventoccurred,butnothowmany
times it did not occur (Chapter 13).
We model processes involving proportional response variables in R by specifying a
generalized linear model (GLM) with family=binomial. The only complication is that
whereaswithPoissonerrorswecouldsimplysayfamily=poisson,withbinomialerrors
we must specifythe number offailures as well as thenumbersof successesby creatinga
two-vectorresponsevariable.Todothiswebindtogethertwovectorsusingcbindintoa
single object, y, comprising the numbers of successes and the number of failures. The
binomial denominator, n, is the total sample, and
number.of.failures<-binomial.denominator–number.of.successes
y<-cbind(number.of.successes,number.of.failures)
Theold-fashionedwayofmodellingthissortofdatawastousethepercentagemortality
as the response variable. There are four problems with this:
(cid:129) the errors are not normally distributed
(cid:129) the variance is not constant
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
PROPORTION DATA 257
(cid:129) the response is bounded (by 100 above and by 0 below)
(cid:129) by calculating the percentage, we lose information of the size of the sample, n, from
which the proportion was estimated
ByusingaGLM,wecauseRtocarryoutaweightedregression,withthesamplesizes,n,
asweights,andemployingthelogitlinkfunctiontoensurelinearity.Therearesomekindsof
proportiondata,suchaspercentagecover,thatarebestanalysedusingconventionallinear
models(withnormalerrorsandconstantvariance)followinpgffiaffiffiffirfficffiffiffisffiiffiffinffiffieffiffiffiffitransformation.Insuch
cases,theresponsevariable,measuredinradians,issin  1 0:01p,wherepispercentage
cover. If, however, the response variable takes the form of a percentage change in some
continuousmeasurement(suchasthepercentagechangeinweightonreceivingaparticular
diet), then rather than arcsine transform the data, it is usually better treated by either
(cid:129) analysisofcovariance(seeChapter9),usingfinalweightastheresponsevariableand
initial weight as a covariate, or
(cid:129) by specifying the response variable as a relative growth rate, measured as log(final
weight/initial weight)
both of which can then be analysed with normal errors without further transformation.
AnalysesofDataonOneandTwoProportions
Forcomparisonsofonebinomialproportionwithaconstant,usebinom.test(seep.98).
For comparison of two samples of proportion data, use prop.test (see p. 100). The
methods of this chapter are required only for more complex models of proportion data,
including regression and contingency tables, where GLMs are used.
AveragesofProportions
Aclassicbeginner’smistakeistoaverageproportionsasyouwouldwithanyotherkindof
realnumber.Wehavefourestimatesofthesexratio:0.2,0.17,0.2and0.53.Theyaddupto
1.1,sothemeanis1.1/4=0.275.Wrong!Thisisnothowyouaverageproportions.These
proportionsarebasedoncountdataandweshouldusethecountstodeterminetheaverage.
For proportion data, the mean success rate is given by:
total number of successes
mean proportion
total number of attempts
Weneedtogobackandlookatthecountsonwhichourfourproportionswerecalculated.It
turnsouttheywere1outof5,1outof6,2outof10and53outof100,respectively.The
total number of successes was 1+1+2+53=57 and the total number of attempts was
5+6+10+100=121. The correct mean proportion is 57/121=0.4711. This is nearly
double the answer we got by doing it the wrong way. So beware.
CountDataonProportions
The traditional transformations of proportion data were arcsine and probit. The arcsine
transformationtookcareoftheerrordistribution,whiletheprobittransformationwasused
258 STATISTICS:ANINTRODUCTIONUSINGR
tolinearizetherelationshipbetweenpercentagemortalityandlogdoseinabioassay.There
isnothingwrongwiththesetransformations,andtheyareavailablewithinR,butasimpler
approach is often preferable, and is likely to produce a model that is easier to interpret.
The major difficulty with modelling proportion data is that the responses are strictly
bounded.Thereisnowaythatthepercentagedyingcanbegreaterthan100%orlessthan
0%.Butifweusesimpletechniquessuchaslinearregressionoranalysisofcovariance,then
the fitted model could quite easily predict negative values or values greater than 100%,
especiallyifthevariancewashighandmanyofthedatawerecloseto0orcloseto100%.
Thelogisticcurveiscommonlyusedtodescribedataonproportions,because,unlikethe
straightlinemodel,itasymptotesat0and1sothatnegativeproportionsandresponsesof
morethan100%cannotbepredicted.Throughoutthisdiscussionweshalluseptodescribe
the proportion of individuals observed to respond in a given way. Because much of their
jargonwasderivedfromthetheoryofgambling,statisticianscallthesesuccesses,although
toademographermeasuringdeathratesthismayseemalittleeccentric.Theindividualsthat
respond in other ways (the statistician’s failures) are therefore 1 p and we shall call the
proportion of failures q (so that p+q=1). The third variable is the size of the sample,
n,fromwhichpwasestimated(itisthebinomialdenominator,andthestatistician’snumber
of attempts).
An important point about the binomial distribution is that the variance is not constant.
In fact, the variance of a binomial distribution with mean np is:
s2 npq
so that the variance changes with the mean like this:
Thevarianceislowwhenpisveryhighorverylow(whenallormostoftheoutcomesare
identical),andthevarianceisgreatestwhenp=q=0.5.Aspgetssmaller,sothebinomial
distributiongetscloserandclosertothePoissondistribution.Youcanseewhythisissoby
PROPORTION DATA 259
considering the formula for the variance of the binomial (above). Remember that for the
Poisson,thevarianceisequaltothemean,s2 np.Now,aspgetssmaller,soqgetscloser
and closer to 1, so the variance of the binomial converges to the mean:
s2 npqnp q1
Odds
The logistic model for p as a function of x looks like this:
eabx
p
1eabx
andtherearenoprizesforrealizingthatthemodelisnotlinear.Whenconfrontedwithanew
equation like this, it is a good idea to work out its behaviour at the limits. This involves
askingwhatthevalueofyiswhenx=0,andwhatthevalueofyiswhenxisverylarge(say,
x=infinity)?
When x=0 then p=exp(a)/(1+exp(a)), so this is the intercept. There are two results
thatyoushouldmemorize:exp( ∞ )=∞ andexp( ∞ )=1/exp( ∞ )=0.Sowhenxisavery
largepositivenumber,wehavep=1andwhenxisaverylargenegativenumber,wehave
p=0/1=0, so the model is strictly bounded.
Thetrickoflinearizingthelogisticturnsouttoinvolveaverysimpletransformation.You
mayhavecomeacrossthewayinwhichbookmakersspecifyprobabilitiesbyquotingthe
oddsagainstaparticularhorsewinningarace(theymightgiveoddsof2to1onareasonably
goodhorseor25to1onanoutsider).Whattheymeanby2to1isthatiftherewerethree
races,thenourhorsewouldbeexpectedtolosetwoofthemandwinoneofthem(witha
built-in margin of error in favour of the bookmaker, needless to say).
Thisisaratherdifferentwayofpresentinginformationonprobabilitiesthanscientistsare
usedtodealingwith.Thus,wherethescientistmightstateaproportionas0.3333(1success
out of 3), the bookmaker would give odds of 2 to 1. In symbols, this is the difference
betweenthescientiststatingtheprobabilityp,andthebookmakerstatingtheoddsp/q.Now
if we take the odds p/q and substitute this into the formula for the logistic, we get:
(cid:3) (cid:4)
p eabx eabx  1
 1 
q 1eabx 1eabx
which looks awful. But a little algebra shows that:
(cid:3) (cid:4)
p eabx 1  1
 eabx
q 1eabx 1eabx
Now,takingnaturallogsandrecallingthatln exxwillsimplifymattersevenfurther,sothat
(cid:5) (cid:6)
p
ln abx
q
260 STATISTICS:ANINTRODUCTIONUSINGR
Thisgivesalinearpredictor,a+bx,notforpbutforthelogittransformationofp,namely
ln(p/q). Inthejargon ofR, thelogitisthe link function relatingthelinearpredictor tothe
value of p.
Hereispasafunctionofx(leftpanel)andlogit(p)asafunctionofx(rightpanel)forthe
logistic with a=0.2 and b=0.1:
Youmightaskatthisstage‘whynotsimplydoalinearregressionofln(p/q)againstthe
explanatory x variable?’ R has three great advantages here:
(cid:129) it allows for the non-constant binomial variance
(cid:129) it deals with the fact that logits for ps near 0 or 1 are infinite
(cid:129) it allows for differences between the sample sizes by weighted regression
OverdispersionandHypothesisTesting
Allthedifferentmodellingproceduresthatwehavemetinearlierchapterscanalsobeused
withdataonproportions.Factorialanalysisofvariance,analysisofcovarianceandmultiple
regression can all be carried out using GLMs. The only difference is that we assess the
significanceoftermsonthebasisofchi-squared;thisistheincreaseinscaleddeviancethat
results from removal of a term from the current model.
Theimportantpointtobearinmindisthathypothesistestingwithbinomialerrorsisless
clear-cut than with normal errors. While the chi-squared approximation for changes in
scaleddevianceisreasonableforlargesamples(i.e.biggerthanabout30),itispoorerwith
small samples. Most worrisome is the fact that the degree to which the approximation is
satisfactory isitselfunknown.Thismeansthatconsiderablecaremustbeexercisedinthe
interpretation of tests of hypotheses on parameters, especially when the parameters are
marginallysignificantorwhentheyexplainaverysmallfractionofthetotaldeviance.With
binomial or Poisson errors we cannot hope to provide exact p values for our tests of
hypotheses.
AswithPoissonerrors,weneedtoaddressthequestionofoverdispersion(seeChapter13).
When we have obtained the minimal adequate model, the residual scaled deviance
should be roughly equal to the residual degrees of freedom. When the residual deviance
islargerthantheresidualdegreesoffreedomtherearetwopossibilities:eitherthemodelis
misspecified,ortheprobabilityofsuccess,p,isnotconstantwithinagiventreatmentlevel.
PROPORTION DATA 261
Theeffectofrandomlyvaryingpistoincreasethebinomialvariancefromnpqto
s2 npqn n 1σ2
leadingtoalarger-than-expectedresidualdeviance.Thisoccursevenformodelsthatwould
fit well iftherandom variation were correctly specified.
One simple solution is to assume that the variance is not npq but npqs, where s is an
unknownscaleparameter(s>1).Weobtainanestimateofthescaleparameterbydividing
thePearsonchi-squaredbythedegreesoffreedom,andusethisestimateofstocomparethe
resultingscaleddeviances.Toaccomplishthis,weusefamily=quasibinomialrather
than family = binomial when there is overdispersion.
The most important points to emphasize in modelling with binomial errors are:
(cid:129) create a two-column object for the response, using cbind to join together the two
vectors containing the counts of success and failure
(cid:129) checkforoverdispersion(residualdeviancegreaterthanresidualdegreesoffreedom),
andcorrectforitbyusingfamily=quasibinomialratherthanfamily=binomialif
necessary
(cid:129) remember that you do not obtain exact p values with binomial errors; the chi-
squared approximations are soundforlarge samples, butsmallsamplesmaypresent
a problem
(cid:129) the fitted values are counts, like the response variable
(cid:129) because of this the raw residuals are also in counts
(cid:129) thelinearpredictorthatyouseewhenyouaskforsummary(model)isinlogits(thelog
of the odds, ln(p/q))
(cid:129) you can back-transform from logits (z) to proportions (p) by p=1/(1+1/exp(z))
Applications
You can do as many kinds of modelling in a GLM as in a linear model: here we show
examples of:
(cid:129) regression with binomial errors (continuous explanatory variables)
(cid:129) analysis of deviance with binomial errors (categorical explanatory variables)
(cid:129) analysis of covariance with binomial errors (both kinds of explanatory variables)
LogisticRegressionwithBinomialErrors
Thisexampleconcernssexratiosininsects(theproportionofallindividualsthataremales).
Inthespeciesinquestion,ithasbeenobservedthatthesexratioishighlyvariable,andan
experimentwassetuptoseewhetherpopulationdensitywasinvolvedindeterminingthe
fraction of males.
262 STATISTICS:ANINTRODUCTIONUSINGR
numbers<-read.csv("c:\\temp\\sexratio.csv")
numbers
density females males
1 1 1 0
2 4 3 1
3 10 7 3
4 22 18 4
5 55 22 33
6 121 41 80
7 210 52 158
8 444 79 365
Itcertainlylooksasifthereareproportionallymoremalesathighdensity,butweshould
plot the data as proportions to see this more clearly:
attach(numbers)
windows(7,4)
par(mfrow=c(1,2))
p<-males/(males+females)
plot(density,p,ylab="Proportionmale")
plot(log(density),p,ylab="Proportionmale")
Evidently,alogarithmictransformationoftheexplanatoryvariableislikelytoimprove
the model fit. We shall see in a moment.
Does increasing population density lead to a significant increase in the proportion of
malesinthepopulation?Or,moresuccinctly,isthesexratiodensitydependent?Itcertainly
looks from the plot as if it is.
Theresponsevariableisamatchedpairofcountsthatwewishtoanalyseasproportion
data using a GLM with binomial errors. First we bind together the vectors of male and
female counts into a single object that will be the response in our analysis:
y<-cbind(males,females)
Thismeansthatywillbeinterpretedinthemodelastheproportionofallindividualsthat
were male. The model is specified like this:
model<-glm(y∼density,binomial)
PROPORTION DATA 263
Thissaysthattheobjectcalledmodelgetsageneralizedlinearmodelinwhichy(the
sex ratio) is modelled as a function of a single continuous explanatory variable called
density, using an error distribution from the family = binomial. The output looks
like this:
summary(model)
Coefficients:
Estimate Std. Error z value Pr (>|z|)
(Intercept) 0.0807368 0.1550376 0.521 0.603
density 0.0035101 0.0005116 6.862 6.81e-12 ***
(Dispersionparameterforbinomialfamilytakentobe1)
Nulldeviance:71.159 on7 degreesoffreedom
Residualdeviance:22.091 on6 degreesoffreedom
AIC:54.618
NumberofFisherScoringiterations:4
The model table looks just as it would for a straightforward regression. The first
parameter is the intercept and the second is the slope of the graph of sex ratio against
populationdensity.Theslopeishighlysignificantlysteeperthanzero(proportionatelymore
malesathigherpopulationdensity;p=6.81×10  12).Wecanseeiflogtransformationof
the explanatory variable reduces the residual deviance below 22.091:
model<-glm(y∼log(density),binomial)
summary(model)
Coefficients:
Estimate Std. Error z value Pr (>|z|)
(Intercept) -2.65927 0.48758 -5.454 4.92e-08 ***
log(density) 0.69410 0.09056 7.665 1.80e-14 ***
(Dispersionparameterforbinomialfamilytakentobe1)
Nulldeviance:71.1593 on7 degreesoffreedom
Residualdeviance: 5.6739 on6 degreesoffreedom
AIC:38.201
NumberofFisherScoringiterations:4
Thisisabigimprovement,soweshalladoptit.Thereisatechnicalpointhere,too.Ina
GLMlikethis,itisassumedthattheresidualdevianceisthesameastheresidualdegreesof
freedom.Iftheresidualdevianceislargerthantheresidualdegreesoffreedom,thisiscalled
overdispersion. It means that there is extra, unexplained variation, over and above the
binomialvarianceassumedbythemodelspecification.Inthemodelwithlog(density)there
isnoevidenceofoverdispersion(residualdeviance=5.67on6d.f.),whereasthelackoffit
introduced by the curvature in our first model caused substantial overdispersion (residual
264 STATISTICS:ANINTRODUCTIONUSINGR
deviance=22.09on6d.f.)whenwehaddensityinsteadoflog(density)astheexplanatory
variable.
Model checking involves the use of plot(model). As you will see, there is no
worryingly great pattern in the residuals against the fitted values, and the normal plot is
reasonably linear. Point number 4 is highly influential (it has a big value of Cook’s
distance),andpointnumber8hashighleverage(butthemodelisstillsignificantwiththis
point omitted). We conclude that the proportion of animals that are males increases
significantlywithincreasingdensity,andthatthelogisticmodelislinearizedbylogarithmic
transformationoftheexplanatoryvariable(populationdensity).Wefinishbydrawingthe
fitted line though the scatterplot:
xv<-seq(0,6,0.1)
plot(log(density),p,ylab="Proportionmale",pch=21,bg="blue")
lines(xv,predict(model,list(density=exp(xv)),
type="response"),col="brown")
Notetheuseoftype="response"toback-transformfromthelogitscaletotheS-shaped
proportion scale and exp(xv) to back-transform the logs from the x axis to densities as
requiredbythemodelformula(wheretheyarethenloggedagain).Asyoucansee,themodel
isverypoorfitforthelowestdensityandforlog(density)≈3,butareasonablygoodfitover
therestoftherange.Isuspectthatthereplicationwassimplytoolowatverylowpopulation
densitiestogetanaccurateestimateofthesexratio.Ofcourse,weshouldnotdiscountthe
possibility that the data point is not an outlier, but rather that the model is wrong.
ProportionDatawithCategoricalExplanatoryVariables
This example concerns the germination of seeds of two genotypes of the parasitic plant
Orobanche and two extracts from host plants (bean and cucumber) that were used to
stimulate germination. It is a two-way factorial analysis of deviance.
PROPORTION DATA 265
germination<-read.csv("c:\\temp\\germination.csv")
attach(germination)
names(germination)
[1]"count" "sample" "Orobanche""extract"
The response variable count is the number of seeds that germinated out of a batch of
size=sample.Sothenumberthatdidn’tgerminateissample–count,andweconstruct
the response vector like this:
y<-cbind(count,sample-count)
Each of the categorical explanatory variables has two levels:
levels(Orobanche)
[1]"a73""a75"
levels(extract)
[1]"bean" "cucumber"
WewanttotestthehypothesisthatthereisnointeractionbetweenOrobanchegenotype
(a73ora75)andplant extract (beanorcucumber)onthegermination rateoftheseeds.
This requires a factorial analysis using the asterisk * operator like this:
model<-glm(y∼Orobanche*extract,binomial)
summary(model)
Coefficients:
Estimate Std.Error z value Pr (>|z|)
(Intercept) -0.4122 0.1842 -2.238 0.0252 *
Orobanchea75 -0.1459 0.2232 -0.654 0.5132
extractcucumber 0.5401 0.2498 2.162 0.0306 *
Orobanchea75:extractcucumber 0.7781 0.3064 2.539 0.0111 *
(Dispersionparameterforbinomialfamilytakentobe1)
Nulldeviance:98.719 on20 degreesoffreedom
Residualdeviance:33.278 on17 degreesoffreedom
AIC:117.87
NumberofFisherScoringiterations:4
Atfirstglance,itlooksasifthereisahighlysignificantinteraction(p=0.0111).Butwe
needtocheckthatthemodelissound.Thefirstthingistocheckforisoverdispersion.The
residual deviance is 33.278 on 17 d.f. so the model is quite badly overdispersed:
33.279/17
[1]1.957588
266 STATISTICS:ANINTRODUCTIONUSINGR
The overdispersion factor is almost 2. The simplest way to take this into account is
to use what is called an ‘empirical scale parameter’ to reflect the fact that the errors
are not binomial as we assumed, but were larger than this (overdispersed) by a factor
of 1.9576. We refit the model using quasibinomial to account for the
overdispersion:
model<-glm(y∼Orobanche*extract,quasibinomial)
Then we use update to remove the interaction term in the normal way:
model2<-update(model,∼.-Orobanche:extract)
TheonlydifferenceisthatweuseanFtestinsteadofachi-squaredtesttocomparethe
original and simplified models:
anova(model,model2,test="F")
AnalysisofDevianceTable
Model1: y∼Orobanche*extract
Model2: y∼Orobanche+extract
Resid.DfResid. Dev Df Deviance F Pr(>F)
1 17 33.278
2 18 39.686 -1 -6.408 3.4419 0.08099 .
Now you see that the interaction is not significant (p=0.081). There is no compelling
evidence that different genotypes of Orobanche respond differently to the two plant
extracts. The next step is to see if any further model simplification is possible:
anova(model2,test="F")
AnalysisofDevianceTable
Model:quasibinomial,link:logit
Response:y
Termsaddedsequentially(firsttolast)
Df Deviance Resid. Df Resid. Dev F Pr(>F)
NULL 20 98.719
Orobanche 1 2.544 19 96.175 1.1954 0.2887
extract 1 56.489 18 39.686 26.5412 6.692e-05 ***
PROPORTION DATA 267
There is a highly significant difference between the two plant extracts on germination
rate,butitisnotobviousthatweneedtokeepOrobanchegenotypeinthemodel.Wetry
removing it:
model3<-update(model2,∼.-Orobanche)
anova(model2,model3,test="F")
AnalysisofDevianceTable
Model1:y∼Orobanche+extract
Model2:y∼extract
Resid. Df Resid. Dev Df Deviance FPr(>F)
1 18 39.686
2 19 42.751 -1 -3.065 1.44010.2457
ThereisnojustificationforretainingOrobancheinthemodel.Sotheminimaladequate
model contains just two parameters:
coef(model3)
(Intercept)extractcucumber
-0.5121761 1.0574031
What,exactly,dothesetwonumbersmean?Rememberthatthecoefficientsarefromthe
linearpredictor.Theyareonthetransformedscale,sobecauseweareusingquasi-binomial
errors,theyareinlogits(ln(p/(1 p)).Toturnthemintothegerminationratesforthetwo
plantextractsrequiresalittlecalculation.Togofromalogitxtoaproportionp,youneedto
do the following sum:
1
p
1
1
ex
So our first x value is  0.5122 and we calculate
1/(1+1/(exp(-0.5122)))
[1]0.3746779
Thissaysthatthemeangerminationrateoftheseedswiththefirstplantextractwas37%.
What about the parameter for cucumber extract (1.057)? Remember that with categorical
explanatory variables the parameter values are differences between means. So to get the
second germination rate we add 1.057 to the intercept before back-transforming:
1/(1+1/(exp(-0.5122+1.0574)))
[1]0.6330212
Thissaysthatthegerminationratewasnearlytwiceasgreat(63%)withthesecondplant
extract(cucumber).Obviouslywewanttogeneralizethisprocess,andalsotospeedupthe
268 STATISTICS:ANINTRODUCTIONUSINGR
calculationsoftheestimatedmeanproportions.Wecanusepredicttohelphere,because
type="response" makes predictions on the back-transformed scale automatically:
tapply(predict(model3,type="response"),extract,mean)
bean cucumber
0.3746835 0.6330275
Itisinterestingtocomparethesefigureswiththeaveragesoftherawproportions.Firstwe
need to calculate the proportion germinating, p, in each sample:
p<-count/sample
Then we can find the average the germination rates for each extract:
tapply(p,extract,mean)
bean cucumber
0.3487189 0.6031824
You see that this gives different answers. Not too different in this case, it’s true, but
different nonetheless. The correct way to average proportion data is to add up the total
counts for the different levels of abstract, and only then to turn them into proportions:
tapply(count,extract,sum)
bean cucumber
148 276
Thismeansthat148seedsgerminatedwithbeanextractand276withcucumber.Buthow
many seeds were involved in each case?
tapply(sample,extract,sum)
bean cucumber
395 436
Thismeansthat395seedsweretreatedwithbeanextractand436seedsweretreatedwith
cucumber. So the answers we want are 148/395 and 276/436 (i.e. the correct mean
proportions). We automate the calculation like this:
as.vector(tapply(count,extract,sum))/
as.vector(tapply(sample,extract,sum))
[1]0.3746835 0.6330275
ThesearethecorrectmeanproportionsasproducedbytheGLM.Themoralhereisthat
youcalculatetheaverageofproportionsbyusingtotalcountsandtotalsamplesandnotby
averaging the raw proportions.
PROPORTION DATA 269
To summarize this analysis:
(cid:129) make a two-column response vector containing the successes and failures
(cid:129) use glm with family=binomial (you don’t need to include family=)
(cid:129) fit the maximal model (in this case it had four parameters)
(cid:129) test for overdispersion
(cid:129) if, as here, you find overdispersion then use quasibinomial errors
(cid:129) begin model simplification by removing the interaction term
(cid:129) this was non-significant once we had adjusted for overdispersion
(cid:129) try removing main effects (we didn’t need Orobanche genotype in the model)
(cid:129) use plot to obtain your model-checking diagnostics
(cid:129) back transform using predict with the option type="response" to obtain means
AnalysisofCovariancewithBinomialData
Thisexampleconcernsfloweringinfivevarietiesofperennialplants.Replicatedindividuals
inafullyrandomizeddesignweresprayedwithoneofsixdosesofacontrolledmixtureof
growth promoters. After 6 weeks, plants were scored as flowering or not flowering. The
countoffloweringindividualsformstheresponsevariable.ThisisanANCOVA because
wehavebothcontinuous(dose)andcategorical(variety)explanatoryvariables.Weuse
logistic regression because the response variable is a count (flowered) that can be
expressed as a proportion (flowered/number).
props<-read.csv("c:\\temp\\flowering.csv")
attach(props)
names(props)
[1]"flowered""number" "dose" "variety"
y<-cbind(flowered,number-flowered)
pf<-flowered/number
pfc<-split(pf,variety)
dc<-split(dose,variety)
Withcountdatalikethese,itiscommonforpointsfromdifferenttreatmentstohideone
another. Note the use of jitter to move points left or right and up or down by a small
random amount so that all of the symbols at a different point can be seen:
plot(dose,pf,type="n",ylab="Proportionflowered")
points(jitter(dc[[1]]),jitter(pfc[[1]]),pch=21,bg="red")
points(jitter(dc[[2]]),jitter(pfc[[2]]),pch=22,bg="blue")
points(jitter(dc[[3]]),jitter(pfc[[3]]),pch=23,bg="gray")
points(jitter(dc[[4]]),jitter(pfc[[4]]),pch=24,bg="green")
points(jitter(dc[[5]]),jitter(pfc[[5]]),pch=25,bg="yellow")
270 STATISTICS:ANINTRODUCTIONUSINGR
Thereisclearlyasubstantialdifferencebetweentheplantvarietiesintheirresponsetothe
flowering stimulant. The modelling proceeds in the normal way. We begin by fitting the
maximal model with different slopes and intercepts for each variety (estimating 10
parameters in all):
model1<-glm(y∼dose*variety,binomial)
summary(model1)
Themodelexhibitssubstantialoverdispersion(51.083/20>2),sowefitthemodelagain
with quasi-binomial errors:
model2<-glm(y∼dose*variety,quasibinomial)
summary(model2)
Coefficients:
Estimate Std.Error t value Pr (>|t|)
(Intercept) -4.59165 1.56314 -2.937 0.00814 **
dose 0.41262 0.15195 2.716 0.01332 *
varietyB 3.06197 1.65555 1.850 0.07922 .
varietyC 1.23248 1.79934 0.685 0.50123
varietyD 3.17506 1.62828 1.950 0.06534 .
varietyE -0.71466 2.34511 -0.305 0.76371
dose:varietyB -0.34282 0.15506 -2.211 0.03886 *
dose:varietyC -0.23039 0.16201 -1.422 0.17043
dose:varietyD -0.30481 0.15534 -1.962 0.06380 .
dose:varietyE -0.00649 0.20130 -0.032 0.97460
(Dispersionparameterforquasibinomialfamilytakentobe2.293557)
Nulldeviance:303.350 on29 degreesoffreedom
Residualdeviance: 51.083 on20 degreesoffreedom
AIC:NA
NumberofFisherScoringiterations:5
PROPORTION DATA 271
Doweneedtoretaintheinteractionterm(itappearstobesignificantinonlyonecase)?
We leave it out and compare the two models using anova:
model3<-glm(y∼dose+variety,quasibinomial)
anova(model2,model3,test="F")
AnalysisofDevianceTable
Model1:y∼dose*variety
Model2:y∼dose+variety
Resid. Df Resid. Dev Df Deviance F Pr(>F)
1 20 51.083
2 24 96.769 -4 -45.686 4.9798 0.005969**
Yes,wedo.Theinteractiontermismuchmoresignificantthanindicatedbyanyoneof
the t tests (p=0.005969).
Letusdrawthefivefittedcurvesthroughthescatterplot.Thevaluesonthedoseaxisneed
to go from 0 to 30:
xv<-seq(0,32,0.25)
length(xv)
[1]129
This means we shall need to provide the predict function with 129 repeats of each
factor level in turn:
yv<-predict(model3,list(dose=xv,variety=rep("A",129)),
type="response")
lines(xv,yv,col="red")
yv<-predict(model3,list(dose=xv,variety=rep("B",129)),
type="response")
lines(xv,yv,col="blue")
yv<-predict(model3,list(dose=xv,variety=rep("C",129)),
type="response")
lines(xv,yv,col="gray")
yv<-predict(model3,list(dose=xv,variety=rep("D",129)),
type="response")
lines(xv,yv,col="green")
yv<-predict(model3,list(dose=xv,variety=rep("E",129)),
type="response")
lines(xv,yv,col="yellow")
272 STATISTICS:ANINTRODUCTIONUSINGR
Asyoucansee,themodelisareasonablefitfortwoofthevarieties(AandE,represented
red circles and yellow down-triangles, respectively), not bad for one variety (C, grey
diamonds)butverypoorfortwoofthem:B(bluesquares)andD(greenup-triangles).For
several varieties, the model overestimates the proportion flowering at zero dose, and for
variety B there seems to be some inhibition of flowering at the highest dose because the
graph falls from 90% flowering at dose 16 to just 50% at dose 32 (the fitted model is
assumedtobeasymptotic).VarietyDappearstobeasymptotingatlessthan80%flowering.
These failures of the model focus attention for future work.
Themoralisthatjustbecausewehaveproportiondata,thatdoesnotmeanthatthedata
will necessarily be well described by the logistic. For instance, in order to describe the
responseofvarietyB, the model wouldneedtohavea hump, ratherthan to asymptoteat
p=1forlargedoses,andtomodelvarietyDthemodelwouldneedanextraparameterto
estimate an asymptotic value that was less than 100%.
FurtherReading
Hosmer,D.W.andLemeshow,S.(2000)AppliedLogisticRegression,2ndedn,JohnWiley&Sons,
NewYork.
15
Binary Response Variable
Manystatisticalproblemsinvolvebinaryresponsevariables.Forexample,weoftenclassify
thingsasdeadoralive,occupiedorempty,healthyordiseased,maleorfemale,literateor
illiterate, mature or immature, solvent or insolvent, employed or unemployed, and it is
interestingtounderstandthefactorsthatareassociatedwithanindividualbeinginoneclass
ortheother.Inastudyofcompanyinsolvency,forinstance,thedatawouldconsistofalist
of measurements made on the insolvent companies (their age, size, turnover, location,
management experience, workforce training, and so on) and a similar list for the solvent
companies.Thequestionthenbecomeswhich,ifany,oftheexplanatoryvariablesincrease
the probability of an individual company being insolvent.
Theresponsevariablecontainsonly0sor1s;forexample,0torepresentdeadindividuals
and1torepresentliveones.Thus,thereisonlyasinglecolumnofnumbersfortheresponse,
in contrast to proportion data where two vectors (successes and failures) were bound
togethertoformtheresponse(seeChapter14).AnalternativeisallowedbyRinwhichthe
valuesoftheresponsevariablearerepresentedbyatwo-levelfactor(like‘dead’or‘alive’,
‘male’ or ‘female’, etc.).
ThewaythatRtreatsbinarydataistoassumethatthevaluesoftheresponsecomefroma
binomialtrialwithsamplesize1.Iftheprobabilitythatanindividualisdeadisp,thenthe
probabilityofobtainingy(whereyiseitherdeadoralive,0or1)isgivenbyanabbreviated
form of the binomial distribution with n=1, known as the Bernoulli distribution:
P ypy 1 p1 y
The random variable y has a mean of p and a variance of p(1 p), and the object is to
determinehowtheexplanatoryvariablesinfluencethevalueofp.Thetricktousingbinary
responsevariableseffectivelyistoknowwhenitisworthusingthem,andwhenitisbetterto
lump the successes and failures togetherand analyse the total counts of dead individuals,
occupied patches, insolvent firms or whatever. The question you need to ask yourself is
whetherornotyouhaveuniquevaluesofoneormoreexplanatoryvariablesforeachand
everyindividualcase.Iftheansweris‘yes’,thenanalysiswithabinaryresponsevariableis
likelytobefruitful.Iftheansweris‘no’,thenthereisnothingtobegained,andyoushould
reduceyourdatabyaggregatingthecountstotheresolutionatwhicheachcountdoeshavea
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
274 STATISTICS:ANINTRODUCTIONUSINGR
unique set of explanatory variables. For example, suppose that all your explanatory
variables were categorical (say sex (male or female), employment (employed or
unemployed) and region (urban or rural)). In this case there is nothing to be gained
fromanalysisusingabinaryresponsevariablebecausenoneoftheindividualsinthestudy
haveuniquevaluesofanyoftheexplanatoryvariables.Itmightbeworthwhileifyouhad
eachindividual’sbodyweight,forexample;thenyoucouldaskwhether,whenyoucontrol
forsexandregion,heavypeoplearemorelikelytobeunemployedthanlightpeople.Inthe
absence of unique values for any explanatory variables, there are two useful options:
(cid:129) analysethedataasacontingencytableusingPoissonerrors,withthecountofthetotal
number of individuals in each of the eight contingencies (2×2×2) as the response
variable (see Chapter 13) in a dataframe with just eight rows
(cid:129) decide which of your explanatory variables is the key (perhaps you are interested in
genderdifferences),thenexpressthedataasproportions(thenumberofmalesandthe
numberoffemales)andrecodethebinaryresponseasacountofatwo-levelfactor–the
analysisisnowofproportiondata(e.g.theproportionofallindividualsthatarefemale)
using binomial errors (see Chapter 14)
If you do have unique measurements of one or more explanatory variables for each
individual,thesearelikelytobecontinuousvariablessuchasbodyweight,income,medical
history, distance to the nuclear reprocessing plant, geographic isolation, and so on. This
being thecase, successful analyses of binary response data tend tobe multiple regression
analysesorcomplexanalysesofcovariance,andyoushouldconsultChapters10and11for
details on model simplification and model criticism.
Inordertocarryoutmodellingonabinaryresponsevariablewetakethefollowingsteps:
(cid:129) createasinglevectorcontaining0sand1s(oroneoftwofactorlevels)astheresponse
variable
(cid:129) use glm with family=binomial
(cid:129) you can change the link function from the default logit to complementary log-log
(cid:129) fit the model in the usual way
(cid:129) testsignificancebydeletionoftermsfromthemaximalmodel,andcomparethechange
in deviance with chi-squared
(cid:129) note that there is no such thing as overdispersion with a binary response variable,
and hence no need to change to using quasi-binomial when the residual deviance
is large
(cid:129) plot(model)israrelyinformativewithbinaryresponsevariables,somodelchecking
is more than usually challenging
Thechoiceoflinkfunctionisgenerallymadebytryingbothlinksandselectingthelink
thatgivesthelowestdeviance.Thelogitlinkthatweusedearlierissymmetricinpandq,but
the complementary log-log link is asymmetric.
BINARY RESPONSEVARIABLE 275
IncidenceFunctions
In this example, the response variable is called incidence; a value of 1 means that an
islandwasoccupiedbyaparticularspeciesofbird,and0meansthatthebirddidnotbreed
there. The explanatory variables are the area of the island (km2) and the isolation of the
island (distance from the mainland, km).
island<-read.csv("c:\\temp\\isolation.csv")
attach(island)
names(island)
[1]"incidence""area" "isolation"
There are two continuous explanatory variables, so the appropriate analysis is multiple
regression.Theresponseisbinary,soweshalldologisticregressionwithbinomialerrors.
Webeginbyfittingacomplexmodelinvolvinganinteractionbetweenisolationandarea:
model1<-glm(incidence∼area*isolation,binomial)
Then we fit a simpler model with only main effects for isolation and area:
model2<-glm(incidence∼area+isolation,binomial)
Now we compare the two models using anova:
anova(model1,model2,test="Chi")
AnalysisofDevianceTable
Model1:incidence∼area*isolation
Model2:incidence∼area+isolation
Resid. Df Resid.Dev Df Deviance Pr(>Chi)
1 46 28.252
2 47 28.402 -1 -0.15043 0.6981
The simpler model is notsignificantly worse, so we accept this for thetime being, and
inspect the parameter estimates and standard errors:
summary(model2)
Coefficients:
Estimate Std.Error zvalue Pr(>|z|)
(Intercept) 6.6417 2.9218 2.273 0.02302*
area 0.5807 0.2478 2.344 0.01909*
isolation -1.3719 0.4769 -2.877 0.00401**
(Dispersionparameterforbinomialfamilytakentobe1)
Nulldeviance:68.029 on49 degreesoffreedom
Residualdeviance:28.402 on47 degreesoffreedom
AIC:34.402
NumberofFisherScoringiterations:6
276 STATISTICS:ANINTRODUCTIONUSINGR
Theestimatesandtheirstandarderrorsareinlogits.Areahasasignificantpositiveeffect
(largerislandsaremorelikelytobeoccupied),butisolationhasaverystrongnegativeeffect
(isolatedislandsaremuchlesslikelytobeoccupied).Thisistheminimaladequatemodel.
Weshouldplotthefittedmodelthroughthescatterplotofthedata.Itismucheasiertodothis
for each variable separately, like this:
windows(7,4)
par(mfrow=c(1,2))
xv<-seq(0,9,0.01)
modela<-glm(incidence∼area,binomial)
modeli<-glm(incidence∼isolation,binomial)
yv<-predict(modela,list(area=xv),type="response")
plot(area,incidence,pch=21,bg="yellow")
lines(xv,yv,col="blue")
xv2<-seq(0,10,0.1)
yv2<-predict(modeli,list(isolation=xv2),type="response")
plot(isolation,incidence,pch=21,bg="yellow")
lines(xv2,yv2,col="red")
This is well and good, but it is very difficult to know how good the fit of the model is
whenthedataareshownonlyas0sor1s.Itissensibletocomputeoneormoreintermediate
probabilities from the data, and to show these empirical estimates (ideally with their
standard errors) on the plot in order to judge whether the fitted line is a reasonable
description of the data.
Forthepurposesofdemonstration,wedividetherangesofareaandofisolationinto(say)
three,counthowmanysuccessesandfailuresoccurredineachinterval,calculatethemean
proportionincidenceinepachffiffiffiffitffihffiffiiffiffirffidffiffiffi,ffiffipffiffi,ffiffiaffiffindaddtheseestimatestotheplotaspointsalongwith
theirstandarderrorbars p 1 p=n.Startbyusingcuttoobtainthebreakpointsonthe
two axes:
BINARY RESPONSEVARIABLE 277
ac<-cut(area,3)
ic<-cut(isolation,3)
tapply(incidence,ac,sum)
(0.144,3.19] (3.19,6.23] (6.23,9.28]
7 8 14
tapply(incidence,ic,sum)
(2.02,4.54](4.54,7.06](7.06,9.58]
12 17 0
There were seven data points indicating occupation (success) in the lowest third of the
area axis and 14 in the highest third. There were 12 data points indicating occupation
(success)inthelowestthirdofrangeofvaluesforisolationandnoneinthehighestthird.
Notetheconventionforlabellingintervals:(a,b]meansincludetheright-handendpoint,b
(the square bracket), but not the left one, a (the round bracket).
Now count the total number of islands in each interval using table:
table(ac)
ac
(0.144,3.19] (3.19,6.23] (6.23,9.28]
21 15 14
table(ic)
ic
(2.02,4.54](4.54,7.06](7.06,9.58]
12 25 13
Theprobabilityofoccupationisgivenbydividingthenumberofsuccessesbythenumber
of cases:
tapply(incidence,ac,sum)/table(ac)
(0.144,3.19] (3.19,6.23] (6.23,9.28]
0.3333333 0.5333333 1.0000000
tapply(incidence,ic,sum)/table(ic)
(2.02,4.54](4.54,7.06](7.06,9.58]
1.00 0.68 0.00
pffiffiffiffiffiffiffiffiffiffi
Theideaistoplotthesemeanproportionsandtheirstandarderrors( pq=n)alongwith
theregressionlinefromthemodeltoseehowclosethemodelgetstothethreecalculated
proportions:
xv<-seq(0,9,0.01)
yv<-predict(modela,list(area=xv),type="response")
plot(area,incidence,pch=21,bg="yellow")
lines(xv,yv,col="blue")
278 STATISTICS:ANINTRODUCTIONUSINGR
d<-(max(area)-min(area))/3
left<-min(area)+d/2
mid<-left+d
right<-mid+d
xva<-c(left,mid,right)
pa<-as.vector(tapply(incidence,ac,sum)/table(ac))
se<-sqrt(pa*(1-pa)/table(ac))
xv<-seq(0,9,0.01)
yv<-predict(modela,list(area=xv),type="response")
lines(xv,yv,col="blue")
points(xva,pa,pch=16,col="red")
for(iin1:3)lines(c(xva[i],xva[i]),
c(pa[i]+se[i],pa[i]-se[i]),col="red")
xv2<-seq(0,10,0.1)
yv2<-predict(modeli,list(isolation=xv2),type="response")
plot(isolation,incidence,pch=21,bg="yellow")
lines(xv2,yv2,col="red")
d<-(max(isolation)-min(isolation))/3
left<-min(isolation)+d/2
mid<-left+d
right<-mid+d
xvi<-c(left,mid,right)
pi<-as.vector(tapply(incidence,ic,sum)/table(ic))
se<-sqrt(pi*(1-pi)/table(ic))
points(xvi,pi,pch=16,col="blue")
for(iin1:3)lines(c(xvi[i],xvi[i]),
c(pi[i]+se[i],pi[i]-se[i]),col="blue")
Youcanseeatoncethatthefitfortheright-handgraphofincidenceagainstisolationis
excellent;thelogisticisaverygoodmodelforthesedata.Incontrast,thefitfortheleft-hand
graph of incidence against area is poor; at low values of area the model (blue line)
underestimatestheobserveddata(redpointwitherrorbars)whileforintermediatevaluesof
area the model (blue line) overestimates the observed data (red point with error bars).
BINARY RESPONSEVARIABLE 279
Thisapproachofplottingthetwoeffectsseparatelywouldnotwork,ofcourse,ifthere
wereasignificantinteractionbetweenareaandisolation;thenyouwouldneedtoproduce
conditioning plots of incidence against area for different degrees of isolation.
ANCOVAwithaBinaryResponseVariable
Inthis example thebinaryresponsevariableisparasiteinfection (infected ornot)and the
explanatoryvariablesareweightandage(continuous)andsex(categorical).Webeginwith
data inspection:
infection<-read.csv("c:\\temp\\infection.csv")
attach(infection)
names(infection)
[1]"infected""age" "weight" "sex"
Forthecontinuousexplanatoryvariables,weightandage,itisusefultolookatbox-and-
whisker plots of the data:
windows(7,4)
par(mfrow=c(1,2))
plot(infected,weight,xlab="Infection",ylab="Weight",
col="lightblue")
plot(infected,age,xlab="Infection",ylab="Age",col="lightgreen")
Infectedindividualsaresubstantiallylighterthanuninfectedindividuals,andoccurina
much narrower range ofages.To seetherelationshipbetween infectionand gender (both
categorical variables) we can use table:
table(infected,sex)
sex
infected female male
absent 17 47
present 11 6
Thisindicatesthattheinfectionismuchmoreprevalentinfemales(11/28)thaninmales(6/53).
280 STATISTICS:ANINTRODUCTIONUSINGR
Wegetdowntobusiness,asusual,byfittingamaximalmodelwithdifferentslopesfor
each level of the categorical variable:
model<-glm(infected∼age*weight*sex,family=binomial)
summary(model)
Coefficients:
Estimate Std.Error zvalue Pr(>|z|)
(Intercept) -0.109124 1.375388 -0.079 0.937
age 0.024128 0.020874 1.156 0.248
weight -0.074156 0.147678 -0.502 0.616
sexmale -5.969109 4.278066 -1.395 0.163
age:weight -0.001977 0.002006 -0.985 0.325
age:sexmale 0.038086 0.041325 0.922 0.357
weight:sexmale 0.213830 0.343265 0.623 0.533
age:weight:sexmale -0.001651 0.003419 -0.483 0.629
(Dispersionparameterforbinomialfamilytakentobe1)
Nulldeviance:83.234 on80 degreesoffreedom
Residualdeviance:55.706 on73 degreesoffreedom
AIC:71.706
NumberofFisherScoringiterations:6
Itcertainlydoesnotlookasifanyofthehigh-orderinteractionsaresignificant.Insteadof
usingupdateandanovaformodelsimplification,wecanusesteptocomputetheAICfor
each term in turn:
model2<-step(model)
Start: AIC=71.71
First, it tests whether the three-way interaction is required:
Df Deviance AIC
-age:weight:sex 1 55.943 69.943
<none> 55.706 71.706
Step: AIC=69.94
ThiscausesareductioninAICofjust71.7–69.9=1.8andhenceisnotsignificant,sothe
three-way interaction is eliminated.
Next,itlooksatthetwo-wayinteractionsanddecideswhichofthethreetodeletefirst:
Df Deviance AIC
-weight:sex 1 56.122 68.122
-age:sex 1 57.828 69.828
<none> 55.943 69.943
-age:weight 1 58.674 70.674
Step: AIC=68.12
BINARY RESPONSEVARIABLE 281
Only the removal of the weight–sex interaction causes a reduction in AIC, so this
interactionindeletedandtheothertwointeractionsareretained.Letusseeifwewouldhave
been this lenient:
summary(model2)
Call:
glm(formula=infected∼age+weight+sex+age:weight+age:sex,
family=binomial)
Coefficients:
Estimate Std.Error zvalue Pr(>|z|)
(Intercept) -0.391572 1.264850 -0.310 0.7569
age 0.025764 0.014918 1.727 0.0842 .
weight -0.036493 0.128907 -0.283 0.7771
sexmale -3.743698 1.786011 -2.096 0.0361 *
age:weight -0.002221 0.001365 -1.627 0.1037
age:sexmale 0.020464 0.015199 1.346 0.1782
(Dispersionparameterforbinomialfamilytakentobe1)
Nulldeviance:83.234 on80 degreesoffreedom
Residualdeviance:56.122 on75 degreesoffreedom
AIC:68.122
Neither of the two interactions retained by stepwould figure in our model (p>0.10).
We shall use updateto simplify model2:
model3<-update(model2,∼.-age:weight)
anova(model2,model3,test="Chi")
AnalysisofDevianceTable
Model1:infected∼age+weight+sex+age:weight+age:sex
Model2:infected∼age+weight+sex+age:sex
Resid. Df Resid.Dev Df Deviance Pr(>Chi)
1 75 56.122
2 76 58.899 -1 -2.777 0.09562 .
So there is no really persuasive evidence of an age–weight term (p=0.096).
model4<-update(model2,∼.-age:sex)
anova(model2,model4,test="Chi")
AnalysisofDevianceTable
Model1:infected∼age+weight+sex+age:weight+age:sex
Model2:infected∼age+weight+sex+age:weight
Resid. Df Resid.Dev Df Deviance Pr(>Chi)
1 75 56.122
2 76 58.142 -1 -2.0203 0.1552
282 STATISTICS:ANINTRODUCTIONUSINGR
Note that we are testing all the two-way interactions by deletion from the model that
contains all two-way interactions (model2): p=0.1552, so nothing there, then.
What about the three main effects?
model5<-glm(infected∼age+weight+sex,family=binomial)
summary(model5)
Coefficients:
Estimate Std.Error zvalue Pr(>|z|)
(Intercept) 0.609369 0.803288 0.759 0.448096
age 0.012653 0.006772 1.868 0.061701 .
weight -0.227912 0.068599 -3.322 0.000893 ***
sexmale -1.543444 0.685681 -2.251 0.024388 *
(Dispersionparameterforbinomialfamilytakentobe1)
Nulldeviance:83.234 on80 degreesoffreedom
Residualdeviance:59.859 on77 degreesoffreedom
AIC:67.859
NumberofFisherScoringiterations:5
Weight is highly significant, as we expected from the initial boxplot, sex is quite
significant,andageismarginally significant.Itisworth establishingwhether thereisany
evidenceofnon-linearityintheresponseofinfectiontoweightorage.Wemightbeginby
fitting quadratic terms for the two continuous explanatory variables:
model6<-glm(infected∼age+weight+sex+I(weight^2)+I(age^2),
family=binomial)
summary(model6)
Coefficients:
Estimate Std.Error zvalue Pr(>|z|)
(Intercept) -3.4475839 1.7978359 -1.918 0.0552 .
age 0.0829364 0.0360205 2.302 0.0213 *
weight 0.4466284 0.3372352 1.324 0.1854
sexmale -1.2203683 0.7683288 -1.588 0.1122
I(weight^2) -0.0415128 0.0209677 -1.980 0.0477 *
I(age^2) -0.0004009 0.0002004 -2.000 0.0455 *
(Dispersionparameterforbinomialfamilytakentobe1)
Nulldeviance:83.234 on80 degreesoffreedom
Residualdeviance:48.620 on75 degreesoffreedom
AIC:60.62
NumberofFisherScoringiterations:6
Evidently, both relationships are significantly non-linear. It is worth looking at these
non-linearitiesinmoredetail,toseeifwecandobetterwithotherkindsofmodels(e.g.non-
parametric smoothers, piecewise linear models or step functions). A good start is often a
generalized additive model when we have continuous covariates:
BINARY RESPONSEVARIABLE 283
library(mgcv)
model7<-gam(infected∼sex+s(age)+s(weight),family=binomial)
plot.gam(model7)
Thesenon-parametricsmoothersareexcellentatshowingthehumpedrelationshipbetween
infection and age, and at highlighting the possibility of a threshold at weight 8 in the
relationshipbetweenweightandinfection.
WecannowreturntoaGLMtoincorporatetheseideas.Weshallfitageandage^2as
before,buttryapiecewiselinearfitforweight,estimatingthethresholdweightatarange
ofvalues(say,8–14)andselectingthethresholdthatgivesthelowestresidualdeviance;this
turnsouttobeathresholdof12(abithigherthanitappearsfromthegamplot,above).The
piecewise regression is specified by the term:
I((weight–12)*(weight>12))
The I (‘as is’) is necessary to stop the * being evaluated as an interaction term in the
modelformula.Whatthisexpressionsaysis:regressinfectiononthevalueofweight–12,
butonlydothiswhenweight>12istrue;otherwise,assumethatinfectionisindependent
of weight.
model8<-glm(infected∼sex+age+I(age^2)+
I((weight-12)*(weight>12)),family=binomial)
summary(model8)
Coefficients:
Estimate Std.Error zvalue Pr(>|z|)
(Intercept) -2.7511382 1.3678824 -2.011 0.0443 *
sexmale -1.2864683 0.7349201 -1.750 0.0800 .
age 0.0798629 0.0348184 2.294 0.0218 *
I(age^2) -0.0003892 0.0001955 -1.991 0.0465 *
I((weight-12)*(weight>12)) -1.3547520 0.5350853 -2.532 0.0113 *
(Dispersionparameterforbinomialfamilytakentobe1)
Nulldeviance:83.234 on80 degreesoffreedom
Residualdeviance:48.687 on76 degreesoffreedom
AIC:58.687
NumberofFisherScoringiterations:7
284 STATISTICS:ANINTRODUCTIONUSINGR
model9<-update(model8,∼.-sex)
anova(model8,model9,test="Chi")
model10<-update(model8,∼.-I(age^2))
anova(model8,model10,test="Chi")
Theeffectofsexoninfectionisnotquitesignificant(p=0.071forachi-squaredteston
deletion),soweleaveitout.Thequadratictermforagedoesnotlookhighlysignificanthere
(p=0.0465), but a deletion test gives p=0.011, so we retain it. The minimal adequate
model is therefore model9:
summary(model9)
Coefficients:
Estimate Std.Error zvalue Pr(>|z|)
(Intercept) -3.1207552 1.2665593 -2.464 0.0137*
age 0.0765784 0.0323376 2.368 0.0179*
I(age^2) -0.0003843 0.0001846 -2.081 0.0374*
I((weight-12)*(weight>12)) -1.3511706 0.5134681 -2.631 0.0085**
(Dispersionparameterforbinomialfamilytakentobe1)
Nulldeviance:83.234 on80 degreesoffreedom
Residualdeviance:51.953 on77 degreesoffreedom
AIC:59.953
NumberofFisherScoringiterations:7
Weconcludethereisahumpedrelationshipbetweeninfectionandage,andathreshold
effect of weight on infection. The effect of sex is marginal, but might repay further
investigation (p=0.071).
FurtherReading
Collett, D.(1991) Modelling BinaryData,Chapman& Hall, London.
Cox, D.R.andSnell, E.J.(1989)Analysis of BinaryData,Chapman &Hall, London.
16
Death and Failure Data
Time-to-deathdata,anddataonfailuretimes,areoftenencounteredinstatisticalmodelling.
The mainproblemisthat thevarianceinsuchdata isalmostalwaysnon-constant,andso
standardmethodsareinappropriate.Iftheerrorsaregammadistributed,thenthevarianceis
proportionaltothesquareofthemean(recallthatwithPoissonerrors,thevarianceisequal
tothemean).Itisstraightforwardtodealwithsuchdatausingageneralizedlinear model
(GLM) with gamma errors.
This case study has 50 replicates in each of three treatments: an untreated control, low
dosageandhighdosageofanovelcancertreatment.Theresponseistheageatdeathforthe
rats (expressed as an integer number of months):
mortality<-read.csv("c:\\temp\\deaths.csv")
attach(mortality)
names(mortality)
[1]"death" "treatment"
tapply(death,treatment,mean)
control high low
3.46 6.88 4.70
Theanimalsreceivingthehighdoselivedroughlytwiceaslongastheuntreatedcontrols.
The low dose increased life expectancy by more than 35%. The variance in age at death,
however, is not constant:
tapply(death,treatment,var)
control high low
0.4167347 2.4751020 0.8265306
The variance is much greater for the longer-lived individuals, so we should not use
standardstatisticalmodelswhichassumeconstantvarianceandnormalerrors.Butwecan
use a GLM with gamma errors:
model<-glm(death∼treatment,Gamma)
summary(model)
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
286 STATISTICS:ANINTRODUCTIONUSINGR
Coefficients:
Estimate Std.Error tvalue Pr(>|t|)
(Intercept) 0.289017 0.008327 34.708 <2e-16***
treatmenthigh -0.143669 0.009321 -15.414 <2e-16***
treatmentlow -0.076251 0.010340 -7.374 1.11e-11***
(DispersionparameterforGammafamilytakentobe0.04150576)
Nulldeviance:17.7190 on149 degreesoffreedom
Residualdeviance: 5.8337 on147 degreesoffreedom
AIC:413.52
NumberofFisherScoringiterations:4
Thelinkfunctionwithgammaerrorsisthereciprocal:thatiswhytheparameterforthe
highdoseappearsasanegativeterminthesummarytable:themeanvalueforhighdoseis
calculated as 0.289 0.1437=0.1453, and 1/0.1453=6.882. Checking the model using
plot(model) shows that it is reasonably well behaved (you might like to compare the
behaviour of lm(death∼treatment)). We conclude that all three treatment levels are
significantly different from one another.
A common difficulty with data on time at death is that some (or even many) of the
individualsdonotdieduringthetrial,sotheirageatdeathremainsunknown(theymight
DEATH ANDFAILURE DATA 287
recover, they might leave the trial, or the experiment might end before they die). These
individualsaresaidtobecensored.Censoringmakestheanalysismuchmorecomplicated,
becausethecensoredindividualsprovidesomeinformation(weknowtheageatwhichthey
werelastseenalive)butthedataareofadifferenttypefromtheinformationonageatdeath
which is the response variable in the main analysis. There is a whole field of statistical
modelling for such data: it is called survival analysis.
detach(mortality)
SurvivalAnalysiswithCensoring
Thenextexamplecomesfromastudyofmortalityin150wildmalesheep.Therewerethree
experimentalgroups,andtheanimalswerefollowedfor50months.Thegroupsweretreated
withthreedifferentmedicinesagainsttheirgutparasites:groupAreceivedaboluswithahigh
doseofworm-killer,groupBreceivedalowdose,andgroupCreceivedtheplacebo(abolus
withnoworm-killingcontents).Theinitialbodymassofeachindividual(weight)wasrecorded
asacovariate.Themonthinwhicheachanimaldied(death)wasrecorded,andanimalswhich
surviveduptothe50thmonth(theendofthestudy)wererecordedasbeingcensored(theythus
havecensoringindicatorstatus=0,whereastheanimalsthatdiedallhavestatus=1).
library(survival)
sheep<-read.csv("c:\\temp\\sheep.deaths.csv")
attach(sheep)
names(sheep)
[1]"death" "status""weight""group"
The overall survivorship curves for the three groups of animals are obtained like this:
plot(survfit(Surv(death,status)∼group),col=c(2,3,4),
xlab="Ageatdeath(months)")
288 STATISTICS:ANINTRODUCTIONUSINGR
The crosses + at the end of the survivorship curves for groups A (red) and B (green)
indicatethattherewascensoringinthesegroups(notalloftheindividualsweredeadatthe
end of the experiment). Parametric regression in survival models uses the survreg
function, for which you can specify a wide range of different error distributions. Here
weusetheexponentialdistributionforthepurposesofdemonstration(youcanchosefrom
dist="extreme","logistic","gaussian"or "exponential"andfrom link=
"log" or "identity"). We fit the full analysis of covariance model to begin with:
model<-survreg(Surv(death,status)∼weight*group,dist="exponential")
summary(model)
Call:
survreg(formula=Surv(death,status)∼weight*group,dist=
"exponential")
Value Std.Error z p
(Intercept) 3.8702 0.3854 10.041 1.00e-23
weight -0.0803 0.0659 -1.219 2.23e-01
groupB -0.8853 0.4508 -1.964 4.95e-02
groupC -1.7804 0.4386 -4.059 4.92e-05
weight:groupB 0.0643 0.0674 0.954 3.40e-01
weight:groupC 0.0796 0.0674 1.180 2.38e-01
Scalefixedat1
Exponentialdistribution
Loglik(model)=-480.6 Loglik(interceptonly)=-502.1
Chisq=43.11on5degreesoffreedom,p=3.5e-08
NumberofNewton-RaphsonIterations:5
n=150
Modelsimplificationproceedsinthenormalway.Youcoulduseupdate,buthere(for
varietyonly)werefitprogressivelysimplermodelsandtestthemusinganova.Firstwetake
out the different slopes for each group:
model2<-survreg(Surv(death,status)∼weight+group,dist="exponential")
anova(model,model2,test="Chi")
TermsResid.Df -2*LL TestDf Deviance P(>|Chi|)
1 weight*group 144961.1800 NA NA NA
2 weight+group 146962.9411 -weight:group-2 -1.761142 0.4145462
The interaction is not significant so we leave it out and try deleting weight:
model3<-survreg(Surv(death,status)∼group,dist="exponential")
anova(model2,model3,test="Chi")
TermsResid.Df -2*LL TestDf Deviance P(>|Chi|)
1 weight+group 146962.9411 NA NA NA
2 group 147963.9393 -weight-1 -0.9981333 0.3177626
DEATH ANDFAILURE DATA 289
This is not significant, so we leave it out and try deleting group:
model4<-survreg(Surv(death,status)∼1,dist="exponential")
anova(model3,model4,test="Chi")
TermsResid.Df -2*LL TestDf Deviance P(>|Chi|)
1 group 147 963.9393 NA NA NA
2 1 149 1004.2865 -2 -40.34721 1.732661e-09
Thisishighlysignificant,soweadditback.Theminimaladequatemodelismodel3with
the three-level factor group, but there is no evidence that initial body weight had any
influence on survival.
summary(model3)
Call:
survreg(formula=Surv(death,status)∼group,dist="exponential")
Value Std.Error z p
(Intercept) 3.467 0.167 20.80 3.91e-96
groupB -0.671 0.225 -2.99 2.83e-03
groupC -1.386 0.219 -6.34 2.32e-10
Scalefixedat1
Exponentialdistribution
Loglik(model)=-482 Loglik(interceptonly)=-502.1
Chisq=40.35on2degreesoffreedom,p=1.7e-09
NumberofNewton-RaphsonIterations:5
n=150
Weneedtoretainallthreegroups(groupBissignificantlydifferentfrombothgroupA
and group C).
It is straightforward to compare error distributions for the same model structure:
model3<-survreg(Surv(death,status)∼group,dist="exponential")
model4<-survreg(Surv(death,status)∼group,dist="extreme")
model5<-survreg(Surv(death,status)∼group,dist="gaussian")
model6<-survreg(Surv(death,status)∼group,dist="logistic")
anova(model3,model4,model5,model6)
TermsResid.Df -2*LL TestDf Deviance Pr(>Chi)
1 group 147 963.9393 NA NA NA
2 group 146 1225.3512 = 1-261.411949 NA
3 group 146 1178.6582 = 0 46.692975 NA
4 group 146 1173.9478 = 0 4.710457 NA
Our initial choice of exponential was clearly the best, giving much the lowest residual
deviance (963.94).
290 STATISTICS:ANINTRODUCTIONUSINGR
You can immediately see the advantage of doing proper survival analysis when you
comparethepredictedmeanagesatdeathfrommodel3withthecrudearithmeticaverages
of the raw data on age at death:
tapply(predict(model3,type="response"),group,mean)
A B C
32.05555 16.38635 8.02
tapply(death,group,mean)
A B C
23.08 14.42 8.02
Ifthereisnocensoring(asingroupC,wherealltheindividualsdied)thentheestimated
mean ages at death are identical. But when there is censoring, the arithmetic mean
underestimates the age at death, and when the censoring is substantial (as in group A)
this underestimate is very large (23.08 vs. 32.06 months).
FurtherReading
Cox, D.R.andOakes, D.(1984)Analysis ofSurvival Data,Chapman &Hall, London.
Kalbfleisch,J.andPrentice,R.L.(1980)TheStatisticalAnalysisofFailureTimeData,JohnWiley&
Sons,New York.
Appendix
Essentials of the R Language
RasaCalculator
The command line after the screen prompt > is an excellent calculator:
>log(42/7.3)
[1]1.749795
By default, logs in R are base e (natural or Napierian, not base 10 logs), but you
canspecifyanybaseyouwant,asasecondargumenttothelogfunction.Hereislogbase
2 of 16:
>log(16,2)
[1]4
Each line can have many characters, but if you want to evaluate a complicated
expression, you might like to continue it on one or more further lines for clarity. The
wayyoudothisisbyendingthelineataplacewherethelineisobviouslyincomplete(e.g.
with a trailing comma, operator, or with more left parentheses than right parentheses,
implyingthatmorerightparentheseswillfollow).Whencontinuationisexpected,theline
prompt changes from > to +
>5+6+3+6+4+2+4+8+
+ 3+2+7
[1]50
Notethatthe+continuationpromptdoesnotcarryoutarithmeticplus.Ifyouhavemadea
mistake,andyouwanttogetridofthe+promptandreturntothe>prompt,thenpressthe
Esc key and use the Up arrow to edit the last (incomplete) line.
From here onwards (and throughout the book), the prompt character > is omitted.
Thematerialthatyoushouldtypeonthecommandlineisshowninredfontinthisbook.
Just press the Return key to see the answer. The output from R is shown in dark blue
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
292 STATISTICS:ANINTRODUCTIONUSINGR
CourierNewfont,whichusesabsoluteratherthanproportionalspacing,sothatcolumns
of numbers remain neatly aligned on the page and on the screen.
Twoormoreexpressionscanbeplacedonasinglelinesolongastheyareseparatedby
semicolons:
2+3; 5*7; 3-7
[1]5
[1]35
[1]-4
Alloftheusualcalculationscanbedonedirectly,andthestandardorderofprecedence
applies:powersandrootsareevaluatedfirst,thenmultiplicationanddivision,thenfinally
additionandsubtraction.Althoughthereisanamedfunctionforsquareroots,sqrt,roots
aregenerallycalculatedasfractionalpowers.Exponentiation(powers)usesthecaret(‘hat’)
^operator(not**asinsomecomputinglanguageslikeFortranorGLIM).Sothecuberoot
of 8 is
8^(1/3)
[1]2
The exponentiation operator ^ groups right to left, but all other operators group left to
right.Thus,2^2^3is2^8,not4^3,whereas1-1-1is-1,not1.Usebracketsas
jy  y j
necessarytooverridetheprecedence.Forattest,wherethevaluerequiredis A B and
SE
diff
the numbers are 5.7, 6.8 and 0.38, you type:
abs(5.7-6.8)/0.38
[1]2.894737
ThedefaultnumberofdigitsprintedoutbyRis7(asabove).Youcancontrolthenumber
of digits printed using the digitsoption like this:
options(digits=3)
abs(5.7-6.8)/0.38
[1]2.89
Built-inFunctions
Allthemathematicalfunctionsyoucouldeverwantarehere(TableA.1).Wehavealready
met the log function; the antilog function to the base e is exp:
exp(1)
[1]2.718282
APPENDIX: ESSENTIALS OF THE RLANGUAGE 293
ThetrigonometricfunctionsinRmeasureanglesinradians.Acircleis2π radians,and
this is 360°, so a right-angle (90°) is π=2 radians. R knows the value of π as pi:
pi
[1]3.141593
sin(pi/2)
[1]1
cos(pi/2)
[1]6.123032e-017
Noticethatthecosineofaright-angledoesnotcomeoutasexactlyzero,eventhoughthe
sine came out as exactly 1. The e-017 means ‘times 10  17 ’. While this is a very small
number it is clearly not exactly zero (so you need to be careful when testing for exact
equality of real numbers; see p. 313).
TableA.1 MathematicalfunctionsusedinR
Function Meaning
log(x) logtobaseeofx
exp(x) antilogofx(=2.7818 x )
log(x,n) logtobasenofx
log10(x) logtobase10ofx
sqrt(x) squarerootofx
factorial(x) x!
choose(n,x) binomialcoefficients
n!/(x!(n–x)!)
gamma(x) Γ x
(x–1)!forintegerx
lgamma(x) naturallogofgamma(x)
floor(x) greatestinteger<x
ceiling(x) smallestinteger>x
trunc(x) closestintegertoxbetweenxand0:trunc
(1.5)=1,trunc( 1.5)= 1
truncislikefloorforpositivevaluesandlike
ceilingfornegativevalues
round(x,digits=0) roundthevalueofxtoaninteger
signif(x,digits=6) givextosixdigitsinscientificnotation
runif(n) generatesnrandomnumbersbetween
0and1fromauniformdistribution
cos(x) cosineofxinradians
sin(x) sineofxinradians
(continued)
294 STATISTICS:ANINTRODUCTIONUSINGR
TableA.1 (Continued)
Function Meaning
tan(x) tangentofxinradians
acos(x),asin(x),atan(x) inverse trigonometric transformations of real or
complexnumbers.
acosh(x),asinh(x),atanh(x) inverse hyperbolic trigonometric transformations
onrealorcomplexnumbers
abs(x) the absolute value of x, ignoring the minus sign if
thereisone
NumberswithExponents
For very big numbers or very small numbers R uses the following scheme:
1.2e3 means1200becausethee3means‘movethedecimalpoint3placestothe
right’
1.2e-2 means0.012becausethee-2means‘movethedecimalpoint2placestothe
left’
3.9+4.5i isacomplexnumberwithreal(3.9)andimaginary(4.5)parts,andiisthe
squarerootof 1
ModuloandIntegerQuotients
Integer quotients and remainders are obtained using the notation %/% (percent, divide,
percent)and%%(percent,percent)respectively.Supposewewanttoknowtheintegerpartof
a division – say, how many 13s are there in 119:
119%/%13
[1]9
Nowsupposewewantedtoknowtheremainder(whatisleftoverwhen119isdividedby
13), known in maths as modulo:
119%%13
[1]2
Moduloisveryusefulfortestingwhethernumbersareoddoreven:oddnumbersare1
modulo 2 and even numbers are 0 modulo 2:
9%%2
[1]1
8%%2
[1]0
APPENDIX: ESSENTIALS OF THE RLANGUAGE 295
Likewise, you use modulo to test if one number is an exact multiple of some other
number. For instance, to find out whether 15 421 is a multiple of 7, type:
15421%%7==0
[1]TRUE
Assignment
Objects obtain values in R by assignment (‘x gets a value’). This is achieved by the
getsarrowwhichisacompositesymbolmadeupfrom‘lessthan’and‘minus’<-withouta
space between them. Thus, to create a scalar constant x with value 5 we type
x<-5
and not x = 5. Notice that there is a potential ambiguity if you get the spacing wrong.
Compareourx<-5meaning‘xgets5’andx<-5whichisalogicalquestion,asking‘is
x less than minus 5?’ and producing the answer TRUE or FALSE.
Rounding
Varioussortsofrounding(roundingup,roundingdown,roundingtothenearestinteger)
can be done easily. Take 5.7 as an example. The ‘greatest integer less than’ function is
floor:
floor(5.7)
[1]5
The ‘next integer’ function is ceiling:
ceiling(5.7)
[1]6
Youcanroundtothenearestintegerbyadding0.5tothenumberthenusingfloor.There
isabuilt-infunctionforthis,butwecaneasilywriteoneofourowntointroducethenotion
of writing functions. Call it rounded, then define it as a function like this:
rounded<-function(x) floor(x+0.5)
Now we can use the new function:
rounded(5.7)
[1]6
rounded(5.4)
[1]5
296 STATISTICS:ANINTRODUCTIONUSINGR
InfinityandThingsthatAreNotaNumber(NaN)
Calculations can lead to answers that are plus infinity, represented in R by Inf:
3/0
[1]Inf
or minus infinity, which is –Inf in R:
-12/0
[1]-Inf
Calculations involving infinity can be evaluated:
exp(-Inf)
[1]0
0/Inf
[1]0
(0:3)^Inf
[1] 0 1InfInf
Thesyntax0:3isveryuseful.Itgeneratersaseries(0,1,2,3inthiscase)andthefunction
isevaluatedforeachofthevaluestoproduceavectorofanswers.Inthiscase,thevectoris
of length = 4.
Other calculations, however, lead to quantities that are not numbers. These are repre-
sented in R by NaN (‘not a number’). Here are some of the classic cases:
0/0
[1]NaN
Inf-Inf
[1]NaN
Inf/Inf
[1]NaN
YouneedtounderstandclearlythedistinctionbetweenNaNandNA(thisstandsfor‘not
available’andisthemissingvaluesymbolinR;seebelow).Therearebuilt-inteststocheck
whether a number is finite or infinite:
is.finite(10)
[1]TRUE
APPENDIX: ESSENTIALS OF THE RLANGUAGE 297
is.infinite(10)
[1]FALSE
is.infinite(Inf)
[1]TRUE
MissingValues(NA)
Missingvaluesindataframesarearealsourceofirritationbecausetheyaffectthewaythat
model-fittingfunctionsoperate,andtheycangreatlyreducethepowerofthemodellingthat
we would like to do.
Somefunctionsdonotworkwiththeirdefaultsettingswhentherearemissingvaluesin
the data, and mean is a classic example of this:
x<-c(1:8,NA)
mean(x)
[1]NA
Inordertocalculatethemeanofthenon-missingvalues,youneedtospecifythattheNA
are to be removed, using the na.rm=TRUE argument:
mean(x,na.rm=T)
[1]4.5
Tocheckforthelocationofmissingvalueswithinavector,usethefunctionis.na(x).
Hereisanexamplewherewewanttofindthelocations(7and8)ofmissingvalueswithina
vector called vmv:
(vmv<-c(1:6,NA,NA,9:12))
[1] 1 2 3 4 5 6 NA NA 9 10 11 12
Notetheuseofroundbracketstogettheanswerprintedaswellasallocated.Makingan
index of the missing values in an array could use the seq function:
seq(along=vmv)[is.na(vmv)]
[1]78
However, the result is achieved more simply using which like this:
which(is.na(vmv))
[1]78
Ifthemissingvaluesaregenuinecountsofzero,youmightwanttoedittheNAto0.Use
the is.na function to generate subscripts for this:
(vmv[is.na(vmv)]<-0)
[1] 1 2 3 4 5 6 0 0 9 10 11 12
298 STATISTICS:ANINTRODUCTIONUSINGR
Alternatively, use the ifelse function like this:
vmv<-c(1:6,NA,NA,9:12)
ifelse(is.na(vmv),0,vmv)
[1] 1 2 3 4 5 6 0 0 9 10 11 12
Operators
R uses the following operator tokens
+ -* / %% ^ arithmetic
> >= < <= == != relational
! & | logical
∼ modelformulae
<- -> assignment
$ listindexing(the‘elementname’operator)
: createasequence
Severaloftheoperatorshavedifferentmeaninginsidemodelformulas.Thusinamodel
formula*indicatesthemaineffectsplusinteraction,:indicatestheinteractionbetweentwo
variables and ^ means all interactions up to the indicated power.
CreatingaVector
Vectors are variables with one or more values of the same type: logical, integer, real,
complex,string(orcharacter)orraw.Valuescanbeassignedtovectorsinmanydifferent
ways. They can be generated by R: here the vector called y gets the sequence of integer
values 10 to 16 using : (colon), the sequence operator:
y<-10:16
You can type the values into the command line, using the concatenation function c:
y<-c(10,11,12,13,14,15,16)
Alternatively, you can enter then numbers from the keyboard one at a time using scan:
y<-scan()
1:10
2:11
3:12
4:13
5:14
6:15
7:16
8:
Read7items
APPENDIX: ESSENTIALS OF THE RLANGUAGE 299
PresstheEnterkeytwicetoindicatethatdatainputiscomplete.However,thecommonest
waytoallocatevaluestoavectoristoreadthedatafromanexternalfile,usingread.csv
or read.table (p. 25).
NamedElementswithinVectors
Itisoftenusefultohavethevaluesinavectorlabelledinsomeway.Forinstance,ifourdata
are counts of 0, 1, 2, . . . occurrences in a vector called counts,
(counts<-c(25,12,7,4,6,2,1,0,2))
[1] 25 12 7 4 6 2 1 0 2
sothattherewere25zeros,12onesandsoon,itwouldbeusefultonameeachofthecounts
with the relevant number 0, 1, . . . , 8:
names(counts)<-0:8
Now when we inspect the vector called counts we see both the names and the
frequencies:
counts
0 1 2 3 4 5 6 7 8
2512 7 4 6 2 1 0 2
Ifyouhavecomputedatableofcounts,andyouwanttoremovethenames,thenuse
the as.vector function like this:
(st<-table(rpois(2000,2.3)))
0 1 2 3 4 5 6 7 8 9
205455510431233102 43 13 7 1
as.vector(st)
[1] 205 455 510 431 233 102 43 13 7 1
VectorFunctions
OneofthegreatstrengthsofRisitsabilitytoevaluatefunctionsoverentirevectors,thereby
avoidingtheneedforloopsandsubscripts.Themostimportantvectorfunctionsareshown
in Table A.2.
300 STATISTICS:ANINTRODUCTIONUSINGR
TableA.2 VectorfunctionsinR
Operation Meaning
max(x) maximumvalueinx
min(x) minimumvalueinx
sum(x) totalofallthevaluesinx
mean(x) arithmeticaverageofthevaluesinx
median(x) medianvalueinx
range(x) vectorofmin(x)andmax(x)
var(x) samplevarianceofx,withdegreesoffreedom=length(x) 1
cor(x,y) correlationbetweenvectorsxandy
sort(x) asortedversionofx
rank(x) vectoroftheranksofthevaluesinx
order(x) an integer vector containing the permutation to sort x into ascending
order
quantile(x) vectorcontainingtheminimum,lowerquartile,median,upper
quartile,andmaximumofx
cumsum(x) vectorcontainingthesumofalloftheelementsuptothatpoint
cumprod(x) vectorcontainingtheproductofalloftheelementsuptothatpoint
cummax(x) vector ofnon-decreasingnumberswhich arethecumulativemaxima
ofthevaluesinxuptothatpoint.
cummin(x) vector of non-increasing numbers which are the cumulative minima
ofthevaluesinxuptothatpoint.
pmax(x,y,z) vector, of length equal to the longest of x, y, or z containing the
maximumofx,yorzfortheithpositionineach
pmin(x,y,z) vector, of length equal to the longest of x, y, or z containing the
minimumofx,yorzfortheithpositionineach
colMeans(x) columnmeansofdataframeormatrixx
colSums(x) columntotalsofdataframeormatrixx
rowMeans(x) rowmeansofdataframeormatrixx
rowSums(x) rowtotalsofdataframeormatrixx
SummaryInformationfromVectorsbyGroups
Oneofthemostimportantandusefulvectorfunctionstomasteristapply.The‘t’stands
for ‘table’ and the idea is to apply a function to produce a table from the values in the
vector,basedononeormoregroupingvariables(oftenthegroupingisbyfactorlevels).This
sounds much more complicated than it really is:
data<-read.csv("c:\\temp\\daphnia.csv")
attach(data)
names(data)
[1]"Growth.rate""Water" "Detergent" "Daphnia"
APPENDIX: ESSENTIALS OF THE RLANGUAGE 301
TheresponsevariableisGrowth.rateandtheotherthreevariablesarefactors.Suppose
we want the mean growth rate for each Detergent:
tapply(Growth.rate,Detergent,mean)
BrandA BrandB BrandC BrandD
3.88 4.01 3.95 3.56
Thisproducesatablewithfourentries,oneforeachlevelofthefactorcalledDetergent.
Toproduceatwo-dimensionaltableweputthetwogroupingvariablesinalist.Herewe
calculate the median growth rate for Water type and Daphnia clone:
tapply(Growth.rate,list(Water,Daphnia),median)
Clone1Clone2Clone3
Tyne 2.87 3.91 4.62
Wear 2.59 5.53 4.30
The first variable in the list creates the rows of the table and the second the
columns.
SubscriptsandIndices
Whilewetypicallyaimtoapplyfunctionstovectorsasawhole,therearecircumstances
where we want to select only some of the elements of a vector. This selection is done
using subscripts (also known asindices ). Subscripts have square brackets [2], while
functions have round brackets (2). Subscripts on vectors, matrices, arrays and data-
frames have one set of square brackets [6],[3,4]or[2,3,2,1], while subscripts on
lists have double square brackets [[2]] or [[i,j]] (see p. 309). When there are two
subscripts to an object like a matrix or a dataframe, the first subscript refers to the row
number(therowsaredefinedasmarginnumber1)andthesecondsubscriptreferstothe
columnnumber(thecolumnsaremarginnumber2).Thereisanimportantandpowerful
conventioninRsuchthatwhenasubscriptappearsasablankitisunderstoodtomean
‘all of’. Thus
(cid:129) [,4] means all rows in column 4 of an object
(cid:129) [2,] means all columns in row 2 of an object
Thereisanother indexing convention inRwhichisused toextract named components
fromobjectsusingthe$operatorlikethis:model$coeformodel$resid(p.317).Thisis
known as ‘indexing tagged lists’ using the element names operator $.
WorkingwithVectorsandLogicalSubscripts
Take the example of a vector containing the 11 numbers 0 to 10:
x<-0:10
302 STATISTICS:ANINTRODUCTIONUSINGR
There are two quite different kinds of things we might want to do with this. We might
want to add up the values of the elements:
sum(x)
[1]55
Alternatively, we might want to count the elements that passed some logical criterion.
Suppose we wanted to know how many of the values were less than 5:
sum(x<5)
[1]5
You see the distinction. We use the vector function sum in both cases. But sum(x)
adds up the values of the xs and sum(x<5) counts up the number of cases that pass
the logical condition ‘x is less than 5’. This works because of coercion (p. 314).
Logical TRUE has been coerced to numeric 1 and logical FALSE has been coerced to
numeric 0.
That’sallwellandgood,buthowdoyouaddupthevaluesofjustsomeoftheelementsof
x?Wespecifyalogicalcondition,butwedon’twanttocountthenumberofcasesthatpass
thecondition,wewanttoaddupallthevaluesofthecasesthatpass.Thisisthefinalpieceof
the jigsaw, and involves the use of logical subscripts. Note that when we counted the
numberofcases,thecountingwasappliedtotheentirevector,usingsum(x<5).Tofind
the sum of the x values that are less than 5, we write:
sum(x[x<5])
[1]10
Let us look at this in more detail. The logical condition x<5 is either true or false:
x<5
[1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE
[10]FALSE FALSE
Youcanimaginefalseasbeingnumeric0andtrueasbeingnumeric1.Thenthevectorof
subscripts [x<5] is five 1s followed by six 0s:
1*(x<5)
[1]11111000000
Now imagine multiplying the values of x by the values of the logical vector:
x*(x<5)
[1]01234000000
APPENDIX: ESSENTIALS OF THE RLANGUAGE 303
Whenthefunctionsumisapplied,itgivesustheanswerwewant:thesumofthevaluesof
the numbers 0+1+2+3+4=10.
sum(x*(x<5))
[1]10
Thisproducesthesameanswerassum(x[x<5]),butisratherlesselegant.Supposewe
wanttoworkoutthesumofthethreelargestvaluesinavector.Therearetwosteps:firstsort
the vector into descending order; then add up the values of the first three elements of the
sorted array. Let us do this in stages. First, the values of y:
y<-c(8,3,5,7,6,6,8,9,2,3,9,4,10,4,11)
Nowifyouapplysorttothis,thenumberswillbeinascendingsequence,andthismakes
life slightly harder for the present problem:
sort(y)
[1] 2 3 3 4 4 5 6 6 7 8 8 9 91011
We can use the reverse function rev like this (use the Up arrow key to save typing):
rev(sort(y))
[1]1110 9 9 8 8 7 6 6 5 4 4 3 3 2
Sotheanswertoourproblemis11+10+9=30.Buthowtocomputethis?Wecanuse
specificsubscriptstodiscoverthecontentsofanyelementofavector.Wecanseethat10is
the second element of the sorted array. To compute this we just specify the subscript [2]:
rev(sort(y))[2]
[1]10
Arangeofsubscriptsissimplyaseriesgeneratedusingthecolonoperator.Wewantthe
subscripts 1 to 3, so this is:
rev(sort(y))[1:3]
[1]1110 9
So the answer to the exercise is just:
sum(rev(sort(y))[1:3])
[1]30
Note that we have not changed the vector y in any way, nor have we created any new
space-consuming vectors during intermediate computational steps.
304 STATISTICS:ANINTRODUCTIONUSINGR
AddresseswithinVectors
Therearetwoimportantfunctionsforfindingaddresseswithinarrays.Thefunctionwhich
is very easy to understand. The vector y (see above) looks like this:
y
[1] 8 3 5 7 6 6 8 9 2 3 9 410 411
Suppose we wanted to know the addresses of the elements of y that contained values
bigger than 5:
which(y>5)
[1] 1 4 5 6 7 8111315
Noticethattheanswertothisenquiryisasetofsubscripts.Wedon’tusesubscriptsinside
thewhichfunctionitself.Thefunctionisappliedtothewholearray.Toseethevaluesofy
that are larger than 5 we just type:
y[y>5]
[1] 8 7 6 6 8 9 91011
Notethatthisisashortervectorthanyitself,becausevaluesof5orlesshavebeenleftout.
length(y)
[1]15
length(y[y>5])
[1]9
TrimmingVectorsUsingNegativeSubscripts
An extremely useful facility is to use negative subscripts to drop terms from a vector.
Suppose we wanted a new vector, z, to contain everything but the first element of x:
x<-c(5,8,6,7,1,5,3)
z<-x[-1]
z
[1]867153
Ourtaskistocalculateatrimmedmeanofxwhichignoresboththesmallestandlargest
values(i.e.wewanttoleaveoutthe1andthe8inthisexample).Therearetwostepstothis.
Firstwesortthevectorx.Thenweremovethefirstelementusingx[-1]andthelastusing
x[-length(x)].Wecandobothdropsatthesametimebyconcatenatingbothinstructions
like this: -c(1,length(x)). Then we use the built-in function mean:
trim.mean<-function(x)mean(sort(x)[-c(1,length(x))])
APPENDIX: ESSENTIALS OF THE RLANGUAGE 305
The answer should be mean(c(5,6,7,5,3)) = 26/5 = 5.2. Let us try it out:
trim.mean(x)
[1]5.2
LogicalArithmetic
Arithmeticinvolvinglogicalexpressionsisveryusefulinprogrammingandinselectionof
variables(seeTableA.3).Iflogicalarithmeticisunfamiliartoyou,thenperseverewithit,
becauseitwillbecomeclearhowusefulitis,oncethepennyhasdropped.Thekeythingto
understand is that logical expressions evaluate to either true or false (represented in R by
TRUE or FALSE), and that R can coerce TRUE or FALSE into numerical values: 1 for
TRUE and 0 for FALSE.
TableA.3 Logicaloperations
Symbol Meaning
! logicalnot
& logicaland
| logicalor
< lessthan
<= lessthanorequalto
> greaterthan
>= greaterthanorequalto
== logicalequals(double=)
!= notequal
&& andwithif
|| orwithif
xor(x,y) exclusiveor
isTRUE(x) anabbreviationofidentical(TRUE,x)
Repeats
Youwilloftenwanttogeneraterepeatsofnumbersorcharacters,forwhichthefunctionis
rep.Theobjectthatisnamedinthefirstargumentisrepeatedanumberoftimesasspecified
in the second argument. At its simplest, we would generate five 9s like this:
rep(9,5)
[1]99999
Youcanseetheissuesinvolvedbyacomparisonofthesethreeincreasinglycomplicated
uses of the rep function:
rep(1:4,2)
[1]12341234
rep(1:4,each=2)
[1]11223344
306 STATISTICS:ANINTRODUCTIONUSINGR
rep(1:4,each=2,times=3)
[1]112233441122334411223344
Wheneachelementoftheseriesistoberepeatedadifferentnumberoftimes,thenthe
second argument must be a vector of the same length as the vector comprising the first
argument. Suppose that we need four 9s, one 15, four 21s and two 83s. We use the
concatenationfunctionctocreatetwovectors,oneforthetargetnumbersandanotherfor
the repeats of each element of the target:
rep(c(9,15,21,83),c(4,1,4,2))
[1] 9 9 9 915212121218383
GenerateFactorLevels
Thefunctiongl(‘generatelevels’)isusefulwhenyouwanttoencodelongvectorsoffactor
levels. The syntax for the three arguments is this:
gl(‘up to’, ‘with repeats of’, ‘to total length’)
Hereisthesimplestcasewherewewantfactorlevelsupto4withrepeatsof3repeated
only once (i.e. to total length=12):
gl(4,3)
[1]111222333444
Levels:1234
Here is the function when we want that whole pattern repeated twice:
gl(4,3,24)
[1]111222333444111222333444
Levels:1234
If the total length is not a multiple of the length of the pattern, the vector is truncated:
gl(4,3,20)
[1]11122233344411122233
Levels:1234
If you want text for the factor levels, rather than numbers, use labelslike this:
gl(3,2,24,labels=c("A","B","C"))
[1]AABBCCAABBCCAABBCCAABBCC
Levels:ABC
GeneratingRegularSequencesofNumbers
For regular series of whole numbers use the colon operator (: as on p. 296). When the
intervalisnot1.0youneedtousetheseqfunction.Ingeneral,thethreeargumentstoseq
APPENDIX: ESSENTIALS OF THE RLANGUAGE 307
are:initialvalue,finalvalue,andincrement(ordecrementforadecliningsequence).Here
we want to go from 0 up to 1.5 in steps of 0.2:
seq(0,1.5,0.2)
[1]0.00.20.40.60.81.01.21.4
Notethatseqstopsbeforeitgetstothesecondargument(1.5)iftheincrementdoesnot
match exactly (our sequence stops at 1.4). If you want to seq downwards, the third
argument needs to be negative
seq(1.5,0,-0.2)
[1]1.51.31.10.90.70.50.30.1
Again,zerodidnotmatchthedecrement,sowasexcludedandthesequencestoppedat
0.1. If you want to create a sequence of the same length as an existing vector, then use
alonglike this. Suppose that our existing vector, x, contains 18 random numbers from a
normal distribution with a mean of 10.0 and a standard deviation of 2.0:
x<-rnorm(18,10,2)
Supposealsothatwewanttogenerateasequenceofthesamelengthasthis(18)startingat
88 and stepping down to exactly 50 for x[18]:
seq(88,50,along=x)
[1]88.0000085.7647183.5294181.2941279.0588276.8235374.5882472.35294
[9]70.1176567.8823565.6470663.4117661.1764758.9411856.7058854.47059
[17]52.2352950.00000
Thisisusefulwhenyoudonotwanttogotothetroubleofworkingoutthesizeofthe
increment but you do know the starting value (88 in this case) and the final value (50).
Matrices
There are several ways of making a matrix. You can create one directly like this:
X<-matrix(c(1,0,0,0,1,0,0,0,1),nrow=3)
X
[,1][,2] [,3]
[1,] 1 0 0
[2,] 0 1 0
[3,] 0 0 1
Here, by default, the numbers are entered columnwise. The class and attributes of
X indicate that it is a matrix of 3 rows and 3 columns (these are its dimattributes):
308 STATISTICS:ANINTRODUCTIONUSINGR
class(X)
[1]"matrix"
attributes(X)
$dim
[1]33
In the next example, the data in the vector appear row-wise, so we indicate this with
byrow=T:
vector<-c(1,2,3,4,4,3,2,1)
V<-matrix(vector,byrow=T,nrow=2)
V
[,1][,2][,3][,4]
[1,] 1 2 3 4
[2,] 4 3 2 1
Anotherwaytoconvertavectorintoamatrixisbyprovidingthevectorobjectwithtwo
dimensions (rows and columns) using the dim function like this:
dim(vector)<-c(4,2)
and we can check that vector has now become a matrix:
is.matrix(vector)
[1]TRUE
Weneedtobecareful,however,becausewehavemadenoallowanceatthisstageforthe
fact that the data were entered row-wise into vector:
vector
[,1][,2]
[1,] 1 4
[2,] 2 3
[3,] 3 2
[4,] 4 1
The matrix we want is the transpose, t, of this matrix:
(vector<-t(vector))
[,1][,2][,3][,4]
[1,] 1 2 3 4
[2,] 4 3 2 1
APPENDIX: ESSENTIALS OF THE RLANGUAGE 309
CharacterStrings
InR,characterstringsaredefinedbydoublequotationmarks.Webeingbydefiningaphrase:
phrase<-"thequickbrownfoxjumpsoverthelazydog"
The function called substr is used to extract substrings of a specified number of
charactersfromacharacterstring.Hereisthecodetoextractthefirst,thefirstandsecond,
the first, second and third, and so on (up to 20) characters from our phrase:
q<-character(20)
for(iin1:20) q[i]<- substr(phrase,1,i)
q
[1] "t" "th" "the"
[4] "the" "theq" "thequ"
[7] "thequi" "thequic" "thequick"
[10]"thequick" "thequickb" "thequickbr"
[13]"thequickbro" "thequickbrow" "thequickbrown"
[16]"thequickbrown" "thequickbrownf" "thequickbrownfo"
[19]"thequickbrownfox" "thequickbrownfox"
Thesecondargumentinsubstristhenumberofthecharacteratwhichextractionisto
begin(inthiscasealwaysthefirst),andthethirdargumentisthenumberofthecharacterat
whichextractionistoend(inthiscase,theith).Tosplitupacharacterstringintoindividual
characters, we use strsplit like this:
strsplit(phrase,split=character(0))
[[1]]
[1] "t""h""e""""q""u""i""c""k""""b""r""o""w""n"""
[17] "f""o""x""""j""u""m""p""s""""o""v""e""r"
[31] """t""h""e""""l""a""z""y""""d""o""g"
The table function is useful for counting the number of occurrences of characters of
different kinds:
table(strsplit(phrase,split=character(0)))
abcdefghijklmnopqrstuvwxyz
811113112111111411212211111
Thisdemonstratesthatallofthelettersofthealphabetwereusedatleastoncewithinour
phrase,andthattherewereeightblankswithinphrase.Thissuggestsawayofcountingthe
numberofwordsinaphrase,given that thiswillalwaysbeonemore than thenumberof
blanks:
words<-1+table(strsplit(phrase,split=character(0)))[1]
words
9
310 STATISTICS:ANINTRODUCTIONUSINGR
It is easy to switch between upper and lower cases using the toupper and tolower
functions:
toupper(phrase)
[1]"THEQUICKBROWNFOXJUMPSOVERTHELAZYDOG"
tolower(toupper(phrase))
[1]"thequickbrownfoxjumpsoverthelazydog"
WritingFunctionsinR
FunctionsinRareobjectsthatcarryoutoperationsonargumentsthataresuppliedtothem
and return one or more values. The syntax for writing a function is
function (argument list) body
Thefirstcomponentofthefunctiondeclarationisthekeywordfunctionwhichindicates
toRthatyouwanttocreateafunction.Anargumentlistisacomma-separatedlistofformal
arguments.Aformalargumentcanbeasymbol(i.e.avariablenamelikexory),astatement
oftheformsymbol=expression(e.g.pch=16)orthespecialformalargument...(dot
dot dot). The body can be any valid R expression or set of R expressions. Generally, the
body is a group of expressions contained in curly brackets { } with each expression on a
separateline.Functionsaretypicallyassignedtosymbols,buttheydonotneedtobe.This
will only begin to mean anything after you have seen several examples in operation.
ArithmeticMeanofaSingleSample
P P
The mean is the sum of the numbers y divided by the number of numbers n 1
(summing over the number of numbers in the vector called y). The R function for n is
P
length(y) and for y is sum(y), so a function to compute arithmetic means is:
arithmetic.mean<-function(x) sum(x)/length(x)
We should test the function with some data where we know the right answer:
y<-c(3,3,4,5,5)
arithmetic.mean(y)
[1]4
Needless to say, there is a built- in function for arithmetic means called mean:
mean(y)
[1]4
MedianofaSingleSample
The median (or 50th percentile) is the middle value of the sorted values of a vector of
numbers:
sort(y)[ceiling(length(y)/2)
APPENDIX: ESSENTIALS OF THE RLANGUAGE 311
There is slight hitch here, of course, because if the vector contains an even number of
numbers,thenthereisnomiddlevalue.Thelogicisthatweneedtoworkoutthearithmetic
averageofthetwovaluesofyoneithersideofthemiddle.Thequestionnowarisesastohow
weknow,ingeneral,whetherthevectorycontainsanoddoranevennumberofnumbers,so
thatwecandecidewhichofthetwomethodstouse.Thetrickhereistousemodulo2(p.46).
Nowwehaveallthetoolsweneedtowriteageneralfunctiontocalculatemedians.Letus
call the function med and define it like this:
med<-function(x){
odd.even<-length(x)%%2
if(odd.even==0) (sort(x)[length(x)/2]+sort(x)[1+length(x)/2])/2
else sort(x)[ceiling(length(x)/2)]
}
Noticethatwhentheifstatementistrue(i.e.wehaveanevennumberofnumbers)then
the expression immediately following the if function is evaluated (this is the code for
calculatingthemedianwith anevennumberofnumbers). Whentheifstatementisfalse
(i.e. we have an odd number of numbers, and odd.even == 1) then the expression
followingtheelsefunctionisevaluated(thisisthecodeforcalculatingthemedianwithan
oddnumberofnumbers).Letustryitout,firstwiththeodd-numberedvectory,thenwith
theeven-numberedvectory[-1],afterthefirstelementofy(whichisy[1]=3)hasbeen
dropped (using the negative subscript):
med(y)
[1]4
med(y[-1])
[1]4.5
Again,youwon’tbesurprisedthatthereisabuilt-infunctionforcalculatingmedians,and
helpfully it is called median.
LoopsandRepeats
Theclassic,Fortran-likeloopisavailableinR.Thesyntaxisalittledifferent,buttheideais
identical;yourequestthatanindex,i,takesonasequenceofvalues,andthatoneormore
linesofcommandsareexecutedasmanytimesastherearedifferentvaluesofi.Hereisa
loop executed five times with the values of i: we print the square of each value
for(iin1:5)print(i^2)
[1]1
[1]4
[1]9
[1]16
[1]25
312 STATISTICS:ANINTRODUCTIONUSINGR
Formultiplelinesofcode,youusecurlybrackets{}toenclosematerialoverwhichthe
loopistowork.Notethatthe‘hardreturn’(theEnterkey)attheendofeachcommandlineis
anessentialpartofthestructure(youcanreplacethehardreturnsbysemicolonsifyoulike,
but clarity is improved if you put each command on a separate line).
Here is a function that uses the whilefunction in converting a specified number to its
binaryrepresentation.Thetrickisthatthesmallestdigit(0forevenor1foroddnumbers)is
always at the right-hand side of the answer (in location 32 in this case):
binary<-function(x) {
if(x==0)return(0)
i<-0
string<-numeric(32)
while(x>0) {
string[32-i]<-x%%2
x<-x%/%2
i<-i+1 }
first<-match(1,string)
string[first:32] }
Theleadingzeros(1tofirst–1)withinthestringarenotprinted.Werunthefunctionto
find the binary representation of the numbers 15 through 17:
sapply(15:17,binary)
[[1]]
[1] 1 1 1 1
[[2]]
[1] 10000
[[3]]
[1] 10001
TheifelseFunction
Sometimes you want to do one thing if a condition is true and a different thing if the
condition is false (rather than do nothing, as in the last example). The ifelse function
allowsyoutodothisforentirevectorswithoutusingforloops.Wemightwanttoreplace
any negative values of y by  1 and any positive values and zero by +1:
z<-ifelse(y<0, -1,1)
EvaluatingFunctionswithapply
The apply function is used for applying functions to the rows (subscript 1) or columns
(subscript 2) of matrices or dataframes:
(X<-matrix(1:24,nrow=4))
[,1][,2][,3][,4][,5][,6]
[1,] 1 5 9 13 17 21
[2,] 2 6 10 14 18 22
[3,] 3 7 11 15 19 23
[4,] 4 8 12 16 20 24
APPENDIX: ESSENTIALS OF THE RLANGUAGE 313
Here are the row totals (four of them) across margin 1 of the matrix:
apply(X,1,sum)
[1]66727884
Here are the column totals (six of them) across margin 2 of the matrix:
apply(X,2,sum)
[1]102642587490
Notethatinbothcases,theanswerproducedbyapplyisavectorratherthanamatrix.
Youcanapplyfunctionstotheindividualelementsofthematrixratherthantothemargins.
The margin you specify influences only the shape of the resulting matrix.
apply(X,1,sqrt)
[,1] [,2] [,3] [,4]
[1,] 1.000000 1.414214 1.732051 2.000000
[2,] 2.236068 2.449490 2.645751 2.828427
[3,] 3.000000 3.162278 3.316625 3.464102
[4,] 3.605551 3.741657 3.872983 4.000000
[5,] 4.123106 4.242641 4.358899 4.472136
[6,] 4.582576 4.690416 4.795832 4.898979
apply(X,2,sqrt)
[,1] [,2] [,3] [,4] [,5] [,6]
[1,] 1.000000 2.236068 3.000000 3.605551 4.123106 4.582576
[2,] 1.414214 2.449490 3.162278 3.741657 4.242641 4.690416
[3,] 1.732051 2.645751 3.316625 3.872983 4.358899 4.795832
[4,] 2.000000 2.828427 3.464102 4.000000 4.472136 4.898979
TestingforEquality
Youneedtobecarefulinprogrammingwhenyouwanttotestwhetherornottwocomputed
numbers are equal. R will assume that you mean ‘exactly equal’ and what that means
dependsuponmachineprecision.Mostnumbersareroundedto53binarydigitsaccuracy.
Typicallytherefore,twofloatingpointnumberswillnotreliablybeequalunlesstheywere
computedbythesamealgorithm,andnotalwayseventhen.Youcanseethisbysquaringthe
square root of 2: surely these values are the same?
x<-sqrt(2)
x*x== 2
[1]FALSE
We can see by how much the two values differ by subtraction:
x*x–2
[1]4.440892e-16
314 STATISTICS:ANINTRODUCTIONUSINGR
TestingandCoercinginR
Objectshaveatype,andyoucantestthetypeofanobjectusinganis.typefunction(see
TableA.4).Forinstance,mathematicalfunctionsexpectnumericinputandtext-processing
functions expect character input. Some types of objects can be coerced into other types
(againseeTableA.4).AfamiliartypeofcoercionoccurswhenweinterprettheTRUEand
FALSEoflogicalvariablesasnumeric1and0,respectively.Factorlevelscanbecoercedto
numbers. Numbers can be coerced into characters, but non-numeric characters cannot be
coerced into numbers.
as.numeric(factor(c("a","b","c")))
[1]1 2 3
as.numeric(c("a","b","c"))
[1] NA NA NA
Warningmessage:
NAsintroducedbycoercion
as.numeric(c("a","4","c"))
[1]NA 4 NA
Warningmessage:
NAsintroducedbycoercion
If you try to coerce complex numbers to numeric the imaginary part will be discarded.
Note that is.complex and is.numericare never both TRUE.
TableA.4 Testingandcoercing
Type Testing Coercing
Array is.array as.array
Character is.character as.character
Complex is.complex as.complex
Dataframe is.data.frame as.data.frame
Double is.double as.double
Factor is.factor as.factor
Raw is.raw as.raw
List is.list as.list
Logical is.logical as.logical
Matrix is.matrix as.matrix
Numeric is.numeric as.numeric
Timeseries(ts) is.ts as.ts
Vector is.vector as.vector
APPENDIX: ESSENTIALS OF THE RLANGUAGE 315
Weoftenwanttocoercetablesintotheformofvectorsasasimplewayofstrippingoff
their dimnames (using as.vector), and to turn matrices into dataframes (as.data.
frame).Alotoftestinginvolvesthe‘not’operator!infunctionstoreturnanerrormessage
if the wrong type is supplied. For instance, if you were writing a function to calculate
geometricmeansyoumightwanttotesttoensurethattheinputwasnumericusingthe!is.
numeric function
geometric<-function(x){
if(!is.numeric(x)) stop("Inputmustbenumeric")
exp(mean(log(x))) }
Here is what happens when you try to work out the geometric mean of character data:
geometric(c("a","b","c"))
Erroringeometric(c("a","b","c")):Inputmustbenumeric
Youmightalsowanttocheckthattherearenozeros ornegativenumbers intheinput,
because it would make no sense to try to calculate a geometric mean of such data:
geometric<-function(x){
if(!is.numeric(x)) stop("Inputmustbenumeric")
if(min(x)<=0)stop("Inputmustbegreaterthanzero")
exp(mean(log(x))) }
Testing this:
geometric(c(2,3,0,4))
Erroringeometric(c(2,3,0,4)):Inputmustbegreaterthanzero
But when the data are OK there will be no messages, just the numeric answer:
geometric(c(10,1000,10,1,1))
[1]10
DatesandTimesinR
Themeasurementoftimeishighlyidiosyncratic.Successiveyearsstartondifferentdaysof
theweek.Therearemonthswithdifferentnumbersofdays.Leapyearshaveanextradayin
February.AmericansandBritonsputthedayandthemonthindifferentplaces:3/4/2006is4
Marchinonecaseand3Aprilinanother.Occasionalyearshaveanadditional‘leapsecond’
addedtothembecausefrictionfromthetidesisslowingdowntherotationoftheearthfrom
when the standard time was set on the basis of the tropical year in 1900. The cumulative
effectofhavingsettheatomicclocktooslowaccountsforthecontinualneedtoinsertleap
seconds(32ofthemsince1958).Thereiscurrentlyadebateaboutabandoningleapseconds
andintroducinga‘leapminute’everycenturyorso,instead.Calculationsinvolvingtimes
are complicated by the operation of time zones and daylight saving schemes in different
316 STATISTICS:ANINTRODUCTIONUSINGR
countries. All these things mean that working with dates and times is excruciatingly
complicated. Fortunately, R has a robust system for dealing with this complexity. To see
how R handles dates and times, have a look at Sys.time():
Sys.time()
[1]"2015-03-2308:56:26GMT"
Theanswerisstrictlyhierarchicalfromlefttoright:thelongesttimescale(year=2015)
comesfirst,thenmonth(March=03)thenday(the23rd)separatedbyhyphens,thenthereis
a blank space and the time hours first (08 in the 24-hour clock) then minutes (56), then
seconds(26)separatedbycolons.Finally,thereisacharacterstringexplainingthetimezone
(GMT=Greenwich Mean Time). Now obviously, there is a lot going on here. To
accommodate this, R uses the Portable Operating System Interface system (POSIX) for
representing times and dates:
class(Sys.time())
[1]"POSIXct""POSIXt"
Therearetwoquitedifferentwaysofexpressingtime,eachusefulinadifferentcontext.
Fordoinggraphsandregressionanalyseswheretimeistheexplanatoryvariable,youwant
timetobeacontinuousnumber.Butwhereyouwanttodosummarystatistics(say,monthly
meansacrossalongrunofyears),youwantmonthtobeafactorwith12levels.InRthis
distinctionishandledbythetwoclassesPOSIXctandPOSIXlt.Thefirstclass,withsuffix
ct, you can think of as continuous time (i.e. a number of seconds). The other class, with
suffixlt,youcanthinkofaslisttime(i.e.anamedlistofvectors,representingallofthe
various categorical descriptions of thetime, including day of the week and so forth). Itis
hard to remember these acronyms, but it is well worth making the effort. You can see
(above)thatSys.timeisofbothclassctandclasst.Timesthatareofclassltarealsoof
classt.WhatthismeansisthatclassPOSIXttellsyouthatanobjectisatime,butnotwhat
sort of time it is; it does not distinguish between continuous time ct and list times lt.
Naturally, you can easily convert from one representation to the other:
time.list<-as.POSIXlt(Sys.time())
class(time.list)
[1]"POSIXlt""POSIXt"
unlist(time.list)
sec min hour mday mon year wday yday isdst
26 56 8 23 2 115 1 81 0
HereyouseetheninecomponentsofthelistofclassPOSIXlt.Thetimeisrepresentedby
thenumberofseconds(sec),minutes(min)andhours(houronthe24-hourclock).Next
comesthedayofthemonth(mday,startingfrom1),thenthemonthoftheyear(mon,starting
(non-intuitively,perhaps)atJanuary=0;youcanthinkofitas‘monthscompletedsofar’),
thentheyear(startingat0=1900).Thedayoftheweek(wday)iscodedfromSunday=0
APPENDIX: ESSENTIALS OF THE RLANGUAGE 317
toSaturday=6.Thedaywithintheyear(yday)iscodedfrom0=January1(youcanthink
of it as ‘days completed so far’). Finally, there is a logical variable isdst which asks
whether daylight saving time is in operation (0=FALSE in this case because we are on
Greenwich Mean Time). The ones you are most likely to use includeyear(to get yearly
meanvalues),mon(togetmonthlymeans)andwday(togetmeansforthedifferentdaysof
the week, useful for answering questions like ‘are Fridays different from Mondays?’).
Youcanusetheelementnameoperator$toextractpartsofthedateandtimefromthis
object using the names: sec,min,hour,mday,mon,year,wday,yday and isdst.
Hereweextractthedayoftheweek(date$wday=0meaningSunday)andtheJuliandate
(day of the year after 1 January as date$yday):
time.list$wday
[1]1
time.list$yday
[1]81
meaningthattodayisaMondayandthe82nddayoftheyear(81daysarecompletedsofar).
ItisimportanttoknowhowtoreaddatesandtimesintoR.Hereweillustratetwoofthe
commonest methods:
(cid:129) Excel dates
(cid:129) dates stored in separate variables for year, month, day, hour, minute, second
TheExceldateconventionusesforwardslashestoseparatethenumericalcomponents.
TheEnglishwaytogiveadateisday/month/yearandtheAmericanwayismonth/day/year.
Single-figure days and months can have a leading zero. So 03/05/2015 means 3 May in
England but 5 March in America. Because of the appearance of the slashes these are
characterstringsratherthannumbers,andRwillinterpretthemasfactorswhentheyareread
fromafileintoadataframe.Youhavetoconvertthisfactorintoadate–timeobjectusinga
functioncalledstrptime(youcanrememberthisas‘strippingthetime’outofacharacter
string). We start by reading the Excel dates into a dataframe:
data<-read.table("c:\\temp\\date.txt",header=T)
head(data)
x date
1 3 15/06/2014
2 1 16/06/2014
3 6 17/06/2014
4 7 18/06/2014
5 8 19/06/2014
6 9 20/06/2014
As things stand, the date is not recognized by R:
class(date)
[1]"factor"
318 STATISTICS:ANINTRODUCTIONUSINGR
The strptime function takes the name of the factor (date) and a format statement
showingexactlywhateachelementofthecharacterstringrepresentsandwhatsymbolsare
usedasseparators(forwardslashesinthiscase).Inourexample,thedayofthemonthcomes
first(%d)thenaslash,thenthemonth(%m)thenanotherslash,thentheyearinfull(%Y;ifthe
yearwasjustthelasttwodigits‘15’thentheappropriatecodewouldbe%yinlowercase).
We give the translated date a new name like this:
Rdate<-strptime(date,"%d/%m/%Y")
class(Rdate)
[1]"POSIXlt""POSIXt"
That’s more like it. Now we can do date-related things. For instance, what isthe mean
valueofxforeachdayoftheweek?Youneedtoknowthatthenameofthedayoftheweek
within the list called Rdate is wday, then
tapply(x,Rdate$wday,mean)
0 1 2 3 4 5 6
5.660 2.892 5.092 7.692 8.692 9.692 8.892
The value was lowest on Mondays (day 1) and highest on Fridays (day 5).
Forcaseswhereyourdatafilehasseveralvariablesrepresentingthecomponentsofdate
oratime(say,hours,minutesandsecondsseparately),youusethepastefunctiontojoin
thecomponentsupintoasinglecharacterstring,usingtheappropriateseparators(hyphens
for dates, colons for times). Here is the data file:
time<-read.csv("c:\\temp\\times.csv")
attach(time)
head(time)
hrs min sec experiment
1 2 23 6 A
2 3 16 17 A
3 3 2 56 A
4 2 45 0 A
5 3 4 42 A
6 2 56 25 A
Create a vector of times y using paste with a colon separator:
y<-paste(hrs,min,sec,sep=":")
y
[1]"2:23:6" "3:16:17" "3:2:56" "2:45:0" "3:4:42" "2:56:25"
[7]"3:12:28" "1:57:12" "2:22:22" "1:42:7" "2:31:17" "3:15:16"
[13]"2:28:4" "1:55:34" "2:17:7" "1:48:48"
YouthenneedtoconvertthesecharacterstringsintosomethingthatRwillrecognizeasa
dateand/oratime.IfyouusestrptimethenRwillautomaticallyaddtoday’sdatetothe
APPENDIX: ESSENTIALS OF THE RLANGUAGE 319
POSIXctobject.Notethat"%T"isashortcutfor"%H:%M:%S".Formoredetailsandafull
list of codes, see The R Book (Crawley, 2013).
strptime(y,"%T")
[1]"2014-01-2202:23:06""2014-01-2203:16:17""2014-01-2203:02:56"
[4]"2014-01-2202:45:00""2014-01-2203:04:42""2014-01-2202:56:25"
....
Whenyouonlyhavetimesratherthandatesandtimes,thenyoumaywanttousetheas.
difftimefunction rather than as.POSIXct to create the variables to work with:
(Rtime<-as.difftime(y))
Timedifferencesinhours
[1]2.385000 3.271389 3.048889 2.750000 3.078333 2.940278 3.207778
[8]1.953333 2.372778 1.701944 2.521389 3.254444 2.467778 1.926111
[15]2.285278 1.813333
Thisconvertsallofthetimesintodecimalhoursinthiscase(itwouldconverttominutesif
therewerenohoursintheexamples).Hereisthemeantimeforeachofthetwoexperiments
(A and B):
tapply(Rtime,experiment,mean)
A B
2.829375 2.292882
CalculationswithDatesandTimes
You can subtract one date from another, but you cannot add two dates together. The
following calculations are allowed for dates and times:
(cid:129) time+number
(cid:129) time number
(cid:129) time1 time2
(cid:129) time1 ‘logical operation’ time2
where the logical operations are one of ==,!=, <, <=, > or>=.
ThethingyouneedtograspisthatyoushouldconvertyourdatesandtimesintoPOSIXlt
objects before starting to do any calculations. Once they are POSIXlt objects, it is
straightforward to calculate means, differences and so on. Here we want to calculate the
number of days between two dates, 22 October 2003 and 22 October 2005:
y2<-as.POSIXlt("2018-10-22")
y1<-as.POSIXlt("2015-10-22")
320 STATISTICS:ANINTRODUCTIONUSINGR
Now you can do calculations with the two dates:
y2-y1
Timedifferenceof1096days
Notethatyoucannotaddtwodates.Itiseasytocalculatedifferencesbetweentimesusing
thissystem.Notethatthedatesareseparatedbyhyphenswhereasthetimesareseparatedby
colons:
y3<-as.POSIXlt("2018-10-2209:30:59")
y4<-as.POSIXlt("2018-10-2212:45:06")
y4-y3
Timedifferenceof3.235278hours
Alternatively, you can do these calculations using the difftime function:
difftime("2018-10-2212:45:06","2018-10-2209:30:59")
Timedifferenceof3.235278hours
Hereisanexampleoflogicaloperationondatetimeobjects.Wewanttoknowwhether
y4 was later than y3:
y4>y3
[1]TRUE
UnderstandingtheStructureofanRObjectUsingstr
We finish with a simple but important function for understanding what kind of object is
representedbyagivennameinthecurrentRsession.Welookatthreeobjectsofincreasing
complexity: a vector of numbers, a list of various classes and a linear model.
x<-runif(23)
str(x)
num[1:23]0.9710.230.6450.6970.537...
We see that x is a number (num) in a vector of length 23 ([1:23]) with the first five
values as shown (0.971, . . . ).
In this example of intermediate complexity the variable called basket is a list:
basket<-list(rep("a",4),c("b0","b1","b2"),9:4,gl(5,3))
basket
[[1]]
[1] "a""a""a""a"
[[2]]
[1] "b0""b1""b2"
[[3]]
[1] 987654
[[4]]
[1] 11 1 2 2 2 3 3 3 4 4 4 5 5 5
Levels:1 2 3 4 5
APPENDIX: ESSENTIALS OF THE RLANGUAGE 321
The first element isa character vector of length 4 (all "a"), while thesecond has three
differentcharacterstrings.Thethirdisanumericvectorandthefourthisafactoroflength
12 with four levels. Here is what you get from str:
str(basket)
Listof4
$:chr[1:4]"a""a""a""a"
$:chr[1:3]"b0""b1""b2"
$:int[1:6]987654
$:Factorw/5levels"1","2","3","4",..:1112223334...
Firstyouseethelengthofthelist(4)thentheattributesofeachofthefourcomponents:
thefirsttwoarecharacterstrings(chr),thethirdcontainsintegers(int)andthefourthisa
factor with five levels.
Finally, we look at an example of a complex structure – a quadratic linear model:
xv<-seq(0,30)
yv<-2+0.5*xv+rnorm(31,0,2)
model<-lm(yv∼xv+I(xv^2))
str(model)
Listof12
$coefficients:Namednum[1:3]2.513170.388090.00269
..-attr(*,"names")=chr[1:3]"(Intercept)""xv""I(xv^2)"
$residuals :Namednum[1:31]-1.712-1.8694.5110.436-0.207...
..-attr(*,"names")=chr[1:31]"1""2""3""4"...
$effects :Namednum[1:31]-50.96523.339-1.0680.6460.218...
..-attr(*,"names")=chr[1:31]"(Intercept)""xv""I(xv^2)"""...
$rank :int3
$fitted.values:Namednum[1:31]2.512.93.33.74.11...
..-attr(*,"names")=chr[1:31]"1""2""3""4"...
$assign :int[1:3]012
$qr :Listof5
..$qr :num[1:31,1:3]-5.570.180.180.180.18...
....-attr(*,"dimnames")=Listof2
......$:chr[1:31]"1""2""3""4"...
......$:chr[1:3]"(Intercept)""xv""I(xv^2)"
....-attr(*,"assign")=int[1:3]012
..$qraux:num[1:3]1.181.241.13
..$pivot:int[1:3]123
..$tol :num1e-07
..$rank:int3
..-attr(*,"class")=chr"qr"
$df.residual :int28
$xlevels :Namedlist()
$call :languagelm(formula=yv∼xv+I(xv^2))
$terms :Classes’terms’,’formula’length3yv∼xv+I(xv^2)
....-attr(*,"variables")=languagelist(yv,xv,I(xv^2))
322 STATISTICS:ANINTRODUCTIONUSINGR
....-attr(*,"factors")=int[1:3,1:2]010001
......-attr(*,"dimnames")=Listof2
........$:chr[1:3]"yv""xv""I(xv^2)"
........$:chr[1:2]"xv""I(xv^2)"
....-attr(*,"term.labels")=chr[1:2]"xv""I(xv^2)"
....-attr(*,"order")=int[1:2]11
....-attr(*,"intercept")=int1
....-attr(*,"response")=int1
....-attr(*,".Environment")=<environment:R_GlobalEnv>
....-attr(*,"predvars")=languagelist(yv,xv,I(xv^2))
....-attr(*,"dataClasses")=Namedchr[1:3]"numeric""numeric"
"numeric"
......-attr(*,"names")=chr[1:3]"yv""xv""I(xv^2)"
$model :’data.frame’: 31obs.of 3variables:
..$yv :num[1:31]0.8021.0357.8114.1383.901...
..$xv :int[1:31]0123456789...
..$I(xv^2):Class’AsIs’ num[1:31]0149162536496481...
..-attr(*,"terms")=Classes’terms’,’formula’length3yv∼xv+
I(xv^2)
......-attr(*,"variables")=languagelist(yv,xv,I(xv^2))
......-attr(*,"factors")=int[1:3,1:2]010001
........-attr(*,"dimnames")=Listof2
..........$:chr[1:3]"yv""xv""I(xv^2)"
..........$:chr[1:2]"xv""I(xv^2)"
......-attr(*,"term.labels")=chr[1:2]"xv""I(xv^2)"
......-attr(*,"order")=int[1:2]11
......-attr(*,"intercept")=int1
......-attr(*,"response")=int1
......-attr(*,".Environment")=<environment:R_GlobalEnv>
......-attr(*,"predvars")=languagelist(yv,xv,I(xv^2))
......-attr(*,"dataClasses")=Namedchr[1:3]"numeric""numeric"
"numeric"
........-attr(*,"names")=chr[1:3]"yv""xv""I(xv^2)"
-attr(*,"class")=chr"lm"
Thiscomplexstructureconsistsofalistof12objects,coveringeverydetailofthemodel
from the variables, their values, the coefficients, the residuals, the effects, and so on.
Reference
Crawley,M.J. (2013) TheR Book, 2ndedn,JohnWiley &Sons, Chichester.
FurtherReading
Chambers,J.M.andHastie,T.J.(1992)StatisticalModelsinS,Wadsworth&Brooks/Cole,Pacific
Grove,CA.
R Development Core Team (2014) R: A Language and Environment for Statistical Computing, R
Foundationfor Statistical Computing, Vienna,Avaialble fromhttp://www.R-project.org.
Venables,W.N.andRipley,B.D.(2002)ModernAppliedStatisticswithS-PLUS,4thedn,Springer-
Verlag, NewYork.
Index
EntriesinboldareRfunctions
1parameter“1”astheintercept,114 ainterceptinlinearregression,114
1:6generateasequence1to6,297,298 aprioricontrasts,212
==(“doubleequals”)logicalEQUALS,113, ablinefunctionforaddingstraightlinestoa
155,161,253 plots,116
!=logicalNOTEQUAL,98 afterAncova,191
forbarplot,251 inAnova,152
influencetesting,159,200 withalinearmodelasitsargument,150
withsubsets,133 abline(h=3)drawahorizontalline,51,82,
/division,294 83
/nestingofexplanatoryvariables,171,173 abline(lm(y∼x))drawalinewithaandb
"\n"newlineinoutput,withcat,310 estimatedfromthelinearmodely∼x,136,
%%modulo,46 140
&logicalAND,29 abline(v=10)drawaverticallineatx=10,
jconditioning(“given”),302 51,82
()argumentstofunctions,26 absenceofevidence,3
(a,b]fromandincludinga,uptobutnot,70 acceptancenullhypothesis,4
includingb,277 ageeffectslongitudinaldata,178
*maineffectsandinteractiontermsinamodel, age-at-deathdatausingglm,226,228
170 aggregationandrandomization,10
*multiplication,295 aggregationcountdata,253
:generateasequence;e.g.1:6,28,298 AICAkaike’sInformationCriterion,236,238
[[]]subscriptsforlists,301 airpollutioncorrelations,197,205
[]subscripts,26,299 aliasingintroduction,161,239
[a,b)includebbutnota,70 analysisofcovariance,seeAncova
\\doublebackslashinfilepaths,20 analysisofdeviancecountdata,237
^forpowersandroots,47,292 proportiondata,268
{}indefiningfunctions,43 analysisofvariance,seeAnova
inforloops,119 Ancova,185
<-getsoperator,25 contrasts,31
<lessthan,57 ordermatters,187
>greaterthan,44 subscripts,254,297
1stQuartilewithsummary,27,67 withbinaryresponse,279
3rdQuartilewithsummary,30,67 withcountdata,247
Statistics:AnIntroductionUsingR,SecondEdition.MichaelJ.Crawley.
©2015JohnWiley&Sons,Ltd.Published2015byJohnWiley&Sons,Ltd.
324 INDEX
anovaandAnova:anovaisanRfunctionfor association,contingencytables,105
comparingtwomodels,whileAnovastands asymptoticexponentialinnon-linear
foranalysisofvariance,271 regression,135
anovaanalysisofdeviance,266 attachadataframe,25,42,66
Ancova,189 autocorrelationrandomeffects,177
comparingmodels,186 averageofproportions,268
functionforcomparingmodels,141 averagingspeeds,49
modelsimplification,170 axischangeticmarklocations,144,271,277
non-linearregression,140
test="Chi",244,274,284 bslopeinlinearregression,114
test="F",266 b=SSXY/SSX,123
withcontrasts,212 barplotfactorialexperiments,167
Anovaessenceof,150 frequencies,250
choice,1 negativebinomialdistribution,252
introduction,150 tableusingtapply,242
longhandcalculationsforone-way,156 twodatasetscompared,117
modelformula,141,174,222,229 witherrorbars,162
one-way,150 withtwosetsofbars,251
Anovatableinregression,128 Bernoullidistributionn=,273
one-wayAnova,158 binaryresponsevariable,2
andnon-orthogonaldata,188 Ancova,279
antilogsexp,47,292 introduction,273
antsintrees,105 binom.testexactbinomialtest,99
aovfunctionforfittinglinearmodelswith binomialvariance/meanratio,252
categoricalexplanatoryvariables, binomialdataintroduction,256
132,146 binomialdenominator,256
analysisofvariancemodels,123 binomialdistributiondbinomdensity
competitionexample,214 function,257
Errorforratsexample,179 pbinomprobabilities
factorialexperiments,168 qbinomquantiles
modelforanalysisofvariance,157 rbinomrandomnumbers
multipleerrortermsusingError,171 binomialerrorsglm,229
withcontrasts,212 logitlink,257,274
appearanceofgraphs,improvements,38 binomialtestcomparingtwoproportionswith
arcsinetransformationofpercentagedata, prop.test,98
257 binomialtrialsBernoullidistribution,273
arithmeticmeandefinition,43 blankplotsusetype="n",59
withsummary,29 blocks,9
arrayfunctioncreatinganarrayspecifyingits splitplotdesign,173
dimensions,303,304 andpairedt-test,97
array,301 bootstrapconfidenceintervalformean,62
as.characterforlabels,251,254,314 hypothesistestingwithsinglesamples,81
inbarplotlabels,254 boundedcountdata, 227
as.matrix,107 boundedproportiondata,227,228
as.numeric,177 boxandwhiskerplots,seeboxplot
as.POSIX,319 boxplotfunction,67,93
as.vector,180 gardenozone,90
toestimateproportions,241,247 notch=Tforhypothesistesting,
withtapply,255,268 93,167
assignment,<-not=,295 withsplit,309
INDEX 325
cconcatenationfunction, 298,306 col="red"colourinbarplot,254
makingavector,44,45,54,296 columntotalsincontingencytables,100
calculator,291 columnsselectingfromanarray,301
cancerwithdistanceexample,235 selectingusingsubscripts,27
canonicallinkfunctionsglm,232 columnwisedataentryformatrices,104
Cartesiancoordinates,119 comparingtwomeans,90
categoricalvariablesindataframes,1,23 comparingtwoproportions,98,99
usecuttocreatefromcontinuous,279 comparingtwovariances,88
cbindfunctiontobindcolumnstogether competitionexperiment,162,214
inAncova,269 concatenationfunction,c, 298,306
makingcontrasts,212 confidenceintervalsaserrorbars,165
proportiondata,262,265 introduction,62
creatingtheresponsevariableforproportion constantvarianceglm,17,229
data,256 modelchecking,134
ceilingfunctionfor“thesmallestintegergreater contingencytablesdangersofaggregation,244
than”,44 introduction,100
censoringintroduction,287 ratherthanbinaryanalysis,273
central,afunctionforcentraltendency,42–44 continuousvariables,1
centrallimittheorem,introduction,42,70,72, converttocategoricalusingcut,305
73 indataframes,23
centraltendencycentralfunction,42–44 usingcuttocreatecategoricalvariables,277
introduction,42 contr.treatmenttreatmentcontrasts,216
chanceandvariation,2 contrastcoefficients,213
charactermodeforvariable,251 contrastconventionscompared,213
chisquaredcomparingtwodistributions,255 contrastsumofsquaresexamplebyhand,222
test="Chi",245 contrastsAncova,212
distributionpchisqprobabilitesqchisq asfactorattribute,214,215
quantiles,255 Helmert,224
chisq.testPearson’sChi-squaredtest,104 introduction,161,212
chi-squarecontingencytables,100 sum,224
choiceofmodel,usuallyacompromise,135 treatment,222
choosecombinatorialfunctioninR,293 contrasts=c("contr.treatment",
classicaltests,88,90 "contr.poly"))options,212
cleartheworkspacerm(list=ls()),22 controls,7
clumps,selectingarandomindividual,11 Cook’sdistanceplotinmodelchecking,135
coefextractcoefficientsfromamodelobject,267 corcorrelationinR,110
coefficientsAncova,189 paireddata,110
Anova,219 cor.testscaledependentcorrelation,111
binaryinfection,279 significanceofcorrelation,111
coeffunction,267 correct=Finchisq.test,105
extract,asinmodel$coef,267 correctedsumsofsquaresAncova,188
factorialexperiments,168 one-wayAnova,158
gam,146 correctionfactorhierarchicaldesigns,182
glmwithGammaerrors,281,282 correlationandpaired-samplet-test,96
quadraticregression,140 contingencytables,103
regression,118,133 introduction,108
regressionwithproportiondata,262 partial,111
treatmentcontrasts,161 problemsofscale-dependence,112,113
withcontrasts,216 varianceofdifferences,111
cohorteffectsinlongitudinaldata,178 correlationcoefficientr,108
326 INDEX
correlationofexplanatoryvariablesmodel growth,168
checking,114 hump,141,146
multipleregression,193 induced,244
correlationstructure,randomeffects,173 infection,279
countdataanalysisofdeviance,237 isolation,278
analysisusingcontingencytables,100 jaws,142
Fisher’sExactTest,107 light,80
introduction,234 oneway,150
negativebinomialdistribution,252 ozone,197–199
onproportions,258 paired,110
counting,usetable,305 pollute,203
usingsum(d>0),96 productivity,113
elementsofvectorsusingtablefunction,69 rats,176
counts,1 sexratio,262
covarianceandthevarianceofadifference,92 sheep,287
introduction,108 skewdata,64,85
pairedsamples,97 smoothing,146
criticalvalueandrejectionofthenull,92 splits,309
hypothesis,92 splityield,173
F-test,88 streams,97
ruleofthumbfort=2,9,83 sulphur.dioxide,203
Student’st,92 t.test.data,93
cross-sectionalstudieslongitudinaldata, tannin,117
178 twosample,109
cumprodcumulativeproductfunction,300 worms,25
currentmodel,194 yvalues,42
curvatureandmodelsimplification,200 datadredgingusingcor,110
inregression,135 dataediting,68
modelchecking,134 dataexploration,193
multipleregression,193 dataframe,introduction,23
curvesonplots,AncovawithPoissonerrors,248 datasummaryonesamplecase,66
cut,producecategorydatafromcontinuous,274 dataframecreateusingcbind,157
createusingread.table,299
d.f.,seedegreesoffreedom namethesameasvariablename,310
dangersofcontingencytables,244 datesandtimesinR,315
data,fittingmodelsto,193 deathdataintroduction,285
dataAncovacontrasts,231 deerjawsexample,142
cases,250 degreeoffitr2,133
cells,237 degreesoffreedomcheckingfor
clusters,235 pseudoreplication,177
compensation,186 contingencytables,103
competition,162,214 definition,53
Daphnia,300,301 factorialexperiments,168
deaths,285 inapairedt-test,97
decay,136 inanFtestoftwovariances,58,88
f.test.data,89 inAnova,153
fisher,107 indifferentmodels,217
flowering,269 innesteddesigns,177
gardens,56 inthelinearpredictor,229
germination,264 modelsimplification,144
INDEX 327
numberofparameters,54 emptyplotsusetype="n",59
one-wayAnova,150,158 equals,logical==(“doubleequals”),34
spottingpseudoreplication,7 Errorwithaov,introduction,132
deletiontests,stepsinvolved,194,284 multipleerrortermsinaov,171
densityfunctionbinomial, errorbars,functionfordrawing,163
negativebinomial,248 leastsignificantdifference,166
Normal,296 onproportions,175
Poisson,251 overlapandsignificance,165
derivedvariableanalysislongitudinaldata,178 errorcorrection,81,96
detachadataframe,21,34,90,287 errorstructureintroduction,173,174
deviations,introduction,51 modelcriticism,274
dietsupplementexample,168 errorsumofsquaresSSEinregression,118
difffunctiongeneratingdifferences,69 errorvariancecontrastsumofsquares,222
differencesvs.pairedt-test,97 inregression,125
differencesbetweenmeansaliasing,239 error.barsfunctionforplotting,164
inAnovamodelformula,160 errorsPoissonforcountdata,234
differencesbetweenslopesAncova,190 etathelinearpredictor,229
differencesbetweeninterceptsAncova, 190 evennumbers,%%2iszero,46
difftime,320 everythingvaries,2
dimdimensionsofanobject,307,308 exactbinomialtestbinom.test,98,99
dimensionsofamatrix,107 ExceldatesinR,317
dimensionsofanarray,301 exitafunctionusingstop,315
dimensionsofanobjectx-1:12;dim(x)<-c expantilogs(basee)inR,47
(3,4),307,308 predictedvalue,131,140
division/,46 withglmandquasipoissonerrors,236
dnbinomfunctionforprobabilitydensityofthe expectationofthevectorproduct,108
negativebinomial,253 expectedfrequenciesE=RxC/G,101
dnorm,76 Fisher’sExactTest,105
plotof,83 negativebinomialdistribution,252
probabilitydensityoftheNormaldistribution, experiment,8
72 experimentaldesign,7
dredgingthroughdatausingcor,110 explainedvariationinAnova,154
dropelementsofanarrayusingnegative inregression,125
subscripts,304 explanatorypowerofdifferentmodels,125
dropthelastelementofanarrayusinglength, explanatoryvariables,1
296 continuousregression,114
dtdensityfunctionofStudent’st,plotof,83 dangersofaggregation,244
dummyvariablesintheAnovamodelformula, specifying,seepredict
16 transformation,135
durationofexperiments,14 uniquevaluesforeachbinaryresponse,274
exponentialerrors,insurvivalanalysis,288
E=RxC/Gexpectedfrequenciesin expression,complextextonplots,294
contingencytables,101 extract$,309
eachinrepeats,311 extremevaluedistributioninsurvivalanalysis,289
edges,selectingarandomindividual,11 extrinsicaliasing,17
effectsizeandpower,9 eyecolour,contingencytables,100
factorialexperiments,168
fixedeffects,174 FaslogicalFalse,27
one-wayAnova,150,158 Fratio,89
elsewiththeiffunction,46 inregression,128
328 INDEX
F-test,comparingtwovariances,58 errorbars,163
factor,numericalfactorlevels,177 exitusingstop,315
factorlevelsFisher’sExactTest,107 forasigntest,95
generatewithgl,293 forvariance,54
informative,177,200 leverage,135,139
inmodelformula,166 median,46
factorial,Fisher’sExactTest,105 negativebinomialdistribution,252
factorialdesigns,introduction,150
factorialexperimentsintroduction,168 gamgeneralizedadditivemodels,146
factor-levelreductioninmodelsimplification, dataexploration,193
172 introduction,141
factorscategoricalvariablesinAnova,1,150 library(mgcv),142
indataframes,23 withabinaryresponse,283
plot,162 y∼s(x),142
failuredata,introduction,2,285 Gammadistribution,variance/meanratio,252
failuresproportiondata,256 Gammaerrorsglm,229,232,285
FALSEorF,influencetesting,164 introduction,285,286
logicalvariable,27 gardenA,56
falsifiablehypotheses,3 Gaussiandistributioninsurvivalanalysis,289
family=binomialbinaryresponsevariable,274 generalizedadditivemodels,seegam,
proportiondata,269 generalizedlinearmodel,seeglm,
family=poissonforcountdata,234 generatefactorlevelsgl,306
famousfive;sums,sumsofsquaresandsumsof geometricmean,definition,47,315
products,122 glgeneratelevelsforfactors,306
filenames,25 glmanalysisofdeviance,238,262
fillcolourforlegends,254 Ancovawithbinomialerrors,262
inbarplotlegend,242,254 Ancovawithpoissonerrors,248
fisher.testFisher’sExactTest,105,106 binaryinfection,282
with2argumentsasfactorlevels,107 binaryresponsevariable,274
Fisher’sExactTest,contingencytables,105 cancersexample,235
Fisher’sF-Test,seeF-test, Gammaerrors,286
fitofdifferentmodels,137,194,230 proportiondata,262
fittedvaluesdefinition,120 regressionwithproportiondata,263
proportiondata,262 saturatedmodelwithPoissonerrors,248
fittingmodelstodata,193 gradient,seeslope,
fixedeffects,introduction,174 graphs,twoadjacent,par(mfrow=c(1,2)),143
forloops,119,121,164,299,311 graphs,twobytwoarray,
drawingerrorbars,163 par(mfrow=c(2,2)),198,199
forplottingresiduals,120 GregorMendeleffect,15
negativebinomialdistribution,252 groupingrandomeffects,176
residualsinAnova,151
withablineandsplit,247 h,leveragemeasure,135
formula,modelforAnova,157,158 haircolour,contingencytables,100
F-ratio,contrastsumofsquares,220 harmonicmean,49,304
one-wayAnova,158 header=T,317
frequenciescountdata,234 HelmertcontrastsAncova,216,224
usingtable,237,305 example,222
frequencydistributions,introduction,250 heteroscedasticityintroduction,35,202
F-test,introduction,88 modelchecking,134
functionswritteninR,43,302 multipleregression,197
INDEX 329
hierarchicaldesigns,correctionfactor,178 termsmodelformulae,174
hierarchyrandomeffects,182 termsinmultipleregression,195,196
ratsexample,179 interaction.plotsplitplotexample,172
histfunctionforproducinghistograms,42 interactionsfactorialexperiments,150
speed,80 selectingvariables,203
values,85 valueoftreemodels,203
withbootstrap,81 intercepta,114
withskew,84 calculationslonghand,124
histograms,,seehist differencesbetweenintercepts,190
history(Inf)forlistofinputcommands,20 estimate,124
honestsignificantdifferencesTukeyHSD,18 maximumlikelihoodestimate,6,118
horizontallinesonplotabline(h=3),82,83 treatmentcontrasts,161
howmanysamples?plotofvarianceandsample interceptsAncova,230
size,60 interquartilerange,80
humpedrelationshipssignificancetesting,146 plots,162
modelsimplification,144 intrinsicaliasing,17
testingfor,155 inverse,andharmonicmeans,49
testingabinaryresponsemodel,284
hypothesesgoodandbad,3 kofthenegativebinomialdistribution,252
hypothesestesting,81 key,see,seelegend,
usingchi-square,103 kindsofyears,15
withF,89 knownvaluesinasystemoflinearequations,320
kurtosisdefinition,86
I“asis”inmultipleregression errorstructure,114
modelformulas,298 functionfor,86,87
identitylinkglm,231 values,87
Normalerrors,97,229
iffunction,46 labelschangingfontsize,cex.lab
ifwithlogicalsubscripts,28,29 forbarplot,251
incidencefunctionsusinglogisticregression, leastsignificantdifference(LSD)errorbars,
273,275 166
independence,9 introduction,165,166
independenceassumptionincontingencytables, least-squaresestimatesofslopeandinterceptin
100 linearregression,118
independenceoferrors,15 legendbarplotwithtwosetsofbars,
randomeffects,173 243,254
indexinone-variableplots,153 plotfunctionforkeys,167
induceddefencesexample,244 lengthfunctionfordeterminingthelengthofa
infectionexample,279 vector,43,44,51,69,84
inferencewithsinglesamples,81 dropthelastelementofanarray,301
influenceintroduction inasigntestfunction,95,97
modelchecking,134 lengthwithtapply,318
one-wayAnova,150 levelsoffactors,1
testinginmultipleregression,194,200 levels,generatewithgl,293
informativefactorlevels,fixedeffects,174 levels,usesplittoseparatevectors,309
initialconditions, 16 levelsintroduction,170
inputfromkeyboardusingscan(),298 modelsimplification,170
insecticide,11 proportiondata,262
interaction,multipleregression,193 regressioninAncova,185
termswithcontinuousexplanatoryvariables,114 withcontrasts,215
330 INDEX
“levelsgets”comparingtwodistributions,252 loesslocalregressionnon-parametricmodels,
factor-levelreduction,170 18
withcontrasts,217 logexponentialdecay,140
leverageandSSX,124 loglogarithms(basee)inR,47,291,292
leveragefunction,135,139 loglinkforcountdata,231,232
influencetesting,135 logodds,logit,232
libraryctestforclassicaltests logtransformationinmultipleregression,202,
mgcvforgam,146,198,283 263
nlmeformixedeffectsmodels,178 logarithmsandvariability,48
survivalforsurvivalanalysis,287 logicalsubscripts,301,302
treefortreemodels,197,200 logicaltestsusingsubscripts,27
linearfunction,6 logicalvariables,TorF,27,29
linearmixedeffectsmodellme,178 indataframes,23
linearpredictorintroduction logisticmodel,caveats,264
logitlink,257 logisticS-shapedmodelforproportiondata,264
linearregressionexampleusinggrowthand distributioninsurvivalanalysis,289
tannin,117 logisticregression,binaryresponsevariable,
linearizingthelogistic,259 275
linesaddslinestoaplots(cf.points),68,115 example,261
binaryresponsevariable,279 logitlinkbinomialerrors,260
drawingerrorbars,163 definition,259,260
dtanddnorm,83 log-linearmodelsforcountdata,235
exponentialdecay,140 longitudinaldataanalysis,178
forerrorswithproportiondata,278 loopsinR,seeforloops
non-linearregression,140 LSDleastsignificantdifference,166
orderedxvalues,141 plots,166
overhistograms,72 ltylinetype(e.g.dottedislty=2),64
polynomialregression,136
showingresiduals,120 m 3 thirdmoment,84
type="response"forproportiondata,264 m
4
fourthmoment,86
withglmandquasipoissonerrors,236 marginaltotalsincontingencytables,104
withqt,82 marginsincontingencytables,101
withsubscripts,152 matrices,columnwisedataentry,104
link,logforcountdata,234 matrixfunctioninR,104
linkfunctioncomplementarylog-log,274 withnrow,312
logit,260,274 matrixmultiplication%*%,312
list,innon-linearregression,142 maximum.withsummary,30
lists,subscripts,301,320 max,67
liver,ratsexample,180 maximumlikelihooddefinition,6
lm estimatesinlinearregression,118
lmfitalinearmodellm(y∼x),119 estimateofkofthenegativebinomial,252
Ancova,187 meanfunctiondeterminingarithmeticmean,
inregression,130 44
linearmodels,120 mean,arithmetic,65,160,297
thepredictfunction,120 geometric,47
lmelinearmixedeffectsmodel,173 harmonic,49
handlingpseudoreplication,173 meanageatdeathwithcensoring,290
locatorfunctionfordeterminingcoordinateson meansquareddeviation,introduction,54
asplot,116 means,tapplyfortables,161,250
withbarplot,167 two-waytablesusingtapply,168
INDEX 331
measurementerror,179 anddegreesoffreedom,54
medfunctionfordeterminingmedians,46,67 andpower,9
medianbuilt-infunction,46 andstandarderror,61
withsummary,29
writingafunction,47 namesinbarplot,168,251
mgcv,binomial,146,198,283 namesofvariablesinadataframe,25,66
Michelson’slightdata,80 naturalexperiments,14
minimaladequatemodel,5,6,144 negativebinomialdistributiondefinition,252
analysisofdeviance,240 dnbinomdensityfunction,253
multipleregression,197 negativecorrelationincontingencytables,109
minimum,min,withsummary,30,67 negativeskew,80,84,86
mixedeffectsmodels,18 negativesubscriptstodropelementsofanarray,
library(nlme),178 304
mode,themostfrequentvalue,42 nestedAnova,modelformulae,183
modelforAnova,159 nestingmodelformulae,176
contingencytables,100 ofexplanatoryvariables,%in%,176
linearregression,159 newlineofoutputusing"\n",302
modelchecking,introduction,134 nicenumbersinmodelsimplification,196
inregression,134,135 nlmelibraryformixedeffectsmodels,178
modelcriticism,introduction,274 non-linearmixedeffectsmodel,178
modelformulaforAnova,174 nlsnon-linearleastsquaresmodels,142
modelobjects,genericfunctions,19 non-constantvariancecountdata,138,177,202
modelselection,5 modelcriticism,274
modelsimplificationanalysisofdeviance,265, proportiondata,256
266 non-linearleastsquares,seenls
Ancova,188 non-linearmixedeffectsmodel,seenlme
caveats,196 non-linearregressionintroduction,142
factorialexperiments,168 non-lineartermsinmodelformulae,195
factor-levelreduction,222–224 useofnls,143,144
multipleregression,193,211 non-linearityinregression,282,283
non-linearregression,142 non-Normalerrorsintroduction,229
withcontrasts,216 countdata,234
model,structureofalinearmodelsusingstr, modelchecking,134,135
321 modelcriticism,274
modulo%% proportiondata,256
forbarplot,251 non-orthogonaldataobservationalstudies,16
remainder,46 ordermatters,187
withlogicalsubscripts,301 non-parametricsmoothersgam,38,146,147,
momentsofadistribution,83 198,283
multiplecomparisons,167 pairs,197
multipleerrorterms,introduction,181 withabinaryresponse,273
multiplegraphsperpage, NormalandStudent’stdistributionscompared,
par(mfrow=c(1,2)),143 83
multipleregression,introduction,193 Normalcalculationsusingz,76,77
difficultiesin,203 Normalcurve,drawingthe,78
minimaladequatemodel,197 Normaldistribution,introduction,70
numberofparameters,161,255 dnormdensityfunction,72,73,79,83
quadraticterms,140 pnormprobabilities,75
multiplication,*,302 qnormquantiles,75,76
n,samplesize,10 rnormrandomnumbers,58,307
332 INDEX
Normalerrorsidentitylink,229,231,257,260 overdispersionandtransformationof
modelchecking,134 explanatoryvariables,263
Normalq-qplotinmodelchecking,134 nosuchthingwithbinarydata,275
normality,testsof,79 proportiondata,257,258
notequal,!=,98 usequasibinomialforproportiondata,
notch=Tinboxplotforsignificancetesting,93, 261,266
167 usequasipoissonforcountdata,236
plotsforAnova,166 over-parameterizationinmultipleregression,
withboxplot,282 206
nrow,numberofrowsinamatrix,312 ozoneandlettucegrowthingardens,56,
n-shapedhumpedrelationships,141 157
nuisancevariables,marginaltotalsin
contingencytables,244 ΠGreekPi,meaningtheproductof,47
nullhypotheses,3 pnumberofparameters,54,55
rejectionandcriticalvalues,129 andinfluence,135
withF-tests,88,89 inthelinearpredictor,115,116
nullmodely∼1,186 estimatedparametersinthemodel,103
numbersasfactorlevels,177 pvalues,3
numeric,definitionofthemodeofavariable, comparedfort-testandWilcoxonRankSum
63,81 Test,96
pairedsamplest-test,97
observationaldata,7 pairsmutli-panelscatterplots,197
observedfrequenciesincontingencytables, panel.smoothinpairs,197
102 pargraphicsparameters,135,294
Occam’sRazor,8 par(mfrow=c(1,1))singlegraphperpage,
andchoiceoftest,88 136
contingencytables,100 par(mfrow=c(1,2))twographssidebyside,
oddnumbers,%%2isone,46 143,251,262,276,279
odds,p/q,definition,259 par(mfrow=c(2,2))fourplotsina2×2array,
one-samplet-test,98 198
one-wayAnovaintroduction,150 parallellinesinAncova,189
optionscontrasts=c("contr.helmert", parameterestimationinnon-linearregression,
"contr.poly")),224 138
contrasts=c("contr.sum","contr. parameters2-parametermodel,5,6
poly")),228 inmultipleregression,211
contrasts=c("contr.treatment", parsimony,7
"contr.poly")),217,222,228 partialcorrelation,introduction,111
orderfunction,303 pastetoconcatenatetext,298
insortingdataframes,28 pathanalysis,111
withscatterplots,141 pathnameforfiles,25
withsubscripts,303 pchwithsplit,247
ordermattersAncova,187,192 pch=35,136
non-orthogonaldata,16 solidcircleplottingsymbols,117
ordering,introduction,303 withsplit,306
orthogonalcontrasts,212,213 pchisqcumulativeprobabilityofchisquared
orthogonaldesigns,16 distribution,255
Anovatables,129,132 Pearson’schi-squareddefinition,102
outliersdefinition,66,80 forcomparingtwodistributions,255
inboxandwhiskerplots,93 Pearson’sProduct-MomentCorrelation,
newlineusing"\n",303 cor.test,113
INDEX 333
percentagedataandthearcsinetransformation, pairsformanyscatterplots,190
257 forbinaryresponseexample,279
fromcounts,256 plottingsymblolspchinplot,116,136
percentiles,67 pnormprobabilitiesfromtheNormal
plots,162 distribution,74
inboxandwhiskerplots,69 probabilitiesofzvalues,77
withsummary,29 pointsaddingpointstoaplot(cf.lines),59,
pfcumulativeprobabilityfromtheF 119
distribution,58 withgamplot,146
inF-tests,89 withsplit, 269,309
inregression,128 withsubscripts,254
one-wayAnova,158 Poissondistributiondefinition,250
piece-wiseregression,withabinaryresponse, dpoisdensityfunction,251
283 rpoisrandomnumbergenerator,299
PivotTableinExcel,25 poissonerrorscountdata,234,235
plot5,50,59,64,83,135 glmforcountdata,229,235
ablineforaddingstraightlines,117 pollution,exampleofmultipleregression,
addingpointstoaplot,59 203,204
binaryresponsevariable,279 polygonfunctionforshadingcomplexshapes,
boxandwhisker,162 79
compensationexample,186 polynomialregression,introduction,140
correlation,109 populationgrowth,simulationmodel,263
countdata,235 positivecorrelation,andpaired-sample
growthandtannin,117 t-test,98
inAnova,150 contingencytables,105
inerrorchecking,35,36 POSIX,316
las=1forverticalaxislabels,299 power,probabilityofrejectingafalsenull
multipleusingpairs,193,200 hypothesis,9
multipleusingpar(mfrow=c(1,2)),141 functionsforestimatingsamplesize,10
non-linearscatterplot,140 power.t.test,10
proportiondata,262,269 powers^,47,292
regressionwithproportiondata,264 p/q,seeodds
scaledependentcorrelation,113 predict,functiontopredictvaluesfromamodel
thelocatorfunctionfordetermining forspecifiedvaluesoftheexplanatory
coordinates,119 variables,120
type="n"forblankplottingarea,82,115, binaryresponsevariable,279
190,301,309 non-linearregression,142
withindex,153 polynomialregression,140
withsplit,269 type="response"forproportiondata,
plot(model)introduction,134 256,264
forgam,146,198 withglmandquasipoissonerrors,236
andtransformationofexplanatoryvariables, predictedvalue,standarderrorof^y,130
262 predictions,14
fortreemodels,197 probabilities,contingencytables,100
glmwithGammaerrors,286 probabilitydensity,binomialdistribution,72
modelchecking,134 Normal,72
multipleregression,211 negativebinomialdistribution,252
one-wayAnova,150 Poissondistribution,250
plot.gamwithabinaryresponse,283 products,cumprodfunctionforcumulative
plots,boxandwhisker,162 products,105
334 INDEX
prop.testbinomialtestforcomparingtwo quasipoissonanalysisofdeviance,236,239
proportions,100 familyforoverdispersedcountdata,236
proportion,transformationfromlogit,262,
267 rcorrelationcoefficient,108
proportiondataintroduction,1,256 intermsofcovariance,110
analysisofdeviance,264 intermsofSSXY,109
Ancova,269 Rdownload,xii
binomialerrors,256,260 Rlanguage,xi
ratherthanbinaryanalysis,274 r2asameasureofexplanatorypowerofamodel,
proportionsfromtapplywithas.vector,256, 133
268 definition,131
pseudoreplication,15 r2=SSR/SSY, 133
analysiswith,173 randomeffectsintroduction,176
checkingdegreesoffreedom,176 longitudinaldata,178
removingit,178 uninformativefactorlevels,180,181
splitplots,173 randomnumbersfromthenormaldistribution,
ptcumulativeprobabilitiesofStudent’st 58
distribution rnorm,59,307
gardenozone,93 fromthePoissondistribution,rpois,299
testforskew,85 fromtheuniformdistribution,runif,293,
320
qchisqquantilesofthechi-squaredistribution, randomizationinsamplingandexperimental
104 design,7,10
qfquantilesoftheFdistribution,88 randomizingvariableselection,208
contrastsumofsquares,222 rangefunctionreturningmaximumand
inregression,129 minimum,50,300
one-wayAnova,155 rankfunctioninR,95
qnormquantilesoftheNormaldistribution,75 read.tableintroduction,25,42,55
qqlineintroduction,79 readingdatafromafile,25,296
qqnormintroduction,79 reciprocallinkwithGammaerrors,231,232
inregression,135 reciprocals,49,286
qtquantilesofthetdistribution,64 regressionintroduction,114
confidenceintervalformean,165 anovatable,128
criticalvalueofStudent’st,92 atdifferentfactorlevelsAncova,185
quadraticregression.introduction,140 binaryresponsevariable,273
multipleregression,196 byeye,117
inabinaryresponsemodel,282 calculationslonghand,121,122
modelformulae,298 choice,1
quantilefunctioninR,64 exponentialdecay,137
ofthechi-squaredistributionusingqchisq, linear,114
104 logistic,261
oftheFdistributionusibngqf,88, non-linear,140
129,155 parameterestimationinnon-linear,138
oftheNormaldistributionusingqnorm,75 piece-wise,283
ofthetdistributionusibngqt,62 polynomial,140
quartileplots,163 predictinnon-linear,140
withsummary,29 quadratic,140
quasibinomialanalysisofdeviance,261,266 summaryinnon-linear,141
familyforoverdispersedproportiondata, testingforhumpedrelationships,141
261 testingfornon-linearity,134
INDEX 335
rejectioncriticalvalues,94 Slanguage,background,xi
nullhypothesis,3,4 P s(x)smootheringam,146
usingF-tests,89 P y y(cid:1)0proof,52
relativegrowthratewithpercentagedata,257 (y a bx)=0proof,122
removingvariableswithrm,22,297 sample,functionforsamplingatrandomfroma
repfunctionforgeneratingrepeats,95,96,293 vector,71
errorbars,163 withreplacement,replace=T,81
forsubjectidentities,307 selectingvariables,203
LSDbars,166,167 forshuffling,replace=F, 208
repeatfunction,96 samplesizeanddegreesoffreedom,54
text,94,95 samplingwithreplacement;samplewith
repeatedmeasures,9 replace=T,63
randomeffects,176 saturatedmodel,194,195
repeats,generatingrepeats,seerep contingencytables,244
replace=Tsamplingwithreplacement,81 savingyourworkfromanRsession,20
replication7,8,9 scalelocationplot,usedinmodelchecking,
checkingwithtable,164 135
residualdevianceinproportiondata,261 scaleparameter,overdispersion,260
residualerrors,4 scale-dependentcorrelation,113
residualplotsinmodelchecking,132 scan()inputfromkeyboard,299
residualsdefinition,3,120 scatter,measuringdegreeoffitwithr2,133
extractresidualsfromamodelobject,122 scatterplot,graphicforregression,114
inAnova,150 sdstandarddeviationfunctioninR,72
modelchecking,134 seedproductioncompensationexample,190
patternandheteroscedasticity,202 selectingarandomindividual,10
response,predictwithtype="response", selectingcertaincolumnsofanarray,301
247,268,276 selectingcertainrowsofanarray,301
responsevariableandthechoiceofmodel,1,114 selectionofmodels,introduction,117
regression,114 self-startingfunctionsinnon-linearregression,
typesof,2 142
revwithorderinsortingdataframes,28 seqgenerateaseries,64,72,76,83,297
rev(sort(y))sortintoreverseorder,303 valuesforxaxisinpredict,236
rmremovingvariablesfromtheworkspace,22 sequencegeneration,seeseq
rm(list=ls())cleareverything,22 serialcorrelation,66
rnormrandomnormallydistributednumbers, randomeffects,173
59,307 sexdiscrimination,testofproportions,100
roots,^(fraction),47,292 shufflingusingsample,208
incalculatinggeometricmean,47 signtestdefinition,95
rownamesindataframes,23 gardenozone,96
rowtotalscontingencytables,101 significance,3
row.namesinread.table,25 inboxplotsusingnotch=T,93
rowsselectingfromanarray,301 ofcorrelationusingcor.test,113
selectingusingsubscripts,27 overlapoferrorbars,165
rulesofthumb significantdifferencesincontingencytables,
parametersinmultipleregressionp/3,202 102
power80%requiresn>=16 s2/d2,10 simplicity,seeOccam’sRazor
t>2issignificant,83 simplification,seemodelsimplification
runifuniformrandomnumbers,293,320 simulationexperimentonthecentrallimit
theorem,72
ΣGreekSigma,meaningsummation,43 singlesampletests,66
336 INDEX
skewdefinition,84 SSXcorrectedsumofsquaresofx,122
asymmetricconfidenceintervals,64,65 calculationslonghand,124,125
functionfor,84 SSXYcorrectedsumofproducts,109,123
inhistograms,71 Ancova,187,188
negative,86 calculationslonghand,124,125
values,85 shortcutformula,124
slopeb,114 SSYtotalsumofsquaresdefined,122
calculationslonghand,124 calculationslonghand,123,124
definition,115 inAnova,150
differencesbetweenslopes,190 nullmodel,145,146
maximumlikelihoodestimate,6,118 one-wayAnova,158
standarderror,129 SSY=SSR+SSE,128
slopesAncova,230 standarddeviation,sdfunctioninR,72
removalinmodelsimplification,189 andskew,84
smoothinggam,18 incalculatingz,77
modelformulae,298 standarderror
panel.smoothinpairs,197 aserrorbars,164
sortfunctionforsortingavector,44,303 differencebetweentwomeans,92,160
rev(sort(y))forreverseorder,303 Helmertcontrasts,224
sortingadataframe,27 mean,61,160
sorting,introduction,303 ofkurtosis,86
spacesinvariablenamesorfactorlevels,25 ofskew,84
spatialautocorrelationrandomeffects,177 ofslopeandinterceptinlinearregression,
spatialcorrelationandpairedt-test,98 129
spatialpseudoreplication,15 standardnormaldeviate,seez
Spearman’sRankCorrelation,113 start,initialparametervaluesinnls,141
splitforspeciesdata,269 statisticalmodelling,introduction,187,199
proportiondata,270,271 statuswithcensoring,287
separateonthebasisoffactorlevels, stepautomatedmodelsimplification,274,
190,306 280
split-plotsErrorterms,174,175 str,thestructureofanRobject,320
introduction,174 straightline,6
differentplottingsymbols,305 stronginference,14
spreadsheetsanddataframes,24 strptime,inR,315
sqrtsquarerootfunctioninR,63,64,84 Student’st-distributionintroduction,82
squarerootfunction,seesqrt ptprobabilities,85,94
SSAexplainedvariationinAnova,154 qtquantiles,75,88,104
one-wayAnova,158 Student’st-teststatistic,91
shortcutformula,157 normalerrorsandconstantvariance,96,
SSCcontrastsumofsquares,222 97
SSEerrorsumofsquares,118 subjects,randomeffects,174
inAncova,189 subscripts[]introduction,301
inAnova,153 barplotwithtwosetsofbars,251
inregression,133 dataselection,161
one-wayAnova,158 factor-levelreduction,170
thesumofthesquaresoftheresiduals,120 forcomputingsubsetsofdata,115
S-shapedcurvelogistic,264 indataframes,33
SSRAncova,188 inlists[[]],197,301
inregression,133 incalculationsforAnova,153,154
regressionsumofsquares,125 influencetesting,199,200
INDEX 337
lmforAncova,231 sumsofsquaresinhierarchicaldesigns,182
residualsinAnova,151 suppressaxislabellingxaxt="n",277
withorder,303 survfitplotsurvivorshipcurves,287
usingthewhichfunction,68 survivalanalysisintroduction
subsetinmodelchecking,134 library(survival),287
influencetesting,176 survivorshipcurves,plot(surfit),288
multipleregression,199 survreganalysisofdeviance,288
subsetsofdatausinglogicalsubscripts,301 symbolsinmodelformulae,298
substitute,complextextonplots symbolsonplotscomplextextonplots,293
inplotlabels,163 differentsymbols,305
successes,proportiondata,256 Sys.time,316
sulphurdioxide,multipleregression,203
sumfunctionforcalculatingtotals, 43, TlogicalTrue,302
51,84 tdistribution,seeStudent’stdistribution,
sumcontrasts,224 t.testgardenozone,94
sumofsquaresintroduction,52 onesample,98
computation,51,54 paired=T,98
contrastsumofsquares,223 table,functionforcountingelementsinvectors,
shortcutformula,55 70
summaryintroduction binaryresponsevariable,273
analysisofdeviance,267 checkingreplication,168,170
Ancova,191 countingfrequencies,251,254,255
Ancovawithpoissonerrors,244 countingvaluesinavector,303
factorialexperiments,168 determiningfrequencydistribution,237,250
glmwithGammaerrors,286 withcut,277
glmwithpoissonerrors,235 tablesofmeansintroduction,304
inregression,130 tapplyonproportions,277
non-linearregression,139,140 tailsoftheNormaldistribution,74,75
ofavector,67 tailsoftheNormalandStudent’stcompared,83
regressionwithproportiondata,262 tapplyfortablesofmeans,161,191,240,304
speed,80 forproportions,268
splitplotaov,171 functioninR,95
withdataframes,25 meanageatdeath,285
withquasipoissonerrors,235 meanageatdeathwithcensoring,290
summary(model) reducingvectorlengths,177
gam,146 tableoftotals,withsum,95,157
piece-wiseregression,284 tableofvariances,withvar,285
withsurvreg,287 two-waytablesofmeans,168
summary.aov withcontrasts,219
Ancova,188 withcountdata,237
inregression,131 withcut,277
one-wayAnova,158 withlength,305
summary.lm temporalautocorrelationrandomeffects,177
Ancova,231 temporalpseudoreplication,15,177
effectsizesinAnova,159 teststatisticforStudent’st,91
factorialexperiments,168 test="Chi"contingencytable,244
Helmertcontrasts,224 test="F"anova,266
inAnova,214,215 testsofhypotheses,14,81,260
two-wayAnova,154 testsofnormality,79
withcontrasts,215 text(model)fortreemodels,199,200
338 INDEX
theory,8 type="n"forblankplots,59,64,152,190
three-wayAnova,modelformulae,150 proportiondata,272
thresholdsinpiece-wiseregression,283 withsplit,309
ties,problemsinWilcoxonRankSumTest,95 type="response",modeloutputonback-
tilde∼means“ismodelledasafunctionof”in transformedscale
lmoraov,117 Ancovawithpoissonerrors,246
modelformulae,298 withbinarydata,273
timeanddateinR,315 withproportiondata,264,268,272
timeatdeath,1
timeseries,randomeffects,176 unexplainedvariation,5
timeseries,9,15 inAnova,152,153
time-at-deathdata,introduction,285 inregression,125
transformation uniformrandomnumberswithruniffunction,
arcsineforpercentagedata,257 293,320
countdata,234 uninformativefactorlevels,69
explanatoryvariables,105,262,263 ratsexample,180
fromlogittop,261,267 unlist,316
linearmodels,117 unplannedcomparisons,aposterioricontrasts,
logistic,259 212,213
modelcriticism,274 unreliability,estimationof,60,61
modelformulae,298 intercept,129,130
thelinearpredictor,229 predictedvalue,131
transpose,usingconcatenate,c,308 slope,127
transposefunctionforamatrix,t updateinmodelsimplification, 144,
treatmentcontrastsintroduction,161,222 afterstep,281,282
treatmenttotals,contrastsumofsquares,223 analysisofdeviance,237,240,264
inAnova,155,156 contingencytable,244
treemodels,199,200,203 multipleregression,196,197,200,201
advantagesof,203 usingvariancetoestimateunreliability,60,61
dataexploration,193 testinghypotheses,59
ozoneexample,197
trees,selectingarandomindividual,10 varvariancefunctioninR,55,64,83,84,88,
Tribolium,11 89,291
logicalvariable,23 var(x,y)functionforcovariance,108,109
t-testdefinition,91 var.testF-testinR,57,58
pairedsamples,97 forgardenozone,89
ruleofthumbfort=9,165 variablenamesindataframes,25
TukeyHSD,Tukey’sHonestsignificant variance,definitionandderivation,50
differences,18 andcorrectedsumsofsquares,122,123
twosampleproblems,88 andpower,9
t-testwithpaireddata,97 andsamplesize,58,59
two-parametermodel,linearregression,135,144 andstandarderror,60,61
two-tailedtests,57,61,94 constancyinaglm,226,227,229
Fisher’sExactTest,105 countdata,234
two-wayAnova,modelformulae,154 dataontime-at-death,286
TypeIErrors,4,104 F-testtocomparetwovariances,58
TypeIIErrors,4 formula,54,55
type="b"bothpointsandlines,64 gammadistribution,285
type="l"lineratherthanpointsinplot, inAnova,150
72,76 minimizingestimators,6
INDEX 339
ofadifference,91,113 WilcoxonRankSumTest,91,95
ofthebinomialdistribution,252,253,255 non-normalerrors,88
plotagainstsamplesize,63 wormsdataframe,25
randomeffects,173 writingfunctionsinR,seefunctions
sumofsquares/degreesoffreedom,54,128
varfunctioninR,43 x,continuousexplanatoryvariableinregression,
VCA,variancecomponentsanalysis,178, 114
179 xlablabelsforthexaxis,58,63
variancecomponentsanalysis,178,179 inAnova,150
ratsexample,179
varianceconstancymodelchecking,134
yresponsevariableinregression,114
variancefunction,randomeffects,177
y∼1nullmodel,145,146
variance/meanratio
y∼x-1removingtheintercept,114,115
aggregationincountdata,253
Yates’correctionPearson’sChi-squaredtest,
examples,116
104
variation,12
yaxt="n"suppressaxislabelling,295
usinglogsingraphics,86
yieldexperiment,splitplotexample,173,174
varietyandsplit,269
ylablabelsfortheyaxis,58,63
VCA,seevariancecomponentsanalysis,
inAnova,150
vectorfunctionsinR,299
ylimcontrollingthescaleoftheyaxisinplots
inAnova,150
weakinference,14
webaddressofthisbook,xii
proportiondata,256 zoftheNormaldistribution,76
WelchTwoSamplet-test,94 approximationinWilcoxonRankSumTest,
which,Rfunctiontofindsubscripts,84,86 95
whiskersinboxandwhiskerplots,67 zerotermnegativebinomialdistribution,252
wilcox.testWilcoxonRankSumTest,91,95,96 Poissondistribution,250,251
WILEY END USER LICENSE
AGREEMENT
Go to www.wiley.com/go/eula to access Wiley's ebook
EULA.